<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 20]
- [cs.CR](#cs.CR) [Total: 23]
- [cs.AI](#cs.AI) [Total: 36]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [$\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection](https://arxiv.org/abs/2507.10583)
*Daniil Orel,Indraneil Paul,Iryna Gurevych,Preslav Nakov*

Main category: cs.SE

TL;DR: 论文提出了DroidCollection，一个包含多种编程语言和代码生成模型的开放数据集，并开发了DroidDetect检测器，用于评估和改进机器生成代码的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有检测器在多样化的编程语言和领域中的泛化能力不足，且易受对抗性样本的影响，因此需要更全面的数据集和改进的检测方法。

Method: 构建了DroidCollection数据集，包含多种代码样本和对抗性样本，并开发了基于多任务目标的DroidDetect检测器。

Result: 实验表明现有检测器泛化能力差，但通过对抗性数据训练可以改进性能；同时，度量学习和不确定性重采样能有效提升检测效果。

Conclusion: DroidCollection和DroidDetect为机器生成代码检测提供了更全面的解决方案，对抗性数据和改进的训练方法能显著提升检测性能。

Abstract: In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most
extensive open data suite for training and evaluating machine-generated code
detectors, comprising over a million code samples, seven programming languages,
outputs from 43 coding models, and over three real-world coding domains.
Alongside fully AI-generated samples, our collection includes human-AI
co-authored code, as well as adversarial samples explicitly crafted to evade
detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite
of encoder-only detectors trained using a multi-task objective over
$\texttt{DroidCollection}$. Our experiments show that existing detectors'
performance fails to generalise to diverse coding domains and programming
languages outside of their narrow training data. Additionally, we demonstrate
that while most detectors are easily compromised by humanising the output
distributions using superficial prompting and alignment approaches, this
problem can be easily amended by training on a small amount of adversarial
data. Finally, we demonstrate the effectiveness of metric learning and
uncertainty-based resampling as means to enhance detector training on possibly
noisy distributions.

</details>


### [2] [ARPaCCino: An Agentic-RAG for Policy as Code Compliance](https://arxiv.org/abs/2507.10584)
*Francesco Romeo,Luigi Arena,Francesco Blefari,Francesco Aurelio Pironti,Matteo Lupinacci,Angelo Furfaro*

Main category: cs.SE

TL;DR: ARPaCCino是一个结合LLM、RAG和工具验证的系统，用于自动化生成和验证Policy as Code规则，提升IaC环境中的策略合规性。


<details>
  <summary>Details</summary>
Motivation: Policy as Code的采用受限于策略语言的复杂性和配置错误的风险，ARPaCCino旨在解决这些问题。

Method: ARPaCCino利用LLM、RAG和工具验证，从自然语言描述生成Rego规则，并验证和优化IaC配置。

Result: 实验证明ARPaCCino能生成正确策略，识别不合规基础设施并进行修正，即使使用小型LLM也有效。

Conclusion: ARPaCCino展示了基于RAG的架构在提升PaC自动化、可靠性和可访问性方面的潜力。

Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance
policies into machine-readable formats, enabling automated enforcement in
Infrastructure as Code (IaC) environments. However, its adoption is hindered by
the complexity of policy languages and the risk of misconfigurations. In this
work, we present ARPaCCino, an agentic system that combines Large Language
Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation
to automate the generation and verification of PaC rules. Given natural
language descriptions of the desired policies, ARPaCCino generates formal Rego
rules, assesses IaC compliance, and iteratively refines the IaC configurations
to ensure conformance. Thanks to its modular agentic architecture and
integration with external tools and knowledge bases, ARPaCCino supports policy
validation across a wide range of technologies, including niche or emerging IaC
frameworks. Experimental evaluation involving a Terraform-based case study
demonstrates ARPaCCino's effectiveness in generating syntactically and
semantically correct policies, identifying non-compliant infrastructures, and
applying corrective modifications, even when using smaller, open-weight LLMs.
Our results highlight the potential of agentic RAG architectures to enhance the
automation, reliability, and accessibility of PaC workflows.

</details>


### [3] [Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime](https://arxiv.org/abs/2507.10590)
*Mojtaba Eshghie*

Main category: cs.SE

TL;DR: Meta Self-Refining框架通过元校正层修复语言模型管道中的竞争性软约束问题，提高效率。


<details>
  <summary>Details</summary>
Motivation: 语言模型管道在动态优化输出时，面对竞争性软约束会导致低效的回溯循环，需要一种方法修复这一问题。

Method: 引入Meta Self-Refining框架，通过监控执行历史检测振荡性失败，并调用元修复器LM生成平衡竞争需求的策略指令。

Result: 实验表明，该框架能成功修复回溯循环，使语言模型程序更高效。

Conclusion: Meta Self-Refining有效解决了竞争性软约束问题，提升了语言模型管道的性能。

Abstract: Language Model (LM) pipelines can dynamically refine their outputs against
programmatic constraints. However, their effectiveness collapses when faced
with competing soft constraints, leading to inefficient backtracking loops
where satisfying one constraint violates another. We introduce Meta
Self-Refining, a framework that equips LM pipelines with a meta-corrective
layer to repair these competitions at runtime/inference-time. Our approach
monitors the pipeline's execution history to detect oscillatory failures. Upon
detection, it invokes a meta-repairer LM that analyzes the holistic state of
the backtracking attempts and synthesizes a strategic instruction to balance
the competing requirements. This self-repair instruction guides the original LM
out of a failing refining loop towards a successful output. Our results show
Meta Self-Refining can successfully repair these loops, leading to more
efficient LM programs.

</details>


### [4] [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
*Peng Ding*

Main category: cs.SE

TL;DR: Toolregistry是一个协议无关的工具管理库，通过统一接口简化工具注册、表示、执行和生命周期管理，显著减少集成代码并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用的工具集成方法存在碎片化、协议限制和实现复杂性问题，导致开发成本高。

Method: 提出Toolregistry，一个协议无关的工具管理库，提供统一接口管理工具。

Result: Toolregistry减少60-80%的集成代码，性能提升3.1倍，兼容OpenAI函数调用标准。

Conclusion: Toolregistry显著提升开发效率和代码可维护性，已在开源社区发布。

Abstract: Large Language Model (LLM) applications are increasingly relying on external
tools to extend their capabilities beyond text generation. However, current
tool integration approaches suffer from fragmentation, protocol limitations,
and implementation complexity, leading to substantial development overhead.
This paper presents Toolregistry, a protocol-agnostic tool management library
that simplifies tool registration, representation, execution, and lifecycle
management via a unified interface. Our evaluation demonstrates that
\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x
performance improvements through concurrent execution, and 100% compatibility
with OpenAI function calling standards. Real-world case studies show
significant improvements in development efficiency and code maintainability
across diverse integration scenarios. \toolregistry is open-source and
available at https://github.com/Oaklight/ToolRegistry, with comprehensive
documentation at https://toolregistry.readthedocs.io/.

</details>


### [5] [SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications](https://arxiv.org/abs/2507.10640)
*Labiba Farah,Mohammad Ridwan Kabir,Shohel Ahmed,MD Mohaymen Ul Anam,Md. Sakibul Islam*

Main category: cs.SE

TL;DR: 论文提出SENSOR工具和GRACE模型，用于自动分类用户评论中的隐私相关需求或问题，帮助开发者高效处理隐私问题。


<details>
  <summary>Details</summary>
Motivation: 社交媒体的隐私问题日益突出，但手动分类海量用户评论效率低下，现有工具缺乏对隐私相关需求或问题的专门分类。

Method: 提出GRACE模型（基于GRU、CBOW和注意力机制），用于自动标注用户评论，并通过SENSOR工具实现分类。分析了16000条用户评论，人工标注后训练模型。

Result: GRACE模型表现最佳（宏F1分数：0.9434，宏ROC-AUC：0.9934，准确率：95.10%），SENSOR工具能有效辅助开发者处理隐私问题。

Conclusion: SENSOR和GRACE为开发者提供了高效处理隐私相关评论的工具，有助于提升用户隐私和信任。

Abstract: The widespread use of social media applications has raised significant
privacy concerns, often highlighted in user reviews. These reviews also provide
developers with valuable insights into improving apps by addressing issues and
introducing better features. However, the sheer volume and nuanced nature of
reviews make manual identification and prioritization of privacy-related
concerns challenging for developers. Previous studies have developed software
utilities to automatically classify user reviews as privacy-relevant,
privacy-irrelevant, bug reports, feature requests, etc., using machine
learning. Notably, there is a lack of focus on classifying reviews specifically
as privacy-related feature requests, privacy-related bug reports, or
privacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated
online annotation tool designed to help developers annotate and classify user
reviews into these categories. For automating the annotation of such reviews,
this paper introduces the annotation model, GRACE (GRU-based Attention with
CBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words
(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven
popular social media apps on Google Play Store, including Instagram, Facebook,
WhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were
analyzed. Two annotators manually labelled the reviews, achieving a Cohen's
Kappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement
for training machine learning models. Among the models tested, GRACE
demonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:
0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates
significant potential to assist developers with extracting and addressing
privacy-related feature requests or bug reports from user reviews, enhancing
user privacy and trust.

</details>


### [6] [A Code Comprehension Benchmark for Large Language Models for Code](https://arxiv.org/abs/2507.10641)
*Jayant Havare,Saurav Chaudhary,Ganesh Ramakrishnan,Kaushik Maharajan,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: 大型语言模型在代码生成和补全任务中表现优异，但缺乏对代码语义的深层理解。通过针对代码理解任务进行微调，模型性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有模型在需要深层语义理解的任务（如代码调试和优化）中表现不佳，原因是其预训练目标仅关注表面语法模式。

Method: 提出对模型进行针对代码理解任务的微调，使用大规模数据集以增强对代码语义的理解。

Result: 微调后，模型在代码理解任务中表现显著提升，特别是QWQ-32B模型准确率从70%提升至83.47%。

Conclusion: 微调能有效提升模型对代码语义的理解能力，其中DPO微调的Codestral-22B在主观评分任务中表现最佳。

Abstract: Large Language Models have shown impressive capabilities in coding tasks like
code generation and code completion, as they have been trained on a large
amount of code data. Also, since one of the core pretraining objectives is Next
Token Prediction, these models tends to learn surface-level syntactic patterns
in code. However, this does not guarantee code comprehension ability i.e. the
ability to capture the semantics of the code. In our opinion, this is the
reason why these models often underperform on tasks that require deeper
semantic understanding, such as code debugging and code optimization. To
address this, we propose fine-tuning these models specifically for code
comprehension tasks using large-scale datasets, enabling them to develop a more
robust understanding of code semantics. We evaluate three code models of
varying sizes on a suite of code comprehension tasks designed to assess
semantic understanding beyond surface-level syntactic pattern matching. In
particular, we analyze performance on the Subjectivity Grading Task and observe
that model performance improves after fine-tuning on relevant downstream tasks.
The most significant improvement is seen in the QWQ-32B model, where accuracy
increases from 70% to 83.47%. A similar or explainable trend is observed across
other models, clearly indicating an enhancement in code comprehension ability.
Among the models studied, the DPO-fine-tuned Codestral-22B achieves the highest
micro-accuracy of 87.66% on the Subjectivity Grading Task.

</details>


### [7] [CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance](https://arxiv.org/abs/2507.10646)
*Myeongsoo Kim,Shweta Garg,Baishakhi Ray,Varun Kumar,Anoop Deoras*

Main category: cs.SE

TL;DR: CodeAssistBench (CAB) 是一个新的基准框架，用于评估多轮编程辅助在真实环境中的表现，填补了现有基准在项目环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如InfiBench和StackEval）局限于单轮交互和孤立环境，无法反映真实项目中的复杂需求。CAB旨在解决这一问题。

Method: CAB通过从GitHub问题自动生成可扩展数据集，并利用容器化代码库进行多轮交互评估。

Result: 测试集包含3,286个真实编程问题，覆盖7种语言。评估显示，模型在Stack Overflow上表现良好（70-83%成功率），但在CAB中仅解决16.49%的问题。

Conclusion: CAB揭示了模型在复杂项目环境中的能力差距，强调了真实编程辅助的挑战。

Abstract: Programming assistants powered by large language models have transformed
software development, yet most benchmarks focus narrowly on code generation
tasks. Recent efforts like InfiBench and StackEval attempt to address this gap
using Stack Overflow data but remain limited to single-turn interactions in
isolated contexts, require significant manual curation, and fail to represent
complete project environments. We introduce CodeAssistBench (CAB), the first
benchmark framework for evaluating multi-turn programming assistance in
realistic settings that address real-world questions about actual codebases.
Unlike existing programming Q&A benchmarks, CAB automatically generates
scalable datasets from question-related GitHub issues using configurable
parameters (e.g., repository creation date, star count, programming languages),
and includes automatic containerization of codebases for evaluation. It then
evaluates models through simulated users in these containerized environments
with full codebase access. Using this framework, we constructed a test set of
3,286 real-world programming questions across 231 repositories, spanning seven
programming languages and diverse problem domains. Our evaluation of leading
LLMs reveals a substantial capability gap: while models perform well on Stack
Overflow questions with success rates of 70-83%, they resolve only up to 16.49%
of CAB's recent issues. This discrepancy highlights the challenges of providing
assistance in complex, project-specific contexts versus answering standalone
questions.

</details>


### [8] [Toward Realistic Evaluations of Just-In-Time Vulnerability Prediction](https://arxiv.org/abs/2507.10729)
*Duong Nguyen,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: 论文评估了即时漏洞预测（JIT-VP）在真实场景中的有效性，发现其在现实数据中性能显著下降，并提出需领域特定方法解决数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前JIT-VP评估依赖理想化数据集，缺乏对现实场景中数据不平衡的考量，研究旨在填补这一空白。

Method: 引入包含100多万个提交的大规模公开数据集，评估八种先进JIT-VP技术在真实条件下的性能，并探索处理数据不平衡的方法。

Result: 现实条件下JIT-VP性能大幅下降（如PR-AUC从0.805降至0.016），现有不平衡处理技术效果不佳。

Conclusion: 强调真实评估的重要性，并呼吁开发领域特定技术以解决JIT-VP中的数据不平衡问题。

Abstract: Modern software systems are increasingly complex, presenting significant
challenges in quality assurance. Just-in-time vulnerability prediction (JIT-VP)
is a proactive approach to identifying vulnerable commits and providing early
warnings about potential security risks. However, we observe that current
JIT-VP evaluations rely on an idealized setting, where the evaluation datasets
are artificially balanced, consisting exclusively of vulnerability-introducing
and vulnerability-fixing commits.
  To address this limitation, this study assesses the effectiveness of JIT-VP
techniques under a more realistic setting that includes both
vulnerability-related and vulnerability-neutral commits. To enable a reliable
evaluation, we introduce a large-scale public dataset comprising over one
million commits from FFmpeg and the Linux kernel. Our empirical analysis of
eight state-of-the-art JIT-VP techniques reveals a significant decline in
predictive performance when applied to real-world conditions; for example, the
average PR-AUC on Linux drops 98\% from 0.805 to 0.016. This discrepancy is
mainly attributed to the severe class imbalance in real-world datasets, where
vulnerability-introducing commits constitute only a small fraction of all
commits.
  To mitigate this issue, we explore the effectiveness of widely adopted
techniques for handling dataset imbalance, including customized loss functions,
oversampling, and undersampling. Surprisingly, our experimental results
indicate that these techniques are ineffective in addressing the imbalance
problem in JIT-VP. These findings underscore the importance of realistic
evaluations of JIT-VP and the need for domain-specific techniques to address
data imbalance in such scenarios.

</details>


### [9] [GenAI-Enabled Backlog Grooming in Agile Software Projects: An Empirical Study](https://arxiv.org/abs/2507.10753)
*Kasper Lien Oftebro,Anh Nguyen-Duc,Kai-Kristian Kemell*

Main category: cs.SE

TL;DR: 研究探讨了生成式AI助手能否在不牺牲准确性和透明度的前提下，自动化敏捷软件开发中的待办事项整理。通过设计科学方法开发的Jira插件，结合向量数据库和GPT-4o模型，实现了高精度和效率提升。


<details>
  <summary>Details</summary>
Motivation: 随着产品待办事项规模和复杂度的增加，冗余、过时或定义不清的任务堆积，导致优先级排序和决策过程复杂化。

Method: 通过设计科学循环开发了一个Jira插件，利用向量数据库嵌入待办事项，通过余弦相似度检测重复项，并借助GPT-4o模型提出合并、删除或新增任务的建议。

Result: AI辅助的待办事项整理实现了100%的精确度，并将完成时间减少了45%。

Conclusion: 该工具展示了在优化待办事项细化流程和提升用户体验方面的潜力。

Abstract: Effective backlog management is critical for ensuring that development teams
remain aligned with evolving requirements and stakeholder expectations.
However, as product backlogs consistently grow in scale and complexity, they
tend to become cluttered with redundant, outdated, or poorly defined tasks,
complicating prioritization and decision making processes. This study
investigates whether a generative-AI (GenAI) assistant can automate backlog
grooming in Agile software projects without sacrificing accuracy or
transparency. Through Design Science cycles, we developed a Jira plug-in that
embeds backlog issues with the vector database, detects duplicates via cosine
similarity, and leverage the GPT-4o model to propose merges, deletions, or new
issues. We found that AI-assisted backlog grooming achieved 100 percent
precision while reducing the time-to-completion by 45 percent. The findings
demonstrated the tool's potential to streamline backlog refinement processes
while improving user experiences.

</details>


### [10] [Towards a Closer Collaboration Between Practice and Research in Agile Software Development Workshop: A Summary and Research Agenda](https://arxiv.org/abs/2507.10785)
*Michael Neumann,Eva-Maria Schön,Mali Senapathi,Maria Rauschenberger,Tiago Silva da Silva*

Main category: cs.SE

TL;DR: 本文总结了首次国际研讨会的成果，探讨了敏捷软件开发中研究与实际应用之间的差距及其解决策略。


<details>
  <summary>Details</summary>
Motivation: 敏捷软件开发虽广泛应用，但研究与实际实施之间存在显著差距，需促进两者的协作。

Method: 通过国际研讨会收集参与者意见，分析主要主题和因素。

Result: 识别了导致差距的因素、解决策略及需进一步研究的挑战。

Conclusion: 研讨会为缩小研究与实践的差距提供了方向，但仍需更多研究。

Abstract: Agile software development principles and values have been widely adopted
across various industries, influencing products and services globally. Despite
its increasing popularity, a significant gap remains between research and
practical implementation. This paper presents the findings of the first
international workshop designed to foster collaboration between research and
practice in agile software development. We discuss the main themes and factors
identified by the workshop participants that contribute to this gap, strategies
to bridge it, and the challenges that require further research attention.

</details>


### [11] [How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow](https://arxiv.org/abs/2507.10818)
*Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: LLMs推荐Python库时倾向于第三方库，但存在可用性差距，如库名与安装包不匹配，且缺乏安装指导。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在编程任务中推荐库的行为，以了解其对软件功能、安全和维护的影响。

Method: 对六种先进LLMs进行实证研究，通过Stack Overflow的Python问题测试其库推荐行为。

Result: LLMs偏好成熟、流行的第三方库，但4.6%的库因名称问题无法自动解析，仅两种模型提供安装指导。

Conclusion: LLM生成的代码在依赖管理上存在不足，需改进以提高可靠性和可用性。

Abstract: Software libraries are central to the functionality, security, and
maintainability of modern code. As developers increasingly turn to Large
Language Models (LLMs) to assist with programming tasks, understanding how
these models recommend libraries is essential. In this paper, we conduct an
empirical study of six state-of-the-art LLMs, both proprietary and open-source,
by prompting them to solve real-world Python problems sourced from Stack
Overflow. We analyze the types of libraries they import, the characteristics of
those libraries, and the extent to which the recommendations are usable out of
the box. Our results show that LLMs predominantly favour third-party libraries
over standard ones, and often recommend mature, popular, and permissively
licensed dependencies. However, we also identify gaps in usability: 4.6% of the
libraries could not be resolved automatically due to structural mismatches
between import names and installable packages, and only two models (out of six)
provided installation guidance. While the generated code is technically valid,
the lack of contextual support places the burden of manually resolving
dependencies on the user. Our findings offer actionable insights for both
developers and researchers, and highlight opportunities to improve the
reliability and usability of LLM-generated code in the context of software
dependencies.

</details>


### [12] [Past, Present and Future: Exploring Adaptive AI in Software Development Bots](https://arxiv.org/abs/2507.10822)
*Omar Elsisi,Glaucia Melo*

Main category: cs.SE

TL;DR: 本文探讨了自适应AI驱动的对话代理在软件开发中的作用，强调其动态、上下文感知的辅助能力，并分析了其从简单查询系统到高级AI解决方案的演变。


<details>
  <summary>Details</summary>
Motivation: 研究自适应AI对话代理如何提升软件开发的生产力和协作效率，同时解决数据隐私和伦理问题。

Method: 通过分析自适应AI代理的机器学习与自然语言处理技术，以及其在GitHub Copilot等工具中的应用。

Result: 自适应AI代理能提供个性化、响应迅速的帮助，显著提升开发效率。

Conclusion: 自适应AI对话代理有望通过实时定制支持彻底改变软件开发，但需解决隐私和伦理挑战。

Abstract: Conversational agents, such as chatbots and virtual assistants, have become
essential in software development, boosting productivity, collaboration, and
automating various tasks. This paper examines the role of adaptive AI-powered
conversational agents in software development, highlighting their ability to
offer dynamic, context-aware assistance to developers. Unlike traditional
rule-based systems, adaptive AI agents use machine learning and natural
language processing to learn from interactions and improve over time, providing
more personalized and responsive help. We look at how these tools have evolved
from simple query-based systems to advanced AI-driven solutions like GitHub
Copilot and Microsoft Teams bots. We also explore the challenges of integrating
adaptive AI into software development processes. The study aims to assess the
benefits and limitations of these systems, address concerns like data privacy
and ethical issues, and offer insights into their future use in the field.
Ultimately, adaptive AI chatbots have great potential to revolutionize software
development by delivering real-time, customized support and enhancing the
efficiency of development cycles.

</details>


### [13] [Evaluating Generated Commit Messages with Large Language Models](https://arxiv.org/abs/2507.10906)
*Qunhong Zeng,Yuxia Zhang,Zexiong Ma,Bo Jiang,Ningyuan Sun,Klaas-Jan Stol,Xingyu Mou,Hui Liu*

Main category: cs.SE

TL;DR: 该论文探讨了利用大型语言模型（LLMs）作为自动评估提交消息质量的工具，通过实验证明其接近人类评估水平，优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 提交消息在软件开发中至关重要，但实践中质量参差不齐，传统自动评估指标存在局限性，需要资源密集型的人工评估。

Method: 通过系统实验，结合Chain-of-Thought推理和少量示例，使用先进的LLMs评估提交消息质量。

Result: LLM评估器显著优于传统指标，接近人类水平，同时保持可接受的再现性、鲁棒性和公平性。

Conclusion: LLMs为提交消息评估提供了可扩展的高质量替代方案，减少了人工评估的需求。

Abstract: Commit messages are essential in software development as they serve to
document and explain code changes. Yet, their quality often falls short in
practice, with studies showing significant proportions of empty or inadequate
messages. While automated commit message generation has advanced significantly,
particularly with Large Language Models (LLMs), the evaluation of generated
messages remains challenging. Traditional reference-based automatic metrics
like BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit
message quality, as they assume a one-to-one mapping between code changes and
commit messages, leading researchers to rely on resource-intensive human
evaluation. This study investigates the potential of LLMs as automated
evaluators for commit message quality. Through systematic experimentation with
various prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs
combining Chain-of-Thought reasoning with few-shot demonstrations achieve near
human-level evaluation proficiency. Our LLM-based evaluator significantly
outperforms traditional metrics while maintaining acceptable reproducibility,
robustness, and fairness levels despite some inherent variability. This work
conducts a comprehensive preliminary study on using LLMs for commit message
evaluation, offering a scalable alternative to human assessment while
maintaining high-quality evaluation.

</details>


### [14] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: SWE-MERA是一个动态更新的基准，旨在解决SWE-bench中的数据污染问题，通过自动化收集GitHub问题并严格验证质量。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如SWE-bench）存在数据污染问题，如解决方案泄漏和测试用例不足，影响了评估的可靠性。

Method: 采用自动化管道收集真实GitHub问题，并进行严格质量验证，确保数据质量。

Result: 生成了约10,000个潜在任务，目前有300个样本可用，评估显示对最新LLMs具有强区分能力。

Conclusion: SWE-MERA通过动态更新和严格验证，提供了一个更可靠的基准，可用于评估LLMs在软件工程中的表现。

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>


### [15] [MT4DP: Data Poisoning Attack Detection for DL-based Code Search Models via Metamorphic Testing](https://arxiv.org/abs/2507.11092)
*Gong Chen,Wenjie Liu,Xiaoyuan Xie,Xunzhu Tang,Tegawendé F. Bissyandé,Songqiang Chen*

Main category: cs.SE

TL;DR: MT4DP是一种基于蜕变测试的数据投毒攻击检测框架，用于深度学习代码搜索模型，通过语义等效蜕变关系检测攻击，显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法对深度学习代码搜索模型的数据投毒攻击检测效果不足，需提出更有效的解决方案。

Method: MT4DP利用高频率词作为潜在投毒目标，生成语义等效查询并重新排序，通过方差计算揭示攻击。

Result: 实验显示MT4DP在F1分数和精度上分别比基线方法提升191%和265%。

Conclusion: MT4DP有效提升数据投毒攻击检测能力，推动相关研究发展。

Abstract: Recently, several studies have indicated that data poisoning attacks pose a
severe security threat to deep learning-based (DL-based) code search models.
Attackers inject carefully crafted malicious patterns into the training data,
misleading the code search model to learn these patterns during training.
During the usage of the poisoned code search model for inference, once the
malicious pattern is triggered, the model tends to rank the vulnerability code
higher. However, existing detection methods for data poisoning attacks on
DL-based code search models remain insufficiently effective. To address this
critical security issue, we propose MT4DP, a Data Poisoning Attack Detection
Framework for DL-based Code Search Models via Metamorphic Testing. MT4DP
introduces a novel Semantically Equivalent Metamorphic Relation (SE-MR)
designed to detect data poisoning attacks on DL-based code search models.
Specifically, MT4DP first identifies the high-frequency words from search
queries as potential poisoning targets and takes their corresponding queries as
the source queries. For each source query, MT4DP generates two semantically
equivalent follow-up queries and retrieves its source ranking list. Then, each
source ranking list is re-ranked based on the semantic similarities between its
code snippets and the follow-up queries. Finally, variances between the source
and re-ranked lists are calculated to reveal violations of the SE-MR and warn
the data poisoning attack. Experimental results demonstrate that MT4DP
significantly enhances the detection of data poisoning attacks on DL-based code
search models, outperforming the best baseline by 191% on average F1 score and
265% on average precision. Our work aims to promote further research into
effective techniques for mitigating data poisoning threats on DL-based code
search models.

</details>


### [16] [Automata Models for Effective Bug Description](https://arxiv.org/abs/2507.11146)
*Tom Yaacov,Gera Weiss,Gal Amram,Avi Hayoun*

Main category: cs.SE

TL;DR: 论文提出了一种基于自动机学习和测试技术的方法，用于生成简洁且信息丰富的错误描述，包括失败解释（FE）、最终失败解释（EFE）和早期检测（ED）。


<details>
  <summary>Details</summary>
Motivation: 调试复杂系统耗时且困难，需要更高效的方法来生成有意义的错误行为总结。

Method: 通过自动机学习和测试技术，提取关键测试模式，排除无关信息，生成错误描述。

Result: 在多种测试模式和实际基准测试中验证了方法的有效性，能够生成紧凑且信息丰富的错误描述。

Conclusion: 该方法显著提升了错误检测和理解的能力。

Abstract: Debugging complex systems is a crucial yet time-consuming task. This paper
presents the use of automata learning and testing techniques to obtain concise
and informative bug descriptions. We introduce the concepts of Failure
Explanations (FE), Eventual Failure Explanations (EFE), and Early Detection
(ED) to provide meaningful summaries of failing behavior patterns. By factoring
out irrelevant information and focusing on essential test patterns, our
approach aims to enhance bug detection and understanding. We evaluate our
methods using various test patterns and real-world benchmarks, demonstrating
their effectiveness in producing compact and informative bug descriptions.

</details>


### [17] [New Formulation of DNN Statistical Mutation Killing for Ensuring Monotonicity: A Technical Report](https://arxiv.org/abs/2507.11199)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: 论文提出了一种基于Fisher精确检验的统计突变杀死准则，解决了DeepCrime违反单调性的问题。


<details>
  <summary>Details</summary>
Motivation: DeepCrime的统计突变杀死准则存在单调性被违反的缺陷，即扩大测试集可能导致先前被杀死的突变不再被分类为杀死。

Method: 采用Fisher精确检验重新定义统计突变杀死准则，保持统计严谨性的同时确保单调性。

Result: 新方法在保持统计严谨性的同时解决了单调性问题。

Conclusion: 提出的Fisher精确检验方法有效解决了DeepCrime的单调性问题，同时保持了统计严谨性。

Abstract: Mutation testing has emerged as a powerful technique for evaluating the
effectiveness of test suites for Deep Neural Networks. Among existing
approaches, the statistical mutant killing criterion of DeepCrime has leveraged
statistical testing to determine whether a mutant significantly differs from
the original model. However, it suffers from a critical limitation: it violates
the monotonicity property, meaning that expanding a test set may result in
previously killed mutants no longer being classified as killed. In this
technical report, we propose a new formulation of statistical mutant killing
based on Fisher exact test that preserves the statistical rigour of it while
ensuring monotonicity.

</details>


### [18] [An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling](https://arxiv.org/abs/2507.11272)
*Anh Nguyen-Duc,Chien Vu Manh,Bao Anh Tran,Viet Phuong Ngo,Luan Le Chi,Anh Quang Nguyen*

Main category: cs.SE

TL;DR: MARAUS是一个结合多智能体、检索增强和LLM的对话AI平台，用于越南高等教育招生咨询，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的解决方案多为原型或合成基准，MARAUS填补了实际部署的空白，为低资源教育环境提供实用方案。

Method: 结合混合检索、多智能体协调和LLM生成，与越南运输技术大学合作进行技术开发和实际评估。

Result: 处理6000+实际用户查询，平均准确率92%，幻觉率从15%降至1.45%，响应时间低于4秒，成本仅11.58美元。

Conclusion: MARAUS为低资源教育环境中部署智能RAG系统提供了可行方案和实用经验。

Abstract: This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University
Admission System), a real-world deployment of a conversational AI platform for
higher education admissions counseling in Vietnam. While large language models
(LLMs) offer potential for automating advisory tasks, most existing solutions
remain limited to prototypes or synthetic benchmarks. MARAUS addresses this gap
by combining hybrid retrieval, multi-agent orchestration, and LLM-based
generation into a system tailored for real-world university admissions. In
collaboration with the University of Transport Technology (UTT) in Hanoi, we
conducted a two-phase study involving technical development and real-world
evaluation. MARAUS processed over 6,000 actual user interactions, spanning six
categories of queries. Results show substantial improvements over LLM-only
baselines: on average 92 percent accuracy, hallucination rates reduced from 15
precent to 1.45 percent, and average response times below 4 seconds. The system
operated cost-effectively, with a two-week deployment cost of 11.58 USD using
GPT-4o mini. This work provides actionable insights for the deployment of
agentic RAG systems in low-resource educational settings.

</details>


### [19] [RefModel: Detecting Refactorings using Foundation Models](https://arxiv.org/abs/2507.11346)
*Pedro Simões,Rohit Gheyi,Rian Melo,Jonhnanthan Oliveira,Márcio Ribeiro,Wesley K. G. Assunção*

Main category: cs.SE

TL;DR: 研究探讨了使用基础模型（如Phi4-14B、Claude 3.5 Sonnet等）检测代码重构的可行性，开发了工具RefModel，并在性能和通用性上超越了传统静态分析工具。


<details>
  <summary>Details</summary>
Motivation: 传统重构检测工具依赖复杂规则和静态分析，难以扩展和通用化，因此研究探索了基础模型的潜力。

Method: 使用Phi4-14B、Claude 3.5 Sonnet等模型，评估了858个人工生成的Java程序重构和44个真实项目中的重构，并与传统工具对比。

Result: RefModel在部分情况下优于传统工具，Claude 3.5 Sonnet和Gemini 2.5 Pro在真实场景中识别了97%的重构，且能泛化到Python和Golang。

Conclusion: 基础模型在重构检测中表现优异，提供了自然语言解释，且定义简单，具有广泛的应用潜力。

Abstract: Refactoring is a common software engineering practice that improves code
quality without altering program behavior. Although tools like ReExtractor+,
RefactoringMiner, and RefDiff have been developed to detect refactorings
automatically, they rely on complex rule definitions and static analysis,
making them difficult to extend and generalize to other programming languages.
In this paper, we investigate the viability of using foundation models for
refactoring detection, implemented in a tool named RefModel. We evaluate
Phi4-14B, and Claude 3.5 Sonnet on a dataset of 858 single-operation
transformations applied to artificially generated Java programs, covering
widely-used refactoring types. We also extend our evaluation by including
Gemini 2.5 Pro and o4-mini-high, assessing their performance on 44 real-world
refactorings extracted from four open-source projects. These models are
compared against RefactoringMiner, RefDiff, and ReExtractor+. RefModel is
competitive with, and in some cases outperform, traditional tools. In
real-world settings, Claude 3.5 Sonnet and Gemini 2.5 Pro jointly identified
97% of all refactorings, surpassing the best-performing static-analysis-based
tools. The models showed encouraging generalization to Python and Golang. They
provide natural language explanations and require only a single sentence to
define each refactoring type.

</details>


### [20] [Security Debt in Practice: Nuanced Insights from Practitioners](https://arxiv.org/abs/2507.11362)
*Chaima Boufaied,Taher Ghaleb,Zainab Masood*

Main category: cs.SE

TL;DR: 论文通过定性实证研究探讨了软件从业者对安全债务（SDs）的认知、管理和沟通方式，强调了在软件开发生命周期中加强安全实践的必要性。


<details>
  <summary>Details</summary>
Motivation: 软件行业因时间、资源限制和功能优先于安全的做法，导致安全债务积累，但缺乏实证研究探讨从业者如何应对这一问题。

Method: 采用半结构化访谈，调查了22名来自不同角色、组织和国家的软件从业者，围绕四个研究问题展开。

Result: 研究发现从业者对SDs的认知和管理存在差异，部分人优先考虑交付速度而非安全，需加强安全实践和风险沟通。

Conclusion: 需在SDLC中更紧密地整合安全实践，平衡资源与安全任务，并关注CIA三要素。

Abstract: With the increasing reliance on software and automation nowadays, tight
deadlines, limited resources, and prioritization of functionality over security
can lead to insecure coding practices. When not handled properly, these
constraints cause unaddressed security vulnerabilities to accumulate over time,
forming Security Debts (SDs). Despite their critical importance, there is
limited empirical evidence on how software practitioners perceive, manage, and
communicate SDs in real-world settings. In this paper, we present a qualitative
empirical study based on semi-structured interviews with 22 software
practitioners across various roles, organizations, and countries. We address
four research questions: i) we assess software practitioners' knowledge of SDs
and awareness of associated security risks, ii) we investigate their behavior
towards SDs, iii) we explore common tools and strategies used to mitigate SDs,
and iv) we analyze how security risks are communicated within teams and to
decision makers. We observe variations in how practitioners perceive and manage
SDs, with some prioritizing delivery speed over security, while others
consistently maintain security as a priority. Our findings emphasize the need
for stronger integration of security practices across the Software Development
Life Cycle (SDLC), more consistent use of mitigation strategies, better
balancing of deadlines, resources, and security-related tasks, with attention
to the Confidentiality, Integrity, and Availability (CIA) triad.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [21] [When and Where do Data Poisons Attack Textual Inversion?](https://arxiv.org/abs/2507.10578)
*Jeremy Styborski,Mingzhi Lyu,Jiayou Lu,Nupur Kapur,Adams Kong*

Main category: cs.CR

TL;DR: 论文分析了扩散模型（DMs）中毒攻击对文本反转（TI）的影响，提出了一种可视化方法（Semantic Sensitivity Maps），并发现DMs在低噪声样本中表现出非均匀学习行为。基于此，作者提出了Safe-Zone Training（SZT）防御机制，显著提升了TI的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 中毒攻击对扩散模型的鲁棒性构成重大威胁，尤其是对文本反转（TI）这一广泛使用的个性化技术。研究旨在分析攻击的时机和位置，并提出有效的防御方法。

Method: 1. 引入Semantic Sensitivity Maps可视化中毒攻击对文本嵌入的影响；2. 发现DMs在低噪声样本中表现出非均匀学习行为；3. 提出Safe-Zone Training（SZT）防御机制，包括JPEG压缩、限制高时间步训练和损失掩码。

Result: 实验表明，SZT显著提升了TI对多种中毒攻击的鲁棒性，生成质量优于现有防御方法。

Conclusion: SZT通过结合多种防御策略，有效提升了扩散模型在文本反转任务中的安全性，为未来研究提供了新方向。

Abstract: Poisoning attacks pose significant challenges to the robustness of diffusion
models (DMs). In this paper, we systematically analyze when and where poisoning
attacks textual inversion (TI), a widely used personalization technique for
DMs. We first introduce Semantic Sensitivity Maps, a novel method for
visualizing the influence of poisoning on text embeddings. Second, we identify
and experimentally verify that DMs exhibit non-uniform learning behavior across
timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias
and inject adversarial signals predominantly at lower timesteps. Lastly, we
observe that adversarial signals distract learning away from relevant concept
regions within training data, corrupting the TI process. Based on these
insights, we propose Safe-Zone Training (SZT), a novel defense mechanism
comprised of 3 key components: (1) JPEG compression to weaken high-frequency
poison signals, (2) restriction to high timesteps during TI training to avoid
adversarial signals at lower timesteps, and (3) loss masking to constrain
learning to relevant regions. Extensive experiments across multiple poisoning
methods demonstrate that SZT greatly enhances the robustness of TI against all
poisoning attacks, improving generative quality beyond prior published
defenses. Code: www.github.com/JStyborski/Diff_Lab Data:
www.github.com/JStyborski/NC10

</details>


### [22] [Breaking a 5-Bit Elliptic Curve Key using a 133-Qubit Quantum Computer](https://arxiv.org/abs/2507.10592)
*Steve Tippeconnic*

Main category: cs.CR

TL;DR: 实验通过量子攻击成功破解5位椭圆曲线密码密钥，使用IBM的133量子比特设备，展示了量子计算的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在密码学中的应用，验证Shor算法对椭圆曲线密码的实际攻击效果。

Method: 使用15量子比特电路（10逻辑比特和5辅助比特），在IBM的133量子比特设备上运行，通过量子干涉提取密钥。

Result: 实验成功从16,384次运行中提取出密钥k=7，验证了量子干涉的有效性。

Conclusion: 量子计算对椭圆曲线密码构成实际威胁，相关代码和数据已公开供复现。

Abstract: This experiment breaks a 5-bit elliptic curve cryptographic key using a
Shor-style quantum attack. Executed on IBM's 133-qubit ibm_torino with Qiskit
Runtime 2.0, a 15-qubit circuit, comprised of 10 logical qubits and 5 ancilla,
interferes over an order-32 elliptic curve subgroup to extract the secret
scalar k from the public key relation Q = kP, without ever encoding k directly
into the oracle. From 16,384 shots, the quantum interference reveals a diagonal
ridge in the 32 x 32 QFT outcome space. The quantum circuit, over 67,000 layers
deep, produced valid interference patterns despite extreme circuit depth, and
classical post-processing revealed k = 7 in the top 100 invertible (a, b)
results. All code, circuits, and raw data are publicly available for
replication.

</details>


### [23] [LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents](https://arxiv.org/abs/2507.10610)
*Zihe Yan,Zhuosheng Zhang*

Main category: cs.CR

TL;DR: LaSM是一种层间缩放机制，通过选择性放大关键层的注意力和MLP模块，有效防御GUI代理的弹出式环境注入攻击，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有的防御方法要么需要昂贵的重新训练，要么在归纳干扰下表现不佳，而GUI代理对弹出式攻击高度脆弱。

Method: 通过研究攻击如何改变GUI代理的注意力行为，发现层间注意力差异模式，并基于此提出LaSM机制。

Result: 在12种弹出式扰动和4种模型骨干上的实验显示，LaSM显著提高了防御成功率，结合提示级警报后稳健性超过98%。

Conclusion: 注意力错位是MLLM代理的核心漏洞，LaSM通过选择性层间调制有效解决了这一问题。

Abstract: Graphical user interface (GUI) agents built on multimodal large language
models (MLLMs) have recently demonstrated strong decision-making abilities in
screen-based interaction tasks. However, they remain highly vulnerable to
pop-up-based environmental injection attacks, where malicious visual elements
divert model attention and lead to unsafe or incorrect actions. Existing
defense methods either require costly retraining or perform poorly under
inductive interference. In this work, we systematically study how such attacks
alter the attention behavior of GUI agents and uncover a layer-wise attention
divergence pattern between correct and incorrect outputs. Based on this
insight, we propose \textbf{LaSM}, a \textit{Layer-wise Scaling Mechanism} that
selectively amplifies attention and MLP modules in critical layers. LaSM
improves the alignment between model saliency and task-relevant regions without
additional training. Extensive experiments across 12 types of pop-up
perturbations and 4 different model backbones show that LaSM consistently
enhances the defense success rate. When combined with prompt-level alerts, LaSM
achieves over 98\% robustness even under strong inductive attacks. Our findings
reveal that attention misalignment is a core vulnerability in MLLM agents and
can be effectively addressed through selective layer-wise modulation.

</details>


### [24] [Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats](https://arxiv.org/abs/2507.10621)
*Quanyan Zhu*

Main category: cs.CR

TL;DR: 论文探讨了如何结合博弈论和基于大型语言模型（LLM）的智能代理来提升网络安全的主动性和智能化。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全方法依赖人工响应和脆弱的启发式规则，缺乏理论与实践的紧密结合。

Method: 通过博弈论建模对抗行为，结合LLM代理将抽象策略转化为实际决策，并设计模块化、自适应的系统。

Result: 提出了博弈论与LLM代理协同的新框架，为网络安全提供更丰富的理论基础和解决方案。

Conclusion: 博弈论与智能代理的结合为构建安全、智能和自适应的网络系统提供了新路径。

Abstract: Protecting cyberspace requires not only advanced tools but also a shift in
how we reason about threats, trust, and autonomy. Traditional cybersecurity
methods rely on manual responses and brittle heuristics. To build proactive and
intelligent defense systems, we need integrated theoretical frameworks and
software tools. Game theory provides a rigorous foundation for modeling
adversarial behavior, designing strategic defenses, and enabling trust in
autonomous systems. Meanwhile, software tools process cyber data, visualize
attack surfaces, verify compliance, and suggest mitigations. Yet a disconnect
remains between theory and practical implementation.
  The rise of Large Language Models (LLMs) and agentic AI offers a new path to
bridge this gap. LLM-powered agents can operationalize abstract strategies into
real-world decisions. Conversely, game theory can inform the reasoning and
coordination of these agents across complex workflows. LLMs also challenge
classical game-theoretic assumptions, such as perfect rationality or static
payoffs, prompting new models aligned with cognitive and computational
realities. This co-evolution promises richer theoretical foundations and novel
solution concepts. Agentic AI also reshapes software design: systems must now
be modular, adaptive, and trust-aware from the outset.
  This chapter explores the intersection of game theory, agentic AI, and
cybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic,
Bayesian, and signaling games) and solution concepts. We then examine how LLM
agents can enhance cyber defense and introduce LLM-driven games that embed
reasoning into AI agents. Finally, we explore multi-agent workflows and
coordination games, outlining how this convergence fosters secure, intelligent,
and adaptive cyber systems.

</details>


### [25] [Spectral Feature Extraction for Robust Network Intrusion Detection Using MFCCs](https://arxiv.org/abs/2507.10622)
*HyeYoung Lee,Muhammad Nadeem,Pavel Tsoi*

Main category: cs.CR

TL;DR: 提出了一种基于MFCC和ResNet-18的物联网网络流量异常检测方法，通过自适应频谱特征表示和深度学习模型提升分类效果。


<details>
  <summary>Details</summary>
Motivation: 物联网网络扩展带来的安全漏洞问题亟需高效的异常检测技术。

Method: 利用可学习的MFCC和ResNet-18模型，将原始信号转换为高维空间以增强分类效果。

Result: 在CICIoT2023、NSL-KDD和IoTID20数据集上验证了方法的有效性。

Conclusion: 结合自适应信号处理和深度学习架构，为物联网网络提供了鲁棒且可扩展的异常检测方案。

Abstract: The rapid expansion of Internet of Things (IoT) networks has led to a surge
in security vulnerabilities, emphasizing the critical need for robust anomaly
detection and classification techniques. In this work, we propose a novel
approach for identifying anomalies in IoT network traffic by leveraging the
Mel-frequency cepstral coefficients (MFCC) and ResNet-18, a deep learning model
known for its effectiveness in feature extraction and image-based tasks.
Learnable MFCCs enable adaptive spectral feature representation, capturing the
temporal patterns inherent in network traffic more effectively than traditional
fixed MFCCs. We demonstrate that transforming raw signals into MFCCs maps the
data into a higher-dimensional space, enhancing class separability and enabling
more effective multiclass classification. Our approach combines the strengths
of MFCCs with the robust feature extraction capabilities of ResNet-18, offering
a powerful framework for anomaly detection. The proposed model is evaluated on
three widely used IoT intrusion detection datasets: CICIoT2023, NSL-KDD, and
IoTID20. The experimental results highlight the potential of integrating
adaptive signal processing techniques with deep learning architectures to
achieve robust and scalable anomaly detection in heterogeneous IoT network
landscapes.

</details>


### [26] [Crypto-Assisted Graph Degree Sequence Release under Local Differential Privacy](https://arxiv.org/abs/2507.10627)
*Xiaojian Zhang,Junqing Wang,Kerui Chen,Peiyuan Zhao,Huiyuan Bai*

Main category: cs.CR

TL;DR: 论文提出了一种名为CADR-LDP的高效框架，结合加密技术和差分隐私机制，用于发布近似实际度分布的度序列。通过优化阈值选择和边缘添加过程，解决了现有方法在通信成本和准确性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在发布图的度序列时，存在阈值选择困难、通信成本高和准确性不足的问题。论文旨在提出一种更高效的解决方案。

Method: CADR-LDP框架结合了加密辅助的最优阈值选择方法（Optimal-θ-Selection）和局部投影的边缘添加方法（LPEA-LOW），优先处理低度节点以减少误差。

Result: 理论分析证明CADR-LDP满足ε-节点局部差分隐私，实验结果表明其在八个图数据集上优于现有方法。

Conclusion: CADR-LDP通过优化阈值选择和边缘添加过程，显著提高了度序列发布的效率和准确性。

Abstract: Given a graph $G$ defined in a domain $\mathcal{G}$, we investigate locally
differentially private mechanisms to release a degree sequence on $\mathcal{G}$
that accurately approximates the actual degree distribution. Existing solutions
for this problem mostly use graph projection techniques based on edge deletion
process, using a threshold parameter $\theta$ to bound node degrees. However,
this approach presents a fundamental trade-off in threshold parameter
selection. While large $\theta$ values introduce substantial noise in the
released degree sequence, small $\theta$ values result in more edges removed
than necessary. Furthermore, $\theta$ selection leads to an excessive
communication cost. To remedy existing solutions' deficiencies, we present
CADR-LDP, an efficient framework incorporating encryption techniques and
differentially private mechanisms to release the degree sequence. In CADR-LDP,
we first use the crypto-assisted Optimal-$\theta$-Selection method to select
the optimal parameter with a low communication cost. Then, we use the LPEA-LOW
method to add some edges for each node with the edge addition process in local
projection. LPEA-LOW prioritizes the projection with low-degree nodes, which
can retain more edges for such nodes and reduce the projection error.
Theoretical analysis shows that CADR-LDP satisfies $\epsilon$-node local
differential privacy. The experimental results on eight graph datasets show
that our solution outperforms existing methods.

</details>


### [27] [Access Control for Information-Theoretically Secure Key-Document Stores](https://arxiv.org/abs/2507.10730)
*Yin Li,Sharad Mehrota,Shantanu Sharma,Komal Kumari*

Main category: cs.CR

TL;DR: 提出了一种基于密钥的访问控制技术，用于安全外包键值存储，支持关键词检索且防止数据泄露。


<details>
  <summary>Details</summary>
Motivation: 解决外包键值存储中的数据安全、访问权限和查询输出大小泄露问题。

Method: 采用Shamir的秘密共享技术，提供无条件安全性，支持关键词检索并防止恶意访问。

Result: 在500,000文件中处理5,000个关键词耗时231.5ms，能检测并阻止恶意客户端和服务器。

Conclusion: 该方法在保证高效访问的同时，提供了强大的安全性和防篡改能力。

Abstract: This paper presents a novel key-based access control technique for secure
outsourcing key-value stores where values correspond to documents that are
indexed and accessed using keys. The proposed approach adopts Shamir's
secret-sharing that offers unconditional or information-theoretic security. It
supports keyword-based document retrieval while preventing leakage of the data,
access rights of users, or the size (\textit{i}.\textit{e}., volume of the
output that satisfies a query). The proposed approach allows servers to detect
(and abort) malicious clients from gaining unauthorized access to data, and
prevents malicious servers from altering data undetected while ensuring
efficient access -- it takes 231.5ms over 5,000 keywords across 500,000 files.

</details>


### [28] [3S-Attack: Spatial, Spectral and Semantic Invisible Backdoor Attack Against DNN Models](https://arxiv.org/abs/2507.10733)
*Jianyao Yin,Luca Arnaboldi,Honglong Chen,Pascal Berrang*

Main category: cs.CR

TL;DR: 提出了一种新型后门攻击3S-attack，通过结合空间、频谱和语义域实现隐蔽性，利用Grad-CAM提取语义特征作为触发器，并通过频谱嵌入和像素限制降低可检测性。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击研究多集中在空间和频谱域，语义域较少涉及，3S-attack旨在填补这一空白，实现更隐蔽的攻击。

Method: 利用Grad-CAM和预训练模型提取语义特征作为触发器，嵌入频谱域后通过像素限制最小化与良性样本的差异。

Result: 实验证明3S-attack在多种数据集上具有高度隐蔽性，现有防御方法难以检测。

Conclusion: 3S-attack展示了后门攻击的新方向，强调了AI安全需要更强的防御机制。

Abstract: Backdoor attacks involve either poisoning the training data or directly
modifying the model in order to implant a hidden behavior, that causes the
model to misclassify inputs when a specific trigger is present. During
inference, the model maintains high accuracy on benign samples but
misclassifies poisoned samples into an attacker-specified target class.
Existing research on backdoor attacks has explored developing triggers in the
spatial, spectral (frequency), and semantic (feature) domains, aiming to make
them stealthy. While some approaches have considered designing triggers that
are imperceptible in both spatial and spectral domains, few have incorporated
the semantic domain. In this paper, we propose a novel backdoor attack, termed
3S-attack, which is stealthy across the spatial, spectral, and semantic
domains. The key idea is to exploit the semantic features of benign samples as
triggers, using Gradient-weighted Class Activation Mapping (Grad-CAM) and a
preliminary model for extraction. The trigger is then embedded in the spectral
domain, followed by pixel-level restrictions after converting the samples back
to the spatial domain. This process minimizes the distance between poisoned and
benign samples, making the attack harder to detect by existing defenses and
human inspection. Extensive experiments on various datasets, along with
theoretical analysis, demonstrate the stealthiness of 3S-attack and highlight
the need for stronger defenses to ensure AI security. Our code is available at:
https://anonymous.4open.science/r/anon-project-3776/

</details>


### [29] [Contrastive-KAN: A Semi-Supervised Intrusion Detection Framework for Cybersecurity with scarce Labeled Data](https://arxiv.org/abs/2507.10808)
*Mohammad Alikhani,Reza Kazemi*

Main category: cs.CR

TL;DR: 提出了一种基于半监督对比学习的实时入侵检测系统，利用Kolmogorov-Arnold网络（KAN）解决数据标注不足的问题，并在多个基准数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 在第四次工业革命时代，网络安全和入侵检测系统对物联网和工业物联网至关重要，但标注数据稀缺和高成本阻碍了机器学习模型的有效训练。

Method: 采用半监督对比学习框架和KAN网络，利用大量未标注数据区分正常和攻击行为。

Result: 在UNSW-NB15、BoT-IoT和Gas Pipeline数据集上，仅用少量标注样本即优于现有方法，KAN在检测准确性和鲁棒性上均优于传统MLP。

Conclusion: 该方法在多类分类和安全关键环境中表现优异，具有可解释性和规则提取潜力。

Abstract: In the era of the Fourth Industrial Revolution, cybersecurity and intrusion
detection systems are vital for the secure and reliable operation of IoT and
IIoT environments. A key challenge in this domain is the scarcity of labeled
cyber-attack data, as most industrial systems operate under normal conditions.
This data imbalance, combined with the high cost of annotation, hinders the
effective training of machine learning models. Moreover, rapid detection of
attacks is essential, especially in critical infrastructure, to prevent
large-scale disruptions. To address these challenges, we propose a real-time
intrusion detection system based on a semi-supervised contrastive learning
framework using the Kolmogorov-Arnold Network (KAN). Our method leverages
abundant unlabeled data to distinguish between normal and attack behaviors
effectively. We validate our approach on three benchmark datasets: UNSW-NB15,
BoT-IoT, and Gas Pipeline, using only 2.20 percent, 1.28 percent, and 8 percent
of labeled samples, respectively, to simulate real-world conditions.
Experimental results show that our method outperforms existing contrastive
learning-based approaches. We further compare KAN with a traditional multilayer
perceptron (MLP), demonstrating KAN's superior performance in both detection
accuracy and robustness under limited supervision. KAN's ability to model
complex relationships and its learnable activation functions are also explored
and visualized, offering interpretability and potential for rule extraction.
The method supports multi-class classification and proves effective in
safety-critical environments where reliability is paramount.

</details>


### [30] [Reporte de vulnerabilidades en IIoT. Proyecto DEFENDER](https://arxiv.org/abs/2507.10819)
*Pedro Almansa Jiménez,Lorenzo Fernández Maimó,Ángel Luis Peráles Gómez*

Main category: cs.CR

TL;DR: 该技术报告全面研究了工业物联网（IIoT）设备，分析了其场景、漏洞及安全威胁，并提出了最新的安全对策，特别强调了机器学习在工业网络安全中的作用。


<details>
  <summary>Details</summary>
Motivation: 工业物联网设备在关键工业环境中广泛应用，但其安全性面临严峻挑战，需要系统性的研究和解决方案。

Method: 报告通过分类IIoT设备、分析其功能和漏洞，并结合实际攻击案例，提出安全对策。

Result: 报告揭示了IIoT设备的漏洞和攻击模式，并总结了有效的安全措施，尤其是机器学习技术的应用。

Conclusion: IIoT设备的安全问题需多维度解决，机器学习在提升工业网络安全中具有重要潜力。

Abstract: The main objective of this technical report is to conduct a comprehensive
study on devices operating within Industrial Internet of Things (IIoT)
environments, describing the scenarios that define this category and analysing
the vulnerabilities that compromise their security. To this end, the report
seeks to identify and examine the main classes of IIoT devices, detailing their
characteristics, functionalities, and roles within industrial systems. This
analysis enables a better understanding of how these devices interact and
fulfil the requirements of critical industrial environments. The report also
explores the specific contexts in which these devices operate, highlighting the
distinctive features of industrial scenarios and the conditions under which the
devices function. Furthermore, it analyses the vulnerabilities affecting IIoT
devices, outlining their vectors, targets, impact, and consequences. The report
then describes the typical phases of an attack, along with a selection of
real-world documented incidents. These cases are classified according to the
taxonomy presented in Section 3, providing a comprehensive view of the
potential threats to security and assessing the impact these vulnerabilities
may have on industrial environments. Finally, the report presents a compilation
of some of the most recent and effective security countermeasures as potential
solutions to the security challenges faced by industrial systems. Special
emphasis is placed on the role of Machine Learning in the development of these
approaches, underscoring its importance in enhancing industrial cybersecurity.

</details>


### [31] [REAL-IoT: Characterizing GNN Intrusion Detection Robustness under Practical Adversarial Attack](https://arxiv.org/abs/2507.10836)
*Zhonghao Zhan,Huichi Zhou,Hamed Haddadi*

Main category: cs.CR

TL;DR: 论文提出REAL-IoT框架，用于评估GNN在物联网环境中的鲁棒性，揭示其在分布漂移和真实攻击下的性能下降，并探索LLM增强鲁棒性的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有GNN入侵检测系统评估局限于单一数据集，且对抗鲁棒性评估缺乏真实性，导致高估其性能。

Method: 提出REAL-IoT框架，整合多数据集评估泛化能力，并基于真实IoT测试床生成攻击数据集，同时探索LLM过滤可疑流量的方法。

Result: 评估显示GNN模型在真实场景下性能下降，LLM过滤可提升鲁棒性。

Conclusion: 强调真实威胁建模和严格评估对开发鲁棒物联网入侵检测系统的重要性。

Abstract: Graph Neural Network (GNN)-based network intrusion detection systems (NIDS)
are often evaluated on single datasets, limiting their ability to generalize
under distribution drift. Furthermore, their adversarial robustness is
typically assessed using synthetic perturbations that lack realism. This
measurement gap leads to an overestimation of GNN-based NIDS resilience. To
address the limitations, we propose \textbf{REAL-IoT}, a comprehensive
framework for robustness evaluation of GNN-based NIDS in IoT environments. Our
framework presents a methodology that creates a unified dataset from canonical
datasets to assess generalization under drift. In addition, it features a novel
intrusion dataset collected from a physical IoT testbed, which captures network
traffic and attack scenarios under real-world settings. Furthermore, using
REAL-IoT, we explore the usage of Large Language Models (LLMs) to analyze
network data and mitigate the impact of adversarial examples by filtering
suspicious flows. Our evaluations using REAL-IoT reveal performance drops in
GNN models compared to results from standard benchmarks, quantifying their
susceptibility to drift and realistic attacks. We also demonstrate the
potential of LLM-based filtering to enhance robustness. These findings
emphasize the necessity of realistic threat modeling and rigorous measurement
practices for developing resilient IoT intrusion detection systems.

</details>


### [32] [BandFuzz: An ML-powered Collaborative Fuzzing Framework](https://arxiv.org/abs/2507.10845)
*Wenxuan Shi,Hongwei Li,Jiahao Yu,Xinqian Sun,Wenbo Guo,Xinyu Xing*

Main category: cs.CR

TL;DR: 协作模糊测试通过结合多个模糊测试工具，动态选择适合不同程序的组合，提供更稳定和通用的性能。然而，现有框架面临计算资源需求增加和资源分配效率低的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试工具依赖特定假设，而协作模糊测试放宽这些假设，旨在提供更通用的解决方案。

Method: 通过动态组合多个模糊测试工具，适应不同程序的需求。

Result: 协作模糊测试在性能上更稳定和通用，但资源消耗和分配效率是主要限制。

Conclusion: 协作模糊测试是通用模糊测试的有前景方向，但需解决资源问题。

Abstract: Collaborative fuzzing has recently emerged as a technique that combines
multiple individual fuzzers and dynamically chooses the appropriate
combinations suited for different programs. Unlike individual fuzzers, which
rely on specific assumptions to maintain their effectiveness, collaborative
fuzzing relaxes the assumptions on target programs, providing constant and
robust performance across various programs. Ideally, collaborative fuzzing
should be a more promising direction toward generic fuzzing solutions, as it
mitigates the need for manual cherry-picking of individual fuzzers. However,
the effectiveness of existing collaborative fuzzing frameworks is limited by
major challenges, such as the need for additional computational resources
compared to individual fuzzers and the inefficient allocation of resources
among the various fuzzers.

</details>


### [33] [PhreshPhish: A Real-World, High-Quality, Large-Scale Phishing Website Dataset and Benchmark](https://arxiv.org/abs/2507.10854)
*Thomas Dalton,Hemanth Gowda,Girish Rao,Sachin Pargi,Alireza Hadj Khodabakhshi,Joseph Rombs,Stephan Jou,Manish Marwah*

Main category: cs.CR

TL;DR: 论文介绍了PhreshPhish，一个高质量的大规模钓鱼网站数据集，解决了现有数据集的问题，并提出了基准测试套件以促进钓鱼检测研究的进步。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击对经济和声誉造成严重损害，但现有数据集存在质量低、泄漏和基准率不现实等问题，阻碍了机器学习在钓鱼检测中的应用。

Method: 提出PhreshPhish数据集，规模更大、质量更高；设计基准测试套件，减少泄漏、增加任务难度、增强多样性并调整基准率。

Result: PhreshPhish数据集质量显著优于现有公开数据集，基准测试套件为模型评估提供了更真实的标准。

Conclusion: PhreshPhish数据集和基准测试套件的发布将推动钓鱼检测研究的标准化和进一步发展。

Abstract: Phishing remains a pervasive and growing threat, inflicting heavy economic
and reputational damage. While machine learning has been effective in real-time
detection of phishing attacks, progress is hindered by lack of large,
high-quality datasets and benchmarks. In addition to poor-quality due to
challenges in data collection, existing datasets suffer from leakage and
unrealistic base rates, leading to overly optimistic performance results. In
this paper, we introduce PhreshPhish, a large-scale, high-quality dataset of
phishing websites that addresses these limitations. Compared to existing public
datasets, PhreshPhish is substantially larger and provides significantly higher
quality, as measured by the estimated rate of invalid or mislabeled data
points. Additionally, we propose a comprehensive suite of benchmark datasets
specifically designed for realistic model evaluation by minimizing leakage,
increasing task difficulty, enhancing dataset diversity, and adjustment of base
rates more likely to be seen in the real world. We train and evaluate multiple
solution approaches to provide baseline performance on the benchmark sets. We
believe the availability of this dataset and benchmarks will enable realistic,
standardized model comparison and foster further advances in phishing
detection. The datasets and benchmarks are available on Hugging Face
(https://huggingface.co/datasets/phreshphish/phreshphish).

</details>


### [34] [From Alerts to Intelligence: A Novel LLM-Aided Framework for Host-based Intrusion Detection](https://arxiv.org/abs/2507.10873)
*Danyu Sun,Jinghuai Zhang,Jiacen Xu,Yu Zheng,Yuan Tian,Zhou Li*

Main category: cs.CR

TL;DR: 论文提出了一种基于大型语言模型（LLM）的定制化HIDS系统SHIELD，通过整合多种技术解决LLM的局限性，显著提升了入侵检测性能。


<details>
  <summary>Details</summary>
Motivation: HIDS在部署中常因高误报率、结果不一致等问题受到批评，而LLM因其语义分析能力有望改进HIDS，但直接应用LLM效果不佳。

Method: 开发了SHIELD系统，结合事件级MAE、攻击证据识别与扩展、确定性数据增强（DDA）等技术，优化LLM的检测能力。

Result: 在三个日志数据集上的实验表明，SHIELD性能优于5种代表性HIDS。

Conclusion: SHIELD展示了LLM在入侵检测中的潜力，为未来研究奠定了基础。

Abstract: Host-based intrusion detection system (HIDS) is a key defense component to
protect the organizations from advanced threats like Advanced Persistent
Threats (APT). By analyzing the fine-grained logs with approaches like data
provenance, HIDS has shown successes in capturing sophisticated attack traces.
Despite the progresses embarked by the research community and industry, HIDS
still frequently encounters backlash from their operators in the deployed
environments, due to issues like high false-positive rate, inconsistent
outcomes across environments and human-unfriendly detection results. Large
Language Models (LLMs) have great potentials to advance the state of HIDS,
given their extensive knowledge of attack techniques and their ability to
detect anomalies through semantic analysis, anchored by recent studies. Yet,
our preliminary analysis indicates that building an HIDS by naively prompting
an LLM is unlikely to succeed. In this work, we explore the direction of
building a customized LLM pipeline for HIDS and develop a system named SHIELD.
SHIELD addresses challenges related to LLM's token limits, confusion of
background noises, etc., by integrating a variety of techniques like
event-level Masked Autoencoder (MAE) for attack window detection, attack
evidence identification and expansion, Deterministic Data Augmentation (DDA)
for profiling normal activities, and multi-purpose prompting that guides the
LLM to conduct precise and interpretable attack investigations. Extensive
experiments on three log datasets (DARPA-E3, NodLink-simulated-data and
ATLASv2) show that SHIELD consistently achieves outstanding performance in
comparison with 5 representative HIDS. These findings highlight the potential
of LLMs as powerful tools for intrusion detection and pave the way for future
research in this domain.

</details>


### [35] [MalCodeAI: Autonomous Vulnerability Detection and Remediation via Language Agnostic Code Reasoning](https://arxiv.org/abs/2507.10898)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy,Noha El Kachach*

Main category: cs.CR

TL;DR: MalCodeAI是一种语言无关的多阶段AI管道，用于自主代码安全分析和修复，结合了代码分解和语义推理，在14种编程语言中提供可扩展且准确的结果。


<details>
  <summary>Details</summary>
Motivation: 传统漏洞检测工具的局限性及网络威胁的复杂性增加，需要新的软件系统安全方法。

Method: 使用经过微调的Qwen2.5-Coder-3B-Instruct模型，通过LoRA优化，分两阶段进行功能分解和漏洞检测修复。

Result: 在验证损失和开发者评估中表现优异，功能分解阶段验证损失0.397，漏洞检测阶段0.199，开发者评分高。

Conclusion: MalCodeAI为智能、可解释且以开发者为中心的软件安全解决方案提供了重要进展。

Abstract: The growing complexity of cyber threats and the limitations of traditional
vulnerability detection tools necessitate novel approaches for securing
software systems. We introduce MalCodeAI, a language-agnostic, multi-stage AI
pipeline for autonomous code security analysis and remediation. MalCodeAI
combines code decomposition and semantic reasoning using fine-tuned
Qwen2.5-Coder-3B-Instruct models, optimized through Low-Rank Adaptation (LoRA)
within the MLX framework, and delivers scalable, accurate results across 14
programming languages. In Phase 1, the model achieved a validation loss as low
as 0.397 for functional decomposition and summarization of code segments after
200 iterations, 6 trainable layers, and a learning rate of 2 x 10^(-5). In
Phase 2, for vulnerability detection and remediation, it achieved a best
validation loss of 0.199 using the same number of iterations and trainable
layers but with an increased learning rate of 4 x 10^(-5), effectively
identifying security flaws and suggesting actionable fixes. MalCodeAI supports
red-hat-style exploit tracing, CVSS-based risk scoring, and zero-shot
generalization to detect complex, zero-day vulnerabilities. In a qualitative
evaluation involving 15 developers, the system received high scores in
usefulness (mean 8.06/10), interpretability (mean 7.40/10), and readability of
outputs (mean 7.53/10), confirming its practical value in real-world
development workflows. This work marks a significant advancement toward
intelligent, explainable, and developer-centric software security solutions.

</details>


### [36] [DVFS: A Dynamic Verifiable Fuzzy Search Service for Encrypted Cloud Data](https://arxiv.org/abs/2507.10927)
*Jie Zhang,Xiaohong Li,Man Zheng,Zhe Hou,Guangdong Bai,Ruitao Feng*

Main category: cs.CR

TL;DR: DVFS是一种动态可验证的模糊搜索服务，解决了云存储中加密数据检索的安全与效率矛盾，通过自适应安全模糊搜索、双存储库版本控制和区块链验证系统实现高效、安全的动态操作。


<details>
  <summary>Details</summary>
Motivation: 云存储中的加密数据检索存在安全与效率的矛盾，现有解决方案无法同时满足高安全性和高效率的需求。

Method: DVFS结合了局部敏感哈希和虚拟二叉树的自适应安全模糊搜索方法，支持动态更新的双存储库版本控制机制，以及基于区块链的验证系统。

Result: DVFS将搜索复杂度从线性降至亚线性（O(log n)），同时消除了分支泄漏风险，并通过区块链验证确保操作的正确性和完整性。

Conclusion: DVFS成功解决了安全与性能的矛盾，为加密数据检索提供了高效、安全且可信的动态操作方案。

Abstract: Cloud storage introduces critical privacy challenges for encrypted data
retrieval, where fuzzy multi-keyword search enables approximate matching while
preserving data confidentiality. Existing solutions face fundamental trade-offs
between security and efficiency: linear-search mechanisms provide adaptive
security but incur prohibitive overhead for large-scale data, while tree-based
indexes improve performance at the cost of branch leakage vulnerabilities.
  To address these limitations, we propose DVFS - a dynamic verifiable fuzzy
search service with three core innovations: (1) An \textit{adaptive-secure
fuzzy search} method integrating locality-sensitive hashing with virtual binary
trees, eliminating branch leakage while reducing search complexity from linear
to sublinear ($O(\log n)$ time); (2) A \textit{dual-repository version control}
mechanism supporting dynamic updates with forward privacy, preventing
information leakage during operations; (3) A \textit{blockchain-based
verification system} that ensures correctness and completeness via smart
contracts, achieving $O(\log n)$ verification complexity.
  Our solution advances secure encrypted retrieval by simultaneously resolving
the security-performance paradox and enabling trustworthy dynamic operations.

</details>


### [37] [Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking](https://arxiv.org/abs/2507.11137)
*Yuan Yao,Jin Song,Jian Jin*

Main category: cs.CR

TL;DR: NeuralMark是一种基于哈希水印过滤器的神经网络水印方法，旨在保护深度神经网络的版权，抵御伪造和覆盖攻击。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络作为有价值的数字资产需要版权保护，而现有的权重水印方法易受攻击。

Method: 利用哈希函数从密钥生成不可逆二进制水印，作为过滤器选择嵌入参数，并结合平均池化抵抗微调和剪枝攻击。

Result: 在13种卷积和Transformer架构、5个图像分类任务和1个文本生成任务中验证了其有效性和鲁棒性。

Conclusion: NeuralMark设计巧妙，适用范围广，理论安全边界明确，实践效果显著。

Abstract: As valuable digital assets, deep neural networks necessitate robust ownership
protection, positioning neural network watermarking (NNW) as a promising
solution. Among various NNW approaches, weight-based methods are favored for
their simplicity and practicality; however, they remain vulnerable to forging
and overwriting attacks. To address those challenges, we propose NeuralMark, a
robust method built around a hashed watermark filter. Specifically, we utilize
a hash function to generate an irreversible binary watermark from a secret key,
which is then used as a filter to select the model parameters for embedding.
This design cleverly intertwines the embedding parameters with the hashed
watermark, providing a robust defense against both forging and overwriting
attacks. An average pooling is also incorporated to resist fine-tuning and
pruning attacks. Furthermore, it can be seamlessly integrated into various
neural network architectures, ensuring broad applicability. Theoretically, we
analyze its security boundary. Empirically, we verify its effectiveness and
robustness across 13 distinct Convolutional and Transformer architectures,
covering five image classification tasks and one text generation task. The
source codes are available at https://github.com/AIResearch-Group/NeuralMark.

</details>


### [38] [FacialMotionID: Identifying Users of Mixed Reality Headsets using Abstract Facial Motion Representations](https://arxiv.org/abs/2507.11138)
*Adriano Castro,Simon Hanisch,Matin Fallahi,Thorsten Strufe*

Main category: cs.CR

TL;DR: 研究探讨了混合现实头戴设备中面部运动数据的隐私风险，发现用户可通过数据被高精度识别，且情绪状态也可被推断。


<details>
  <summary>Details</summary>
Motivation: 随着混合现实设备普及，面部运动数据作为行为生物特征可能引发隐私问题，需评估其是否会导致用户身份或敏感信息泄露。

Method: 研究收集了116名参与者在三种头戴设备上的面部、眼部和头部运动数据，分析其抽象表示形式的识别能力。

Result: 用户可从数据中以98%的平衡准确率被重新识别，且情绪状态推断准确率达86%。

Conclusion: 面部运动数据在混合现实环境中存在显著的隐私风险，需引起重视。

Abstract: Facial motion capture in mixed reality headsets enables real-time avatar
animation, allowing users to convey non-verbal cues during virtual
interactions. However, as facial motion data constitutes a behavioral
biometric, its use raises novel privacy concerns. With mixed reality systems
becoming more immersive and widespread, understanding whether face motion data
can lead to user identification or inference of sensitive attributes is
increasingly important.
  To address this, we conducted a study with 116 participants using three types
of headsets across three sessions, collecting facial, eye, and head motion data
during verbal and non-verbal tasks. The data used is not raw video, but rather,
abstract representations that are used to animate digital avatars. Our analysis
shows that individuals can be re-identified from this data with up to 98%
balanced accuracy, are even identifiable across device types, and that
emotional states can be inferred with up to 86% accuracy. These results
underscore the potential privacy risks inherent in face motion tracking in
mixed reality environments.

</details>


### [39] [Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities](https://arxiv.org/abs/2507.11155)
*Yiting Qu,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 论文研究了视觉语言模型（VLMs）在识别多模态不安全概念时的表现，提出了UnsafeConcepts数据集和基于强化学习的优化方法，提升了模型对齐能力。


<details>
  <summary>Details</summary>
Motivation: 探讨VLMs在不同模态（文本和图像）下识别不安全概念的能力，并解决其存在的模态差距问题。

Method: 构建UnsafeConcepts数据集，评估8种流行VLMs的感知和对齐能力，提出基于PPO的强化学习方法来优化模型对齐。

Result: 发现VLMs能准确感知不安全概念但有时误判为安全，开源VLMs存在模态差距；提出的RL方法优于基线方法。

Conclusion: 研究为推进安全VLMs提供了数据集、评估结果和优化方案。

Abstract: Vision-language models (VLMs) are increasingly applied to identify unsafe or
inappropriate images due to their internal ethical standards and powerful
reasoning abilities. However, it is still unclear whether they can recognize
various unsafe concepts when presented in different modalities, such as text
and images. To address this, we first compile the UnsafeConcepts dataset,
featuring 75 unsafe concepts, i.e., ``Swastika,'' ``Sexual Harassment,'' and
``Assaults,'' along with associated 1.5K images. We then conduct a systematic
evaluation of VLMs' perception (concept recognition) and alignment (ethical
reasoning) capabilities. We assess eight popular VLMs and find that, although
most VLMs accurately perceive unsafe concepts, they sometimes mistakenly
classify these concepts as safe. We also identify a consistent modality gap
among open-source VLMs in distinguishing between visual and textual unsafe
concepts. To bridge this gap, we introduce a simplified reinforcement learning
(RL)-based approach using proximal policy optimization (PPO) to strengthen the
ability to identify unsafe concepts from images. Our approach uses reward
scores based directly on VLM responses, bypassing the need for collecting
human-annotated preference data to train a new reward model. Experimental
results show that our approach effectively enhances VLM alignment on images
while preserving general capabilities. It outperforms baselines such as
supervised fine-tuning (SFT) and direct preference optimization (DPO). We hope
our dataset, evaluation findings, and proposed alignment solution contribute to
the community's efforts in advancing safe VLMs.

</details>


### [40] [LRCTI: A Large Language Model-Based Framework for Multi-Step Evidence Retrieval and Reasoning in Cyber Threat Intelligence Credibility Verification](https://arxiv.org/abs/2507.11310)
*Fengxiao Tang,Huan Li,Ming Zhao,Zongzong Wu,Shisong Peng,Tao Yin*

Main category: cs.CR

TL;DR: LRCTI是一个基于大型语言模型（LLM）的框架，用于多步骤验证网络威胁情报（CTI）的可信度，通过文本摘要、自适应证据检索和自然语言推理模块，显著提升了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统CTI可信度验证方法缺乏鲁棒性和透明度，无法有效处理不完整或嘈杂的情报，因此需要一种更可靠、可解释的解决方案。

Method: LRCTI框架包括文本摘要模块、自适应多步骤证据检索机制和基于提示的自然语言推理模块，逐步验证CTI的可信度。

Result: 在CTI-200和PolitiFact数据集上，LRCTI的F1-Macro和F1-Micro分数分别达到90.9%和93.6%，比现有方法提高了5%以上。

Conclusion: LRCTI提供了一种可扩展、准确且可解释的自动化CTI可信度验证方案，有效解决了传统方法的局限性。

Abstract: Verifying the credibility of Cyber Threat Intelligence (CTI) is essential for
reliable cybersecurity defense. However, traditional approaches typically treat
this task as a static classification problem, relying on handcrafted features
or isolated deep learning models. These methods often lack the robustness
needed to handle incomplete, heterogeneous, or noisy intelligence, and they
provide limited transparency in decision-making-factors that reduce their
effectiveness in real-world threat environments. To address these limitations,
we propose LRCTI, a Large Language Model (LLM)-based framework designed for
multi-step CTI credibility verification. The framework first employs a text
summarization module to distill complex intelligence reports into concise and
actionable threat claims. It then uses an adaptive multi-step evidence
retrieval mechanism that iteratively identifies and refines supporting
information from a CTI-specific corpus, guided by LLM feedback. Finally, a
prompt-based Natural Language Inference (NLI) module is applied to evaluate the
credibility of each claim while generating interpretable justifications for the
classification outcome. Experiments conducted on two benchmark datasets,
CTI-200 and PolitiFact show that LRCTI improves F1-Macro and F1-Micro scores by
over 5%, reaching 90.9% and 93.6%, respectively, compared to state-of-the-art
baselines. These results demonstrate that LRCTI effectively addresses the core
limitations of prior methods, offering a scalable, accurate, and explainable
solution for automated CTI credibility verification

</details>


### [41] [A Review of Privacy Metrics for Privacy-Preserving Synthetic Data Generation](https://arxiv.org/abs/2507.11324)
*Frederik Marinus Trudslev,Matteo Lissandrini,Juan Manuel Rodriguez,Martin Bøgsted,Daniele Dell'Aglio*

Main category: cs.CR

TL;DR: PP-SDG机制通过差分隐私（DP）保护个人数据隐私，但隐私损失（ε）难以解释。本文提出17种隐私度量（PMs）的假设和数学公式，以透明化隐私风险。


<details>
  <summary>Details</summary>
Motivation: 解决差分隐私中隐私损失（ε）难以解释的问题，并通过多种隐私度量（PMs）透明化实际隐私风险。

Method: 提出17种隐私度量的假设和数学公式，详细定义其计算方法。

Result: 明确了17种隐私度量的计算方式，为PP-SDG机制的隐私风险评估提供透明化工具。

Conclusion: 通过定义隐私度量的计算方式，增强了PP-SDG机制隐私风险的透明性和可解释性。

Abstract: Privacy Preserving Synthetic Data Generation (PP-SDG) has emerged to produce
synthetic datasets from personal data while maintaining privacy and utility.
Differential privacy (DP) is the property of a PP-SDG mechanism that
establishes how protected individuals are when sharing their sensitive data. It
is however difficult to interpret the privacy loss ($\varepsilon$) expressed by
DP. To make the actual risk associated with the privacy loss more transparent,
multiple privacy metrics (PMs) have been proposed to assess the privacy risk of
the data. These PMs are utilized in separate studies to assess newly introduced
PP-SDG mechanisms. Consequently, these PMs embody the same assumptions as the
PP-SDG mechanism they were made to assess. Therefore, a thorough definition of
how these are calculated is necessary. In this work, we present the assumptions
and mathematical formulations of 17 distinct privacy metrics.

</details>


### [42] [Demo: Secure Edge Server for Network Slicing and Resource Allocation in Open RAN](https://arxiv.org/abs/2507.11499)
*Adhwaa Alchaab,Ayman Younis,Dario Pompili*

Main category: cs.CR

TL;DR: SnSRIC是一个智能网络切片框架，用于在Open RAN环境中防御DDoS攻击，通过AI动态分配资源并确保安全。


<details>
  <summary>Details</summary>
Motivation: NGRAN需要满足严格的安全、延迟和SLA要求，但面临基础设施安全、资源动态分配和实时重配置的挑战。

Method: SnSRIC采用AI驱动的xApp动态分配PRB，检测异常行为，区分恶意设备，并通过E2接口限制恶意信号。

Result: 系统能够有效防御DDoS攻击，同时保障合法用户的服务连续性。

Conclusion: SnSRIC为NGRAN提供了一种安全、智能的网络切片解决方案。

Abstract: Next-Generation Radio Access Networks (NGRAN) aim to support diverse vertical
applications with strict security, latency, and Service-Level Agreement (SLA)
requirements. These demands introduce challenges in securing the
infrastructure, allocating resources dynamically, and enabling real-time
reconfiguration. This demo presents SnSRIC, a secure and intelligent network
slicing framework that mitigates a range of Distributed Denial-of-Service
(DDoS) attacks in Open RAN environments. SnSRIC incorporates an AI-driven xApp
that dynamically allocates Physical Resource Blocks (PRBs) to active users
while enforcing slice-level security. The system detects anomalous behavior,
distinguishes between benign and malicious devices, and uses the E2 interface
to throttle rogue signaling while maintaining service continuity for legitimate
users.

</details>


### [43] [ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning](https://arxiv.org/abs/2507.11500)
*Zhengyue Zhao,Yingzi Ma,Somesh Jha,Marco Pavone,Chaowei Xiao*

Main category: cs.CR

TL;DR: 论文提出ARMOR框架，通过结构化推理增强LLMs的安全性，显著提升对恶意指令的防御能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的安全对齐方法易被绕过，需要更结构化的推理过程来识别用户真实意图和风险。

Method: 提出ARMOR框架，分三步：检测越狱策略、提取核心意图、应用安全策略分析。

Result: ARMOR在自适应越狱攻击和多个安全基准测试中表现优异，显著优于现有方法。

Conclusion: ARMOR通过结构化推理有效提升LLMs的安全性，为未来安全对齐提供新方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable generative
capabilities. However, their susceptibility to misuse has raised significant
safety concerns. While post-training safety alignment methods have been widely
adopted, LLMs remain vulnerable to malicious instructions that can bypass
safety constraints. Recent efforts have introduced inference-time safety
reasoning (system-2 alignment), where LLMs conduct a reasoning process to
perform safety verification before final response. We show, however, that these
checks are driven by ad-hoc reasoning that diverges from the structured human
process, where they first discern a user's true intent, then evaluate the
associated risk based on the true intent. Consequently, these defenses remain
vulnerable to sophisticated jailbreak prompts that cloak harmful goals in
seemingly benign language. To build secure and safe LLMs, we propose a
reasoning-based safety alignment framework, ARMOR, that replaces the ad-hoc
chains of thought reasoning process with human-aligned, structured one. At
inference, ARMOR (1) detects likely jailbreak strategies, (2) extracts the
user's core intent while discarding deceptive instructions, and (3) applies a
policy-grounded safety analysis to the purified request. ARMOR is evaluated on
adaptive jailbreak attacks and multiple safety benchmarks, and a test-time
scaling is conducted to further improve its performance. Results demonstrate
that ARMOR significantly enhances the robustness against state-of-the-art
adaptive jailbreak attacks and outperforms recent reasoning-based aligned
models across various safety benchmarks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [44] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Main category: cs.AI

TL;DR: SAMEP协议解决了AI代理间记忆共享的短暂性问题，提供持久、安全且可语义搜索的记忆交换框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理架构存在记忆短暂性问题，限制了跨会话和代理间的协作与知识共享。

Method: 提出SAMEP框架，包括分布式记忆存储、向量语义搜索、加密访问控制和标准化API。

Result: 实验显示减少了73%冗余计算，提升了89%上下文相关性，并完全符合监管要求。

Conclusion: SAMEP为持久协作的AI代理生态系统提供了安全、隐私保障的新范式。

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [45] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

Main category: cs.AI

TL;DR: 该研究通过AIM框架证明，在去中心化多智能体强化学习中，内生符号系统可实现自然语义压缩与纳什均衡驱动的语义收敛，无需外部归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过引入归纳偏置促进通信，但本研究质疑其是否过度工程化，探索内生符号系统的潜力。

Method: 采用基于VQ-VAE的AIM框架，分析智能体的神经表征与符号通信行为。

Result: AIM框架实现高效符号通信，符号使用呈现幂律分布，并提出了三项理论见解。

Conclusion: 内生符号系统为连接符号主义与连接主义提供了新途径，未来将探索HQ-VAE增强表达能力。

Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [46] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: 论文提出了一种模块化的多智能体AI视觉分类框架，结合了通用多模态智能体、非视觉推理协调器和RAG模块，用于苹果叶病诊断，显著提升了零样本设置的准确性和可信度。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体AI在零样本设置中的可信度问题，尤其是在视觉和语言理解结合的领域。

Method: 引入模块化框架，包括多模态智能体、协调器和RAG模块，通过置信度校准和图像检索优化信任分配。

Result: 零样本设置下准确率提升77.94%，总体达到85.63%，GPT-4o表现更优校准，Qwen-2.5-VL存在过度自信问题。

Conclusion: 该框架通过分离感知与元推理，实现了可扩展和可解释的多智能体AI，适用于诊断和生物等信任关键领域。

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [47] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

Main category: cs.AI

TL;DR: 论文揭示了大型语言模型（LLMs）在符号推理、算术准确性和逻辑一致性任务中的系统性失败，提出了“计算分裂脑综合征”的概念，指出其根源在于计算执行而非知识访问。


<details>
  <summary>Details</summary>
Motivation: 研究旨在诊断LLMs在任务失败中的结构性问题，揭示其表面流畅性与实际能力之间的差距。

Method: 通过控制实验和架构分析，研究了LLMs在任务中的表现，发现其原则表达与实际应用之间的不一致性。

Result: LLMs作为模式完成引擎强大，但缺乏组合推理的架构支持，导致行为脆弱。

Conclusion: 研究明确了当前LLMs的能力边界，并提出了未来模型需要具备元认知控制、原则提升和结构化执行能力。

Abstract: Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [48] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang,Xue Xiao,Ping Yin,Taotao Xie*

Main category: cs.AI

TL;DR: KG2data是一个结合知识图谱、LLM、ReAct代理和工具使用技术的系统，用于气象领域的数据获取和查询处理，性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在知识密集型领域（如气象学）中通过API调用有效利用工具的能力。

Method: 集成知识图谱、LLM、ReAct代理和工具使用技术，通过虚拟API评估API调用准确性。

Result: KG2data在名称识别失败、幻觉失败和调用正确性方面表现优异（1.43%、0%、88.57%），优于RAG2data和chat2data。

Conclusion: KG2data通过知识图谱解决了LLM在领域特定知识上的限制，为高知识需求领域提供了智能问答和数据分析的新方案。

Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [49] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: 本文提出了一个全面的Web of Agents（WoA）进化概述，揭示了现代协议与早期标准的直接联系，并引入了一个四轴分类法来统一分析不同世代的代理架构。


<details>
  <summary>Details</summary>
Motivation: 研究WoA领域的碎片化问题，揭示现代系统与早期标准（如FIPA和OWL）的关联，以促进对该领域发展的整体理解。

Method: 通过四轴分类法（语义基础、通信范式、智能位置、发现机制）系统化分析代理架构，比较不同世代的技术演变。

Result: 发现智能位置从外部数据或平台转移到代理核心模型（LLM）的范式转变，这是现代Agentic AI的基础。

Conclusion: 新协议虽必要但不足，未来研究应关注去中心化身份、经济模型、安全和治理等社会技术挑战。

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [50] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.AI

TL;DR: 本文提出了一种基于规则的音乐生成方法，通过变异现有曲调生成新音乐。利用Sequitur算法解析曲调为语法结构，随机应用19种变异类型，生成新曲调并分析其变化。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过变异现有曲调生成新音乐，探索曲调在多次变异中的变化规律。

Method: 使用Sequitur算法解析曲调为语法结构（PA），随机应用19种变异类型（如添加、删除、交换等），生成新曲调。

Result: 通过编辑距离、结构复杂度和曲调长度分析变异效果，并评估每种变异类型的影响。

Conclusion: 该方法能有效生成与原曲调相关的新音乐，但仅关注音高序列生成，未涉及其他音乐元素。

Abstract: This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [51] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota,Nicolas Tsesmetzis,Faruk O. Alpak,Apurva Gala,Detlef Hohl*

Main category: cs.AI

TL;DR: 本文探讨了AI对数据中心的能源消耗和温室气体排放的影响，分析了短期（2030年前）和长期（2035年后）的情景，并讨论了AI在减排方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的广泛应用，数据中心的能源消耗和碳排放问题日益突出，需要评估其对环境的实际影响以及潜在的减排能力。

Method: 通过分析数据中心的能源消耗情景和AI在能源生产、供应及消费领域的应用潜力，评估其对CO2排放的短期和长期影响。

Result: 短期内，AI的普及可能导致能源消耗和碳排放增加；但长期来看，AI的自动化和优化能力有望显著减少碳足迹。

Conclusion: 尽管AI初期可能对环境造成压力，但其在气候缓解方面的潜力有望超过负面影响，为可持续发展提供支持。

Abstract: Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [52] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati,Bimal Karki,Saroj Gopali,Akbar Siami Namin*

Main category: cs.AI

TL;DR: 该论文通过深度学习模型检测物联网恶意攻击，评估了多种模型（如GraphSAGE、BERT、TCN等）的性能，其中BERT表现最佳。


<details>
  <summary>Details</summary>
Motivation: 物联网系统流量模式具有时序性和多样性，适合深度学习模型学习，以检测恶意攻击。

Method: 采用GraphSAGE、BERT、TCN、Multi-Head Attention、BI-LSTM等模型，评估其在恶意流量检测中的表现。

Result: BERT表现最优，准确率达99.94%，其他指标（如F1-score）接近100%；Multi-Head Attention提供可解释结果但耗时较长；GraphSAGE训练最快但性能最低。

Conclusion: BERT在捕获时序依赖方面表现卓越，适合物联网恶意攻击检测；其他模型各有优劣，需根据需求选择。

Abstract: This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [53] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Main category: cs.AI

TL;DR: 论文提出将AI辅助检测作为分类任务，通过预处理数据使常见模型能有效分类抽象任务数据，并展示了四种图像和时间序列的预处理方法，验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI在复杂任务中的普及，检测AI辅助变得重要，但抽象任务数据对传统方法具有挑战性。

Method: 构建四种神经网络友好的图像预处理方法和一种时间序列方法，结合CNN-RNN架构进行检测。

Result: 实验表明，预处理后的数据能有效分类，时间序列方法提升了检测性能。

Conclusion: 编码时空信息对检测抽象任务中的AI辅助至关重要，预处理方法具有通用性。

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [54] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi,Bhanu T. Gullapalli,Daiqi Gao,Benjamin M. Marlin,Vivek Shetty,Susan A. Murphy*

Main category: cs.AI

TL;DR: SigmaScheduling动态调整决策点时间，提高移动健康干预的及时性。


<details>
  <summary>Details</summary>
Motivation: 固定间隔的决策点调度对习惯性行为干预效果不佳，尤其是对作息不规律的用户。

Method: 提出SigmaScheduling方法，根据行为时间预测的不确定性动态调整决策点。

Result: 在68名参与者的真实数据中，SigmaScheduling在70%以上的情况下确保决策点早于目标行为。

Conclusion: SigmaScheduling提升了移动健康干预的精准性，适用于时间敏感的习惯性行为。

Abstract: Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [55] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: 研究评估了大型语言模型（LLMs）在主题分析任务中的表现，发现GPT-4o在少量样本提示下表现最佳，可作为定性研究的补充工具。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在需要深度解释和领域专业知识的主题分析任务中的可行性，以替代或辅助专家分析。

Method: 使用两个Reddit数据集，将任务建模为一系列二元分类，采用零样本、单样本和少量样本提示策略，评估五个LLMs的性能。

Result: GPT-4o在少量样本提示下表现最佳（准确率90.9%，F1分数0.71），高流行主题的分布与专家分类接近。

Conclusion: 少量样本LLM方法可自动化主题分析，为定性研究提供可扩展的补充工具。

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [56] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia,Heng Zheng,Shawn Bowers,Bertram Ludäscher*

Main category: cs.AI

TL;DR: AF-XRAY是一个开源工具包，用于探索、分析和可视化法律推理中的抽象论证框架（AFs），帮助非专家识别歧义来源并解释论证接受性。


<details>
  <summary>Details</summary>
Motivation: 法律推理中的论证框架（AFs）存在歧义和解释难题，非专家难以理解。AF-XRAY旨在通过可视化工具解决这一问题。

Method: AF-XRAY提供分层可视化、攻击边分类、替代解决方案叠加可视化，以及关键攻击集识别功能。

Result: AF-XRAY能将歧义场景转化为明确的解决方案，帮助用户定位歧义原因并探索替代方案。

Conclusion: AF-XRAY通过实际法律案例验证，支持目的性法律推理，展示不同假设如何导致不同结论。

Abstract: Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [57] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: NavComposer是一个自动生成高质量导航指令的框架，结合NavInstrCritic进行无标注评估，提升语言导航研究的可扩展性和通用性。


<details>
  <summary>Details</summary>
Motivation: 解决专家提供的导航指令数量有限，合成指令质量不足的问题。

Method: NavComposer通过分解和重组语义实体生成指令，NavInstrCritic从对比匹配、语义一致性和语言多样性三个维度评估指令质量。

Result: 实验证明该方法有效，支持大规模研究。

Conclusion: NavComposer和NavInstrCritic为语言导航研究提供了可扩展且通用的解决方案。

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [58] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu,Ting Chen,Irit Hochberg,Zhoujian Sun,Ruth Edry,Zhengxing Huang,Mor Peleg*

Main category: cs.AI

TL;DR: 研究探讨了使用基于大型语言模型（LLM）的多代理系统（MAS）为多病共存患者提供更安全的治疗建议的可行性和价值，发现单代理系统与多学科团队（MDT）表现相当，但建议仍存在不完整和不必要的药物问题。


<details>
  <summary>Details</summary>
Motivation: 多病共存患者的治疗建议因治疗冲突风险而复杂化，现有决策支持系统存在可扩展性限制，研究旨在模拟MDT协作以解决这一问题。

Method: 设计了单代理和MAS框架，模拟MDT决策过程，通过LLM代理间的讨论解决医疗冲突，并在多病共存患者的治疗规划任务中评估系统性能。

Result: 当前LLM下，单代理GP表现与MDT相当，最佳模型能提供满足所有临床目标的正确建议，但建议不完整且存在不必要的药物冲突。

Conclusion: LLM-MAS系统在多病共存治疗建议中具有潜力，但仍需改进建议完整性和减少药物冲突。

Abstract: Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [59] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Main category: cs.AI

TL;DR: F2STrans是一种分阶段的代码翻译方法，通过功能学习和风格学习提升LLMs的性能，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在代码翻译中正确性和可读性的挑战，以促进实际软件开发中的应用。

Method: 分两阶段：功能学习优化翻译正确性，风格学习提升可读性，并结合新基准测试。

Result: F2STrans显著提升性能，Qwen-1.5B在20种场景中平均优于Qwen-32B和GPT-4。

Conclusion: F2STrans为代码翻译提供了有效的功能与风格优化方法，具有实际应用潜力。

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [60] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang,Keyan Ding,Kehua Feng,Zeyuan Wang,Ming Qin,Xiaotong Li,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: 提出了一种知识引导的偏好优化（KPO）框架，通过蛋白质安全知识图谱整合先验知识，减少有害蛋白质序列的生成风险。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型在功能优化和设计中有优势，但可能生成有害序列，带来生物安全和伦理挑战。

Method: 结合蛋白质安全知识图谱，采用图剪枝策略和强化学习，优化序列生成。

Result: KPO显著降低有害序列生成概率，同时保持高功能性。

Conclusion: KPO为生物技术中生成模型的应用提供了安全保证。

Abstract: Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [61] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Main category: cs.AI

TL;DR: 结合代码文本与结构化建模优势的新方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有Transformer模型在代码结构化分析（如控制流和数据流）上的局限性。

Method: 提出一种结合代码文本与结构化建模的新方法。

Result: 未明确提及具体结果。

Conclusion: 新方法旨在弥补现有LLMs在代码结构化分析上的不足。

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


### [62] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic,Min-Hong Shih*

Main category: cs.AI

TL;DR: 结合卷积神经网络（CNN）和表格数据，准确预测鸟类在特定栖息地的存在情况。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化导致栖息地范围变化，需要一种可靠的方法来追踪鸟类分布。

Method: 利用卫星图像和环境特征（如温度、降水、海拔），结合CNN和表格数据建模。

Result: 模型平均准确率达到85%，能有效预测鸟类分布。

Conclusion: 该方法为理解鸟类迁徙提供了可扩展且可靠的解决方案。

Abstract: Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [63] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt,Tunaberk Almaci,Stefan Feuerriegel,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: ExRec是一个个性化习题推荐框架，结合语义知识追踪和强化学习，解决了现有方法忽略问题语义和学习序列的问题。


<details>
  <summary>Details</summary>
Motivation: 现有习题推荐方法通常忽视问题的语义内容和学习的序列性，ExRec旨在填补这一空白。

Method: ExRec通过端到端流程，从问题标注到知识追踪模型训练，优化强化学习方法，并引入模型基值估计（MVE）改进Q学习。

Result: 在四个实际数学学习任务中验证了ExRec的有效性，能泛化到新问题并生成可解释的学习轨迹。

Conclusion: ExRec展示了知识追踪引导的强化学习在教育个性化中的潜力。

Abstract: We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [64] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang,Qizhen Wu,Lei Chen*

Main category: cs.AI

TL;DR: 提出了一种基于视觉语言模型的指挥官系统，用于解决无人地面车辆对抗中的智能感知到决策推理问题，结合视觉语言模型和轻量级大语言模型，实现了高适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的手工规则方法在复杂战场环境中表现脆弱，而现有强化学习方法缺乏可解释性且主要关注动作操作而非战略决策。

Method: 结合视觉语言模型进行场景理解和轻量级大语言模型进行战略推理，实现感知与决策的统一。

Result: 仿真和消融实验表明，该方法在基线模型对比中胜率超过80%。

Conclusion: 该方法通过模拟人类指挥官的认知过程，实现了高适应性和可解释性的全链条感知-决策推理。

Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [65] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.AI

TL;DR: GoldMine OS是一个研究导向的架构，利用多个专用AI代理自动化并安全地将实物黄金代币化为区块链稳定币（OZ），结合链上智能合约和链下AI代理，满足合规性、流动性和风险管理需求。


<details>
  <summary>Details</summary>
Motivation: 解决实物资产与区块链系统之间的代币化与交易问题，同时满足严格的合规性、流动性和风险管理要求。

Method: 采用链上智能合约进行关键风险控制，链下AI代理进行决策，包括四个协作代理（合规、代币发行、做市、风险控制）和一个协调核心。

Result: 原型系统实现1.2秒内的按需代币发行，做市代理在波动条件下保持0.5%以内的价差，系统在故障注入测试中表现出高韧性，支持5000 TPS和10000并发用户。

Conclusion: 基于AI代理的去中心化交易所能够满足高性能和安全要求，为传统非流动性资产提供民主化访问，并通过多签名代理更新和链上社区投票确保系统透明性和适应性。

Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


### [66] [Defining neurosymbolic AI](https://arxiv.org/abs/2507.11127)
*Lennert De Smet,Luc De Raedt*

Main category: cs.AI

TL;DR: 该论文提出了一个神经符号AI的形式化定义，抽象了其关键组成部分，将神经符号推理定义为逻辑函数和信念函数乘积的积分计算。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI领域缺乏普遍接受的形式化定义，导致难以统一理解神经符号模型和推理的本质。

Method: 引入一个形式化定义，抽象神经符号AI的关键成分，具体定义为逻辑函数与信念函数乘积的积分。

Result: 该定义能够抽象代表神经符号AI系统的关键特征。

Conclusion: 提出的形式化定义有助于统一神经符号AI领域的理解，并为未来研究提供基础。

Abstract: Neurosymbolic AI focuses on integrating learning and reasoning, in
particular, on unifying logical and neural representations. Despite the
existence of an alphabet soup of neurosymbolic AI systems, the field is lacking
a generally accepted formal definition of what neurosymbolic models and
inference really are. We introduce a formal definition for neurosymbolic AI
that makes abstraction of its key ingredients. More specifically, we define
neurosymbolic inference as the computation of an integral over a product of a
logical and a belief function. We show that our neurosymbolic AI definition
makes abstraction of key representative neurosymbolic AI systems.

</details>


### [67] [Collaborative Trustworthiness for Good Decision Making in Autonomous Systems](https://arxiv.org/abs/2507.11135)
*Selma Saidi,Omar Laimona,Christoph Schmickler,Dirk Ziegenbein*

Main category: cs.AI

TL;DR: 提出了一种基于协作数据共享的自主系统可信决策方法，利用感知质量等属性评估系统可信度，并采用BDD进行高效信念聚合与传播。


<details>
  <summary>Details</summary>
Motivation: 自主系统在动态复杂环境中确保安全正确行为存在挑战，需提高其决策的可信度和可靠性。

Method: 利用感知质量等属性评估系统可信度，结合社会认识论定义聚合与传播规则，使用BDD进行信念建模与计算优化。

Result: 提出了一种高效的可信决策框架，通过BDD优化实现协作自动推理。

Conclusion: 该方法提升了自主系统在复杂环境中的可信决策能力，为协作数据共享提供了新思路。

Abstract: Autonomous systems are becoming an integral part of many application domains,
like in the mobility sector. However, ensuring their safe and correct behaviour
in dynamic and complex environments remains a significant challenge, where
systems should autonomously make decisions e.g., about manoeuvring. We propose
in this paper a general collaborative approach for increasing the level of
trustworthiness in the environment of operation and improve reliability and
good decision making in autonomous system. In the presence of conflicting
information, aggregation becomes a major issue for trustworthy decision making
based on collaborative data sharing. Unlike classical approaches in the
literature that rely on consensus or majority as aggregation rule, we exploit
the fact that autonomous systems have different quality attributes like
perception quality. We use this criteria to determine which autonomous systems
are trustworthy and borrow concepts from social epistemology to define
aggregation and propagation rules, used for automated decision making. We use
Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and
propagation, and formulate reduction rules to reduce the size of the BDDs and
allow efficient computation structures for collaborative automated reasoning.

</details>


### [68] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Main category: cs.AI

TL;DR: 论文提出了一种使用答案集编程（ASP）计算组合模块实际最大延迟的方法，以替代传统的静态时序分析，从而提高处理器性能。


<details>
  <summary>Details</summary>
Motivation: 传统静态时序分析计算的是延迟的上界，可能导致处理器性能未被充分利用。

Method: 将问题建模为答案集编程（ASP），并提出非平凡的编码方法。

Result: 实验结果表明，ASP能有效解决硬件设计中的复杂问题。

Conclusion: ASP是解决硬件设计中复杂问题的可行方案。

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [69] [DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion](https://arxiv.org/abs/2507.11229)
*Jin Li,Zezhong Ding,Xike Xie*

Main category: cs.AI

TL;DR: DuetGraph提出了一种双路径全局-局部融合的KG推理机制，通过分离局部和全局信息处理路径解决分数过平滑问题，并结合粗到细优化策略提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有KG推理方法因分数过平滑问题导致推理效果下降，需改进以区分正确与错误答案。

Method: DuetGraph采用双路径机制分离局部（消息传递）和全局（注意力）信息处理，并引入粗到细优化策略划分实体子集。

Result: 实验显示DuetGraph在推理质量和训练效率上均有显著提升，推理质量最高提升8.7%，训练效率加速1.8倍。

Conclusion: DuetGraph通过双路径融合和粗到细优化有效解决了分数过平滑问题，实现了SOTA性能。

Abstract: Knowledge graphs (KGs) are vital for enabling knowledge reasoning across
various domains. Recent KG reasoning methods that integrate both global and
local information have achieved promising results. However, existing methods
often suffer from score over-smoothing, which blurs the distinction between
correct and incorrect answers and hinders reasoning effectiveness. To address
this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with
dual-pathway global-local fusion. DuetGraph tackles over-smoothing by
segregating -- rather than stacking -- the processing of local (via message
passing) and global (via attention) information into two distinct pathways,
preventing mutual interference and preserving representational discrimination.
In addition, DuetGraph introduces a coarse-to-fine optimization, which
partitions entities into high- and low-score subsets. This strategy narrows the
candidate space and sharpens the score gap between the two subsets, which
alleviates over-smoothing and enhances inference quality. Extensive experiments
on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)
performance, with up to an 8.7% improvement in reasoning quality and a
1.8$\times$ acceleration in training efficiency.

</details>


### [70] [Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems](https://arxiv.org/abs/2507.11277)
*Dany Moshkovich,Sergey Zeltyn*

Main category: cs.AI

TL;DR: AgentOps框架为基于LLM的代理系统提供全面的操作管理，涵盖观察、分析、优化和自动化，以应对其不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统软件操作实践无法有效管理LLM代理系统的不确定性，需新方法支持开发者、测试者、SRE和业务用户。

Method: 提出AgentOps自动化管道，包括行为观察、指标收集、问题检测、根因分析、优化建议和运行时自动化六个阶段。

Result: 框架通过自动化管理不确定性，支持代理系统的安全、自适应和高效运行。

Conclusion: AgentOps通过自动化驯服不确定性，推动自改进AI系统的安全与适应性。

Abstract: Large Language Models (LLMs) are increasingly deployed within agentic
systems-collections of interacting, LLM-powered agents that execute complex,
adaptive workflows using memory, tools, and dynamic planning. While enabling
powerful new capabilities, these systems also introduce unique forms of
uncertainty stemming from probabilistic reasoning, evolving memory states, and
fluid execution paths. Traditional software observability and operations
practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing,
analyzing, optimizing, and automating operation of agentic AI systems. We
identify distinct needs across four key roles-developers, testers, site
reliability engineers (SREs), and business users-each of whom engages with the
system at different points in its lifecycle. We present the AgentOps Automation
Pipeline, a six-stage process encompassing behavior observation, metric
collection, issue detection, root cause analysis, optimized recommendations,
and runtime automation. Throughout, we emphasize the critical role of
automation in managing uncertainty and enabling self-improving AI systems-not
by eliminating uncertainty, but by taming it to ensure safe, adaptive, and
effective operation.

</details>


### [71] [Opus: A Prompt Intention Framework for Complex Workflow Generation](https://arxiv.org/abs/2507.11288)
*Théo Fagnoni,Mahsun Altin,Chia En Chung,Phillip Kingston,Alan Tuning,Dana O. Mohamed,Inès Adnani*

Main category: cs.AI

TL;DR: 论文提出了Opus Prompt Intention Framework，通过引入中间层（Intention Capture）提升基于LLM的复杂工作流生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决直接基于用户查询生成工作流时逻辑性和扩展性不足的问题。

Method: 提出Opus Workflow Intention Framework，包括从用户查询中提取Workflow Signals、解析为结构化Workflow Intention对象，并基于此生成工作流。

Result: 在1000个多意图查询-工作流对的基准测试中，语义相似度指标显著提升。

Conclusion: Opus框架显著提高了工作流生成质量，尤其在混合意图场景下表现优异。

Abstract: This paper introduces the Opus Prompt Intention Framework, designed to
improve complex Workflow Generation with instruction-tuned Large Language
Models (LLMs). We propose an intermediate Intention Capture layer between user
queries and Workflow Generation, implementing the Opus Workflow Intention
Framework, which consists of extracting Workflow Signals from user queries,
interpreting them into structured Workflow Intention objects, and generating
Workflows based on these Intentions. Our results show that this layer enables
LLMs to produce logical and meaningful outputs that scale reliably as query
complexity increases. On a synthetic benchmark of 1,000 multi-intent
query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to
Workflow Generation yields consistent improvements in semantic Workflow
similarity metrics. In this paper, we introduce the Opus Prompt Intention
Framework by applying the concepts of Workflow Signal and Workflow Intention to
LLM-driven Workflow Generation. We present a reproducible, customizable
LLM-based Intention Capture system to extract Workflow Signals and Workflow
Intentions from user queries. Finally, we provide empirical evidence that the
proposed system significantly improves Workflow Generation quality compared to
direct generation from user queries, particularly in cases of Mixed Intention
Elicitation.

</details>


### [72] [Contestability in Quantitative Argumentation](https://arxiv.org/abs/2507.11323)
*Xiang Yin,Nico Potyka,Antonio Rago,Timotheus Kampik,Francesca Toni*

Main category: cs.AI

TL;DR: 本文探讨了如何利用边加权定量双极论证框架（EW-QBAFs）实现可争议AI决策，提出了一种基于梯度的方法（G-RAEs）来调整边权重以达到目标论证强度。


<details>
  <summary>Details</summary>
Motivation: 确保AI驱动的决策符合人类偏好，但目前EW-QBAFs在支持可争议性方面研究较少。

Method: 提出G-RAEs方法量化边权重对目标论证强度的影响，并开发迭代算法调整权重。

Result: 实验证明该方法在模拟推荐系统和多层感知器的合成EW-QBAFs中有效。

Conclusion: G-RAEs和迭代算法为EW-QBAFs的可争议性问题提供了实用解决方案。

Abstract: Contestable AI requires that AI-driven decisions align with human
preferences. While various forms of argumentation have been shown to support
contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks
(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs
can be deployed for this purpose. Specifically, we introduce the contestability
problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)
to achieve a desired strength for a specific argument of interest (i.e., a
topic argument). To address this problem, we propose gradient-based relation
attribution explanations (G-RAEs), which quantify the sensitivity of the topic
argument's strength to changes in individual edge weights, thus providing
interpretable guidance for weight adjustments towards contestability. Building
on G-RAEs, we develop an iterative algorithm that progressively adjusts the
edge weights to attain the desired strength. We evaluate our approach
experimentally on synthetic EW-QBAFs that simulate the structural
characteristics of personalised recommender systems and multi-layer
perceptrons, and demonstrate that it can solve the problem effectively.

</details>


### [73] [CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking](https://arxiv.org/abs/2507.11334)
*Yuehao Huang,Liang Liu,Shuangming Lei,Yukai Ma,Hao Su,Jianbiao Mei,Pengxiang Zhao,Yaqing Gu,Yong Liu,Jiajun Lv*

Main category: cs.AI

TL;DR: CogDDN是一个基于视觉语言模型（VLM）的框架，通过模拟人类认知和学习机制，提升机器人在未知环境中的导航和交互能力。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的需求导航（DDN）方法依赖预收集数据，泛化能力有限。CogDDN旨在通过模拟人类认知机制解决这一问题。

Method: CogDDN整合快速和慢速思维系统，语义对齐检测对象与指令，并采用双过程决策模块（启发式和分析式）和链式思维（CoT）推理。

Result: 在AI2Thor模拟器和ProcThor数据集上的评估显示，CogDDN比单视角相机方法性能提升15%，导航准确性和适应性显著提高。

Conclusion: CogDDN通过模拟人类认知机制，显著提升了机器人在未知环境中的导航能力，为未来研究提供了新方向。

Abstract: Mobile robots are increasingly required to navigate and interact within
unknown and unstructured environments to meet human demands. Demand-driven
navigation (DDN) enables robots to identify and locate objects based on
implicit human intent, even when object locations are unknown. However,
traditional data-driven DDN methods rely on pre-collected data for model
training and decision-making, limiting their generalization capability in
unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that
emulates the human cognitive and learning mechanisms by integrating fast and
slow thinking systems and selectively identifying key objects essential to
fulfilling user demands. CogDDN identifies appropriate target objects by
semantically aligning detected objects with the given instructions.
Furthermore, it incorporates a dual-process decision-making module, comprising
a Heuristic Process for rapid, efficient decisions and an Analytic Process that
analyzes past errors, accumulates them in a knowledge base, and continuously
improves performance. Chain of Thought (CoT) reasoning strengthens the
decision-making process. Extensive closed-loop evaluations on the AI2Thor
simulator with the ProcThor dataset show that CogDDN outperforms single-view
camera-only methods by 15%, demonstrating significant improvements in
navigation accuracy and adaptability. The project page is available at
https://yuehaohuang.github.io/CogDDN/.

</details>


### [74] [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](https://arxiv.org/abs/2507.11352)
*Yunhao Yang,Neel P. Bhatt,Christian Ellis,Alvaro Velasquez,Zhangyang Wang,Ufuk Topcu*

Main category: cs.AI

TL;DR: 提出了一种结合自然语言对话与可验证保证的神经符号框架，用于复杂物流决策，提升实时性和安全性。


<details>
  <summary>Details</summary>
Motivation: 物流决策需要快速响应和应对不确定性，现有方法（如整数规划）速度慢且假设理想环境，而大语言模型（LLMs）易产生误解和幻觉。

Method: 开发了一种神经符号框架，将用户请求转换为结构化规划规范，量化不确定性，并在置信度不足时触发交互式澄清循环。

Result: 轻量级模型在100个不确定性过滤示例上微调后，性能超过GPT-4.1，推理延迟降低近50%。

Conclusion: 该框架为复杂物流提供了一种可验证、实时且用户对齐的决策路径。

Abstract: Logistics operators, from battlefield coordinators rerouting airlifts ahead
of a storm to warehouse managers juggling late trucks, often face life-critical
decisions that demand both domain expertise and rapid and continuous
replanning. While popular methods like integer programming yield logistics
plans that satisfy user-defined logical constraints, they are slow and assume
an idealized mathematical model of the environment that does not account for
uncertainty. On the other hand, large language models (LLMs) can handle
uncertainty and promise to accelerate replanning while lowering the barrier to
entry by translating free-form utterances into executable plans, yet they
remain prone to misinterpretations and hallucinations that jeopardize safety
and cost. We introduce a neurosymbolic framework that pairs the accessibility
of natural-language dialogue with verifiable guarantees on goal interpretation.
It converts user requests into structured planning specifications, quantifies
its own uncertainty at the field and token level, and invokes an interactive
clarification loop whenever confidence falls below an adaptive threshold. A
lightweight model, fine-tuned on just 100 uncertainty-filtered examples,
surpasses the zero-shot performance of GPT-4.1 while cutting inference latency
by nearly 50%. These preliminary results highlight a practical path toward
certifiable, real-time, and user-aligned decision-making for complex logistics.

</details>


### [75] [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473)
*Tomek Korbak,Mikita Balesni,Elizabeth Barnes,Yoshua Bengio,Joe Benton,Joseph Bloom,Mark Chen,Alan Cooney,Allan Dafoe,Anca Dragan,Scott Emmons,Owain Evans,David Farhi,Ryan Greenblatt,Dan Hendrycks,Marius Hobbhahn,Evan Hubinger,Geoffrey Irving,Erik Jenner,Daniel Kokotajlo,Victoria Krakovna,Shane Legg,David Lindner,David Luan,Aleksander Mądry,Julian Michael,Neel Nanda,Dave Orr,Jakub Pachocki,Ethan Perez,Mary Phuong,Fabien Roger,Joshua Saxe,Buck Shlegeris,Martín Soto,Eric Steinberger,Jasmine Wang,Wojciech Zaremba,Bowen Baker,Rohin Shah,Vlad Mikulik*

Main category: cs.AI

TL;DR: AI系统通过人类语言思考，为AI安全提供了监控其思维链（CoT）意图的机会。尽管CoT监控不完美，但有潜力，建议进一步研究并投资。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统通过人类语言思考的特性，以监控其意图，提升AI安全性。

Method: 提出监控思维链（CoT）的方法，评估其可行性和局限性。

Result: CoT监控虽不完美，但显示出潜力，建议结合现有安全方法进一步研究。

Conclusion: 建议前沿模型开发者考虑开发决策对CoT可监控性的影响，并投资相关研究。

Abstract: AI systems that "think" in human language offer a unique opportunity for AI
safety: we can monitor their chains of thought (CoT) for the intent to
misbehave. Like all other known AI oversight methods, CoT monitoring is
imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows
promise and we recommend further research into CoT monitorability and
investment in CoT monitoring alongside existing safety methods. Because CoT
monitorability may be fragile, we recommend that frontier model developers
consider the impact of development decisions on CoT monitorability.

</details>


### [76] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: PAiR框架通过整合Perspective-Aware AI与XR，利用用户的多模态数字足迹构建身份模型，实现可解释、情境感知的沉浸式体验。


<details>
  <summary>Details</summary>
Motivation: 当前AI增强的XR系统因用户建模浅薄和认知情境有限而表现不足，PAiR旨在解决这一问题。

Method: 基于Chronicles（多模态数字足迹构建的身份模型），PAiR采用闭环系统动态链接用户状态与沉浸式环境。

Result: 通过Unity引擎中的两个概念验证场景展示了PAiR的实用性。

Conclusion: PAiR为人类-AI交互开辟了新方向，将基于视角的身份模型嵌入沉浸式系统。

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [77] [Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light](https://arxiv.org/abs/2507.11482)
*Mani Hamidi,Terrence W. Deacon*

Main category: cs.AI

TL;DR: 论文提出一个基于开放进化理论的框架，重新审视强化学习的三个核心假设，并探讨其对理论和应用的影响。


<details>
  <summary>Details</summary>
Motivation: 强化学习的三个核心假设（关于代理的定义、学习目标和奖励假设的范围）需要概念性修订，以推动理论和应用的发展。

Method: 借鉴开放进化理论，重新审视这三个假设，并结合生物学学习的背景，探讨进化动力学在个体生命周期内的作用。

Result: 通过进化视角丰富了学习的目标和奖励假设的讨论，但代理问题仍需结合生命起源理论来解决。

Conclusion: 进化理论为强化学习的核心假设提供了新的视角，但代理问题需要进一步结合生命起源的热力学理论来解决。

Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition
of agency, the objective of learning, and the scope of the reward
hypothesis--have been highlighted as key targets for conceptual revision, with
major implications for theory and application. We propose a framework, inspired
by open-ended evolutionary theory, to reconsider these three "dogmas." We
revisit each assumption and address related concerns raised alongside them. To
make our arguments relevant to RL as a model of biological learning, we first
establish that evolutionary dynamics can plausibly operate within living brains
over an individual's lifetime, and are not confined to cross-generational
processes. We begin by revisiting the second dogma, drawing on evolutionary
insights to enrich the "adaptation-rather-than-search" view of learning. We
then address the third dogma regarding the limits of the reward hypothesis,
using analogies from evolutionary fitness to illuminate the scalar reward vs.
multi-objective debate. After discussing practical implications for exploration
in RL, we turn to the first--and arguably most fundamental--issue: the absence
of a formal account of agency. We argue that unlike the other two problems, the
evolutionary paradigm alone cannot resolve the agency question, though it
gestures in a productive direction. We advocate integrating ideas from
origins-of-life theory, where the thermodynamics of sustenance and replication
offer promising foundations for understanding agency and resource-constrained
reinforcement learning in biological systems.

</details>


### [78] [DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering](https://arxiv.org/abs/2507.11527)
*Yinsheng Li,Zhen Dong,Yi Shao*

Main category: cs.AI

TL;DR: DrafterBench是一个用于评估LLM代理在土木工程图纸修订任务中的开源基准，包含12类任务、46个定制功能和1920个任务，旨在全面测试代理的能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏从工业角度（如土木工程）系统评估自动化代理的基准，DrafterBench填补了这一空白。

Method: 通过总结真实图纸文件中的任务，设计包含多种能力的测试集，并开发工具包评估代理的准确性。

Result: DrafterBench提供了任务准确性和错误统计的详细分析，揭示了代理的能力和改进方向。

Conclusion: DrafterBench为LLM在工程应用中的集成提供了深入见解，并识别了改进目标。

Abstract: Large Language Model (LLM) agents have shown great potential for solving
real-world problems and promise to be a solution for tasks automation in
industry. However, more benchmarks are needed to systematically evaluate
automation agents from an industrial perspective, for example, in Civil
Engineering. Therefore, we propose DrafterBench for the comprehensive
evaluation of LLM agents in the context of technical drawing revision, a
representation task in civil engineering. DrafterBench contains twelve types of
tasks summarized from real-world drawing files, with 46 customized
functions/tools and 1920 tasks in total. DrafterBench is an open-source
benchmark to rigorously test AI agents' proficiency in interpreting intricate
and long-context instructions, leveraging prior knowledge, and adapting to
dynamic instruction quality via implicit policy awareness. The toolkit
comprehensively assesses distinct capabilities in structured data
comprehension, function execution, instruction following, and critical
reasoning. DrafterBench offers detailed analysis of task accuracy and error
statistics, aiming to provide deeper insight into agent capabilities and
identify improvement targets for integrating LLMs in engineering applications.
Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,
with the test set hosted at
https://huggingface.co/datasets/Eason666/DrafterBench.

</details>


### [79] [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
*Daniel Jaroslawicz,Brendan Whiting,Parth Shah,Karime Maamari*

Main category: cs.AI

TL;DR: IFScale是一个评估LLM在高密度指令下性能的基准测试，结果显示前沿模型在500条指令下仅68%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅评估单一或少量指令的任务，无法反映实际生产环境中高密度指令的需求。

Method: 引入IFScale基准，包含500条关键词包含指令，评估20个前沿模型在高密度指令下的表现。

Result: 最佳模型在500条指令下准确率仅为68%，模型规模和推理能力与性能下降模式相关。

Conclusion: 研究结果有助于设计高密度指令提示，并揭示了性能与延迟的权衡，所有数据和结果已开源。

Abstract: Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions simultaneously. However, the instruction-following
capabilities of LLMs at high instruction densities have not yet been
characterized, as existing benchmarks only evaluate models on tasks with a
single or few instructions. We introduce IFScale, a simple benchmark of 500
keyword-inclusion instructions for a business report writing task to measure
how instruction-following performance degrades as instruction density
increases. We evaluate 20 state-of-the-art models across seven major providers
and find that even the best frontier models only achieve 68% accuracy at the
max density of 500 instructions. Our analysis reveals model size and reasoning
capability to correlate with 3 distinct performance degradation patterns, bias
towards earlier instructions, and distinct categories of instruction-following
errors. Our insights can help inform design of instruction-dense prompts in
real-world applications and highlight important performance-latency tradeoffs.
We open-source the benchmark and all results for further analysis at
https://distylai.github.io/IFScale.

</details>
