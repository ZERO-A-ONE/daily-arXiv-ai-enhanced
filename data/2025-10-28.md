<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 27]
- [cs.CR](#cs.CR) [Total: 37]
- [cs.AI](#cs.AI) [Total: 83]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments](https://arxiv.org/abs/2510.21902)
*Timothé Boulet,Xavier Hinaut,Clément Moulin-Frier*

Main category: cs.SE

TL;DR: 首次评估软件工程智能体在具身任务控制器生成中的表现，比较不同信息访问条件对性能的影响


<details>
  <summary>Details</summary>
Motivation: 软件工程智能体在传统软件工程任务中表现良好，但在需要信息发现的具身任务中的性能尚未探索

Method: 将Mini-SWE-Agent适配到Minigrid环境中的20个具身任务，比较有无环境源代码访问和不同交互探索能力下的性能

Result: 量化了不同信息访问水平对具身任务中软件工程智能体性能的影响，分析了静态代码分析与动态探索的相对重要性

Conclusion: 确立了具身任务控制器生成为软件工程智能体的重要评估领域，为未来高效推理系统研究提供了基准结果

Abstract: Software Engineering Agents (SWE-Agents) have proven effective for
traditional software engineering tasks with accessible codebases, but their
performance for embodied tasks requiring well-designed information discovery
remains unexplored. We present the first extended evaluation of SWE-Agents on
controller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to
solve 20 diverse embodied tasks from the Minigrid environment. Our experiments
compare agent performance across different information access conditions: with
and without environment source code access, and with varying capabilities for
interactive exploration. We quantify how different information access levels
affect SWE-Agent performance for embodied tasks and analyze the relative
importance of static code analysis versus dynamic exploration for task solving.
This work establishes controller generation for embodied tasks as a crucial
evaluation domain for SWE-Agents and provides baseline results for future
research in efficient reasoning systems.

</details>


### [2] [TOM-SWE: User Mental Modeling For Software Engineering Agents](https://arxiv.org/abs/2510.21903)
*Xuhui Zhou,Valerie Chen,Zora Zhiruo Wang,Graham Neubig,Maarten Sap,Xingyao Wang*

Main category: cs.SE

TL;DR: ToM-SWE是一个双智能体架构，将软件工程智能体与轻量级心理理论智能体配对，通过建模用户心理状态来提升代码任务中的用户意图理解和跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 当前编码智能体在规划、编辑、运行和测试复杂代码库方面能力增强，但在推断和跟踪用户意图方面仍有困难，特别是当指令不明确或依赖上下文时。

Method: 采用双智能体架构：一个主软件工程智能体负责编码任务，一个轻量级心理理论智能体专门建模用户心理状态，包括推断用户目标、约束和偏好，维护用户持久记忆，并向SWE智能体提供用户相关建议。

Result: 在两个软件工程基准测试中，ToM-SWE显著提高了任务成功率和用户满意度。在状态化SWE基准测试中，任务成功率从18.1%提升到59.7%。在三周的专业开发者实际使用研究中，86%的情况下被认为有用。

Conclusion: 状态化用户建模对实用编码智能体具有重要价值，ToM-SWE架构通过心理理论智能体有效提升了用户意图理解和任务执行能力。

Abstract: Recent advances in coding agents have made them capable of planning, editing,
running, and testing complex code bases. Despite their growing ability in
coding tasks, these systems still struggle to infer and track user intent,
especially when instructions are underspecified or context-dependent. To bridge
this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary
software-engineering (SWE) agent with a lightweight theory-of-mind (ToM)
partner agent dedicated to modeling the user's mental state. The ToM agent
infers user goals, constraints, and preferences from instructions and
interaction history, maintains a \textbf{persistent memory} of the user, and
provides user-related suggestions to the SWE agent. In two software engineering
benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task
success rates and user satisfaction. Notably, on the stateful SWE benchmark, a
newly introduced evaluation that provides agents with a user simulator along
with previous interaction histories, ToM-SWE achieves a substantially higher
task success rate of 59.7\% compared to 18.1\% for OpenHands, a
state-of-the-art SWE agent. Furthermore, in a three-week study with
professional developers using ToM-SWE in their daily work, participants found
it useful 86\% of the time, underscoring the value of stateful user modeling
for practical coding agents.

</details>


### [3] [A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case](https://arxiv.org/abs/2510.21933)
*Joao Correia,Daniel Coutinho,Marco Castelluccio,Caio Barbosa,Rafael de Mello,Anita Sarma,Alessandro Garcia,Marco Gerosa,Igor Steinmacher*

Main category: cs.SE

TL;DR: 评估RAG在Mozilla Firefox项目中辅助开发者的效果，发现RAG增强的GPT模型比人类开发者提供更全面的回答，帮助性接近人类水平，但回答不够简洁。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中的使用增加，需要评估RAG在真实开源项目中的有效性，以减轻核心维护者的负担。

Method: 与Mozilla基金会合作，对来自开发者聊天室的真实查询进行实证分析，比较人类开发者、标准GPT和RAG增强GPT的回答，由专家评估帮助性、全面性和简洁性。

Result: RAG辅助回答比人类开发者更全面（62.50% vs 54.17%），帮助性接近（75.00% vs 79.17%），但不够简洁且冗长。

Conclusion: RAG工具在OSS中有应用潜力，可减轻维护者负担而不损失回答质量，未来需要优化检索机制和提高回答简洁性。

Abstract: The use of Large Language Models (LLMs) to support tasks in software
development has steadily increased over recent years. From assisting developers
in coding activities to providing conversational agents that answer newcomers'
questions. In collaboration with the Mozilla Foundation, this study evaluates
the effectiveness of Retrieval-Augmented Generation (RAG) in assisting
developers within the Mozilla Firefox project. We conducted an empirical
analysis comparing responses from human developers, a standard GPT model, and a
GPT model enhanced with RAG, using real queries from Mozilla's developer chat
rooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses
based on helpfulness, comprehensiveness, and conciseness. The results show that
RAG-assisted responses were more comprehensive than human developers (62.50% to
54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to
enhance developer assistance. However, the RAG responses were not as concise
and often verbose. The results show the potential to apply RAG-based tools to
Open Source Software (OSS) to minimize the load to core maintainers without
losing answer quality. Toning down retrieval mechanisms and making responses
even shorter in the future would enhance developer assistance in massive
projects like Mozilla Firefox.

</details>


### [4] [ArchISMiner: A Framework for Automatic Mining of Architectural Issue-Solution Pairs from Online Developer Communities](https://arxiv.org/abs/2510.21966)
*Musengamana Jean de Dieu,Ruiyin Li,Peng Liang,Mojtaba Shahin,Muhammad Waseem,Arif Ali Khan,Bangchao Wang,Mst Shamima Aktar*

Main category: cs.SE

TL;DR: ArchISMiner框架用于从Stack Overflow等开发者社区挖掘架构知识，包含ArchPI组件识别架构相关帖子，ArchISPE组件提取架构问题-解决方案对，显著提高了架构知识发现的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: Stack Overflow等社区论坛包含丰富的软件开发知识，但架构知识由于内容量大且分散，难以有效提取。开发者需要手动筛选帖子，过程耗时且容易出错。

Method: ArchISMiner框架包含两个组件：ArchPI使用多种ML/DL模型、预训练语言模型和大语言模型自动识别架构相关帖子；ArchISPE采用间接监督方法，结合BERT嵌入和TextCNN特征提取架构问题-解决方案对。

Result: ArchPI在架构相关帖子检测中达到F1分数0.960，ArchISPE在架构问题和解决方案提取上分别达到F1分数0.883和0.894，优于SE和NLP领域的基线方法。用户研究验证了提取内容的质量。

Conclusion: ArchISMiner能够帮助架构师和开发者更准确高效地从开发者社区识别架构相关帖子和提取相关架构知识，并已在三个额外论坛上应用，发布了包含18K+架构问题-解决方案对的数据集。

Abstract: Stack Overflow (SO), a leading online community forum, is a rich source of
software development knowledge. However, locating architectural knowledge, such
as architectural solutions remains challenging due to the overwhelming volume
of unstructured content and fragmented discussions. Developers must manually
sift through posts to find relevant architectural insights, which is
time-consuming and error-prone. This study introduces ArchISMiner, a framework
for mining architectural knowledge from SO. The framework comprises two
complementary components: ArchPI and ArchISPE. ArchPI trains and evaluates
multiple models, including conventional ML/DL models, Pre-trained Language
Models (PLMs), and Large Language Models (LLMs), and selects the
best-performing model to automatically identify Architecture-Related Posts
(ARPs) among programming-related discussions. ArchISPE employs an indirect
supervised approach that leverages diverse features, including BERT embeddings
and local TextCNN features, to extract architectural issue-solution pairs. Our
evaluation shows that the best model in ArchPI achieves an F1-score of 0.960 in
ARP detection, and ArchISPE outperforms baselines in both SE and NLP fields,
achieving F1-scores of 0.883 for architectural issues and 0.894 for solutions.
A user study further validated the quality (e.g., relevance and usefulness) of
the identified ARPs and the extracted issue-solution pairs. Moreover, we
applied ArchISMiner to three additional forums, releasing a dataset of over 18K
architectural issue-solution pairs. Overall, ArchISMiner can help architects
and developers identify ARPs and extract succinct, relevant, and useful
architectural knowledge from developer communities more accurately and
efficiently. The replication package of this study has been provided at
https://github.com/JeanMusenga/ArchISPE

</details>


### [5] [FeaGPT: an End-to-End agentic-AI for Finite Element Analysis](https://arxiv.org/abs/2510.21993)
*Yupeng Qi,Ran Xu,Xu Chu*

Main category: cs.SE

TL;DR: FeaGPT是首个通过对话界面实现完整几何-网格-仿真工作流程的框架，能够将工程规范转化为验证的计算结果，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 现有工具仅自动化单个FEA组件，而FeaGPT旨在实现完全集成的几何-网格-仿真-分析(GMSA)流程，通过自然语言界面使高级计算工程工具民主化。

Method: 系统解释工程意图，自动生成物理感知的自适应网格，配置完整的FEA仿真并推断边界条件，通过闭环迭代执行多目标分析。

Result: 实验验证确认了完整的端到端自动化能力，工业涡轮增压器案例成功将自然语言规范转化为验证的CalculiX仿真，432个NACA翼型配置验证了参数化设计探索的可扩展性。

Conclusion: 自然语言界面可以有效地使高级计算工程工具民主化，同时保持分析严谨性。

Abstract: Large language models (LLMs) are establishing new paradigms for engineering
applications by enabling natural language control of complex computational
workflows. This paper introduces FeaGPT, the first framework to achieve
complete geometry-mesh-simulation workflows through conversational interfaces.
Unlike existing tools that automate individual FEA components, FeaGPT
implements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline
that transforms engineering specifications into validated computational results
without manual intervention. The system interprets engineering intent,
automatically generates physics-aware adaptive meshes, configures complete FEA
simulations with proper boundary condition inference, and performs
multi-objective analysis through closed-loop iteration.
  Experimental validation confirms complete end-to-end automation capability.
Industrial turbocharger cases (7-blade compressor and 12-blade turbine at
\SI{110000}{rpm}) demonstrate the system successfully transforms natural
language specifications into validated CalculiX simulations, producing
physically realistic results for rotating machinery analysis. Additional
validation through 432 NACA airfoil configurations confirms scalability for
parametric design exploration. These results demonstrate that natural language
interfaces can effectively democratize access to advanced computational
engineering tools while preserving analytical rigor.

</details>


### [6] [Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review](https://arxiv.org/abs/2510.22003)
*Stefan Julian Kooy,Jean Paul Sebastian Piest,Rob Henk Bemthuis*

Main category: cs.SE

TL;DR: 系统文献综述显示，生成式AI在企业架构工作中主要支持设计构思、工件快速创建和架构决策，但也存在透明度、偏见、隐私等风险，需要新的技能和组织能力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑敏捷软件组织的企业架构工作，但相关证据分散，需要系统梳理其影响、风险和机遇。

Method: 采用Kitchenham和PRISMA协议进行系统文献综述，分析了1,697条记录中的33项研究，涵盖企业、解决方案、领域、业务和IT架构师角色。

Result: 生成式AI最一致地支持：(1)设计构思和权衡探索；(2)工件快速创建和精炼；(3)架构决策支持和知识检索。风险包括不透明性、偏见、上下文错误输出、隐私合规问题和社会惰化。

Conclusion: 研究为负责任地采用生成式AI提供了指导，既能加速数字化转型，又能保护架构完整性，并提出了人机协作在架构领域的研究议程。

Abstract: Generative AI (GenAI) is reshaping enterprise architecture work in agile
software organizations, yet evidence on its effects remains scattered. We
report a systematic literature review (SLR), following established SLR
protocols of Kitchenham and PRISMA, of 1,697 records, yielding 33 studies
across enterprise, solution, domain, business, and IT architect roles. GenAI
most consistently supports (i) design ideation and trade-off exploration; (ii)
rapid creation and refinement of artifacts (e.g., code, models, documentation);
and (iii) architectural decision support and knowledge retrieval. Reported
risks include opacity and bias, contextually incorrect outputs leading to
rework, privacy and compliance concerns, and social loafing. We also identify
emerging skills and competencies, including prompt engineering, model
evaluation, and professional oversight, and organizational enablers around
readiness and adaptive governance. The review contributes with (1) a mapping of
GenAI use cases and risks in agile architecting, (2) implications for
capability building and governance, and (3) an initial research agenda on
human-AI collaboration in architecture. Overall, the findings inform
responsible adoption of GenAI that accelerates digital transformation while
safeguarding architectural integrity.

</details>


### [7] [LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation](https://arxiv.org/abs/2510.22210)
*Gwihwan Go,Quan Zhang,Chijin Zhou,Zhao Wei,Yu Jiang*

Main category: cs.SE

TL;DR: LSPRAG是一个利用语言服务器协议(LSP)为LLM提供精确符号定义和引用的框架，用于实时、语言无关的单元测试生成，显著提高了测试覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有的单元测试生成方法难以跨多种编程语言泛化，且在实时开发环境中表现不佳。虽然LLM提供了有前景的解决方案，但生成高覆盖率测试代码依赖于提供焦点方法的简洁上下文。

Method: LSPRAG框架利用现成的语言服务器协议(LSP)后端，为LLM实时提供精确的符号定义和引用。通过重用成熟的LSP服务器，实现语言感知的上下文检索，最小化每种语言的工程工作量。

Result: 在Java、Go和Python的开源项目上评估，相比基线最佳性能，LSPRAG将行覆盖率提高了：Golang最高174.55%，Java最高213.31%，Python最高31.57%。

Conclusion: LSPRAG通过利用LSP服务器提供精确的符号信息，实现了语言无关的实时单元测试生成，显著提升了测试覆盖率，同时减少了语言特定的工程成本。

Abstract: Automated unit test generation is essential for robust software development,
yet existing approaches struggle to generalize across multiple programming
languages and operate within real-time development. While Large Language Models
(LLMs) offer a promising solution, their ability to generate high coverage test
code depends on prompting a concise context of the focal method. Current
solutions, such as Retrieval-Augmented Generation, either rely on imprecise
similarity-based searches or demand the creation of costly, language-specific
static analysis pipelines. To address this gap, we present LSPRAG, a framework
for concise-context retrieval tailored for real-time, language-agnostic unit
test generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP)
back-ends to supply LLMs with precise symbol definitions and references in real
time. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware
context retrieval, requiring minimal per-language engineering effort. We
evaluated LSPRAG on open-source projects spanning Java, Go, and Python.
Compared to the best performance of baselines, LSPRAG increased line coverage
by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.

</details>


### [8] [Taming Silent Failures: A Framework for Verifiable AI Reliability](https://arxiv.org/abs/2510.22224)
*Guan-Yan Yang,Farn Wang*

Main category: cs.SE

TL;DR: FAME框架结合离线形式化合成和在线运行时监控，为不透明AI组件创建可验证的安全网，在自动驾驶感知系统中成功检测到93.5%的关键安全违规。


<details>
  <summary>Details</summary>
Motivation: AI在安全关键系统中引入新的可靠性问题：静默故障，即AI产生自信但错误的输出，可能带来危险。

Method: FAME框架将离线形式化合成的数学严谨性与在线运行时监控的警惕性相结合。

Result: 在自动驾驶感知系统中，FAME成功检测到93.5%的关键安全违规，这些违规原本是静默的。

Conclusion: FAME代表了从接受概率性能到在下一代系统中强制执行可证明安全的关键转变，为部署可信AI提供了实用且可认证的途径。

Abstract: The integration of Artificial Intelligence (AI) into safety-critical systems
introduces a new reliability paradigm: silent failures, where AI produces
confident but incorrect outputs that can be dangerous. This paper introduces
the Formal Assurance and Monitoring Environment (FAME), a novel framework that
confronts this challenge. FAME synergizes the mathematical rigor of offline
formal synthesis with the vigilance of online runtime monitoring to create a
verifiable safety net around opaque AI components. We demonstrate its efficacy
in an autonomous vehicle perception system, where FAME successfully detected
93.5% of critical safety violations that were otherwise silent. By
contextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards,
we provide reliability engineers with a practical, certifiable pathway for
deploying trustworthy AI. FAME represents a crucial shift from accepting
probabilistic performance to enforcing provable safety in next-generation
systems.

</details>


### [9] [Understanding Self-Admitted Technical Debt in Test Code: An Empirical Study](https://arxiv.org/abs/2510.22249)
*Ibuki Nakamura,Yutaro Kashiwa,Bin Lin,Hajimu Iida*

Main category: cs.SE

TL;DR: 该研究分析了测试代码中的自认技术债务(SATD)，发现测试代码中的SATD与生产代码中的特征不同，且与测试异味无直接关联。研究开发了基于CodeBERT的机器学习模型来自动分类测试代码中的SATD类型。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注生产代码中的SATD，而忽略了测试代码中的SATD，且假设两者特征相似。实际上测试代码中存在大量SATD，且许多实例无法归入生产代码的现有分类中。

Method: 对50个代码库中的17,766个SATD注释(14,987个来自生产代码，2,779个来自测试代码)进行实证研究，分析SATD在测试代码中的分布和类型，并开发机器学习模型自动分类SATD类型。

Result: 测试代码中广泛存在SATD，但与测试异味无直接关联。基于CodeBERT的模型在召回率和F1分数上优于其他机器学习模型，但在不同类型的SATD上表现有差异。

Conclusion: 测试代码中的SATD具有独特特征，需要专门的分类方法。自动分类模型有助于更好地管理测试代码中的技术债务。

Abstract: Developers often opt for easier but non-optimal implementation to meet
deadlines or create rapid prototypes, leading to additional effort known as
technical debt to improve the code later. Oftentimes, developers explicitly
document the technical debt in code comments, referred to as Self-Admitted
Technical Debt (SATD). Numerous researchers have investigated the impact of
SATD on different aspects of software quality and development processes.
However, most of these studies focus on SATD in production code, often
overlooking SATD in the test code or assuming that it shares similar
characteristics with SATD in production code. In fact, a significant amount of
SATD is also present in the test code, with many instances not fitting into
existing categories for the production code. This study aims to fill this gap
and disclose the nature of SATD in the test code by examining its distribution
and types. Moreover, the relation between its presence and test quality is also
analyzed. Our empirical study, involving 17,766 SATD comments (14,987 from
production code, 2,779 from test code) collected from 50 repositories,
demonstrates that while SATD widely exists in test code, it is not directly
associated with test smells. Our study also presents comprehensive categories
of SATD types in the test code, and machine learning models are developed to
automatically classify SATD comments based on their types for easier
management. Our results show that the CodeBERT-based model outperforms other
machine learning models in terms of recall and F1-score. However, the
performance varies on different types of SATD.

</details>


### [10] [Ten Simple Rules for AI-Assisted Coding in Science](https://arxiv.org/abs/2510.22254)
*Eric W. Bridgeford,Iain Campbell,Zijao Chen,Zhicheng Lin,Harrison Ritz,Joachim Vandekerckhove,Russell A. Poldrack*

Main category: cs.SE

TL;DR: 提出了10条AI辅助编码的实用规则，旨在平衡AI能力利用与科学严谨性，确保科学计算中的代码质量与有效性。


<details>
  <summary>Details</summary>
Motivation: AI编码工具在加速软件开发方面展现出潜力，但在科学计算中引发了关于代码质量和科学有效性的关键问题。需要建立规则来平衡AI能力利用与科学方法严谨性。

Method: 围绕四个关键主题制定10条实用规则：问题准备与理解、上下文管理与交互、测试与验证、代码质量保证与迭代改进。强调保持人类在编码决策中的主导权。

Result: 建立了系统性的AI辅助编码框架，帮助研究人员在利用AI加速软件开发的同时，确保代码满足可靠性、可重复性和科学有效性的研究标准。

Conclusion: 这些规则旨在帮助研究人员利用AI的变革潜力进行更快的软件开发，同时确保其代码符合研究诚信所要求的可靠性、可重复性和科学有效性标准。

Abstract: While AI coding tools have demonstrated potential to accelerate software
development, their use in scientific computing raises critical questions about
code quality and scientific validity. In this paper, we provide ten practical
rules for AI-assisted coding that balance leveraging capabilities of AI with
maintaining scientific and methodological rigor. We address how AI can be
leveraged strategically throughout the development cycle with four key themes:
problem preparation and understanding, managing context and interaction,
testing and validation, and code quality assurance and iterative improvement.
These principles serve to emphasize maintaining human agency in coding
decisions, establishing robust validation procedures, and preserving the domain
expertise essential for methodologically sound research. These rules are
intended to help researchers harness AI's transformative potential for faster
software development while ensuring that their code meets the standards of
reliability, reproducibility, and scientific validity that research integrity
demands.

</details>


### [11] [Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus](https://arxiv.org/abs/2510.22318)
*Tuan-Phong Ngo,Bao-Ngoc Duong,Tuan-Anh Hoang,Joshua Dwight,Ushik Shrestha Khwakhali*

Main category: cs.SE

TL;DR: 本文探索如何将大型语言模型(LLMs)与ISTQB认证框架结合用于高等教育，创建了ISTQB对齐数据集并开发了优化提示，评估了LLMs在软件测试教育中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 软件测试在软件工程教育中至关重要，但ISTQB认证框架与最新生成式AI技术的结合应用尚未得到充分探索，需要研究LLMs如何补充ISTQB框架以改进高等教育方法。

Method: 创建了涵盖十多年的ISTQB对齐数据集(28个样本考试和1145个问题)，开发了领域优化的提示模板来提升LLM精度和解释质量，并系统评估了最先进的LLMs在该数据集上的表现。

Result: 研究展示了LLMs在支持ISTQB认证准备方面的潜力，提供了将LLMs整合到软件测试教育中的可行见解和建议。

Conclusion: LLMs在支持ISTQB认证准备方面具有良好前景，为在高等教育中更广泛地应用LLMs于软件工程领域奠定了基础。

Abstract: Software testing is a critical component in the software engineering field
and is important for software engineering education. Thus, it is vital for
academia to continuously improve and update educational methods to reflect the
current state of the field. The International Software Testing Qualifications
Board (ISTQB) certification framework is globally recognized and widely adopted
in industry and academia. However, ISTQB-based learning has been rarely applied
with recent generative artificial intelligence advances. Despite the growing
capabilities of large language models (LLMs), ISTQB-based learning and
instruction with LLMs have not been thoroughly explored. This paper explores
and evaluates how LLMs can complement the ISTQB framework for higher education.
The findings present four key contributions: (i) the creation of a
comprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28
sample exams and 1,145 questions; (ii) the development of a domain-optimized
prompt that enhances LLM precision and explanation quality on ISTQB tasks;
(iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and
(iv) actionable insights and recommendations for integrating LLMs into software
testing education. These findings highlight the promise of LLMs in supporting
ISTQB certification preparation and offer a foundation for their broader use in
software engineering at higher education.

</details>


### [12] [Operationalizing Large Language Models with Design-Aware Contexts for Code Comment Generation](https://arxiv.org/abs/2510.22338)
*Aritra Mitra,Srijoni Majumdar,Anamitra Mukhopadhyay,Partha Pratim Das,Paul D Clough,Partha Pratim Chakrabarti*

Main category: cs.SE

TL;DR: 研究探索使用大型语言模型基于设计文档生成更有效的代码注释，以解决新手程序员注释质量差的问题。


<details>
  <summary>Details</summary>
Motivation: 新手程序员编写的代码注释往往质量低下且无用，增加了代码维护时间。设计文档通常被维护者用来理解代码，因此研究是否可以利用设计文档作为上下文，让LLM生成更有用的注释。

Method: 使用大型语言模型，以设计文档作为上下文来生成代码注释。

Result: 研究关注的是可行性分析，而非具体实验结果。

Conclusion: 研究表明利用设计文档作为上下文，通过LLM生成代码注释具有潜在可行性。

Abstract: Comments are very useful to the flow of code development. With the increasing
commonality of code, novice coders have been creating a significant amount of
codebases. Due to lack of commenting standards, their comments are often
useless, and increase the time taken to further maintain codes. This study
intends to find the usefulness of large language models (LLMs) in these cases
to generate potentially better comments. This study focuses on the feasibility
of design documents as a context for the LLMs to generate more useful comments,
as design documents are often used by maintainers to understand code when
comments do not suffice.

</details>


### [13] [A First Look at the Self-Admitted Technical Debt in Test Code: Taxonomy and Detection](https://arxiv.org/abs/2510.22409)
*Shahidul Islam,Md Nahidul Islam Opu,Shaowei Wang,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 首次对测试代码中的自认技术债务(SATD)进行大规模研究，分析了5万条测试代码注释，识别出615条SATD并构建了15个分类的细粒度分类法，发现现有工具和LLMs在检测测试代码SATD方面效果不佳。


<details>
  <summary>Details</summary>
Motivation: 虽然已有大量研究关注源代码中的SATD，但测试代码中的SATD及其影响尚未得到专门研究，存在重要的知识空白。

Method: 从1000个开源Java项目的160万条注释中随机抽取5万条进行手动分析，识别和分类SATD，并评估现有SATD检测工具和LLMs的性能。

Result: 识别出615条SATD注释，构建了包含15个类别的测试代码SATD分类法；现有工具中MAT表现最佳但召回率中等，开源和专有LLMs检测准确率都很低，主要由于精度不足。

Conclusion: 这是首次对测试代码SATD的大规模分析，提供了对其类型的细致理解，并揭示了当前SATD检测方法的局限性，为未来针对测试代码SATD的研究奠定了基础。

Abstract: Self-admitted technical debt (SATD) refers to comments in which developers
explicitly acknowledge code issues, workarounds, or suboptimal solutions. SATD
is known to significantly increase software maintenance effort. While extensive
research has examined SATD in source code, its presence and impact in test code
have received no focused attention, leaving a significant gap in our
understanding of how SATD manifests in testing contexts.
  This study, the first of its kind, investigates SATD in test code by manually
analyzing 50,000 comments randomly sampled from 1.6 million comments across
1,000 open-source Java projects. From this sample, after manual analysis and
filtering, we identified 615 SATD comments and classified them into 15 distinct
categories, building a taxonomy of test code SATD. To investigate whether test
code SATD can be detected automatically, we evaluated existing SATD detection
tools, as well as both open-source and proprietary LLMs. Among the existing
tools, MAT performed the best, albeit with moderate recall. To our surprise,
both open-source and proprietary LLMs exhibited poor detection accuracy,
primarily due to low precision. These results indicate that neither existing
approaches nor current LLMs can reliably detect SATD in test code.
  Overall, this work provides the first large-scale analysis of SATD in test
code, a nuanced understanding of its types, and the limitations of current SATD
detection methods. Our findings lay the groundwork for future research on test
code-specific SATD.

</details>


### [14] [A Multifaceted View on Discrimination in Software Development Careers](https://arxiv.org/abs/2510.22457)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: 研究发现软件工程领域的歧视问题不仅限于性别和种族，还包括年龄、政治观点、残疾和神经多样性等多种形式，其中年龄和性别歧视最为常见，但政治和宗教观点歧视也值得关注。


<details>
  <summary>Details</summary>
Motivation: 揭示软件工程领域除性别和种族外其他形式歧视的普遍性，这些歧视形式往往被忽视但同样严重。

Method: 对8,717名参与者的开发者国家状况调查进行二次分析，特别关注800份开放式调查回复，分析感知歧视模式及其相关挑战和负面影响。

Result: 年龄和性别相关歧视是最常见的工作场所问题；女性参与者主要将性别视为歧视来源，常与种族、政治观点、年龄或性取向等交叉因素相关；所有性别身份都报告了与照顾责任相关的歧视；女性和非二元性别受访者报告了更高比例的歧视(35%)和心理健康挑战(62%)。

Conclusion: 软件工程领域的歧视具有多面性，研究人员在设计研究时应选择并评估除年龄和性别外的其他相关因素。

Abstract: Conversations around diversity and inclusion in software engineering often
focus on gender and racial disparities. However, the State of the Developer
Nation 2025 survey with 8,717 participants revealed that other forms of
discrimination are similarly prevalent but receive considerably less attention.
This includes discrimination based on age, political perspective, disabilities,
or cognitive differences such as neurodivergence. We conducted a secondary
analysis of 800 open-ended survey responses to examine patterns of perceived
discrimination, as well as related challenges and negative impacts. Our study
covers multiple identity facets, including age, gender, race, and disability.
We found that age- and gender-related discrimination was the most frequently
reported workplace issue, but discrimination based on political and religious
views emerged as further notable concerns. Most of the participants who
identified as female cited gender as the primary source of discrimination,
often accompanied by intersectional factors such as race, political views, age,
or sexual orientation. Discrimination related to caregiving responsibilities
was reported by all gender identities. Regarding the negative impacts of
workplace issues, many participants described modifying their appearance or
behavior in response to gender biases. Gender also appeared to influence
broader career challenges, as women and non-binary respondents reported
experiencing almost all workplace issues at higher rates, particularly
discrimination (35%) and mental health challenges (62%). Our goal is to raise
awareness in the research community that discrimination in software development
is multifaceted, and to encourage researchers to select and assess relevant
facets beyond age and gender when designing software engineering studies.

</details>


### [15] [Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL](https://arxiv.org/abs/2510.22530)
*Sungmin Kang,Sumi Yun,Jingun Hong,Shin Yoo,Gabin An*

Main category: cs.SE

TL;DR: AutoCrashFL是一种基于LLM代理的故障定位方法，仅需崩溃转储和源代码仓库，在工业级软件SAP HANA上比基线方法更有效。


<details>
  <summary>Details</summary>
Motivation: 传统故障定位方法需要动态分析（如覆盖率分析），在大型工业软件中成本过高，难以应用。

Method: 使用LLM代理，仅依赖崩溃转储和源代码仓库进行故障定位，无需昂贵的动态分析。

Result: 在SAP HANA真实崩溃中，AutoCrashFL在top位置识别了30%的崩溃，而基线仅为17%。对复杂bug更有效，并能指示结果置信度。

Conclusion: LLM代理在工业规模部署具有实用性，为大型软件故障定位提供了可行方案。

Abstract: Fault Localization (FL) aims to identify root causes of program failures. FL
typically targets failures observed from test executions, and as such, often
involves dynamic analyses to improve accuracy, such as coverage profiling or
mutation testing. However, for large industrial software, measuring coverage
for every execution is prohibitively expensive, making the use of such
techniques difficult. To address these issues and apply FL in an industrial
setting, this paper proposes AutoCrashFL, an LLM agent for the localization of
crashes that only requires the crashdump from the Program Under Test (PUT) and
access to the repository of the corresponding source code. We evaluate
AutoCrashFL against real-world crashes of SAP HANA, an industrial software
project consisting of more than 35 million lines of code. Experiments reveal
that AutoCrashFL is more effective in localization, as it identified 30%
crashes at the top, compared to 17% achieved by the baseline. Through thorough
analysis, we find that AutoCrashFL has attractive practical properties: it is
relatively more effective for complex bugs, and it can indicate confidence in
its results. Overall, these results show the practicality of LLM agent
deployment on an industrial scale.

</details>


### [16] [DynaCausal: Dynamic Causality-Aware Root Cause Analysis for Distributed Microservices](https://arxiv.org/abs/2510.22613)
*Songhan Zhang,Aoyang Fang,Yifan Yang,Ruiyi Cheng,Xiaoying Tang,Pinjia He*

Main category: cs.SE

TL;DR: DynaCausal是一个动态因果关系感知框架，用于解决云原生微服务系统中的根因分析问题，通过多模态动态信号捕获时空依赖关系，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有根因分析方法在捕捉动态行为和变化服务关系方面存在局限，存在三个关键挑战：级联故障传播建模不足、对噪声干扰和概念漂移的脆弱性、过度依赖服务偏差强度而掩盖真实根因。

Method: DynaCausal通过交互感知表示学习统一多模态动态信号来捕获时变时空依赖关系，引入动态对比机制分离真实故障指标与上下文噪声，采用因果优先成对排序目标显式优化因果归因。

Result: 在公共基准测试上的综合评估显示，DynaCausal始终优于最先进方法，平均AC@1达到0.63，绝对增益从0.25到0.46，在高度动态的微服务环境中提供准确且可解释的诊断。

Conclusion: DynaCausal有效解决了云原生微服务系统中根因分析的三个关键挑战，通过动态因果关系建模实现了更准确和可解释的故障诊断。

Abstract: Cloud-native microservices enable rapid iteration and scalable deployment but
also create complex, fast-evolving dependencies that challenge reliable
diagnosis. Existing root cause analysis (RCA) approaches, even with multi-modal
fusion of logs, traces, and metrics, remain limited in capturing dynamic
behaviors and shifting service relationships. Three critical challenges
persist: (i) inadequate modeling of cascading fault propagation, (ii)
vulnerability to noise interference and concept drift in normal service
behavior, and (iii) over-reliance on service deviation intensity that obscures
true root causes. To address these challenges, we propose DynaCausal, a dynamic
causality-aware framework for RCA in distributed microservice systems.
DynaCausal unifies multi-modal dynamic signals to capture time-varying
spatio-temporal dependencies through interaction-aware representation learning.
It further introduces a dynamic contrastive mechanism to disentangle true fault
indicators from contextual noise and adopts a causal-prioritized pairwise
ranking objective to explicitly optimize causal attribution. Comprehensive
evaluations on public benchmarks demonstrate that DynaCausal consistently
surpasses state-of-the-art methods, attaining an average AC@1 of 0.63 with
absolute gains from 0.25 to 0.46, and delivering both accurate and
interpretable diagnoses in highly dynamic microservice environments.

</details>


### [17] [Does In-IDE Calibration of Large Language Models work at Scale?](https://arxiv.org/abs/2510.22614)
*Roham Koohestani,Agnia Sergeyuk,David Gros,Claudio Spiess,Sergey Titov,Prem Devanbu,Maliheh Izadi*

Main category: cs.SE

TL;DR: 研究发现在IDE环境中应用后验校准并不能显著提高代码生成模型的可靠性，但个性化校准在用户交互数据充足时有效。开发者偏好使用非数值的颜色编码指示器来呈现可靠性信号。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型集成到IDE中正在改变软件工程，但AI生成代码的实用性和可靠性面临挑战。后验校准旨在对齐模型置信度与可接受性度量，但大规模证据有限。

Method: 开发可扩展的校准框架，分析2400万次真实开发者交互数据，评估Platt缩放校准效果。同时进行多阶段设计研究，包括场景设计、半结构化访谈和调查验证。

Result: 通用的后验校准模型平均不能改善模型置信度信号的可靠性。个性化校准有效但高度依赖用户交互数据量。开发者明确偏好使用非数值的颜色编码指示器。

Conclusion: 在IDE环境中，简单的后验校准方法效果有限，需要更复杂的个性化方法。用户界面设计应优先使用直观的颜色编码而非数值显示来传达可靠性信息。

Abstract: The introduction of large language models into integrated development
environments (IDEs) is revolutionizing software engineering, yet it poses
challenges to the usefulness and reliability of Artificial
Intelligence-generated code. Post-hoc calibration of internal model confidences
aims to align probabilities with an acceptability measure. Prior work suggests
calibration can improve alignment, but at-scale evidence is limited. In this
work, we investigate the feasibility of applying calibration of code models to
an in-IDE context. We study two aspects of the problem: (1) the technical
method for implementing confidence calibration and improving the reliability of
code generation models, and (2) the human-centered design principles for
effectively communicating reliability signal to developers. First, we develop a
scalable and flexible calibration framework which can be used to obtain
calibration weights for open-source models using any dataset, and evaluate
whether calibrators improve the alignment between model confidence and
developer acceptance behavior. Through a large-scale analysis of over 24
million real-world developer interactions across multiple programming
languages, we find that a general, post-hoc calibration model based on
Platt-scaling does not, on average, improve the reliability of model confidence
signals. We also find that while dynamically personalizing calibration to
individual users can be effective, its effectiveness is highly dependent on the
volume of user interaction data. Second, we conduct a multi-phase design study
with 3 expert designers and 153 professional developers, combining
scenario-based design, semi-structured interviews, and survey validation,
revealing a clear preference for presenting reliability signals via
non-numerical, color-coded indicators within the in-editor code generation
workflow.

</details>


### [18] [Collaborative LLM Agents for C4 Software Architecture Design Automation](https://arxiv.org/abs/2510.22787)
*Kamil Szczepanik,Jarosław A. Chudziak*

Main category: cs.SE

TL;DR: 提出基于LLM的多智能体系统，自动化生成C4软件架构模型，通过角色专家对话分析需求并生成Context、Container和Component视图，使用混合评估框架验证质量。


<details>
  <summary>Details</summary>
Motivation: 软件架构设计是创建软件系统的基础部分，但生成C4模型仍然是手动且耗时的过程，需要自动化解决方案。

Method: 使用基于LLM的多智能体系统，模拟角色特定专家之间的对话来分析需求并生成C4模型的三个视图，采用混合评估框架进行质量评估。

Result: 在五个规范系统简介上测试，工作流程展示了快速C4模型创建、高编译成功率和高语义保真度，比较四个最先进LLM显示出在架构设计方面的不同优势。

Conclusion: 本研究为自动化软件架构设计及其评估方法做出了贡献，证明了LLM多智能体系统在生成C4模型方面的有效性。

Abstract: Software architecture design is a fundamental part of creating every software
system. Despite its importance, producing a C4 software architecture model, the
preferred notation for such architecture, remains manual and time-consuming. We
introduce an LLM-based multi-agent system that automates this task by
simulating a dialogue between role-specific experts who analyze requirements
and generate the Context, Container, and Component views of the C4 model.
Quality is assessed with a hybrid evaluation framework: deterministic checks
for structural and syntactic integrity and C4 rule consistency, plus semantic
and qualitative scoring via an LLM-as-a-Judge approach. Tested on five
canonical system briefs, the workflow demonstrates fast C4 model creation,
sustains high compilation success, and delivers semantic fidelity. A comparison
of four state-of-the-art LLMs shows different strengths relevant to
architectural design. This study contributes to automated software architecture
design and its evaluation methods.

</details>


### [19] [On the Freshness of Pinned Dependencies in Maven](https://arxiv.org/abs/2510.22815)
*Vasudev Vikram,Yuvraj Agarwal,Rohan Padhye*

Main category: cs.SE

TL;DR: 该论文研究了Maven库依赖版本固定的问题，发现60%以上的项目使用过时的依赖版本，存在安全风险。作者提出了Pin-Freshener方法，利用同行项目的众包测试来提供升级安全性信号，帮助开发者安全更新依赖。


<details>
  <summary>Details</summary>
Motivation: 软件依赖版本固定虽然能确保构建可重现，但会导致使用包含安全漏洞的过时依赖。需要了解依赖固定的频率和后果，并找到安全升级的方法。

Method: 定义了陈旧和新鲜固定的概念，对Maven库进行实证研究，开发了Pin-Freshener原型，利用同行项目的众包测试来评估依赖升级的安全性。

Result: 60%以上流行Maven库的消费者使用陈旧固定依赖，10%的依赖升级到最新次要或补丁版本可减少安全漏洞。Pin-Freshener仅需1-5个额外测试套件就能提供35-100%的额外覆盖率。

Conclusion: Pin-Freshener通过众包测试为开发者提供超出自身测试套件的额外安全性信号，能够帮助3000多个消费者安全执行减少安全漏洞的升级，是对当前实践的改进。

Abstract: Library dependencies in software ecosystems play a crucial role in the
development of software. As newer releases of these libraries are published,
developers may opt to pin their dependencies to a particular version. While
pinning may have benefits in ensuring reproducible builds and avoiding breaking
changes, it bears larger risks in using outdated dependencies that may contain
bugs and security vulnerabilities. To understand the frequency and consequences
of dependency pinning, we first define the concepts of stale and fresh pins,
which are distinguished based on how outdated the dependency is relative to the
release date of the project. We conduct an empirical study to show that over
60% of consumers of popular Maven libraries contain stale pins to their
dependencies, with some outdated versions over a year old. These pinned
versions often miss out on security fixes; we find that 10% of all dependency
upgrades in our dataset to the latest minor or patch version would reduce
security vulnerabilities.
  We prototype an approach called Pin-Freshener that can encourage developers
to freshen their pins by leveraging the insight that crowdsourced tests of peer
projects can provide additional signal for the safety of an upgrade. Running
Pin-Freshener on dependency upgrades shows that just 1-5 additional test suites
can provide 35-100% more coverage of a dependency, compared to that of a single
consumer test suite. Our evaluation on real-world pins to the top 500 popular
libraries in Maven shows that Pin-Freshener can provide an additional signal of
at least 5 passing crowdsourced test suites to over 3,000 consumers to safely
perform an upgrade that reduces security vulnerabilities. Pin-Freshener can
provide practical confidence to developers by offering additional signal beyond
their own test suites, representing an improvement over current practices.

</details>


### [20] [CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs](https://arxiv.org/abs/2510.22986)
*Junjie Huang,Minghua He,Jinyang Liu,Yintong Huo,Domenico Bianculli,Michael R. Lyu*

Main category: cs.SE

TL;DR: CodeAD是一个使用LLM自动合成轻量级Python规则函数进行日志异常检测的新框架，通过分层聚类和锚定采样构建对比日志窗口，采用代理工作流迭代生成、测试和优化规则，在三个公开数据集上比现有方法F1分数平均提升3.6%，处理速度提升4倍，成本低于4美元。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习、深度学习和LLM的日志异常检测方法存在可解释性差、推理成本高、预处理复杂等问题，而基于规则的系统虽然高效透明但需要大量人工工作且难以扩展。

Method: 引入分层聚类和锚定采样策略构建代表性对比日志窗口，使LLM能够识别区分性异常模式；采用代理工作流迭代生成、测试、修复和优化规则，直到满足正确性和抽象性要求。

Result: 在三个公开数据集(BGL、Hadoop、Thunderbird)上的实验表明，CodeAD相比最先进基线方法F1分数平均绝对提升3.6%，处理大型数据集速度提升4倍，每个数据集的LLM调用总成本低于4美元。

Conclusion: CodeAD为在线监控系统提供了一个实用且可扩展的解决方案，能够在真实环境中实现可解释、高效和自动化的日志异常检测。

Abstract: Log-based anomaly detection (LogAD) is critical for maintaining the
reliability and availability of large-scale online service systems. While
machine learning, deep learning, and large language models (LLMs)-based methods
have advanced the LogAD, they often suffer from limited interpretability, high
inference costs, and extensive preprocessing requirements, limiting their
practicality for real-time, high-volume log analysis. In contrast, rule-based
systems offer efficiency and transparency, but require significant manual
effort and are difficult to scale across diverse and evolving environments. In
this paper, We present CodeAD, a novel framework that automatically synthesizes
lightweight Python rule functions for LogAD using LLMs. CodeAD introduces a
hierarchical clustering and anchor-grounded sampling strategy to construct
representative contrastive log windows, enabling LLMs to discern discriminative
anomaly patterns. To ensure robustness and generalizability, CodeAD employs an
agentic workflow that iteratively generates, tests, repairs, and refines the
rules until it meets correctness and abstraction requirements. The synthesized
rules are interpretable, lightweight, and directly executable on raw logs,
supporting efficient and transparent online anomaly detection. Our
comprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird)
demonstrate that CodeAD achieves an average absolute improvement of 3.6% F1
score over the state-of-the-art baselines, while processing large datasets up
to 4x faster and at a fraction of the cost (total LLM invocation cost under 4
USD per dataset). These results highlight CodeAD as a practical and scalable
solution for online monitoring systems, enabling interpretable, efficient, and
automated LogAD in real-world environment.

</details>


### [21] [TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation](https://arxiv.org/abs/2510.23010)
*Ming-Tung Shen,Yuh-Jzer Joung*

Main category: cs.SE

TL;DR: TALM是一个树状结构的多智能体框架，通过动态任务分解、局部重推理和长期记忆机制，提升代码生成的推理灵活性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架存在工作流程僵化和推理恢复成本高的问题，需要更灵活高效的解决方案来处理复杂的代码生成任务。

Method: 采用可扩展的树状协作结构，结合分治策略和长期记忆模块，支持语义查询和先验知识集成，实现隐式自我改进。

Result: 在HumanEval、BigCodeBench和ClassEval基准测试中，TALM展现出强大的推理性能和高效的令牌使用效率。

Conclusion: TALM框架在复杂代码生成任务中具有鲁棒性和实用价值，能够有效提升智能体代码生成的能力。

Abstract: Agentic code generation requires large language models (LLMs) capable of
complex context management and multi-step reasoning. Prior multi-agent
frameworks attempt to address these challenges through collaboration, yet they
often suffer from rigid workflows and high reasoning recovery costs. To
overcome these limitations, we propose TALM (Tree-Structured Multi-Agent
Framework with Long-Term Memory), a dynamic framework that integrates
structured task decomposition, localized re-reasoning, and long-term memory
mechanisms. TALM employs an extensible tree-based collaboration structure. The
parent-child relationships, when combined with a divide-and-conquer strategy,
enhance reasoning flexibility and enable efficient error correction across
diverse task scopes. Furthermore, a long-term memory module enables semantic
querying and integration of prior knowledge, supporting implicit
self-improvement through experience reuse. Experimental results on HumanEval,
BigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently
delivers strong reasoning performance and high token efficiency, highlighting
its robustness and practical utility in complex code generation tasks.

</details>


### [22] [From Online User Feedback to Requirements: Evaluating Large Language Models for Classification and Specification Tasks](https://arxiv.org/abs/2510.23055)
*Manjeshwar Aniruddh Mallya,Alessio Ferrari,Mohammad Amin Zadenoori,Jacek Dąbrowski*

Main category: cs.SE

TL;DR: 评估5个轻量级开源LLM在3个RE任务上的表现：用户请求分类、非功能性需求分类和需求规范生成，发现LLM在分类任务上达到中等至高准确率，在规范生成上质量中等。


<details>
  <summary>Details</summary>
Motivation: 在线用户反馈为需求工程提供有价值信息，但分析面临量大和噪音的挑战。LLM有潜力自动化此过程并超越现有技术，但目前LLM在RE中的应用研究不足。

Method: 在两个反馈数据集上评估5个轻量级开源LLM的性能，分类任务使用标准指标，规范生成通过人工评估质量。

Result: LLM在分类任务上达到中等至高准确率（F1分数0.47-0.68），规范生成质量中等（平均3/5分）。

Conclusion: 轻量级LLM在需求工程任务中展现出潜力，但仍有局限性，需要进一步研究其能力和限制。

Abstract: [Context and Motivation] Online user feedback provides valuable information
to support requirements engineering (RE). However, analyzing online user
feedback is challenging due to its large volume and noise. Large language
models (LLMs) show strong potential to automate this process and outperform
previous techniques. They can also enable new tasks, such as generating
requirements specifications.
  [Question-Problem] Despite their potential, the use of LLMs to analyze user
feedback for RE remains underexplored. Existing studies offer limited empirical
evidence, lack thorough evaluation, and rarely provide replication packages,
undermining validity and reproducibility.
  [Principal Idea-Results] We evaluate five lightweight open-source LLMs on
three RE tasks: user request classification, NFR classification, and
requirements specification generation. Classification performance was measured
on two feedback datasets, and specification quality via human evaluation. LLMs
achieved moderate-to-high classification accuracy (F1 ~ 0.47-0.68) and
moderately high specification quality (mean ~ 3/5).
  [Contributions] We newly explore lightweight LLMs for feedback-driven
requirements development. Our contributions are: (i) an empirical evaluation of
lightweight LLMs on three RE tasks, (ii) a replication package, and (iii)
insights into their capabilities and limitations for RE.

</details>


### [23] [Checkstyle+: Reducing Technical Debt Through The Use of Linters with LLMs](https://arxiv.org/abs/2510.23068)
*Ella Dodor,Cristina V. Lopes*

Main category: cs.SE

TL;DR: Checkstyle+是一个混合方法，通过将大型语言模型能力集成到Checkstyle中，来检测传统基于规则的分析难以发现的代码风格违规。


<details>
  <summary>Details</summary>
Motivation: 传统linter工具如Checkstyle使用基于规则的机制，能有效检测表面违规，但无法处理需要更深层次代码理解的风格规则。

Method: 提出Checkstyle+，将大型语言模型能力与Checkstyle结合，形成混合方法检测语义复杂的风格违规。

Result: 在380个Java代码文件上的评估显示，Checkstyle+在检测语义复杂规则违规方面优于标准Checkstyle。

Conclusion: Checkstyle+通过结合LLM能力，能够有效识别传统静态分析工具无法检测的代码风格问题，提升代码质量分析能力。

Abstract: Good code style improves program readability, maintainability, and
collaboration, and is an integral component of software quality. Developers,
however, often cut corners when following style rules, leading to the wide
adoption of tools such as linters in professional software development
projects. Traditional linters like Checkstyle operate using rigid, rule-based
mechanisms that effectively detect many surface-level violations. However, in
most programming languages, there is a subset of style rules that require a
more nuanced understanding of code, and fall outside the scope of such static
analysis. In this paper, we propose Checkstyle+, a hybrid approach that
augments Checkstyle with large language model (LLM) capabilities, to identify
style violations that elude the conventional rule-based analysis. Checkstyle+
is evaluated on a sample of 380 Java code files, drawn from a broader dataset
of 30,800 real-world Java programs sourced from accepted Codeforces
submissions. The results show that Checkstyle+ achieves superior performance
over standard Checkstyle in detecting violations of the semantically nuanced
rules.

</details>


### [24] [Validating Formal Specifications with LLM-generated Test Cases](https://arxiv.org/abs/2510.23350)
*Alcino Cunha,Nuno Macedo*

Main category: cs.SE

TL;DR: 评估使用预训练大语言模型（LLMs）从自然语言需求自动生成测试用例的有效性，特别关注为Alloy规范语言中的领域模型结构需求生成测试用例。


<details>
  <summary>Details</summary>
Motivation: 验证形式化规范是开发过程中的核心活动，但手动指定测试用例繁琐且容易出错，导致用户可能跳过这一验证任务。

Method: 使用GPT-5等大语言模型从自然语言需求自动生成测试用例，重点关注Alloy规范语言中简单领域模型的结构需求。

Result: GPT-5在生成语法正确、满足（或不满足）给定需求的正负测试用例方面相当有效，能够检测出许多人工编写的错误规范。

Conclusion: 大语言模型在自动化测试用例生成方面具有潜力，特别是在形式化规范验证的背景下。

Abstract: Validation is a central activity when developing formal specifications.
Similarly to coding, a possible validation technique is to define upfront test
cases or scenarios that a future specification should satisfy or not.
Unfortunately, specifying such test cases is burdensome and error prone, which
could cause users to skip this validation task. This paper reports the results
of an empirical evaluation of using pre-trained large language models (LLMs) to
automate the generation of test cases from natural language requirements. In
particular, we focus on test cases for structural requirements of simple domain
models formalized in the Alloy specification language. Our evaluation focuses
on the state-of-art GPT-5 model, but results from other closed- and open-source
LLMs are also reported. The results show that, in this context, GPT-5 is
already quite effective at generating positive (and negative) test cases that
are syntactically correct and that satisfy (or not) the given requirement, and
that can detect many wrong specifications written by humans.

</details>


### [25] [Floating-Point Neural Network Verification at the Software Level](https://arxiv.org/abs/2510.23389)
*Edoardo Manino,Bruno Farias,Rafael Sá Menezes,Fedor Shmarov,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: 该论文提出了NeuroCodeBench 2.0基准测试，包含912个神经网络验证示例，用于评估软件验证工具在神经网络代码上的性能，发现现有工具平均只能正确解决11%的基准测试。


<details>
  <summary>Details</summary>
Motivation: 在安全关键系统中部署神经网络组件前必须证明其正确性，但现有神经网络验证技术无法在软件层面认证无故障。

Method: 通过显式推理神经网络的浮点实现来指定和验证其安全性，构建了包含激活函数、常见层和完整神经网络的验证基准测试，使用纯C语言编写并与SV-COMP格式兼容。

Result: 对8个最先进的软件验证器进行首次严格评估，结果显示现有自动化验证工具平均只能正确解决11%的基准测试，同时产生约3%的错误判断。

Conclusion: 该基准测试的发布已对软件验证工具产生显著积极影响，表明需要改进神经网络代码的验证能力。

Abstract: The behaviour of neural network components must be proven correct before
deployment in safety-critical systems. Unfortunately, existing neural network
verification techniques cannot certify the absence of faults at the software
level. In this paper, we show how to specify and verify that neural networks
are safe, by explicitly reasoning about their floating-point implementation. In
doing so, we construct NeuroCodeBench 2.0, a benchmark comprising 912 neural
network verification examples that cover activation functions, common layers,
and full neural networks of up to 170K parameters. Our verification suite is
written in plain C and is compatible with the format of the International
Competition on Software Verification (SV-COMP). Thanks to it, we can conduct
the first rigorous evaluation of eight state-of-the-art software verifiers on
neural network code. The results show that existing automated verification
tools can correctly solve an average of 11% of our benchmark, while producing
around 3% incorrect verdicts. At the same time, a historical analysis reveals
that the release of our benchmark has already had a significantly positive
impact on the latter.

</details>


### [26] [Tracing Distribution Shifts with Causal System Maps](https://arxiv.org/abs/2510.23528)
*Joran Leest,Ilias Gerostathopoulos,Patricia Lago,Claudia Raibulet*

Main category: cs.SE

TL;DR: 提出ML系统地图——通过分层视图显示环境与ML系统内部之间传播路径的因果图，实现分布偏移的系统性归因


<details>
  <summary>Details</summary>
Motivation: 当前机器学习系统监控困难，标准实践主要关注检测分布偏移而非其原因，根本原因分析依赖人工追踪来确定偏移是由软件故障、数据质量问题还是自然变化引起

Method: ML系统地图方法，通过分层视图使环境与ML系统内部之间的传播路径显式化

Result: 该方法能够系统性地归因分布偏移，为ML系统监控提供更有效的方法

Conclusion: 提出了ML系统地图方法并制定了其开发和评估的研究议程

Abstract: Monitoring machine learning (ML) systems is hard, with standard practice
focusing on detecting distribution shifts rather than their causes. Root-cause
analysis often relies on manual tracing to determine whether a shift is caused
by software faults, data-quality issues, or natural change. We propose ML
System Maps -- causal maps that, through layered views, make explicit the
propagation paths between the environment and the ML system's internals,
enabling systematic attribution of distribution shifts. We outline the approach
and a research agenda for its development and evaluation.

</details>


### [27] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: 提出了一种通过让SWE代理在代码库中引入功能时无意中破坏测试来生成高质量、多样化bug的新方法，相比传统方法更接近真实开发过程，能提供更有效的训练数据。


<details>
  <summary>Details</summary>
Motivation: 高质量bug对于训练下一代基于语言模型的软件工程代理至关重要，但现有方法通过故意生成bug（如对现有代码进行局部扰动）会产生分布外效应，不能反映真实的开发过程。

Method: 指导SWE代理在代码库中引入功能，在此过程中可能无意中破坏测试，从而产生bug。这种方法更接近人类编辑的模式。

Result: 该方法生成的bug在监督微调中提供更高效的训练数据，仅用1200个bug就比其他数据集3000个bug的效果好2%。训练出的FrogBoss（32B参数）在SWE-bench Verified上达到54.6%的pass@1，FrogMini（14B参数）达到45.3%的pass@1。

Conclusion: 通过让代理在引入功能时无意产生bug的方法，能够生成更真实、多样化的bug，为训练软件工程代理提供更有效的训练数据，在SWE-bench基准测试中取得了最先进的结果。

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [28] [$δ$-STEAL: LLM Stealing Attack with Local Differential Privacy](https://arxiv.org/abs/2510.21946)
*Kieu Dang,Phung Lai,NhatHai Phan,Yelong Shen,Ruoming Jin,Abdallah Khreishah*

Main category: cs.CR

TL;DR: 提出了一种名为δ-STEAL的新型模型窃取攻击，通过在对手模型的token嵌入中注入满足局部差分隐私(LDP)保证的噪声，来绕过LLM服务提供商的水印检测器，同时保持对手模型的实用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)部署存在知识产权风险，特别是模型窃取攻击会威胁专有LLM的收入和财务稳定性。水印技术虽然能提供模型可追溯性和知识产权验证，但现有攻击方法仍存在局限性。

Method: δ-STEAL攻击方法：1) 对手查询服务提供商的模型收集输出，形成输入-输出训练对；2) 在微调过程中向对手模型的token嵌入注入满足LDP保证的噪声；3) 通过LDP保护噪声混淆水印信号，使服务提供商难以确定其输出是否被使用。

Result: 实验显示δ-STEAL在轻量级修改下攻击成功率高达96.95%，且不会显著损害对手模型的实用性。LDP中的噪声尺度控制攻击效果与模型实用性之间的权衡。

Conclusion: δ-STEAL攻击对现有知识产权保护方法构成重大风险，即使鲁棒的水印也能被绕过，使对手能够欺骗水印检测器。

Abstract: Large language models (LLMs) demonstrate remarkable capabilities across
various tasks. However, their deployment introduces significant risks related
to intellectual property. In this context, we focus on model stealing attacks,
where adversaries replicate the behaviors of these models to steal services.
These attacks are highly relevant to proprietary LLMs and pose serious threats
to revenue and financial stability. To mitigate these risks, the watermarking
solution embeds imperceptible patterns in LLM outputs, enabling model
traceability and intellectual property verification. In this paper, we study
the vulnerability of LLM service providers by introducing $\delta$-STEAL, a
novel model stealing attack that bypasses the service provider's watermark
detectors while preserving the adversary's model utility. $\delta$-STEAL
injects noise into the token embeddings of the adversary's model during
fine-tuning in a way that satisfies local differential privacy (LDP)
guarantees. The adversary queries the service provider's model to collect
outputs and form input-output training pairs. By applying LDP-preserving noise
to these pairs, $\delta$-STEAL obfuscates watermark signals, making it
difficult for the service provider to determine whether its outputs were used,
thereby preventing claims of model theft. Our experiments show that
$\delta$-STEAL with lightweight modifications achieves attack success rates of
up to $96.95\%$ without significantly compromising the adversary's model
utility. The noise scale in LDP controls the trade-off between attack
effectiveness and model utility. This poses a significant risk, as even robust
watermarks can be bypassed, allowing adversaries to deceive watermark detectors
and undermine current intellectual property protection methods.

</details>


### [29] [Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning](https://arxiv.org/abs/2510.21957)
*Zhixin Pan,Ziyu Shu,Amberbir Alemayoh*

Main category: cs.CR

TL;DR: 提出结合自监督对比学习和神经架构搜索的勒索软件检测框架，使用硬件性能计数器分析运行时行为，实现早期检测和自适应模型架构。


<details>
  <summary>Details</summary>
Motivation: 传统勒索软件检测方法面临特征依赖、响应延迟和适应性不足三大挑战，需要更智能的检测方案。

Method: 集成自监督对比学习与神经架构搜索，使用硬件性能计数器分析行为，设计定制损失函数促进早期检测，自动构建自适应模型架构。

Result: 检测准确率提升16.1%，响应时间缩短6倍，在规避攻击下保持鲁棒性。

Conclusion: 该框架有效解决了勒索软件检测的关键挑战，在准确性和响应速度上显著优于现有方法。

Abstract: Ransomware has become a critical threat to cybersecurity due to its rapid
evolution, the necessity for early detection, and growing diversity, posing
significant challenges to traditional detection methods. While AI-based
approaches had been proposed by prior works to assist ransomware detection,
existing methods suffer from three major limitations, ad-hoc feature
dependencies, delayed response, and limited adaptability to unseen variants. In
this paper, we propose a framework that integrates self-supervised contrastive
learning with neural architecture search (NAS) to address these challenges.
Specifically, this paper offers three important contributions. (1) We design a
contrastive learning framework that incorporates hardware performance counters
(HPC) to analyze the runtime behavior of target ransomware. (2) We introduce a
customized loss function that encourages early-stage detection of malicious
activity, and significantly reduces the detection latency. (3) We deploy a
neural architecture search (NAS) framework to automatically construct adaptive
model architectures, allowing the detector to flexibly align with unseen
ransomware variants. Experimental results show that our proposed method
achieves significant improvements in both detection accuracy (up to 16.1%) and
response time (up to 6x) compared to existing approaches while maintaining
robustness under evasive attacks.

</details>


### [30] [Security Analysis of LTE Connectivity in Connected Cars: A Case Study of Tesla](https://arxiv.org/abs/2510.22024)
*Evangelos Bitsikas,Jason Veara,Aanjhan Ranganathan*

Main category: cs.CR

TL;DR: 对特斯拉车辆LTE连接的黑盒安全分析发现系统性的协议弱点和架构配置错误，包括IMSI捕获、伪基站劫持、不安全回退机制等漏洞，这些漏洞挑战了汽车网络安全监管框架的核心假设。


<details>
  <summary>Details</summary>
Motivation: 虽然移动网络漏洞在智能手机生态系统中已有充分记录，但在安全关键的汽车环境中其影响尚未得到充分研究。现代联网车辆依赖LTE连接实现远程诊断、OTA更新和安全服务，需要评估这些漏洞在汽车环境中的实际影响。

Method: 采用黑盒、非侵入式安全分析方法，对特斯拉车辆（包括Model 3和Cybertruck）的LTE连接进行测试，重点关注协议弱点和架构配置问题。

Result: 发现特斯拉的远程信息处理堆栈易受IMSI捕获、伪基站劫持和不安全回退机制攻击，可能导致服务可用性静默降级。此外，遗留控制平面配置允许静默SMS注入和广播消息欺骗，驾驶员无法察觉。

Conclusion: 这些漏洞不仅影响单个厂商，还挑战了ISO/SAE 21434和UN R155/R156等监管框架的核心假设，这些框架要求现代车辆必须具有安全、可追溯和有弹性的远程信息处理系统才能获得型式批准。

Abstract: Modern connected vehicles rely on persistent LTE connectivity to enable
remote diagnostics, over-the-air (OTA) updates, and critical safety services.
While mobile network vulnerabilities are well documented in the smartphone
ecosystem, their impact in safety-critical automotive settings remains
insufficiently examined. In this work, we conduct a black-box, non-invasive
security analysis of LTE connectivity in Tesla vehicles, including the Model 3
and Cybertruck, revealing systemic protocol weaknesses and architectural
misconfigurations. We find that Tesla's telematics stack is susceptible to IMSI
catching, rogue base station hijacking, and insecure fallback mechanisms that
may silently degrade service availability. Furthermore, legacy control-plane
configurations allow for silent SMS injection and broadcast message spoofing
without driver awareness. These vulnerabilities have implications beyond a
single vendor as they challenge core assumptions in regulatory frameworks like
ISO/SAE 21434 and UN R155/R156, which require secure, traceable, and resilient
telematics for type approval of modern vehicles.

</details>


### [31] [Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models](https://arxiv.org/abs/2510.22085)
*Pavlos Ntais*

Main category: cs.CR

TL;DR: 提出Jailbreak Mimicry方法，通过训练紧凑攻击模型自动生成基于叙事的越狱提示，将对抗性提示发现从手工制作转变为可重复的科学过程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型仍容易受到利用上下文框架绕过安全机制的复杂提示工程攻击，在网络安全应用中构成重大风险。

Method: 使用参数高效微调(LoRA)在Mistral-7B上，基于AdvBench的精选数据集进行训练，实现一次性生成叙事式越狱提示。

Result: 在GPT-OSS-20B上达到81.0%的攻击成功率，对其他模型的攻击成功率分别为GPT-4:66.5%、Llama-3:79.5%、Gemini 2.5 Flash:33.0%，比直接提示提升54倍。

Conclusion: 该方法揭示了当前安全对齐方法的系统性漏洞，特别是在技术领域和基于欺骗的攻击中尤为脆弱，同时分析了防御策略以减轻AI在网络安全中的这些漏洞。

Abstract: Large language models (LLMs) remain vulnerable to sophisticated prompt
engineering attacks that exploit contextual framing to bypass safety
mechanisms, posing significant risks in cybersecurity applications. We
introduce Jailbreak Mimicry, a systematic methodology for training compact
attacker models to automatically generate narrative-based jailbreak prompts in
a one-shot manner. Our approach transforms adversarial prompt discovery from
manual craftsmanship into a reproducible scientific process, enabling proactive
vulnerability assessment in AI-driven security systems. Developed for the
OpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient
fine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench,
achieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out
test set of 200 items. Cross-model evaluation reveals significant variation in
vulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on
Llama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad
applicability and model-specific defensive strengths in cybersecurity contexts.
This represents a 54x improvement over direct prompting (1.5% ASR) and
demonstrates systematic vulnerabilities in current safety alignment approaches.
Our analysis reveals that technical domains (Cybersecurity: 93% ASR) and
deception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable,
highlighting threats to AI-integrated threat detection, malware analysis, and
secure systems, while physical harm categories show greater resistance (55.6%
ASR). We employ automated harmfulness evaluation using Claude Sonnet 4,
cross-validated with human expert assessment, ensuring reliable and scalable
evaluation for cybersecurity red-teaming. Finally, we analyze failure
mechanisms and discuss defensive strategies to mitigate these vulnerabilities
in AI for cybersecurity.

</details>


### [32] [Lightweight and Breach-Resilient Authenticated Encryption Framework for Internet of Things](https://arxiv.org/abs/2510.22100)
*Saif E. Nouma,Attila A. Yavuz*

Main category: cs.CR

TL;DR: 提出了Graphene，第一个对称前向安全和聚合认证加密框架，专为低端物联网设备设计，结合密钥演进策略和离线-在线加密处理，提供突破恢复能力、近最优在线延迟和紧凑性。


<details>
  <summary>Details</summary>
Motivation: 当前物联网中部署的轻量级认证加密标准缺乏关键特性，如密钥泄露恢复能力和紧凑认证标签，以及离线-在线加密等性能增强功能。

Method: 通过结合密钥演进策略、离线-在线加密处理和通用消息认证码(UMACs)，创建前向安全和聚合认证加密框架。提供了两个不同的实例化实现，平衡性能权衡和MAC扩展性。

Result: 在商用硬件和32位ARM Cortex-M4微控制器上的实验评估显示，Graphene相比现有方案具有显著的性能优势，且向后兼容标准加密实现。

Conclusion: Graphene框架有效解决了物联网设备在低能耗对抗环境和低延迟无线信道中的安全需求，提供了突破恢复能力、高性能和紧凑性，并开源实现供公共测试和适配。

Abstract: The Internet of Things (IoT) relies heavily on resource-limited devices to
communicate critical (e.g., military data) information under low-energy
adversarial environments and low-latency wireless channels. Authenticated
Encryption (AE) guarantees confidentiality, authenticity, and integrity, making
it a vital security service for IoT. However, current deployed (lightweight) AE
standards lack essential features like key compromise resiliency and compact
authentication tags, as well as performance enhancements such as offline-online
cryptography. To address these gaps, we propose Graphene, the first (to our
knowledge) symmetric Forward-secure and Aggregate Authenticated Encryption
(FAAE) framework designed for the performance and security demands of low-end
IoT infrastructures. Graphene innovates by synergizing key evolution strategies
and offline-online cryptographic processing with Universal Message
Authentication Codes (UMACs) to guarantee breach-resiliency, near-optimal
online latency, and compactness. We demonstrate Graphene efficiency through two
distinct instantiations, each balancing unique performance trade-offs with
extensibility for diverse MACs. Our experimental evaluation on commodity
hardware and 32-bit ARM Cortex-M4 microcontroller shows Graphene significant
performance gains over existing alternatives. Graphene is also backward
compatible with standard-compliant cryptographic implementations. We release
our implementation as open source for public testing and adaptation.

</details>


### [33] [TPPR: APT Tactic / Technique Pattern Guided Attack Path Reasoning for Attack Investigation](https://arxiv.org/abs/2510.22191)
*Qi Sheng*

Main category: cs.CR

TL;DR: TPPR是一个基于溯源图的APT攻击检测框架，通过异常子图提取、TTP标注、图剪枝和攻击路径推理，能够高效识别APT攻击行为并重构攻击场景。


<details>
  <summary>Details</summary>
Motivation: 现有技术无法有效建立攻击上下文关联，容易将良性系统操作与真实攻击实体混淆，无法准确表征真实的APT行为。

Method: 首先通过异常节点检测、TTP标注和图剪枝提取异常子图，然后使用挖掘的TTP序列模式进行攻击路径推理，最后通过基于置信度的路径评分和合并重构攻击场景。

Result: 在真实企业日志（超过1亿事件）和DARPA TC数据集上的评估显示，TPPR实现了99.9%的图简化（从70万边减少到20边），同时保留了91%的关键攻击节点，在重构精度上比最先进解决方案（SPARSE、DepImpact）分别高出63.1%和67.9%。

Conclusion: TPPR框架能够有效识别APT攻击行为，显著提高攻击场景重构的精度，同时保持攻击场景的完整性。

Abstract: Provenance analysis based on system audit data has emerged as a fundamental
approach for investigating Advanced Persistent Threat (APT) attacks. Due to the
high concealment and long-term persistence of APT attacks, they are only
represented as a minimal part of the critical path in the provenance graph.
While existing techniques employ behavioral pattern matching and data flow
feature matching to uncover latent associations in attack sequences through
provenance graph path reasoning, their inability to establish effective attack
context associations often leads to the conflation of benign system operations
with real attack entities, that fail to accurately characterize real APT
behaviors. We observe that while the causality of entities in the provenance
graph exhibit substantial complexity, attackers often follow specific attack
patterns-specifically, clear combinations of tactics and techniques to achieve
their goals. Based on these insights, we propose TPPR, a novel framework that
first extracts anomaly subgraphs through abnormal node detection,
TTP-annotation and graph pruning, then performs attack path reasoning using
mined TTP sequential pattern, and finally reconstructs attack scenarios through
confidence-based path scoring and merging. Extensive evaluation on real
enterprise logs (more than 100 million events) and DARPA TC dataset
demonstrates TPPR's capability to achieve 99.9% graph simplification (700,000
to 20 edges) while preserving 91% of critical attack nodes, outperforming
state-of-the-art solutions (SPARSE, DepImpact) by 63.1% and 67.9% in
reconstruction precision while maintaining attack scenario integrity.

</details>


### [34] [SecureLearn -- An Attack-agnostic Defense for Multiclass Machine Learning Against Data Poisoning Attacks](https://arxiv.org/abs/2510.22274)
*Anum Paracha,Junaid Arshad,Mohamed Ben Farah,Khalid Ismail*

Main category: cs.CR

TL;DR: SecureLearn是一种双层攻击无关防御方法，用于保护多分类模型免受数据投毒攻击，包含数据净化和特征导向对抗训练两个组件。


<details>
  <summary>Details</summary>
Motivation: 现有防御主要针对特定攻击或特定ML算法，且多关注深度神经网络或二分类器，而传统多分类器在多模态应用中很重要但缺乏安全保护。

Method: 提出SecureLearn双层防御框架：数据净化层和新的特征导向对抗训练层，采用3D评估矩阵（数据投毒攻击、数据净化、对抗训练）进行有效性验证。

Result: 在10%-20%投毒水平下，SecureLearn保持准确率90%以上，召回率和F1分数75%以上；对神经网络，所有选定攻击下召回率和F1分数达97%。

Conclusion: SecureLearn有效增强了传统多分类模型和神经网络的抗攻击能力和鲁棒性，证实了其超越算法特定防御的泛化能力。

Abstract: Data poisoning attacks are a potential threat to machine learning (ML)
models, aiming to manipulate training datasets to disrupt their performance.
Existing defenses are mostly designed to mitigate specific poisoning attacks or
are aligned with particular ML algorithms. Furthermore, most defenses are
developed to secure deep neural networks or binary classifiers. However,
traditional multiclass classifiers need attention to be secure from data
poisoning attacks, as these models are significant in developing multi-modal
applications. Therefore, this paper proposes SecureLearn, a two-layer
attack-agnostic defense to defend multiclass models from poisoning attacks. It
comprises two components of data sanitization and a new feature-oriented
adversarial training. To ascertain the effectiveness of SecureLearn, we
proposed a 3D evaluation matrix with three orthogonal dimensions: data
poisoning attack, data sanitization and adversarial training. Benchmarking
SecureLearn in a 3D matrix, a detailed analysis is conducted at different
poisoning levels (10%-20%), particularly analysing accuracy, recall, F1-score,
detection and correction rates, and false discovery rate. The experimentation
is conducted for four ML algorithms, namely Random Forest (RF), Decision Tree
(DT), Gaussian Naive Bayes (GNB) and Multilayer Perceptron (MLP), trained with
three public datasets, against three poisoning attacks and compared with two
existing mitigations. Our results highlight that SecureLearn is effective
against the provided attacks. SecureLearn has strengthened resilience and
adversarial robustness of traditional multiclass models and neural networks,
confirming its generalization beyond algorithm-specific defenses. It
consistently maintained accuracy above 90%, recall and F1-score above 75%. For
neural networks, SecureLearn achieved 97% recall and F1-score against all
selected poisoning attacks.

</details>


### [35] [Adapting Noise-Driven PUF and AI for Secure WBG ICS: A Proof-of-Concept Study](https://arxiv.org/abs/2510.22283)
*Devon A. Kelly,Christiana Chamon*

Main category: cs.CR

TL;DR: 该研究提出了一种利用宽禁带技术开关噪声作为物理不可克隆函数源和实时威胁指示器的双用途安全框架，结合机器学习实现工业控制系统的硬件级认证和异常检测。


<details>
  <summary>Details</summary>
Motivation: 宽禁带技术虽然提高了电力系统效率，但也带来了独特的传感器损坏和网络安全风险，特别是在高频噪声和复杂网络物理威胁方面。

Method: 采用噪声驱动的物理不可克隆函数和机器学习辅助的异常检测框架，结合混合机器学习模型与自适应贝叶斯滤波，从WBG开关噪声中提取熵作为PUF源。

Result: 在良性场景和攻击场景下的详细模拟显示，系统达到95%的检测准确率和亚毫秒级的处理延迟。

Conclusion: 证明了物理驱动的双用途噪声利用作为可扩展ICS防御原语的可行性，为利用固有设备特性的下一代安全策略奠定了基础。

Abstract: Wide-bandgap (WBG) technologies offer unprecedented improvements in power
system efficiency, size, and performance, but also introduce unique sensor
corruption and cybersecurity risks in industrial control systems (ICS),
particularly due to high-frequency noise and sophisticated cyber-physical
threats. This proof-of-concept (PoC) study demonstrates the adaptation of a
noise-driven physically unclonable function (PUF) and machine learning
(ML)-assisted anomaly detection framework to the demanding environment of
WBG-based ICS sensor pathways. By extracting entropy from unavoidable WBG
switching noise (up to 100 kHz) as a PUF source, and simultaneously using this
noise as a real-time threat indicator, the proposed system unites
hardware-level authentication and anomaly detection. Our approach integrates
hybrid machine learning (ML) models with adaptive Bayesian filtering, providing
robust and low-latency detection capabilities resilient to both natural
electromagnetic interference (EMI) and active adversarial manipulation. Through
detailed simulations of WBG modules under benign and attack
scenarios--including EMI injection, signal tampering, and node
impersonation--we achieve 95% detection accuracy and sub-millisecond processing
latency. These results demonstrate the feasibility of physics-driven, dual-use
noise exploitation as a scalable ICS defense primitive. Our findings lay the
groundwork for next-generation security strategies that leverage inherent
device characteristics, bridging hardware and artificial intelligence (AI) for
enhanced protection of critical ICS infrastructure.

</details>


### [36] [T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model](https://arxiv.org/abs/2510.22300)
*Chenyu Zhang,Tairen Zhang,Lanjun Wang,Ruidong Chen,Wenhui Li,Anan Liu*

Main category: cs.CR

TL;DR: 提出了T2I-RiskyPrompt基准，用于评估文本到图像模型的安全性，包含6,432个有效风险提示，采用分层风险分类法，并提出了基于原因的风险图像检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有风险提示数据集存在三个主要问题：风险类别有限、注释粒度粗、有效性低，需要更全面的基准来评估T2I模型的安全性。

Method: 开发了包含6个主要类别和14个子类别的分层风险分类法，构建了收集和注释风险提示的流程，提出了基于原因的风险图像检测方法。

Result: 构建了6,432个有效风险提示数据集，对8个T2I模型、9种防御方法、5个安全过滤器和5种攻击策略进行了全面评估，提供了9个关键见解。

Conclusion: T2I-RiskyPrompt为评估T2I模型安全性提供了全面基准，并讨论了其在多个研究领域的潜在应用价值。

Abstract: Using risky text prompts, such as pornography and violent prompts, to test
the safety of text-to-image (T2I) models is a critical task. However, existing
risky prompt datasets are limited in three key areas: 1) limited risky
categories, 2) coarse-grained annotation, and 3) low effectiveness. To address
these limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark
designed for evaluating safety-related tasks in T2I models. Specifically, we
first develop a hierarchical risk taxonomy, which consists of 6 primary
categories and 14 fine-grained subcategories. Building upon this taxonomy, we
construct a pipeline to collect and annotate risky prompts. Finally, we obtain
6,432 effective risky prompts, where each prompt is annotated with both
hierarchical category labels and detailed risk reasons. Moreover, to facilitate
the evaluation, we propose a reason-driven risky image detection method that
explicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt,
we conduct a comprehensive evaluation of eight T2I models, nine defense
methods, five safety filters, and five attack strategies, offering nine key
insights into the strengths and limitations of T2I model safety. Finally, we
discuss potential applications of T2I-RiskyPrompt across various research
fields. The dataset and code are provided in
https://github.com/datar001/T2I-RiskyPrompt.

</details>


### [37] [Privacy-Aware Federated nnU-Net for ECG Page Digitization](https://arxiv.org/abs/2510.22387)
*Nader Nemati*

Main category: cs.CR

TL;DR: 提出了一个跨机构的联邦学习框架，用于将ECG页面图像转换为可分析波形，在保护隐私的同时实现多机构协作训练。


<details>
  <summary>Details</summary>
Motivation: 集中式训练与跨机构隐私和部署约束存在冲突，需要在不共享图像的情况下实现ECG数字化。

Method: 使用nnU-Net分割骨干网络，集成FedAvg、FedProx和FedAdam三种聚合器，结合安全聚合和中心差分隐私保护。

Result: FedAdam相比FedAvg和FedProx收敛更快且达到更高性能，接近集中式性能，同时保持隐私保护。

Conclusion: 该框架在保护原始图像和客户端更新的同时，提供了可部署、可审计的隐私保证，适用于多机构环境。

Abstract: Deep neural networks can convert ECG page images into analyzable waveforms,
yet centralized training often conflicts with cross-institutional privacy and
deployment constraints. A cross-silo federated digitization framework is
presented that trains a full-model nnU-Net segmentation backbone without
sharing images and aggregates updates across sites under realistic non-IID
heterogeneity (layout, grid style, scanner profile, noise).
  The protocol integrates three standard server-side aggregators--FedAvg,
FedProx, and FedAdam--and couples secure aggregation with central, user-level
differential privacy to align utility with formal guarantees. Key features
include: (i) end-to-end full-model training and synchronization across clients;
(ii) secure aggregation so the server only observes a clipped, weighted sum
once a participation threshold is met; (iii) central Gaussian DP with Renyi
accounting applied post-aggregation for auditable user-level privacy; and (iv)
a calibration-aware digitization pipeline comprising page normalization, trace
segmentation, grid-leakage suppression, and vectorization to twelve-lead
signals.
  Experiments on ECG pages rendered from PTB-XL show consistently faster
convergence and higher late-round plateaus with adaptive server updates
(FedAdam) relative to FedAvg and FedProx, while approaching centralized
performance. The privacy mechanism maintains competitive accuracy while
preventing exposure of raw images or per-client updates, yielding deployable,
auditable guarantees suitable for multi-institution settings.

</details>


### [38] [PortGPT: Towards Automated Backporting Using Large Language Models](https://arxiv.org/abs/2510.22396)
*Zhaoyang Li,Zheng Yu,Jingyi Song,Meng Xu,Yuxuan Luo,Dongliang Mu*

Main category: cs.CR

TL;DR: PORTGPT是一个基于LLM的自动化补丁回移植工具，通过工具增强实现端到端的自动化，在现有数据集上达到89.15%成功率，在复杂案例上达到62.33%成功率，优于现有最佳工具。


<details>
  <summary>Details</summary>
Motivation: 手动回移植补丁工作量大，现有自动化方法依赖预定义规则，缺乏对复杂补丁的灵活性。

Method: PORTGPT增强LLM，提供代码访问、Git历史总结和基于反馈的自主补丁修订工具，模拟人类推理和验证过程。

Result: 在1815个案例的数据集上达到89.15%成功率，在146个复杂案例上达到62.33%成功率，向Linux内核社区贡献了9个已合并的补丁。

Conclusion: PORTGPT证明了LLM代理在复杂软件工程任务中的有效性，能够成功自动化补丁回移植过程。

Abstract: Patch backporting, the process of migrating mainline security patches to
older branches, is an essential task in maintaining popular open-source
projects (e.g., Linux kernel). However, manual backporting can be
labor-intensive, while existing automated methods, which heavily rely on
predefined syntax or semantic rules, often lack agility for complex patches.
  In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation
of patch backporting in real-world scenarios. PORTGPT enhances an LLM with
tools to access code on-demand, summarize Git history, and revise patches
autonomously based on feedback (e.g., from compilers), hence, simulating
human-like reasoning and verification. PORTGPT achieved an 89.15% success rate
on existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex
cases, both outperforms state-of-the-art of backporting tools. We contributed 9
backported patches from PORTGPT to the Linux kernel community and all patches
are now merged.

</details>


### [39] [ProGQL: A Provenance Graph Query System for Cyber Attack Investigation](https://arxiv.org/abs/2510.22400)
*Fei Shao,Jia Zou,Zhichao Cao,Xusheng Xiao*

Main category: cs.CR

TL;DR: PROGQL框架解决了现有溯源分析技术的不灵活性和内存效率低下的问题，通过领域特定的图搜索语言和优化的查询引擎，支持复杂网络攻击调查。


<details>
  <summary>Details</summary>
Motivation: 现有溯源分析技术存在两个关键挑战：(1) 不灵活且不可扩展，难以整合分析师专业知识；(2) 内存效率低下，通常需要>100GB内存来存储整个事件流，限制了实际环境中的可扩展性和部署。

Method: 提出PROGQL框架，提供领域特定的图搜索语言和精心设计的查询引擎，允许将系统审计事件和专家知识联合表达为图搜索查询。引入新的语言结构支持约束图遍历、边权重计算、沿加权边的值传播和图合并。

Result: 在真实攻击上的评估表明，PROGQL语言在表达各种复杂攻击方面比最先进的图查询语言Cypher更有效，与SOTA PA技术DEPIMPACT的比较进一步证明了PROGQL框架设计带来的可扩展性显著提升。

Conclusion: PROGQL框架通过其领域特定的图搜索语言和优化的查询引擎，有效解决了现有溯源分析技术的局限性，为复杂网络攻击调查提供了更灵活、可扩展且内存高效的解决方案。

Abstract: Provenance analysis (PA) has recently emerged as an important solution for
cyber attack investigation. PA leverages system monitoring to monitor system
activities as a series of system audit events and organizes these events as a
provenance graph to show the dependencies among system activities, which can
reveal steps of cyber attacks. Despite their potential, existing PA techniques
face two critical challenges: (1) they are inflexible and non-extensible,
making it difficult to incorporate analyst expertise, and (2) they are memory
inefficient, often requiring>100GB of RAM to hold entire event streams, which
fundamentally limits scalability and deployment in real-world environments. To
address these limitations, we propose the PROGQL framework, which provides a
domain-specific graph search language with a well-engineered query engine,
allowing PA over system audit events and expert knowledge to be jointly
expressed as a graph search query and thereby facilitating the investigation of
complex cyberattacks. In particular, to support dependency searches from a
starting edge required in PA, PROGQL introduces new language constructs for
constrained graph traversal, edge weight computation, value propagation along
weighted edges, and graph merging to integrate multiple searches. Moreover, the
PROGQL query engine is optimized for efficient incremental graph search across
heterogeneous database backends, eliminating the need for full in-memory
materialization and reducing memory overhead. Our evaluations on real attacks
demonstrate the effectiveness of the PROGQL language in expressing a diverse
set of complex attacks compared with the state-of-the-art graph query language
Cypher, and the comparison with the SOTA PA technique DEPIMPACT further
demonstrates the significant improvement of the scalability brought by our
PROGQL framework's design.

</details>


### [40] [ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole](https://arxiv.org/abs/2510.22536)
*Jotaro Yano*

Main category: cs.CR

TL;DR: 提出跨域ZK协处理器桥接系统，让Solana程序通过Wormhole VAAs在Aztec L2上请求私有执行，包含Solana程序、EVM门户、Aztec合约和链下中继器四个组件。


<details>
  <summary>Details</summary>
Motivation: 构建跨区块链域的私有计算能力，使Solana程序能够利用Aztec L2的零知识证明隐私保护功能，同时通过Wormhole实现安全的跨链通信。

Method: 使用Wormhole VAAs作为认证传输，包含四个核心组件：Solana程序发布消息、EVM门户验证VAAs、Aztec合约消费私有消息、链下中继器传输VAAs。

Result: 设计了状态机、消息格式和安全性证明，确保重放安全、来源认证、最终性对齐、参数绑定、隐私保护、幂等性和活性。

Conclusion: 成功构建了跨域ZK协处理器桥接系统，提供了可复现的测试网运行版本和迁移指南。

Abstract: We formalize a cross-domain "ZK coprocessor bridge" that lets Solana programs
request private execution on Aztec L2 (via Ethereum) using Wormhole Verifiable
Action Approvals (VAAs) as authenticated transport. The system comprises: (i) a
Solana program that posts messages to Wormhole Core with explicit finality;
(ii) an EVM Portal that verifies VAAs, enforces a replay lock, parses a bound
payload secretHash||m from the attested VAA, derives a domain-separated field
commitment, and enqueues an L1->L2 message into the Aztec Inbox (our reference
implementation v0.1.0 currently uses consumeWithSecret(vaa, secretHash); we
provide migration guidance to the payload-bound interface); (iii) a minimal
Aztec contract that consumes the message privately; and (iv) an off-chain
relayer that ferries VAAs and can record receipts on Solana. We present state
machines, message formats, and proof sketches for replay-safety, origin
authenticity, finality alignment, parameter binding (no relayer front-running
of Aztec parameters), privacy, idempotence, and liveness. Finally, we include a
concise Reproducibility note with pinned versions and artifacts to replicate a
public testnet run.

</details>


### [41] [Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers](https://arxiv.org/abs/2510.22555)
*Dongyi Liu,Jiangtong Li,Dawei Cheng,Changjun Jiang*

Main category: cs.CR

TL;DR: 提出了一种跨范式图后门攻击方法CP-GBA，使用图提示学习训练通用子图触发器，解决了现有方法在不同图学习范式间迁移性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图后门攻击的触发器生成器结构简单，过度依赖特定特征，仅适用于单一图学习范式（如图监督学习、图对比学习或图提示学习），在其他范式下迁移性差，攻击成功率低。

Method: 1) 从目标图中提取紧凑且表达性强的触发器集，构建可查询存储库，同时保证类别感知、特征丰富性和结构保真度；2) 首次探索图提示学习的理论迁移性，在基于提示的目标下训练触发器，使其能有效泛化到不同的测试范式。

Result: 在多个真实世界数据集和防御场景下的广泛实验表明，CP-GBA实现了最先进的攻击成功率。

Conclusion: CP-GBA通过图提示学习训练通用子图触发器，解决了图后门攻击在不同学习范式间的迁移性问题，显著提升了攻击效果。

Abstract: Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, where
adversaries implant malicious triggers to manipulate model predictions.
  Existing trigger generators are often simplistic in structure and overly
reliant on specific features, confining them to a single graph learning
paradigm, such as graph supervised learning, graph contrastive learning, or
graph prompt learning.
  This specialized design, which aligns the trigger with one learning
objective, results in poor transferability when applied to other learning
paradigms.
  For instance, triggers generated for the graph supervised learning paradigm
perform poorly when tested within graph contrastive learning or graph prompt
learning environments.
  Furthermore, these simple generators often fail to utilize complex structural
information or node diversity within the graph data.
  These constraints limit the attack success rates of such methods in general
testing scenarios.
  Therefore, to address these limitations, we propose Cross-Paradigm Graph
Backdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferable
graph backdoor attack that employs graph prompt learning(GPL) to train a set of
universal subgraph triggers.
  First, we distill a compact yet expressive trigger set from target graphs,
which is structured as a queryable repository, by jointly enforcing
class-awareness, feature richness, and structural fidelity.
  Second, we conduct the first exploration of the theoretical transferability
of GPL to train these triggers under prompt-based objectives, enabling
effective generalization to diverse and unseen test-time paradigms.
  Extensive experiments across multiple real-world datasets and defense
scenarios show that CP-GBA achieves state-of-the-art attack success rates.

</details>


### [42] [Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study](https://arxiv.org/abs/2510.22561)
*Kaveri Banerjee,Sajal Saha*

Main category: cs.CR

TL;DR: 该论文调查了区块链平台中使用的数字签名方案，分析了它们如何实现不可否认性并增强系统安全性，比较了不同方案在共识协议、智能合约约束和资源限制下的适用性。


<details>
  <summary>Details</summary>
Motivation: 区块链系统依赖去中心化账本和强安全保证，其中不可否认性是关键要求，能防止交易作者否认并支持记录数据的完整性。

Method: 通过检查代表性方案家族及其密码学基础、安全假设和相关属性（包括不可伪造性、抗延展性、聚合支持、多签名或阈值设置、密钥和签名大小、验证成本），比较不同设计在共识协议、智能合约约束和资源限制下的适用性。

Result: 研究突出了影响吞吐量、存储、可扩展性和攻击面的实际权衡，总结了每种方案在区块链环境中的优势和局限性。

Conclusion: 精心选择的数字签名对于实现不可否认性和保持信息完整性至关重要，研究还概述了实施考虑因素和开放方向，如互操作性和后量子准备。

Abstract: Blockchain systems rely on decentralized ledgers and strong security
guarantees. A key requirement is non-repudiation, which prevents denial of
transaction authorship and supports integrity of recorded data. This work
surveys digital signature schemes used in blockchain platforms and analyzes how
they deliver non-repudiation and contribute to overall system security. We
examine representative scheme families and their cryptographic foundations,
security assumptions, and properties relevant to deployment, including
unforgeability, resistance to malleability, support for aggregation and
multisignature or threshold settings, key and signature sizes, and verification
cost. Using these criteria, we compare the suitability of different designs for
consensus protocols, smart contract constraints, and resource limits. We
highlight practical tradeoffs that affect throughput, storage, scalability, and
attack surfaces, and summarize benefits and limitations of each scheme in
blockchain contexts. The study underscores that carefully chosen digital
signatures are central to achieving non-repudiation and preserving information
integrity, and it outlines implementation considerations and open directions
such as interoperability and post-quantum readiness.

</details>


### [43] [FAARM: Firmware Attestation and Authentication Framework for Mali GPUs](https://arxiv.org/abs/2510.22566)
*Md. Mehedi Hasan*

Main category: cs.CR

TL;DR: MOLE攻击通过向Arm Mali GPU的MCU注入恶意固件来破坏GPU TEE。FAARM框架通过EL3安全监视器的数字签名验证、供应商签名固件包和设备上公钥锚点来防止此类固件篡改，仅增加1.34ms延迟。


<details>
  <summary>Details</summary>
Motivation: MOLE攻击暴露了GPU TEE设计中的关键固件级信任漏洞，攻击者可通过内核权限绕过内存保护，窃取敏感数据并篡改推理结果，影响移动SoC和云加速器。

Method: FAARM在EL3安全监视器集成数字签名验证，使用供应商签名固件包和设备上公钥锚点。启动时验证固件完整性和真实性，执行版本检查并锁定固件区域，消除预验证和TOCTOU攻击向量。

Result: FAARM可靠检测并阻止恶意固件注入，在平均1.34ms延迟内拒绝篡改镜像并阻止覆盖尝试，证明强安全性可实现且开销可忽略。

Conclusion: FAARM填补了基于shim的GPU TEE的基本安全漏洞，为移动和云GPU部署提供了实用、可部署的防御方案，提升了安全基线。

Abstract: Recent work has revealed MOLE, the first practical attack to compromise GPU
Trusted Execution Environments (TEEs), by injecting malicious firmware into the
embedded Microcontroller Unit (MCU) of Arm Mali GPUs. By exploiting the absence
of cryptographic verification during initialization, adversaries with kernel
privileges can bypass memory protections, exfiltrate sensitive data at over 40
MB/s, and tamper with inference results, all with negligible runtime overhead.
This attack surface affects commodity mobile SoCs and cloud accelerators,
exposing a critical firmware-level trust gap in existing GPU TEE designs. To
address this gap, this paper presents FAARM, a lightweight Firmware Attestation
and Authentication framework that prevents MOLE-style firmware subversion.
FAARM integrates digital signature verification at the EL3 secure monitor using
vendor-signed firmware bundles and an on-device public key anchor. At boot, EL3
verifies firmware integrity and authenticity, enforces version checks, and
locks the firmware region, eliminating both pre-verification and
time-of-check-to-time-of-use (TOCTOU) attack vectors. We implement FAARM as a
software-only prototype on a Mali GPU testbed, using a Google Colab-based
emulation framework that models the firmware signing process, the EL1 to EL3
load path, and secure memory configuration. FAARM reliably detects and blocks
malicious firmware injections, rejecting tampered images before use and denying
overwrite attempts after attestation. Firmware verification incurs only 1.34 ms
latency on average, demonstrating that strong security can be achieved with
negligible overhead. FAARM thus closes a fundamental gap in shim-based GPU
TEEs, providing a practical, deployable defense that raises the security
baseline for both mobile and cloud GPU deployments.

</details>


### [44] [Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents](https://arxiv.org/abs/2510.22620)
*Julia Bazinska,Max Mathys,Francesco Casucci,Mateo Rojas-Carulla,Xander Davies,Alexandra Souly,Niklas Pfister*

Main category: cs.CR

TL;DR: 提出了威胁快照框架和b³基准，用于系统评估LLM骨干模型对AI代理安全性的影响，发现推理能力提升安全性而模型大小无关


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对LLM骨干模型如何影响AI代理安全性的系统理解，现有框架要么只捕获特定漏洞，要么需要完整代理建模

Method: 引入威胁快照框架，隔离代理执行流程中LLM漏洞显现的特定状态，构建基于194331个众包对抗攻击的b³安全基准

Result: 评估31个流行LLM，发现增强推理能力可提高安全性，而模型大小与安全性无关

Conclusion: 威胁快照框架能系统识别LLM到代理级别的安全风险传播，为代理开发者和模型开发者提供安全改进指导

Abstract: AI agents powered by large language models (LLMs) are being deployed at
scale, yet we lack a systematic understanding of how the choice of backbone LLM
affects agent security. The non-deterministic sequential nature of AI agents
complicates security modeling, while the integration of traditional software
with AI components entangles novel LLM vulnerabilities with conventional
security risks. Existing frameworks only partially address these challenges as
they either capture specific vulnerabilities only or require modeling of
complete agents. To address these limitations, we introduce threat snapshots: a
framework that isolates specific states in an agent's execution flow where LLM
vulnerabilities manifest, enabling the systematic identification and
categorization of security risks that propagate from the LLM to the agent
level. We apply this framework to construct the $\operatorname{b}^3$ benchmark,
a security benchmark based on 194331 unique crowdsourced adversarial attacks.
We then evaluate 31 popular LLMs with it, revealing, among other insights, that
enhanced reasoning capabilities improve security, while model size does not
correlate with security. We release our benchmark, dataset, and evaluation code
to facilitate widespread adoption by LLM providers and practitioners, offering
guidance for agent developers and incentivizing model developers to prioritize
backbone security improvements.

</details>


### [45] [DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake Detection](https://arxiv.org/abs/2510.22622)
*Kangran Zhao,Yupeng Chen,Xiaoyu Zhang,Yize Chen,Weinan Guan,Baicheng Chen,Chengzhe Sun,Soumyya Kanti Datta,Qingshan Liu,Siwei Lyu,Baoyuan Wu*

Main category: cs.CR

TL;DR: 构建了大规模多模态深度伪造数据集Mega-MMDF和首个统一基准DeepfakeBench-MM，用于推进多模态深度伪造检测研究。


<details>
  <summary>Details</summary>
Motivation: 应对生成式AI滥用导致伪造音视频内容泛滥的社会风险，解决训练数据不足和标准化基准缺失的问题。

Method: 通过21种伪造流程组合10种音频伪造、12种视觉伪造和6种音频驱动面部重演方法，构建包含110万伪造样本的大规模数据集，并建立统一评估基准。

Result: 创建了目前最大最丰富的多模态深度伪造数据集，支持5个数据集和11种检测器的评估，揭示了多个关键发现。

Conclusion: Mega-MMDF和DeepfakeBench-MM将作为推进多模态深度伪造检测的基础设施，为未来研究提供重要支撑。

Abstract: The misuse of advanced generative AI models has resulted in the widespread
proliferation of falsified data, particularly forged human-centric audiovisual
content, which poses substantial societal risks (e.g., financial fraud and
social instability). In response to this growing threat, several works have
preliminarily explored countermeasures. However, the lack of sufficient and
diverse training data, along with the absence of a standardized benchmark,
hinder deeper exploration. To address this challenge, we first build Mega-MMDF,
a large-scale, diverse, and high-quality dataset for multimodal deepfake
detection. Specifically, we employ 21 forgery pipelines through the combination
of 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face
reenactment methods. Mega-MMDF currently contains 0.1 million real samples and
1.1 million forged samples, making it one of the largest and most diverse
multimodal deepfake datasets, with plans for continuous expansion. Building on
it, we present DeepfakeBench-MM, the first unified benchmark for multimodal
deepfake detection. It establishes standardized protocols across the entire
detection pipeline and serves as a versatile platform for evaluating existing
methods as well as exploring novel approaches. DeepfakeBench-MM currently
supports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our
comprehensive evaluations and in-depth analyses uncover several key findings
from multiple perspectives (e.g., augmentation, stacked forgery). We believe
that DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as
foundational infrastructures for advancing multimodal deepfake detection.

</details>


### [46] [Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks](https://arxiv.org/abs/2510.22628)
*Md. Mehedi Hasan,Ziaur Rahman,Rafid Mostafiz,Md. Abir Hossain*

Main category: cs.CR

TL;DR: Sentra-Guard是一个实时模块化防御系统，用于检测和缓解针对大语言模型的越狱和提示注入攻击，采用混合架构实现多语言弹性，检测率达到99.96%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，越狱和提示注入攻击日益增多，现有防御系统存在检测率不足、缺乏多语言支持等问题，需要开发更有效的实时防御方案。

Method: 采用混合架构：FAISS索引的SBERT嵌入表示捕获提示语义，结合微调transformer分类器；包含分类器-检索器融合模块动态计算上下文感知风险评分；多语言预处理层自动翻译非英语提示；集成人机交互反馈循环。

Result: 检测率达到99.96%（AUC=1.00，F1=1.00），攻击成功率仅为0.004%，显著优于LlamaGuard-2（1.3%）和OpenAI Moderation（3.7%）等基线方法。

Conclusion: Sentra-Guard建立了对抗性LLM防御的新技术标准，具有透明、可微调、兼容多种LLM后端的特点，支持商业和开源环境的可扩展部署。

Abstract: This paper presents a real-time modular defense system named Sentra-Guard.
The system detects and mitigates jailbreak and prompt injection attacks
targeting large language models (LLMs). The framework uses a hybrid
architecture with FAISS-indexed SBERT embedding representations that capture
the semantic meaning of prompts, combined with fine-tuned transformer
classifiers, which are machine learning models specialized for distinguishing
between benign and adversarial language inputs. It identifies adversarial
prompts in both direct and obfuscated attack vectors. A core innovation is the
classifier-retriever fusion module, which dynamically computes context-aware
risk scores that estimate how likely a prompt is to be adversarial based on its
content and context. The framework ensures multilingual resilience with a
language-agnostic preprocessing layer. This component automatically translates
non-English prompts into English for semantic evaluation, enabling consistent
detection across over 100 languages. The system includes a HITL feedback loop,
where decisions made by the automated system are reviewed by human experts for
continual learning and rapid adaptation under adversarial pressure.
Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and
malicious prompts, enhancing detection reliability and reducing false
positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =
1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading
baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike
black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible
with diverse LLM backends. Its modular design supports scalable deployment in
both commercial and open-source environments. The system establishes a new
state-of-the-art in adversarial LLM defense.

</details>


### [47] [RejSCore: Rejection Sampling Core for Multivariate-based Public key Cryptography](https://arxiv.org/abs/2510.22661)
*Malik Imran,Safiullah Khan,Zain Ul Abideen,Ciara Rafferty,Ayesha Khalid,Muhammad Rashid,Maire O'Neill*

Main category: cs.CR

TL;DR: RejSCore是一个轻量级硬件加速器，专门用于后量子密码学中的拒绝采样，针对QR-UOV方案进行优化，在资源受限设备上实现高效运行。


<details>
  <summary>Details</summary>
Motivation: 后量子多变量公钥密码学方案需要大量操作如拒绝采样，这对资源受限设备构成挑战，而现有硬件设计尚未充分探索拒绝采样。

Method: 采用AES-CTR-128伪随机数生成器，使用轻量级迭代方法进行拒绝采样，降低资源消耗和面积开销。

Result: 在Artix-7 FPGA上实现2042个slice和222MHz频率，在65nm CMOS上实现464,866μm²面积和565MHz频率，使用QR-UOV参数完成操作需要8525个时钟周期。

Conclusion: ADP和PDP评估证实RejSCore适合部署在资源受限和安全关键环境中。

Abstract: Post-quantum multivariate public key cryptography (MPKC) schemes resist
quantum threats but require heavy operations, such as rejection sampling, which
challenge resource-limited devices. Prior hardware designs have addressed
various aspects of MPKC signature generation. However, rejection sampling
remains largely unexplored in such contexts. This paper presents RejSCore, a
lightweight hardware accelerator for rejection sampling in post-quantum
cryptography. It specifically targets the QR-UOV scheme, which is a prominent
candidate under the second-round of the National Institute of Standards and
Technology (NIST) additional digital signature standardization process. The
architecture includes an AES-CTR-128-based pseudorandom number generator.
Moreover, a lightweight iterative method is employed in rejection sampling,
offering reduced resource consumption and area overhead while slightly
increasing latency. The performance of RejSCore is comprehensively evaluated on
Artix-7 FPGAs and 65 nm CMOS technology using the Area-Delay Product (ADP) and
Power-Delay Product (PDP). On Artix-7 and 65 nm CMOS, RejSCore achieves an area
of 2042 slices and 464,866~$\mu m^2$, with operating frequencies of 222 MHz and
565 MHz, respectively. Using the QR-UOV parameters for security level I ($q =
127$, $v = 156$, $m = 54$, $l = 3$), the core completes its operation in 8525
clock cycles. The ADP and PDP evaluations confirm RejSCore's suitability for
deployment in resource-constrained and security-critical environments.

</details>


### [48] [SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking](https://arxiv.org/abs/2510.22726)
*Van Le,Tan Le*

Main category: cs.CR

TL;DR: SpoofTrackBench是一个可复现的模块化基准测试，用于评估实时定位跟踪系统在雷达欺骗攻击下的对抗鲁棒性，支持多种欺骗类型和跟踪架构的性能比较。


<details>
  <summary>Details</summary>
Motivation: 现有的实时定位跟踪系统缺乏标准化的对抗鲁棒性评估基准，难以系统评估不同跟踪架构在雷达欺骗攻击下的性能表现。

Method: 利用Hampton大学Skyler雷达传感器数据集，模拟漂移、幽灵和镜像欺骗攻击，分别评估JPDA和GNN跟踪架构性能，通过分离干净和欺骗检测流、可视化轨迹偏差等方法进行分析。

Result: 开发了包含聚类覆盖、注入感知时间线和场景自适应可视化的可解释框架，能够量化跟踪误差并自动导出评估结果用于可复现比较。

Conclusion: SpoofTrackBench为欺骗感知跟踪管道设立了开放、伦理的基准测试新标准，支持跨架构分析和社区验证。

Abstract: SpoofTrackBench is a reproducible, modular benchmark for evaluating
adversarial robustness in real-time localization and tracking (RTLS) systems
under radar spoofing. Leveraging the Hampton University Skyler Radar Sensor
dataset, we simulate drift, ghost, and mirror-type spoofing attacks and
evaluate tracker performance using both Joint Probabilistic Data Association
(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates
clean and spoofed detection streams, visualizes spoof-induced trajectory
divergence, and quantifies assignment errors via direct drift-from-truth
metrics. Clustering overlays, injection-aware timelines, and scenario-adaptive
visualizations enable interpretability across spoof types and configurations.
Evaluation figures and logs are auto-exported for reproducible comparison.
SpoofTrackBench sets a new standard for open, ethical benchmarking of
spoof-aware tracking pipelines, enabling rigorous cross-architecture analysis
and community validation.

</details>


### [49] [Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies](https://arxiv.org/abs/2510.22944)
*Bin Wang,YiLu Zhong,MiDi Wan,WenJie Yu,YuanBing Ouyang,Yenan Huang,Hui Li*

Main category: cs.CR

TL;DR: 该研究发现提示词质量与AI生成代码安全性密切相关：提示词规范性越低，生成不安全代码的可能性越高。通过构建评估框架和基准数据集，验证了先进提示技术能有效缓解低质量提示带来的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注对抗攻击或模型固有缺陷，但忽视了良性但表述不佳的提示词对生成代码安全性的影响。这是一个普遍存在但研究不足的问题。

Method: 提出包含目标清晰度、信息完整性和逻辑一致性的提示词质量评估框架，构建CWE-BENCH-PYTHON基准数据集，将提示词分为四个规范性等级(L0-L3)，并在多个先进LLM上进行实验。

Result: 实验显示明确的关联性：随着提示词规范性降低，生成不安全代码的可能性持续显著增加。同时发现Chain-of-Thought和Self-Correction等先进提示技术能有效减轻低质量提示带来的安全风险。

Conclusion: 提高用户提示词质量是增强AI生成代码安全性的关键有效策略，这为代码生成安全提供了新的研究方向和实践指导。

Abstract: Large language models (LLMs) have become indispensable for automated code
generation, yet the quality and security of their outputs remain a critical
concern. Existing studies predominantly concentrate on adversarial attacks or
inherent flaws within the models. However, a more prevalent yet underexplored
issue concerns how the quality of a benign but poorly formulated prompt affects
the security of the generated code. To investigate this, we first propose an
evaluation framework for prompt quality encompassing three key dimensions: goal
clarity, information completeness, and logical consistency. Based on this
framework, we construct and publicly release CWE-BENCH-PYTHON, a large-scale
benchmark dataset containing tasks with prompts categorized into four distinct
levels of normativity (L0-L3). Extensive experiments on multiple
state-of-the-art LLMs reveal a clear correlation: as prompt normativity
decreases, the likelihood of generating insecure code consistently and markedly
increases. Furthermore, we demonstrate that advanced prompting techniques, such
as Chain-of-Thought and Self-Correction, effectively mitigate the security
risks introduced by low-quality prompts, substantially improving code safety.
Our findings highlight that enhancing the quality of user prompts constitutes a
critical and effective strategy for strengthening the security of AI-generated
code.

</details>


### [50] [QuantumShield: Multilayer Fortification for Quantum Federated Learning](https://arxiv.org/abs/2510.22945)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.CR

TL;DR: 提出了一种量子安全的联邦学习框架，通过整合量子密钥分发、量子隐形传态、密钥封装机制和后量子密码学等先进协议，保护分布式学习系统免受量子攻击威胁。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，传统密码学方法面临量子攻击的威胁，需要建立能够抵御量子攻击的联邦学习安全架构。

Method: 集成并严格评估了量子密钥分发、量子隐形传态、密钥封装机制和后量子密码学等协议，构建了一个安全可扩展的量子安全联邦学习生态系统。

Result: 通过理论建模和实验验证，详细评估了所提出框架的安全性和性能表现。

Conclusion: 这项工作为下一代在量子时代具有固有安全性的联邦学习系统奠定了坚实基础。

Abstract: In this paper, we propose a groundbreaking quantum-secure federated learning
(QFL) framework designed to safeguard distributed learning systems against the
emerging threat of quantum-enabled adversaries. As classical cryptographic
methods become increasingly vulnerable to quantum attacks, our framework
establishes a resilient security architecture that remains robust even in the
presence of quantum-capable attackers. We integrate and rigorously evaluate
advanced quantum and post-quantum protocols including Quantum Key Distribution
(QKD), Quantum Teleportation, Key Encapsulation Mechanisms (KEM) and
Post-Quantum Cryptography (PQC) to fortify the QFL process against both
classical and quantum threats. These mechanisms are systematically analyzed and
implemented to demonstrate their seamless interoperability within a secure and
scalable QFL ecosystem. Through comprehensive theoretical modeling and
experimental validation, this work provides a detailed security and performance
assessment of the proposed framework. Our findings lay a strong foundation for
next-generation federated learning systems that are inherently secure in the
quantum era.

</details>


### [51] [CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents](https://arxiv.org/abs/2510.22963)
*Zesen Liu,Zhixiang Zhang,Yuchong Xie,Dongdong She*

Main category: cs.CR

TL;DR: 论文提出了CompressionAttack框架，利用提示压缩作为新的攻击面，通过硬压缩和软压缩策略操纵LLM行为，攻击成功率高达80%，现有防御措施无效。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的代理使用提示压缩来降低推理成本，但这种优化效率而非安全性的压缩模块可能被恶意输入操纵，导致语义漂移和LLM行为改变，构成了新的安全风险。

Method: 提出了CompressionAttack框架，包含两种策略：HardCom使用离散对抗编辑进行硬压缩，SoftCom在潜在空间进行扰动实现软压缩。

Result: 在多个LLM上的实验显示攻击成功率高达80%，偏好翻转率达98%，同时具有高度隐蔽性和可迁移性。VSCode Cline和Ollama的案例研究证实了实际影响。

Conclusion: 当前防御措施对CompressionAttack无效，突显了需要更强的保护机制来应对提示压缩带来的安全风险。

Abstract: LLM-powered agents often use prompt compression to reduce inference costs,
but this introduces a new security risk. Compression modules, which are
optimized for efficiency rather than safety, can be manipulated by adversarial
inputs, causing semantic drift and altering LLM behavior. This work identifies
prompt compression as a novel attack surface and presents CompressionAttack,
the first framework to exploit it. CompressionAttack includes two strategies:
HardCom, which uses discrete adversarial edits for hard compression, and
SoftCom, which performs latent-space perturbations for soft compression.
Experiments on multiple LLMs show up to 80% attack success and 98% preference
flips, while remaining highly stealthy and transferable. Case studies in VSCode
Cline and Ollama confirm real-world impact, and current defenses prove
ineffective, highlighting the need for stronger protections.

</details>


### [52] [Advancing Honeywords for Real-World Authentication Security](https://arxiv.org/abs/2510.22971)
*Sudiksha Das,Ashish Kundu*

Main category: cs.CR

TL;DR: 这篇立场论文认为蜜词技术有潜力但需要更多研究来解决平坦性、集成性和可靠性等问题，才能成为实用的可部署解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管蜜词技术已研究超过十年，但尚未被主流认证平台采用。论文旨在分析阻碍蜜词系统广泛使用的技术问题，并提出可部署的框架。

Method: 论文审查了当前蜜词生成、攻击者建模和蜜检查器架构的研究工作，分析了已解决的问题和持续存在的挑战。

Result: 提出了一个可部署框架，将蜜词提供的抗攻击、上下文感知的诱饵创建与现有系统的轻松集成相结合。

Conclusion: 蜜词技术要从学术理念转变为实用安全工具，需要技术进展与安全简单的架构相结合，同时配备自适应响应处理和详细配置检查。

Abstract: Introduced by Juels and Rivest in 2013, Honeywords, which are decoy passwords
stored alongside a real password, appear to be a proactive method to help
detect password credentials misuse. However, despite over a decade of research,
this technique has not been adopted by major authentication platforms. This
position paper argues that the core concept of Honeywords has potential but
requires more research on issues such as flatness, integration, and
reliability, in order to be a practical deployable solution. This paper
examines the current work on Honeyword generation, attacker modeling, and
honeychecker architecture, analyzing the subproblems that have been addressed
and ongoing issues that prevent this system from being more widely used. The
paper then suggests a deployable framework that combines the
attacker-resilient, context-aware decoy creation that Honeywords provide with
easy integration into existing systems. Honeywords will only move from an
academic idea to a practical security tool if technical advances are paired
with secure and straightforward architectures, along with adaptive response
handling and detailed configuration checks.

</details>


### [53] [A Multi-Store Privacy Measurement of Virtual Reality App Ecosystem](https://arxiv.org/abs/2510.23024)
*Chuan Yan,Zeng Li,Kunlin Cai,Liuhuo Wan,Ruomai Ren,Yiran Shen,Guangdong Bai*

Main category: cs.CR

TL;DR: 本研究首次对VR应用生态系统中的隐私实践进行了全面的多商店分析，涵盖5个主要应用商店的6,565个应用，揭示了显著的隐私合规问题。


<details>
  <summary>Details</summary>
Motivation: VR应用收集敏感数据但缺乏领域特定监管，导致各应用商店的隐私实践存在显著差异，需要系统评估当前隐私保护现状。

Method: 使用自然语言处理、逆向工程和静态分析的多方面方法，评估VR应用的声明性和行为性隐私实践。

Result: 发现所有商店都存在严重隐私合规问题：三分之一的应用未声明敏感数据使用，21.5%的应用未提供有效隐私政策。

Conclusion: VR应用生态系统的隐私保护仍处于不成熟状态，需要开发者、用户和商店运营商共同关注并实施更严格的隐私合规监管。

Abstract: Virtual Reality (VR) has gained increasing traction among various domains in
recent years, with major companies such as Meta, Pico, and Microsoft launching
their application stores to support third-party developers in releasing their
applications (or simply apps). These apps offer rich functionality but
inherently collect privacy-sensitive data, such as user biometrics, behaviors,
and the surrounding environment. Nevertheless, there is still a lack of
domain-specific regulations to govern the data handling of VR apps, resulting
in significant variations in their privacy practices among app stores.
  In this work, we present the first comprehensive multi-store study of privacy
practices in the current VR app ecosystem, covering a large-scale dataset
involving 6,565 apps collected from five major app stores. We assess both
declarative and behavioral privacy practices of VR apps, using a multi-faceted
approach based on natural language processing, reverse engineering, and static
analysis. Our assessment reveals significant privacy compliance issues across
all stores, underscoring the premature status of privacy protection in this
rapidly growing ecosystem. For instance, one third of apps fail to declare
their use of sensitive data, and 21.5\% of apps neglect to provide valid
privacy policies. Our work sheds light on the status quo of privacy protection
within the VR app ecosystem for the first time. Our findings should raise an
alert to VR app developers and users, and encourage store operators to
implement stringent regulations on privacy compliance among VR apps.

</details>


### [54] [Efficient and Encrypted Inference using Binarized Neural Networks within In-Memory Computing Architectures](https://arxiv.org/abs/2510.23034)
*Gokulnath Rajendran,Suman Deb,Anupam Chattopadhyay*

Main category: cs.CR

TL;DR: 提出了一种保护二值化神经网络模型参数的鲁棒策略，使用物理不可克隆函数生成的密钥在存储前转换参数，在加密权重上执行推理操作，实现最小运行时开销的完全同态加密。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过加密存储模型参数并在运行时解密，会引入显著计算开销，违背内存计算整合计算与存储的核心原则。需要一种在保护BNN模型参数的同时不损害内存计算效率的方法。

Method: 利用物理不可克隆函数生成密钥，在存储到交叉阵列前转换模型参数，然后在加密权重上直接执行推理操作，实现特殊的完全同态加密。

Result: 分析显示，在没有密钥的情况下进行推理会导致性能急剧下降，准确率低于15%，验证了保护策略的有效性。

Conclusion: 该方法有效保护了内存计算架构中的BNN模型参数，同时保持了计算效率，为安全高效的边缘计算提供了解决方案。

Abstract: Binarized Neural Networks (BNNs) are a class of deep neural networks designed
to utilize minimal computational resources, which drives their popularity
across various applications. Recent studies highlight the potential of mapping
BNN model parameters onto emerging non-volatile memory technologies,
specifically using crossbar architectures, resulting in improved inference
performance compared to traditional CMOS implementations. However, the common
practice of protecting model parameters from theft attacks by storing them in
an encrypted format and decrypting them at runtime introduces significant
computational overhead, thus undermining the core principles of in-memory
computing, which aim to integrate computation and storage. This paper presents
a robust strategy for protecting BNN model parameters, particularly within
in-memory computing frameworks. Our method utilizes a secret key derived from a
physical unclonable function to transform model parameters prior to storage in
the crossbar. Subsequently, the inference operations are performed on the
encrypted weights, achieving a very special case of Fully Homomorphic
Encryption (FHE) with minimal runtime overhead. Our analysis reveals that
inference conducted without the secret key results in drastically diminished
performance, with accuracy falling below 15%. These results validate the
effectiveness of our protection strategy in securing BNNs within in-memory
computing architectures while preserving computational efficiency.

</details>


### [55] [A high-capacity linguistic steganography based on entropy-driven rank-token mapping](https://arxiv.org/abs/2510.23035)
*Jun Jiang,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CR

TL;DR: RTMStega是一种基于熵的语言隐写框架，通过秩自适应编码和上下文感知解压缩，显著提升了隐写容量和效率。


<details>
  <summary>Details</summary>
Motivation: 当前语言隐写方法存在容量低和安全性不足的问题：传统修改方法会产生可检测异常，检索方法容量低，生成方法受限于token预测熵。

Method: 提出RTMStega框架，结合秩自适应编码和上下文感知解压缩，通过将秘密消息映射到token概率秩，并基于上下文熵动态调整采样策略。

Result: 实验表明RTMStega将主流生成隐写容量提升3倍，处理时间减少50%以上，同时保持高文本质量。

Conclusion: RTMStega为安全高效的隐蔽通信提供了可信解决方案，在容量、效率和不可感知性之间取得了良好平衡。

Abstract: Linguistic steganography enables covert communication through embedding
secret messages into innocuous texts; however, current methods face critical
limitations in payload capacity and security. Traditional modification-based
methods introduce detectable anomalies, while retrieval-based strategies suffer
from low embedding capacity. Modern generative steganography leverages language
models to generate natural stego text but struggles with limited entropy in
token predictions, further constraining capacity. To address these issues, we
propose an entropy-driven framework called RTMStega that integrates rank-based
adaptive coding and context-aware decompression with normalized entropy. By
mapping secret messages to token probability ranks and dynamically adjusting
sampling via context-aware entropy-based adjustments, RTMStega achieves a
balance between payload capacity and imperceptibility. Experiments across
diverse datasets and models demonstrate that RTMStega triples the payload
capacity of mainstream generative steganography, reduces processing time by
over 50%, and maintains high text quality, offering a trustworthy solution for
secure and efficient covert communication.

</details>


### [56] [KAPG: Adaptive Password Guessing via Knowledge-Augmented Generation](https://arxiv.org/abs/2510.23036)
*Xudong Yang,Jincheng Li,Kaiwen Xing,Zhenjia Xiao,Mingjian Duan,Weili Han,Hu Xiong*

Main category: cs.CR

TL;DR: KAPG是一个知识增强的密码猜测框架，通过将外部词汇知识整合到猜测过程中，结合泄露密码的内部统计知识和反映现实世界趋势的外部信息，显著提升了密码猜测效果。


<details>
  <summary>Details</summary>
Motivation: 传统密码猜测模型主要关注泄露密码中的模式，忽视了社会背景、文化趋势和流行词汇等外部因素对密码选择的影响，这限制了模型对新密码趋势的适应性和长期有效性。

Method: 使用密码前缀作为知识查找的锚点，在生成过程中动态注入相关外部线索，同时保持真实密码的结构规律性，将内部统计知识与外部信息相结合。

Result: 在12个泄露数据集上的实验显示，KAPG在站内和跨站场景下分别比最先进模型平均提升了36.5%和74.7%，在密码重叠和模型效率分析中表现出鲁棒性和计算效率。

Conclusion: KAPG框架通过知识增强显著提升了密码猜测性能，同时开发的KAPSM密码强度计在不同评估设置下准确度显著优于现有工具。

Abstract: As the primary mechanism of digital authentication, user-created passwords
exhibit common patterns and regularities that can be learned from leaked
datasets. Password choices are profoundly shaped by external factors, including
social contexts, cultural trends, and popular vocabulary. Prevailing password
guessing models primarily emphasize patterns derived from leaked passwords,
while neglecting these external influences -- a limitation that hampers their
adaptability to emerging password trends and erodes their effectiveness over
time.
  To address these challenges, we propose KAPG, a knowledge-augmented password
guessing framework that adaptively integrates external lexical knowledge into
the guessing process. KAPG couples internal statistical knowledge learned from
leaked passwords with external information that reflects real-world trends. By
using password prefixes as anchors for knowledge lookup, it dynamically injects
relevant external cues during generation while preserving the structural
regularities of authentic passwords. Experiments on twelve leaked datasets show
that KnowGuess achieves average improvements of 36.5\% and 74.7\% over
state-of-the-art models in intra-site and cross-site scenarios, respectively.
Further analyses of password overlap and model efficiency highlight its
robustness and computational efficiency. To counter these attacks, we further
develop KAPSM, a trend-aware and site-specific password strength meter.
Experiments demonstrate that KAPSM significantly outperforms existing tools in
accuracy across diverse evaluation settings.

</details>


### [57] [zkSTAR: A zero knowledge system for time series attack detection enforcing regulatory compliance in critical infrastructure networks](https://arxiv.org/abs/2510.23060)
*Paritosh Ramanan,H. M. Mohaimanul Islam,Abhiram Reddy Alugula*

Main category: cs.CR

TL;DR: zkSTAR是一个基于zk-SNARKs的工业控制系统网络攻击检测框架，能够在保护数据机密性的同时提供可验证的检测保证。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统面临日益严重的网络威胁，监管机构需要验证检测机制的有效性，但公用事业公司不愿披露敏感操作数据。

Method: 采用基于残差的统计假设检验方法，设计双管齐下的zk-SNARK架构，强制执行状态空间动力学的时间一致性和检测测试的统计一致性。

Result: 通过形式化分析框架的可靠性和零知识属性，并在真实ICS数据集上进行计算实验验证了实际可行性。

Conclusion: zkSTAR为工业控制系统驱动的关键基础设施网络提供了一种可扩展、保护隐私的监管合规替代方案。

Abstract: Industrial control systems (ICS) form the operational backbone of critical
infrastructure networks (CIN) such as power grids, water supply systems, and
gas pipelines. As cyber threats to these systems escalate, regulatory agencies
are imposing stricter compliance requirements to ensure system-wide security
and reliability. A central challenge, however, is enabling regulators to verify
the effectiveness of detection mechanisms without requiring utilities to
disclose sensitive operational data. In this paper, we introduce zkSTAR, a
cyberattack detection framework that leverages zk-SNARKs to reconcile these
requirements and enable provable detection guarantees while preserving data
confidentiality. Our approach builds on established residual-based statistical
hypothesis testing methods applied to state-space detection models.
Specifically, we design a two-pronged zk-SNARK architecture that enforces
temporal consistency of the state-space dynamics and statistical consistency of
the detection tests, allowing regulators to temporally verify alarm correctness
without visibility into utility-level data. We formally analyze the soundness
and zero knowledge properties of our framework and validate its practical
feasibility through computational experiments on real-world ICS datasets. As a
result, our work demonstrates a scalable, privacy-preserving alternative for
regulatory compliance for ICS driven critical infrastructure networks.

</details>


### [58] [Fast-MIA: Efficient and Scalable Membership Inference for LLMs](https://arxiv.org/abs/2510.23074)
*Hiromu Takahashi,Shotaro Ishihara*

Main category: cs.CR

TL;DR: Fast-MIA是一个用于高效评估大型语言模型成员推理攻击的Python库，通过批量推理和统一评估框架解决计算成本高和实现标准化问题。


<details>
  <summary>Details</summary>
Motivation: 由于对版权、安全和数据隐私的日益关注，针对LLM的成员推理攻击成为重要挑战，但研究进展受到高计算成本和缺乏标准化实现的阻碍。

Method: 提供快速批量推理功能，并在统一评估框架下实现了代表性MIA方法，支持简单配置和可扩展性。

Result: 开发了开源工具Fast-MIA，支持大规模、可复现的基准测试，促进LLM研究的可扩展性和透明度。

Conclusion: Fast-MIA作为开源工具，能够有效支持LLM成员推理攻击研究的标准化和规模化发展。

Abstract: We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library
for efficiently evaluating membership inference attacks (MIA) against Large
Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due
to growing concerns over copyright, security, and data privacy, and has
attracted increasing research attention. However, the progress of this research
is significantly hindered by two main obstacles: (1) the high computational
cost of inference in LLMs, and (2) the lack of standardized and maintained
implementations of MIA methods, which makes large-scale empirical comparison
difficult. To address these challenges, our library provides fast batch
inference and includes implementations of representative MIA methods under a
unified evaluation framework. This library supports easy implementation of
reproducible benchmarks with simple configuration and extensibility. We release
Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and
transparent research on LLMs.

</details>


### [59] [Beyond Imprecise Distance Metrics: LLM-Predicted Target Call Stacks for Directed Greybox Fuzzing](https://arxiv.org/abs/2510.23101)
*Yifan Zhang,Xin Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种基于大语言模型（LLMs）的定向灰盒模糊测试方法，用精确的调用栈表示替代静态分析的距离度量，显著提高了漏洞触发效率。


<details>
  <summary>Details</summary>
Motivation: 现有的定向灰盒模糊测试（DGF）方法由于依赖静态分析的复杂距离度量，导致概率计算不精确。静态分析的过度近似使得大量不相关的执行路径被错误地认为可能触发目标漏洞，显著降低了模糊测试效率。

Method: 使用调用栈表示精确控制流，避免静态分析中的错误信息。通过静态分析构建调用图识别可能到达目标位置的方法，然后利用LLMs预测最可能触发漏洞的调用栈序列。执行路径与预测调用栈重叠度更高的种子被优先选择进行变异。

Result: 在真实世界程序套件上，该方法触发漏洞的速度比基线方法快1.86倍到3.09倍。通过定向补丁测试，在最新版本程序中发现了10个新漏洞和2个不完整修复，获得了10个CVE编号。

Conclusion: 这是首个将LLMs集成到DGF核心种子优先级机制的工作，证明了基于LLMs的调用栈预测能够有效指导种子优先级排序，显著提高定向模糊测试的效率。

Abstract: Directed greybox fuzzing (DGF) aims to efficiently trigger bugs at specific
target locations by prioritizing seeds whose execution paths are more likely to
mutate into triggering target bugs. However, existing DGF approaches suffer
from imprecise probability calculations due to their reliance on complex
distance metrics derived from static analysis. The over-approximations inherent
in static analysis cause a large number of irrelevant execution paths to be
mistakenly considered to potentially mutate into triggering target bugs,
significantly reducing fuzzing efficiency. We propose to replace static
analysis-based distance metrics with precise call stack representations. Call
stacks represent precise control flows, thereby avoiding false information in
static analysis. We leverage large language models (LLMs) to predict
vulnerability-triggering call stacks for guiding seed prioritization. Our
approach constructs call graphs through static analysis to identify methods
that can potentially reach target locations, then utilizes LLMs to predict the
most likely call stack sequence that triggers the vulnerability. Seeds whose
execution paths have higher overlap with the predicted call stack are
prioritized for mutation. This is the first work to integrate LLMs into the
core seed prioritization mechanism of DGF. We implement our approach and
evaluate it against several state-of-the-art fuzzers. On a suite of real-world
programs, our approach triggers vulnerabilities $1.86\times$ to $3.09\times$
faster compared to baselines. In addition, our approach identifies 10 new
vulnerabilities and 2 incomplete fixes in the latest versions of programs used
in our controlled experiments through directed patch testing, with 10 assigned
CVE IDs.

</details>


### [60] [Optimizing Optimism: Up to 6.5x Faster zkVM Validty Proofs via Sparse Derivation](https://arxiv.org/abs/2510.23172)
*Mohsen Ahmadvand,Pedro Souto*

Main category: cs.CR

TL;DR: 论文分析了Optimism派生管道在zkVM中的效率问题，提出了针对零知识证明优化的重新设计，实现了6.5倍的派生速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前的Optimism派生管道设计主要关注正确性和活性，没有针对简洁有效性证明进行优化，导致在zkVM中实现时产生显著的开销和成本。

Method: 系统识别当前设计中的低效问题，分析其对证明成本的影响，并提供保持安全性的重新设计方案，专门针对零知识证明进行优化。

Result: 重新设计在zkVM中实现了最高6.5倍的派生速度提升（整体速度提升3.5倍），同时保持相同的安全保证。

Conclusion: 通过针对零知识证明特性进行专门优化，可以显著降低Optimism派生管道在zkVM中的证明成本，同时不牺牲安全性。

Abstract: The Optimism derivation pipeline is engineered for correctness and liveness,
not for succinct validity proofs. A straightforward port to a zkVM imposes
significant overheads, making validity proofs significantly more costly than
necessary. We systematically identify inefficiencies in the current design,
analyze their impact on proving costs, and provide a soundness-preserving
redesign tailored to zk proving. Our redesign achieves up to 6.5x faster
derivation inside zkVMs (3.5x overall speedup) while maintaining identical
safety guarantees.

</details>


### [61] [Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy](https://arxiv.org/abs/2510.23274)
*Weixuan Chen,Qianqian Yang,Shuo Shao,Shunpu Tang,Zhiguo Shi,Shui Yu*

Main category: cs.CR

TL;DR: 提出了一种基于差分隐私的安全语义通信框架，通过可学习模式的DP噪声保护图像传输中的隐私信息，在保证合法用户任务性能的同时显著降低窃听者的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有安全语义通信方法依赖限制性假设（如有利信道条件或已知窃听者模型），存在隐私保护不足的问题，需要更实用的安全语义通信方案。

Method: 使用GAN反演方法从源图像提取解耦语义表示，选择性地对私有语义表示添加可学习模式的差分隐私噪声，通过神经网络对抗训练实现隐私保护。

Result: 相比传统DP方法和直接传输，该方法显著降低窃听者重建质量（LPIPS优势0.06-0.29，FPPSR优势0.10-0.86），同时仅对合法用户任务性能造成轻微影响。

Conclusion: 该框架通过可学习DP噪声和可控隐私预算，有效解决了语义通信中的隐私保护问题，为安全语义通信提供了实用解决方案。

Abstract: While semantic communication (SemCom) improves transmission efficiency by
focusing on task-relevant information, it also raises critical privacy
concerns. Many existing secure SemCom approaches rely on restrictive or
impractical assumptions, such as favorable channel conditions for the
legitimate user or prior knowledge of the eavesdropper's model. To address
these limitations, this paper proposes a novel secure SemCom framework for
image transmission over wiretap channels, leveraging differential privacy (DP)
to provide approximate privacy guarantees. Specifically, our approach first
extracts disentangled semantic representations from source images using
generative adversarial network (GAN) inversion method, and then selectively
perturbs private semantic representations with approximate DP noise. Distinct
from conventional DP-based protection methods, we introduce DP noise with
learnable pattern, instead of traditional white Gaussian or Laplace noise,
achieved through adversarial training of neural networks (NNs). This design
mitigates the inherent non-invertibility of DP while effectively protecting
private information. Moreover, it enables explicitly controllable security
levels by adjusting the privacy budget according to specific security
requirements, which is not achieved in most existing secure SemCom approaches.
Experimental results demonstrate that, compared with the previous DP-based
method and direct transmission, the proposed method significantly degrades the
reconstruction quality for the eavesdropper, while introducing only slight
degradation in task performance. Under comparable security levels, our approach
achieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86
for the legitimate user compared with the previous DP-based method.

</details>


### [62] [Network Intrusion Detection: Evolution from Conventional Approaches to LLM Collaboration and Emerging Risks](https://arxiv.org/abs/2510.23313)
*Yaokai Feng,Kouichi Sakurai*

Main category: cs.CR

TL;DR: 这篇综述系统梳理了网络入侵检测系统(NIDS)的演进历程，从传统的基于签名和神经网络的方法到最近与大语言模型(LLM)的集成，总结了当前技术现状、优势与局限，并探讨了LLM在NIDS中的实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂，传统NIDS方法面临局限性，而LLM技术为网络安全领域带来新的可能性。本文旨在系统分析NIDS技术演进，特别是LLM集成带来的机遇与挑战。

Method: 采用文献综述方法，系统梳理了从传统签名检测、神经网络方法到LLM集成的NIDS技术发展路径，分析了在不同环境(传统网络、自动驾驶车辆、物联网)中的应用研究。

Result: 研究发现：1)基于签名的IDS仍在现代系统中发挥重要作用；2)神经网络检测虽发展二十多年但仍面临实际部署挑战；3)LLM在NIDS中具有应用价值但存在实际挑战和安全风险；4)构建领域特定LLM的策略已被提出。

Conclusion: LLM为NIDS带来新的可能性，但面临实际部署挑战和安全风险。领域特定LLM的构建策略是解决这些挑战的关键方向，未来需要继续探索LLM在网络安全中的平衡应用。

Abstract: This survey systematizes the evolution of network intrusion detection systems
(NIDS), from conventional methods such as signature-based and neural network
(NN)-based approaches to recent integrations with large language models (LLMs).
It clearly and concisely summarizes the current status, strengths, and
limitations of conventional techniques, and explores the practical benefits of
integrating LLMs into NIDS. Recent research on the application of LLMs to NIDS
in diverse environments is reviewed, including conventional network
infrastructures, autonomous vehicle environments and IoT environments.
  From this survey, readers will learn that: 1) the earliest methods,
signature-based IDSs, continue to make significant contributions to modern
systems, despite their well-known weaknesses; 2) NN-based detection, although
considered promising and under development for more than two decades, and
despite numerous related approaches, still faces significant challenges in
practical deployment; 3) LLMs are useful for NIDS in many cases, and a number
of related approaches have been proposed; however, they still face significant
challenges in practical applications. Moreover, they can even be exploited as
offensive tools, such as for generating malware, crafting phishing messages, or
launching cyberattacks. Recently, several studies have been proposed to address
these challenges, which are also reviewed in this survey; and 4) strategies for
constructing domain-specific LLMs have been proposed and are outlined in this
survey, as it is nearly impossible to train a NIDS-specific LLM from scratch.

</details>


### [63] [Authentication Against Insecure Bootstrapping for 5G Networks: Feasibility, Resiliency, and Transitional Solutions in Post-Quantum Era](https://arxiv.org/abs/2510.23457)
*Saleh Darzi,Mirza Masfiqur Rahman,Imtiaz Karim,Rouzbeh Behnia,Attila A Yavuz,Elisa Bertino*

Main category: cs.CR

TL;DR: 本文首次全面评估了将NIST后量子密码标准和传统数字签名集成到5G基站认证中的网络性能，发现直接采用PQC存在可行性问题，并提出BORG作为过渡解决方案。


<details>
  <summary>Details</summary>
Motivation: 5G协议在初始引导阶段缺乏强大的基站认证机制，易受伪基站攻击。传统PKI和基于身份的签名方案无法抵御量子攻击，而NIST-PQC标准在5G基站认证中的适用性尚未探索。

Method: 对NIST-PQC标准和传统数字签名（包括阈值和基于身份方案）在5G基站认证中的网络性能进行全面表征分析，并提出基于分层身份基阈值签名方案的BORG过渡解决方案。

Result: 发现直接PQC采用受协议约束和大签名尺寸阻碍，传统方法因证书链开销存在性能限制。BORG提供后量子伪造检测和分布式信任，适合5G严格要求。

Conclusion: 直接PQC集成不可行，BORG作为有效过渡解决方案，为未来量子弹性5G认证铺平道路。

Abstract: The 5G protocol lacks a robust base station authentication mechanism during
the initial bootstrapping phase, leaving it susceptible to threats such as fake
base station attacks. Conventional solutions, including digital signatures
based on Public Key Infrastructures (PKIs) and identity-based signatures, are
inadequate against quantum-capable adversaries. While integrating NIST's
Post-Quantum Cryptography (PQC) standards is a leading approach for quantum
resistance, their suitability for 5G base station authentication remains
unexplored. Moreover, current solutions are predominantly centralized and lack
security features such as distributed authentication. This work presents, to
our knowledge, the first comprehensive network-level performance
characterization of integrating NIST-PQC standards and conventional digital
signatures (including threshold and identity-based schemes) into 5G base
station authentication. Our findings reveal significant feasibility concerns,
with direct PQC adoption hindered by protocol constraints and large signature
sizes. We also highlight the performance limitations of conventional methods
due to the overhead of certificate chains. To mitigate these challenges, we
propose BORG, a transitional authentication solution based on a Hierarchical
Identity-Based Threshold Signature scheme with a Fail-Stop property. BORG
offers post-mortem post-quantum forgery detection and distributed trust via
threshold and compact signatures, well-suited for 5G's stringent requirements.
Our performance analysis underscores an important warning on the infeasibility
of direct PQC integration and positions BORG as an effective transitional
solution toward future quantum-resilient 5G authentication.

</details>


### [64] [Towards a Functionally Complete and Parameterizable TFHE Processor](https://arxiv.org/abs/2510.23483)
*Valentin Reyes Häusler,Gabriel Ott,Aruna Jayasena,Andreas Peter*

Main category: cs.CR

TL;DR: 提出基于FPGA的TFHE全同态加密硬件加速器，通过改进的可编程自举模块将自举操作性能提升240%-480%，实现完全在FPGA上处理同态电路评估


<details>
  <summary>Details</summary>
Motivation: TFHE虽然具有最快的自举操作性能，但同态电路评估的计算开销仍然很高，阻碍了其广泛应用。现有实现受限于高内存带宽成本，需要硬件加速来克服这一瓶颈

Method: 设计基于FPGA的TFHE处理器架构，实现完全在FPGA上处理指令的功能完整处理器，特别改进了可编程自举模块

Result: 改进的自举模块性能比当前最优实现提升240%到480%的自举操作每秒，实现了高效、紧凑且可扩展的设计

Conclusion: 该设计为完整的基于FPGA的TFHE处理器架构奠定了基础，能够显著提升同态加密的计算性能

Abstract: Fully homomorphic encryption allows the evaluation of arbitrary functions on
encrypted data. It can be leveraged to secure outsourced and multiparty
computation. TFHE is a fast torus-based fully homomorphic encryption scheme
that allows both linear operations, as well as the evaluation of arbitrary
non-linear functions. It currently provides the fastest bootstrapping operation
performance of any other FHE scheme. Despite its fast performance, TFHE suffers
from a considerably higher computational overhead for the evaluation of
homomorphic circuits. Computations in the encrypted domain are orders of
magnitude slower than their unencrypted equivalents. This bottleneck hinders
the widespread adoption of (T)FHE for the protection of sensitive data. While
state-of-the-art implementations focused on accelerating and outsourcing single
operations, their scalability and practicality are constrained by high memory
bandwidth costs. In order to overcome this, we propose an FPGA-based hardware
accelerator for the evaluation of homomorphic circuits. Specifically, we design
a functionally complete TFHE processor for FPGA hardware capable of processing
instructions on the data completely on the FPGA. In order to achieve a higher
throughput from our TFHE processor, we implement an improved programmable
bootstrapping module which outperforms the current state-of-the-art by 240\% to
480\% more bootstrappings per second. Our efficient, compact, and scalable
design lays the foundation for implementing complete FPGA-based TFHE processor
architectures.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [65] [A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue](https://arxiv.org/abs/2510.21720)
*Anant Pareek*

Main category: cs.AI

TL;DR: 本文提出了一个结合人工智能与计算心理学的综合性框架，通过端到端开发流程构建了从预测建模到交互式心理分析的系统。


<details>
  <summary>Details</summary>
Motivation: 利用人工智能技术来建模、理解和交互复杂的人类心理状态，弥合孤立预测建模与交互式心理分析系统之间的差距。

Method: 采用多阶段方法：1）在四个心理学数据集上建立基准性能；2）微调transformer模型并解决工程挑战；3）使用参数高效技术微调生成式大语言模型作为交互式"人格大脑"；4）将整套模型部署为微服务生态系统。

Result: 成功稳定了基于transformer的回归模型用于情感计算，在标准方法失败的情况下实现了有意义的预测性能，并开发了可复现的大规模AI研究方法。

Conclusion: 该工作展示了从研究到部署的完整流程，整合了预测分析与生成对话，为计算心理学和人机交互的未来研究提供了实用模型。

Abstract: The confluence of Artificial Intelligence and Computational Psychology
presents an opportunity to model, understand, and interact with complex human
psychological states through computational means. This paper presents a
comprehensive, multi-faceted framework designed to bridge the gap between
isolated predictive modeling and an interactive system for psychological
analysis. The methodology encompasses a rigorous, end-to-end development
lifecycle. First, foundational performance benchmarks were established on four
diverse psychological datasets using classical machine learning techniques.
Second, state-of-the-art transformer models were fine-tuned, a process that
necessitated the development of effective solutions to overcome critical
engineering challenges, including the resolution of numerical instability in
regression tasks and the creation of a systematic workflow for conducting
large-scale training under severe resource constraints. Third, a generative
large language model (LLM) was fine-tuned using parameter-efficient techniques
to function as an interactive "Personality Brain." Finally, the entire suite of
predictive and generative models was architected and deployed as a robust,
scalable microservices ecosystem. Key findings include the successful
stabilization of transformer-based regression models for affective computing,
showing meaningful predictive performance where standard approaches failed, and
the development of a replicable methodology for democratizing large-scale AI
research. The significance of this work lies in its holistic approach,
demonstrating a complete research-to-deployment pipeline that integrates
predictive analysis with generative dialogue, thereby providing a practical
model for future research in computational psychology and human-AI interaction.

</details>


### [66] [PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation](https://arxiv.org/abs/2510.21721)
*Kentaro Ueda,Takehiro Takayanagi*

Main category: cs.AI

TL;DR: PREFINE框架通过构建伪用户代理和用户特定评分标准，在无需参数更新或直接用户反馈的情况下实现个性化故事生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式反馈或微调，存在用户负担、数据收集、计算成本和隐私等实际问题。需要一种无需参数更新就能实现个性化生成的方法。

Method: 构建伪用户代理从用户交互历史中学习，生成用户特定评分标准，让代理基于这些标准对输出进行批判和精炼。

Result: 在PerDOC和PerMPST数据集上的自动评估显示，PREFINE在保持一般故事质量的同时，获得了更高的胜率和统计显著分数。

Conclusion: 伪用户代理和用户特定评分标准对提升个性化性能至关重要，该方法可扩展到对话系统、教育和推荐等更广泛应用。

Abstract: While recent advances in Large Language Models (LLMs) have improved the
quality of creative text generation, significant challenges remain in producing
personalized stories that reflect individual user preferences. Conventional
approaches rely on explicit feedback or fine-tuning, which presents practical
issues regarding user burden, data collection, computational costs, and
privacy. In this work, we propose PREFINE (Persona-and-Rubric Guided
Critique-and-Refine), a novel framework that extends the Critique-and-Refine
paradigm to personalization. PREFINE constructs a pseudo-user agent from a
user's interaction history and generates user-specific rubrics (evaluation
criteria). By having this agent critique and refine outputs on the user's
behalf based on these tailored rubrics, our method achieves personalized
generation without requiring parameter updates or direct user feedback. We
conducted a comprehensive evaluation on the PerDOC and PerMPST story datasets.
We designed three baseline methods and several model variants to verify the
contribution of each component of our framework. In automatic evaluations
(LLM-as-a-Judge), PREFINE achieved higher win rates and statistically
significant scores than the baselines, without compromising general story
quality. Analysis of the model variants confirmed that both the pseudo-user
agent and the user-specific rubrics are crucial for enhancing personalization
performance. Beyond story generation, our approach holds potential for enabling
efficient personalization in broader applications, such as dialogue systems,
education, and recommendation.

</details>


### [67] [SIGN: Schema-Induced Games for Naming](https://arxiv.org/abs/2510.21855)
*Ryan Zhang,Herbert Woisetscläger*

Main category: cs.AI

TL;DR: 论文提出SIGN（模式引导命名游戏），通过轻量级结构引导多智能体形成一致命名规范，相比无约束自然语言通信，收敛速度更快且一致性提高5.8倍。


<details>
  <summary>Details</summary>
Motivation: 现实AI系统中，大型语言模型智能体在协作时可能形成不一致的命名规范，导致协调失败。协作编程和分布式规划等应用需要可靠、一致的通信，且系统扩展性是关键问题。

Method: 引入SIGN（模式引导命名游戏），研究轻量级结构如何引导规范形成。比较模式引导通信与无约束自然语言通信的效果。

Result: 模式引导通信比无约束自然语言收敛更快，一致性提高达5.8倍。

Conclusion: 最小化结构可以作为高效多智能体协调的简单控制机制，其应用范围可超越命名游戏。

Abstract: Real-world AI systems are tackling increasingly complex problems, often
through interactions among large language model (LLM) agents. When these agents
develop inconsistent conventions, coordination can break down. Applications
such as collaborative coding and distributed planning therefore require
reliable, consistent communication, and scalability is a central concern as
systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming
game that examines how lightweight structure can steer convention formation. We
compare schema-induced communication to unconstrained natural language and find
faster convergence with up to 5.8x higher agreement. These results suggest that
minimal structure can act as a simple control knob for efficient multi-agent
coordination, pointing toward broader applications beyond the naming game.

</details>


### [68] [Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks](https://arxiv.org/abs/2510.21866)
*Javier Marín*

Main category: cs.AI

TL;DR: 研究发现解码器自回归语言模型在知识密集型任务中存在能力上限，参数规模扩展带来的准确率提升有限，而损失函数持续下降，呈现指标分离现象。


<details>
  <summary>Details</summary>
Motivation: 探索解码器自回归语言模型在不同任务上的扩展规律，特别是知识密集型任务与程序性任务的能力差异，为资源分配提供依据。

Method: 系统评估OPT和Pythia模型家族（70M-30B参数），分析损失函数与准确率的关系，通过注意力干预实验测试模型鲁棒性。

Result: 知识检索任务准确率几乎无提升，MMLU数学基准准确率稳定在19-20%；程序性任务则呈现传统扩展模式；注意力扰动导致性能灾难性崩溃。

Conclusion: 对于OPT和Pythia架构的知识密集型应用，参数规模超过1-2B后准确率增益有限，这些发现揭示了特定能力扩展失败现象。

Abstract: We document empirical capability ceilings in decoder-only autoregressive
language models across knowledge-intensive tasks. Systematic evaluation of OPT
and Pythia model families (70M-30B parameters, spanning 240 times scaling)
reveals that knowledge retrieval tasks show negligible accuracy improvement
despite smooth loss reduction. On MMLU mathematics benchmarks, accuracy remains
flat at 19-20% (below 25% random chance) across all scales while cross-entropy
loss decreases by 31%. In contrast, procedural tasks like arithmetic show
conventional scaling where both metrics improve together. Attention
intervention experiments reveal high sensitivity to perturbation: swapping
attention patterns between models causes catastrophic performance collapse
(complete accuracy loss) rather than graceful degradation. These measurements
have immediate engineering implications: for knowledge-intensive applications
using OPT and Pythia architectures, parameter scaling beyond 1-2B offers
minimal accuracy gains despite continued loss improvement. Our findings
quantify capability-specific scaling failures in these model families to inform
resource allocation decisions. Whether these patterns reflect fundamental
constraints of decoder-only architectures or implementation-specific
limitations remains an open question requiring investigation across diverse
architectural approaches.

</details>


### [69] [GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.21881)
*Nannan Shi,Chuanyu Qin,Shipeng Song,Man Luo*

Main category: cs.AI

TL;DR: 该论文提出了GeoThoughts数据集和GeoThought-MLLM模型，通过包含详细推理链的训练数据显著提升了多模态大语言模型在几何推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在文本数学推理上表现良好，但在几何推理任务中性能显著下降，主要由于几何问题的内在复杂性（需要详细图像理解和多步推理）以及现有数据集缺乏规模、多样性和显式推理轨迹。

Method: 开发了GeoThoughts数据集（包含6,243个样本的Geo-Thought-6K和10,834个样本的增强版本Geo-Thought-Augmented-10K），每个样本包含视觉描述、逐步解决方案、显式推理链、反思步骤和最终答案。基于此数据集训练了GeoThought-MLLM多模态数学推理模型。

Result: GeoThought-MLLM在几何任务上超越了现有基准模型，证明使用Chain-of-Thought数据集训练能够提升模型在领域内和领域外几何推理能力。

Conclusion: 训练包含详细推理链的数据集能有效提升几何推理性能，错误主要源于数学概念误解或空间判断错误，通过CoT机制可以纠正这些错误得到正确答案。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities
in text-based mathematical problem solving; however, when adapted to visual
reasoning tasks, particularly geometric problem solving, their performance
substantially declines because geometric problems present unique challenges.
Specifically, these challenges stem from two key factors: first, the intrinsic
complexity of geometry requiring detailed image comprehension and multi-step
reasoning, and second, the limitations of existing datasets which lack
sufficient scale, diversity, and explicit reasoning traces, consequently
hindering effective model training. To address these challenges, we developed
the GeoThoughts dataset, a comprehensive geometric reasoning corpus with two
subsets: Geo-Thought-6K with 6,243 samples and its augmented version
Geo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual
descriptions, step-by-step solutions, explicit reasoning chains, reflection
steps, and final answers. Using this dataset, we developed GeoThought-MLLM, a
mathematical reasoning multimodal model that generates detailed thinking
processes during problem-solving. Our model outperforms existing benchmarks in
geometric tasks, demonstrating that training with our Chain-of-Thought dataset
improves geometric reasoning capabilities across both in-domain and
out-of-domain settings. Finally, we analyze failure cases and observe that
errors primarily arise from incorrect interpretation of mathematical concepts
or spatial misjudgment. By invoking CoT to correct these mistakes, the model
produces correct answers.

</details>


### [70] [Exploration through Generation: Applying GFlowNets to Structured Search](https://arxiv.org/abs/2510.21886)
*Mark Phillip Matovic*

Main category: cs.AI

TL;DR: 将生成流网络(GFlowNets)应用于三个图优化问题：旅行商问题、最小生成树和最短路径，通过训练学习采样与奖励函数成比例的解决方案。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型在组合优化问题中的应用潜力，特别是利用GFlowNets的计算可扩展性优势，在经典精确方法不可行的大规模问题实例中发挥作用。

Method: 使用轨迹平衡损失训练GFlowNets，按顺序构建解决方案：为生成树选择边、为路径选择节点、为旅行选择城市。

Result: 在多种规模的基准实例上，GFlowNets学习找到了最优解，生成的解决方案与经典算法（Dijkstra、Kruskal、TSP精确求解器）的结果一致。

Conclusion: 生成模型可以通过学习策略解决组合优化问题，主要优势在于计算可扩展性：GFlowNets通过训练分摊计算成本，而经典算法每个实例具有固定复杂度。

Abstract: This work applies Generative Flow Networks (GFlowNets) to three graph
optimization problems: the Traveling Salesperson Problem, Minimum Spanning
Tree, and Shortest Path. GFlowNets are generative models that learn to sample
solutions proportionally to a reward function. The models are trained using the
Trajectory Balance loss to build solutions sequentially, selecting edges for
spanning trees, nodes for paths, and cities for tours. Experiments on benchmark
instances of varying sizes show that GFlowNets learn to find optimal solutions.
For each problem type, multiple graph configurations with different numbers of
nodes were tested. The generated solutions match those from classical
algorithms (Dijkstra for shortest path, Kruskal for spanning trees, and exact
solvers for TSP). Training convergence depends on problem complexity, with the
number of episodes required for loss stabilization increasing as graph size
grows. Once training converges, the generated solutions match known optima from
classical algorithms across the tested instances. This work demonstrates that
generative models can solve combinatorial optimization problems through learned
policies. The main advantage of this learning-based approach is computational
scalability: while classical algorithms have fixed complexity per instance,
GFlowNets amortize computation through training. With sufficient computational
resources, the framework could potentially scale to larger problem instances
where classical exact methods become infeasible.

</details>


### [71] [Computational Hardness of Reinforcement Learning with Partial $q^π$-Realizability](https://arxiv.org/abs/2510.21888)
*Shayan Karimi,Xiaoqi Tan*

Main category: cs.AI

TL;DR: 本文研究了在部分$q^{\pi}$-可实现性框架下强化学习的计算复杂性，证明了在此设置下学习$\epsilon$-最优策略是计算困难的。


<details>
  <summary>Details</summary>
Motivation: 研究在部分$q^{\pi}$-可实现性框架下强化学习的计算复杂性，该框架假设策略集$\Pi$中所有策略的价值函数都是线性可实现的，提供了一个函数逼近自然出现的实用模型。

Method: 通过从$\delta$-Max-3SAT和$\delta$-Max-3SAT(b)问题归约到GLinear-$\kappa$-RL（贪婪策略）和SLinear-$\kappa$-RL（softmax策略）实例，建立计算复杂性结果。

Result: 证明了在参数化贪婪策略集下学习$\epsilon$-最优策略是NP困难的；在softmax策略集下，除非NP=RP，否则在随机指数时间假设下存在指数级下界。

Conclusion: 在部分$\pi$-可实现性框架下，通常无法获得积极的计算结果，这与在生成访问模型下的$q^{\pi}$-可实现性形成对比。

Abstract: This paper investigates the computational complexity of reinforcement
learning in a novel linear function approximation regime, termed partial
$q^{\pi}$-realizability. In this framework, the objective is to learn an
$\epsilon$-optimal policy with respect to a predefined policy set $\Pi$, under
the assumption that all value functions for policies in $\Pi$ are linearly
realizable. The assumptions of this framework are weaker than those in
$q^{\pi}$-realizability but stronger than those in $q^*$-realizability,
providing a practical model where function approximation naturally arises. We
prove that learning an $\epsilon$-optimal policy in this setting is
computationally hard. Specifically, we establish NP-hardness under a
parameterized greedy policy set (argmax) and show that - unless NP = RP - an
exponential lower bound (in feature vector dimension) holds when the policy set
contains softmax policies, under the Randomized Exponential Time Hypothesis.
Our hardness results mirror those in $q^*$-realizability and suggest
computational difficulty persists even when $\Pi$ is expanded beyond the
optimal policy. To establish this, we reduce from two complexity problems,
$\delta$-Max-3SAT and $\delta$-Max-3SAT(b), to instances of GLinear-$\kappa$-RL
(greedy policy) and SLinear-$\kappa$-RL (softmax policy). Our findings indicate
that positive computational results are generally unattainable in partial
$q^{\pi}$-realizability, in contrast to $q^{\pi}$-realizability under a
generative access model.

</details>


### [72] [Performance Trade-offs of Optimizing Small Language Models for E-Commerce](https://arxiv.org/abs/2510.21970)
*Josip Tomo Licardo,Nikola Tankovic*

Main category: cs.AI

TL;DR: 本文研究了使用10亿参数的小型开源模型作为商业大语言模型在电子商务意图识别任务中的资源高效替代方案，通过QLoRA微调和后训练量化技术，实现了与GPT-4.1相当的99%准确率，同时显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 商用大语言模型在专业领域部署时面临高计算成本、延迟和运营费用的问题，需要寻找更资源高效的替代方案。

Method: 使用QLoRA对10亿参数的Llama 3.2模型进行微调，在模拟真实用户查询的合成数据集上进行训练，然后应用GPTQ和GGUF后训练量化技术创建GPU和CPU优化版本。

Result: 专业化1B模型达到99%准确率，与GPT-4.1性能相当。4位GPTQ减少41%显存使用但推理速度下降82%，GGUF格式在CPU上实现18倍推理吞吐量提升和90%以上内存消耗减少。

Conclusion: 经过适当优化的小型开源模型不仅是可行的，而且是领域特定应用中更合适的替代方案，能以极低计算成本提供最先进的准确性。

Abstract: Large Language Models (LLMs) offer state-of-the-art performance in natural
language understanding and generation tasks. However, the deployment of leading
commercial models for specialized tasks, such as e-commerce, is often hindered
by high computational costs, latency, and operational expenses. This paper
investigates the viability of smaller, open-weight models as a
resource-efficient alternative. We present a methodology for optimizing a
one-billion-parameter Llama 3.2 model for multilingual e-commerce intent
recognition. The model was fine-tuned using Quantized Low-Rank Adaptation
(QLoRA) on a synthetically generated dataset designed to mimic real-world user
queries. Subsequently, we applied post-training quantization techniques,
creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results
demonstrate that the specialized 1B model achieves 99% accuracy, matching the
performance of the significantly larger GPT-4.1 model. A detailed performance
analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ
reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older
GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF
formats on a CPU achieved a speedup of up to 18x in inference throughput and a
reduction of over 90% in RAM consumption compared to the FP16 baseline. We
conclude that small, properly optimized open-weight models are not just a
viable but a more suitable alternative for domain-specific applications,
offering state-of-the-art accuracy at a fraction of the computational cost.

</details>


### [73] [Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions](https://arxiv.org/abs/2510.21977)
*Ji Huang,Mengfei Li,Shuai Shao*

Main category: cs.AI

TL;DR: 提出了一种名为分布偏移对齐（DSA）的两阶段微调方法，用于提高LLM模拟人类调查响应的准确性和效率，显著减少真实数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有零样本方法存在提示敏感性和低准确性问题，而传统微调方法过度拟合训练数据分布，无法产生比训练集更准确的结果，偏离了使用LLM模拟调查响应的初衷。

Method: DSA是一种两阶段微调方法，通过对齐输出分布和不同背景下的分布偏移，学习分布如何变化而非拟合训练数据。

Result: 在五个公开调查数据集上，DSA始终优于其他方法，可将真实数据需求减少53.48-69.12%，在准确性、鲁棒性和数据节省方面表现优异。

Conclusion: DSA能够提供比训练数据更接近真实分布的结果，证明了在调查模拟中的有效性和效率。

Abstract: Large language models (LLMs) offer a promising way to simulate human survey
responses, potentially reducing the cost of large-scale data collection.
However, existing zero-shot methods suffer from prompt sensitivity and low
accuracy, while conventional fine-tuning approaches mostly fit the training set
distributions and struggle to produce results more accurate than the training
set itself, which deviates from the original goal of using LLMs to simulate
survey responses. Building on this observation, we introduce Distribution Shift
Alignment (DSA), a two-stage fine-tuning method that aligns both the output
distributions and the distribution shifts across different backgrounds. By
learning how these distributions change rather than fitting training data, DSA
can provide results substantially closer to the true distribution than the
training data. Empirically, DSA consistently outperforms other methods on five
public survey datasets. We further conduct a comprehensive comparison covering
accuracy, robustness, and data savings. DSA reduces the required real data by
53.48-69.12%, demonstrating its effectiveness and efficiency in survey
simulation.

</details>


### [74] [Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective](https://arxiv.org/abs/2510.21999)
*Zhenya Huang,Jiayu Liu,Xin Lin,Zhiyuan Ma,Shangzi Xue,Tong Xiao,Qi Liu,Yee Whye Teh,Enhong Chen*

Main category: cs.AI

TL;DR: 这篇论文从人类认知角度系统综述了数学应用题求解研究，总结了5个关键认知能力，回顾了近10年神经网络和LLM求解器的发展，并统一比较了它们在5个主流基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 数学应用题领域缺乏系统性的分类综述和当前发展趋势讨论，需要从人类认知角度全面回顾相关研究，展示AI模型在模拟人类认知能力方面的进展。

Method: 从人类认知视角总结5个关键认知能力，回顾近10年神经网络和LLM求解器，重新运行代表性求解器并在5个主流基准上进行统一性能比较。

Result: 提供了首个从人类推理认知角度全面分析过去十年MWP研究的综述，并对现有方法进行了整体比较，发布了开源代码库。

Conclusion: 该综述有望启发AI推理领域的进一步研究，为理解AI模型如何模拟人类认知能力提供了系统框架。

Abstract: Math word problem (MWP) serves as a fundamental research topic in artificial
intelligence (AI) dating back to 1960s. This research aims to advance the
reasoning abilities of AI by mirroring the human-like cognitive intelligence.
The mainstream technological paradigm has evolved from the early rule-based
methods, to deep learning models, and is rapidly advancing towards large
language models. However, the field still lacks a systematic taxonomy for the
MWP survey along with a discussion of current development trends. Therefore, in
this paper, we aim to comprehensively review related research in MWP solving
through the lens of human cognition, to demonstrate how recent AI models are
advancing in simulating human cognitive abilities. Specifically, we summarize 5
crucial cognitive abilities for MWP solving, including Problem Understanding,
Logical Organization, Associative Memory, Critical Thinking, and Knowledge
Learning. Focused on these abilities, we review two mainstream MWP models in
recent 10 years: neural network solvers, and LLM based solvers, and discuss the
core human-like abilities they demonstrated in their intricate problem-solving
process. Moreover, we rerun all the representative MWP solvers and supplement
their performance on 5 mainstream benchmarks for a unified comparison. To the
best of our knowledge, this survey first comprehensively analyzes the
influential MWP research of the past decade from the perspective of human
reasoning cognition and provides an integrative overall comparison across
existing approaches. We hope it can inspire further research in AI reasoning.
Our repository is released on https://github.com/Ljyustc/FoI-MWP.

</details>


### [75] [LightAgent: Mobile Agentic Foundation Models](https://arxiv.org/abs/2510.22009)
*Yangqin Jiang,Chao Huang*

Main category: cs.AI

TL;DR: LightAgent是一个移动GUI代理基础模型解决方案，通过设备-云协作平衡本地模型成本效益和云端模型高性能，在移动平台上实现高效GUI交互。


<details>
  <summary>Details</summary>
Motivation: 解决移动GUI代理面临的困境：完全在设备上的小模型性能不足，而性能足够的大模型要么无法在移动设备部署，要么云服务成本过高。

Method: 通过两阶段SFT->GRPO训练增强Qwen2.5-VL-3B模型的决策能力，集成高效长推理机制利用历史交互，默认在设备上执行，仅通过实时复杂度评估将复杂子任务升级到云端处理。

Result: 在AndroidLab基准测试和多样化应用中，LightAgent性能接近或匹配更大模型，同时显著降低云成本。

Conclusion: LightAgent通过设备-云协作有效解决了移动GUI代理的性能与成本平衡问题，为移动平台提供了可行的代理基础模型解决方案。

Abstract: With the advancement of multimodal large language models (MLLMs), building
GUI agent systems has become an increasingly promising direction-especially for
mobile platforms, given their rich app ecosystems and intuitive touch
interactions. Yet mobile GUI agents face a critical dilemma: truly on-device
models (4B or smaller) lack sufficient performance, while capable models
(starting from 7B) are either too large for mobile deployment or prohibitively
costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose
LightAgent, a mobile agentic foundation model solution that leverages
device-cloud collaboration to tap the cost-efficiency of on-device models and
the high capability of cloud models, while avoiding their drawbacks.
Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO
training on synthetic GUI data for strong decision-making, integrates an
efficient long-reasoning mechanism to utilize historical interactions under
tight resources, and defaults to on-device execution-only escalating
challenging subtasks to the cloud via real-time complexity assessment.
Experiments on the online AndroidLab benchmark and diverse apps show LightAgent
matches or nears larger models, with a significant reduction in cloud costs.

</details>


### [76] [LLM-AR: LLM-powered Automated Reasoning Framework](https://arxiv.org/abs/2510.22034)
*Rick Chen,Joseph Ternasky,Aaron Ontoyin Yin,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: LLM-AR是一个受神经符号系统启发的框架，将LLM生成的启发式规则转化为概率规则，通过ProbLog自动推理引擎执行，用于预测初创企业成功，实现了59.5%的精确度和8.7%的召回率。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型能够识别模式和有效推理，但其准确性不稳定限制了在高风险决策应用中的采用。本文从风险投资角度研究这一问题，基于创始人特质预测初创企业成功。

Method: 提出LLM-AR管道，将LLM生成的启发式规则转化为概率规则，由ProbLog自动推理引擎执行。采用迭代策略进化循环，结合关联规则挖掘逐步优化预测规则。

Result: 在未见数据上，LLM-AR达到59.5%的精确度和8.7%的召回率，是随机基线精确度的5.9倍。框架可解释且可通过超参数调整，每个决策路径都可供人工检查。

Conclusion: 该框架具有可解释性和可调性，展示了扩展到其他领域的潜力，为高风险决策应用提供了可靠预测模型。

Abstract: Large language models (LLMs) can already identify patterns and reason
effectively, yet their variable accuracy hampers adoption in high-stakes
decision-making applications. In this paper, we study this issue from a venture
capital perspective by predicting idea-stage startup success based on founder
traits. (i) To build a reliable prediction model, we introduce LLM-AR, a
pipeline inspired by neural-symbolic systems that distils LLM-generated
heuristics into probabilistic rules executed by the ProbLog automated-reasoning
engine. (ii) An iterative policy-evolution loop incorporates association-rule
mining to progressively refine the prediction rules.
  On unseen folds, LLM-AR achieves 59.5% precision and 8.7% recall, 5.9x the
random baseline precision, while exposing every decision path for human
inspection. The framework is interpretable and tunable via hyperparameters,
showing promise to extend into other domains.

</details>


### [77] [Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability](https://arxiv.org/abs/2510.22039)
*Po-Chen Kuo,Han Hou,Will Dabney,Edgar Y. Walker*

Main category: cs.AI

TL;DR: 在部分可观测环境中，结合自监督预测编码模块的元强化学习能够学习到更紧凑、可解释的贝叶斯最优信念状态表示，优于传统元强化学习，并带来更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统元强化学习虽然能获得接近贝叶斯最优的策略，但往往无法学习到紧凑、可解释的贝叶斯最优信念状态表示，这种表示效率低下可能限制智能体的适应性和泛化能力。

Method: 受神经科学中预测编码理论和深度强化学习中辅助预测目标的启发，将自监督预测编码模块集成到元强化学习中，通过状态机模拟验证其有效性。

Result: 带预测模块的元强化学习在各种任务中都能生成更可解释的表示，更好地逼近贝叶斯最优信念状态。在需要主动信息搜索的挑战性任务中，只有带预测模块的方法能成功学习最优表示和策略。

Conclusion: 预测学习是指导智能体在部分可观测环境中进行有效表示学习的重要原则，更好的表示学习能带来改进的泛化能力。

Abstract: Learning a compact representation of history is critical for planning and
generalization in partially observable environments. While meta-reinforcement
learning (RL) agents can attain near Bayes-optimal policies, they often fail to
learn the compact, interpretable Bayes-optimal belief states. This
representational inefficiency potentially limits the agent's adaptability and
generalization capacity. Inspired by predictive coding in neuroscience--which
suggests that the brain predicts sensory inputs as a neural implementation of
Bayesian inference--and by auxiliary predictive objectives in deep RL, we
investigate whether integrating self-supervised predictive coding modules into
meta-RL can facilitate learning of Bayes-optimal representations. Through state
machine simulation, we show that meta-RL with predictive modules consistently
generates more interpretable representations that better approximate
Bayes-optimal belief states compared to conventional meta-RL across a wide
variety of tasks, even when both achieve optimal policies. In challenging tasks
requiring active information seeking, only meta-RL with predictive modules
successfully learns optimal representations and policies, whereas conventional
meta-RL struggles with inadequate representation learning. Finally, we
demonstrate that better representation learning leads to improved
generalization. Our results strongly suggest the role of predictive learning as
a guiding principle for effective representation learning in agents navigating
partial observability.

</details>


### [78] [HW/SW Co-design of a PCM/PWM converter: a System Level Approach based in the SpecC Methodology](https://arxiv.org/abs/2510.22046)
*Daniel G. P. Petrini,Braz Izaias da Silva Junior*

Main category: cs.AI

TL;DR: 应用SpecC方法学对PCM-to-PWM转换器进行系统级软硬件协同设计，在满足实时约束的同时降低纯硬件方案成本，避免高端处理器的纯软件实现开销。


<details>
  <summary>Details</summary>
Motivation: 研究系统级软硬件协同设计在中等复杂度设计中的价值，通过早期架构洞察、快速验证和成本/性能权衡分析来优化PCM-to-PWM转换器的实现方案。

Method: 使用SpecC方法学对PCM-to-PWM转换器进行建模和探索，通过系统级评估和快速功能仿真来评估不同的软硬件分区映射方案。

Result: 成功找到了满足实时约束的映射方案，相比纯硬件方案降低了估计成本，同时避免了在高端处理器上纯软件实现的高开销。

Conclusion: 即使对于中等复杂度的设计，系统级协同设计也能提供有价值的早期架构洞察、快速验证和可行的成本/性能权衡分析。

Abstract: We present a case study applying the SpecC methodology within a system-level
hardware/software co-design flow to a PCM-to-PWM converter, the core of a
Class-D audio amplifier. The converter was modeled and explored with SpecC
methodology to derive an HW/SW partition. Using system-level estimates and fast
functional simulation, we evaluated mappings that meet real-time constraints
while reducing estimated cost of an all-hardware solution and avoiding the
expense of a purely software implementation on a high-end processor. Despite
the design's moderate complexity, the results underline the value of
system-level co-design for early architectural insight, rapid validation, and
actionable cost/performance trade-offs. [Original work from 2005; formatting
revised in 2025, with no changes to the results.]

</details>


### [79] [Towards Error-Centric Intelligence II: Energy-Structured Causal Models](https://arxiv.org/abs/2510.22050)
*Marcus Thomas*

Main category: cs.AI

TL;DR: 论文提出从预测准确性转向因果可解释性的概念重构，引入计算解释和能量结构化因果模型，使内部结构可在机制层面进行干预和编辑。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习系统虽然达到最先进性能但因果不透明，无法对特定机制进行外科手术式编辑，因为学习到的潜在变量缺乏因果语义。

Method: 引入计算解释和能量结构化因果模型，其中机制表示为约束而非显式输入输出映射，干预通过对这些约束进行局部手术来实现。

Result: 在ESCM背景下具体实现了结构因果原则LAP和ICM，并证明在温和条件下ESCM恢复标准SCM语义。

Conclusion: 为因果推理提供形式化语言，使系统能够理解而不仅仅是预测，支持在解释层面进行机制级干预。

Abstract: Contemporary machine learning optimizes for predictive accuracy, yet systems
that achieve state of the art performance remain causally opaque: their
internal representations provide no principled handle for intervention. We can
retrain such models, but we cannot surgically edit specific mechanisms while
holding others fixed, because learned latent variables lack causal semantics.
We argue for a conceptual reorientation: intelligence is the ability to build
and refine explanations, falsifiable claims about manipulable structure that
specify what changes and what remains invariant under intervention.
Explanations subsume prediction but demand more: causal commitments that can be
independently tested and corrected at the level of mechanisms. We introduce
computational explanations, mappings from observations to intervention ready
causal accounts. We instantiate these explanations with Energy Structured
Causal Models (ESCMs), in which mechanisms are expressed as constraints (energy
functions or vector fields) rather than explicit input output maps, and
interventions act by local surgery on those constraints. This shift makes
internal structure manipulable at the level where explanations live: which
relations must hold, which can change, and what follows when they do. We
provide concrete instantiations of the structural-causal principles LAP and ICM
in the ESCM context, and also argue that empirical risk minimization
systematically produces fractured, entangled representations, a failure we
analyze as gauge ambiguity in encoder energy pairs. Finally, we show that under
mild conditions, ESCMs recover standard SCM semantics. Building on Part I's
principles (LAP, ICM, CAP) and its definition of intelligence as
explanation-building under criticism, this paper offers a formal language for
causal reasoning in systems that aspire to understand, not merely to predict.

</details>


### [80] [Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms](https://arxiv.org/abs/2510.22052)
*Abhijit Chatterjee,Niraj K. Jha,Jonathan D. Cohen,Thomas L. Griffiths,Hongjing Lu,Diana Marculescu,Ashiqur Rasul,Keshab K. Parhi*

Main category: cs.AI

TL;DR: 本文提出了下一代AI系统的愿景，从当前需要大量数据和能源的大型模型转向轻量级、高能效的领域特定智能体，能够在动态环境中进行推理、规划和决策。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型训练成本高昂（GPT-4训练需50-60GWh能源），存在幻觉问题，且无法部署到关键应用领域。相比之下，人脑仅消耗20W功率，需要更高效、更智能的AI系统。

Method: 提出开发轻量级领域特定多模态模型，具备实时数据处理、先验知识利用和持续学习能力，同时需要重新设计硬件以实现超过现有技术1000倍的能效提升。

Result: 构建了未来AI系统的完整愿景框架，明确了从大型通用模型向专业化智能体的转型路径。

Conclusion: 下一代AI将是从数据密集型大型模型向能效高、专业化、具备推理能力的智能体的演进，这需要硬件和软件架构的根本性变革。

Abstract: The field of artificial intelligence (AI) has taken a tight hold on broad
aspects of society, industry, business, and governance in ways that dictate the
prosperity and might of the world's economies. The AI market size is projected
to grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI
is dominated by large language models that exhibit linguistic and visual
intelligence. However, training these models requires a massive amount of data
scraped from the web as well as large amounts of energy (50--60 GWh to train
GPT-4). Despite these costs, these models often hallucinate, a characteristic
that prevents them from being deployed in critical application domains. In
contrast, the human brain consumes only 20~W of power. What is needed is the
next level of AI evolution in which lightweight domain-specific multimodal
models with higher levels of intelligence can reason, plan, and make decisions
in dynamic environments with real-time data and prior knowledge, while learning
continuously and evolving in ways that enhance future decision-making
capability. This will define the next wave of AI, progressing from today's
large models, trained with vast amounts of data, to nimble energy-efficient
domain-specific agents that can reason and think in a world full of
uncertainty. To support such agents, hardware will need to be reimagined to
allow energy efficiencies greater than 1000x over the state of the art. Such a
vision of future AI systems is developed in this work.

</details>


### [81] [Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies](https://arxiv.org/abs/2510.22095)
*Yankai Chen,Xinni Zhang,Yifei Zhang,Yangning Li,Henry Peng Zou,Chunyu Miao,Weizhi Zhang,Xue Liu,Philip S. Yu*

Main category: cs.AI

TL;DR: 该立场论文提出从脑机接口(BCI)向脑-智能体协作(BAC)的范式扩展，强调将智能体重新定义为主动协作伙伴而非被动脑信号处理器，需要关注伦理数据处理、模型可靠性和人-智能体协作框架。


<details>
  <summary>Details</summary>
Motivation: 脑机接口面临信息传输率低和用户特定校准等限制，虽然最近研究探索了大型语言模型的集成，但部署智能AI仍面临技术障碍和伦理问题，缺乏对这一新兴方向的全面讨论。

Method: 作为立场论文，通过论证和概念分析提出范式扩展，强调重新定义智能体角色，关注伦理数据管理、模型可靠性和协作框架。

Result: 提出了从BCI到BAC的范式转变概念，将智能体定位为主动协作伙伴，为脑-计算机交互领域提供了新的发展方向。

Conclusion: 脑-智能体协作是脑机接口领域的自然演进方向，需要确保这些系统安全、可信且有效，通过伦理框架和可靠技术实现真正的人-智能体协作。

Abstract: Brain-Computer Interfaces (BCIs) offer a direct communication pathway between
the human brain and external devices, holding significant promise for
individuals with severe neurological impairments. However, their widespread
adoption is hindered by critical limitations, such as low information transfer
rates and extensive user-specific calibration. To overcome these challenges,
recent research has explored the integration of Large Language Models (LLMs),
extending the focus from simple command decoding to understanding complex
cognitive states. Despite these advancements, deploying agentic AI faces
technical hurdles and ethical concerns. Due to the lack of comprehensive
discussion on this emerging direction, this position paper argues that the
field is poised for a paradigm extension from BCI to Brain-Agent Collaboration
(BAC). We emphasize reframing agents as active and collaborative partners for
intelligent assistance rather than passive brain signal data processors,
demanding a focus on ethical data handling, model reliability, and a robust
human-agent collaboration framework to ensure these systems are safe,
trustworthy, and effective.

</details>


### [82] [Controllable Mathematical Reasoning via Self-Optimizing Thought Vectors](https://arxiv.org/abs/2510.22132)
*Xuying LI*

Main category: cs.AI

TL;DR: 提出了一种基于自优化思想向量和熵最小化的可控数学推理方法，通过可学习的思想向量动态调节大语言模型的内部推理过程。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型在数学推理中缺乏可控性的问题，开发一种能够动态调节推理过程的方法，使模型能够按照特定控制条件进行专注推理。

Method: 使用可学习的思想向量来动态调制大语言模型的内部推理过程，通过熵最小化奖励来引导推理模式，无需外部奖励标注。在Gemma-2-9B模型上使用GSM8K数据集进行实验。

Result: 在GSM8K数据集上达到90.1%的准确率，可控性得分为0.42。分析显示思想向量形成明显的聚类，并且在不同控制条件下都保持低熵分布。

Conclusion: 该方法成功实现了可控的AI推理，熵基奖励能够有效引导专注的推理模式，验证了该框架在可控AI推理方面的有效性。

Abstract: We present a novel approach for controllable mathematical reasoning that
leverages self-optimizing thought vectors with entropy minimization. Our method
introduces learnable thought vectors that dynamically modulate the internal
reasoning process of large language models. Using Gemma-2-9B on GSM8K, we
achieve 90.1% accuracy with a controllability score of 0.42, demonstrating that
entropy-based rewards effectively guide focused reasoning patterns without
requiring external reward annotations. Our analysis reveals distinct thought
vector clusters and consistent low-entropy distributions across control
conditions, validating our framework for controllable AI reasoning.

</details>


### [83] [Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests](https://arxiv.org/abs/2510.22170)
*Alexandra Yost,Shreyans Jain,Shivam Raval,Grant Corser,Allen Roush,Nina Xu,Jacqueline Hammack,Ravid Shwartz-Ziv,Amirali Abdullah*

Main category: cs.AI

TL;DR: 提出了一个AI心理测量框架，使用情境判断测试和复杂人物角色设计来评估AI系统在需要情感判断和伦理考量领域的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常重复使用人类特质量表或临时角色，限制了行为真实性和领域相关性。

Method: 框架包含三个部分：(1)使用现实场景的情境判断测试评估领域特定能力；(2)整合工业组织心理学和人格心理学设计复杂角色；(3)采用结构化生成方法，结合人口统计先验和回忆录式叙事。

Result: 在执法助手案例研究中，构建了包含8,500个角色、4,000个情境判断测试和300,000个响应的丰富数据集。

Conclusion: 将发布数据集和所有代码，促进AI心理测量研究的发展。

Abstract: AI psychometrics evaluates AI systems in roles that traditionally require
emotional judgment and ethical consideration. Prior work often reuses human
trait inventories (Big Five, \hexaco) or ad hoc personas, limiting behavioral
realism and domain relevance. We propose a framework that (1) uses situational
judgment tests (SJTs) from realistic scenarios to probe domain-specific
competencies; (2) integrates industrial-organizational and personality
psychology to design sophisticated personas which include behavioral and
psychological descriptors, life history, and social and emotional functions;
and (3) employs structured generation with population demographic priors and
memoir inspired narratives, encoded with Pydantic schemas. In a law enforcement
assistant case study, we construct a rich dataset of personas drawn across 8
persona archetypes and SJTs across 11 attributes, and analyze behaviors across
subpopulation and scenario slices. The dataset spans 8,500 personas, 4,000
SJTs, and 300,000 responses. We will release the dataset and all code to the
public.

</details>


### [84] [Dopamine-driven synaptic credit assignment in neural networks](https://arxiv.org/abs/2510.22178)
*Saranraj Nambusubramaniyan,Shervin Safavi,Raja Guru,Andreas Knoblauch*

Main category: cs.AI

TL;DR: 提出了一种基于神经强化学习的无导数优化器Dopamine，用于解决神经网络的信用分配问题，通过权重扰动学习和奖励预测误差调节学习率，在计算效率和内存消耗上优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统反向传播算法在计算效率、内存消耗以及权重传输和更新锁定方面的问题，同时提高神经生物学合理性。

Method: 采用权重扰动学习，利用奖励预测误差（RPE）调节学习率，开发了名为Dopamine的无导数优化器，通过最小化扰动模型与未扰动模型之间的期望结果差异来优化网络。

Result: 在XOR任务和混沌时间序列预测任务中，Dopamine优化器训练的模型收敛更快，性能优于标准权重扰动方法，与基于梯度的算法性能相当，但计算和内存消耗显著减少。

Conclusion: Dopamine优化器不仅找到了稳健的解决方案，性能与最先进的机器学习优化器相当，而且在神经生物学上更合理。

Abstract: Solving the synaptic Credit Assignment Problem(CAP) is central to learning in
both biological and artificial neural systems. Finding an optimal solution for
synaptic CAP means setting the synaptic weights that assign credit to each
neuron for influencing the final output and behavior of neural networks or
animals. Gradient-based methods solve this problem in artificial neural
networks using back-propagation, however, not in the most efficient way. For
instance, back-propagation requires a chain of top-down gradient computations.
This leads to an expensive optimization process in terms of computing power and
memory linked with well-known weight transport and update locking problems. To
address these shortcomings, we take a NeuroAI approach and draw inspiration
from neural Reinforcement Learning to develop a derivative-free optimizer for
training neural networks, Dopamine. Dopamine is developed for Weight
Perturbation (WP) learning that exploits stochastic updating of weights towards
optima. It achieves this by minimizing the regret, a form of Reward Prediction
Error (RPE) between the expected outcome from the perturbed model and the
actual outcome from the unperturbed model. We use this RPE to adjust the
learning rate in the network (i.e., creating an adaptive learning rate
strategy, similar to the role of dopamine in the brain). We tested the Dopamine
optimizer for training multi-layered perceptrons for XOR tasks, and recurrent
neural networks for chaotic time series forecasting. Dopamine-trained models
demonstrate accelerated convergence and outperform standard WP, and give
comparable performance to gradient-based algorithms, while consuming
significantly less computation and memory. Overall, the Dopamine optimizer not
only finds robust solutions and comparable performance to the state-of-the-art
Machine Learning optimizers but is also neurobiologically more plausible.

</details>


### [85] [OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling](https://arxiv.org/abs/2510.22192)
*Haoyang Liu,Jie Wang,Yuyang Cai,Xiongwei Han,Yufei Kuang,Jianye Hao*

Main category: cs.AI

TL;DR: OptiTree提出了一种基于树搜索的自适应问题分解方法，通过将复杂运筹学问题分解为更简单的子问题来提升建模能力，相比现有方法在基准测试中取得了超过10%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 运筹学优化建模是技术性很强的过程，现有基于大语言模型的方法采用固定步骤分解，难以处理复杂数学结构的问题，需要更灵活的自适应分解方法。

Method: 开发了一个建模树，基于问题分类和复杂度层次组织运筹学问题，通过递归搜索树结构识别简单子问题，并自适应整合层次化建模思路来合成全局模型。

Result: 实验表明OptiTree在具有挑战性的基准测试中显著提升了建模准确率，相比最先进方法取得了超过10%的改进。

Conclusion: OptiTree通过树搜索和自适应问题分解有效解决了复杂运筹学问题的建模挑战，为自动化优化建模提供了新的解决方案。

Abstract: Optimization modeling is one of the most crucial but technical parts of
operations research (OR). To automate the modeling process, existing works have
leveraged large language models (LLMs), prompting them to break down tasks into
steps for generating variables, constraints, and objectives. However, due to
the highly complex mathematical structures inherent in OR problems, standard
fixed-step decomposition often fails to achieve high performance. To address
this challenge, we introduce OptiTree, a novel tree search approach designed to
enhance modeling capabilities for complex problems through adaptive problem
decomposition into simpler subproblems. Specifically, we develop a modeling
tree that organizes a wide range of OR problems based on their hierarchical
problem taxonomy and complexity, with each node representing a problem category
and containing relevant high-level modeling thoughts. Given a problem to model,
we recurrently search the tree to identify a series of simpler subproblems and
synthesize the global modeling thoughts by adaptively integrating the
hierarchical thoughts. Experiments show that OptiTree significantly improves
the modeling accuracy compared to the state-of-the-art, achieving over 10\%
improvements on the challenging benchmarks. The code is released at
https://github.com/MIRALab-USTC/OptiTree/tree/main.

</details>


### [86] [PACR: Progressively Ascending Confidence Reward for LLM Reasoning](https://arxiv.org/abs/2510.22255)
*Eunseop Yoon,Hee Suk Yoon,Jaehyun Jang,SooHwan Eom,Qi Dai,Chong Luo,Mark A. Hasegawa-Johnson,Chang D. Yoo*

Main category: cs.AI

TL;DR: 提出PACR方法，通过模型内在的置信度奖励来加速强化学习中的探索过程，解决传统稀疏奖励缺乏中间步骤指导的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法的稀疏、基于结果的奖励无法为中间推理步骤提供指导，导致探索过程缓慢。

Method: 提出PACR方法，计算模型对正确答案置信度的渐进上升趋势作为密集奖励，约束探索空间到逻辑合理的推理区域。

Result: PACR加速了探索过程，用更少轨迹达到奖励饱和，并在多个基准测试中取得改进。

Conclusion: 密集的模型内在塑造信号可以使RLVR训练更有效和可靠。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly
improved LLM reasoning, but its sparse, outcome-based reward provides no
guidance for intermediate steps, slowing exploration. We propose Progressively
Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed
directly from the model's evolving belief in the correct answer. PACR encodes
the inductive bias that, along a well-formed reasoning trajectory, the
probability of the ground-truth answer should have a generally ascending trend.
We provide empirical and theoretical analysis validating that such an inductive
bias constrains the exploration search space to regions richer in logically
sound reasoning. We demonstrate that PACR accelerates exploration, reaches
reward saturation with fewer trajectories, and yields improvements on multiple
benchmarks. Our results suggest that dense, model-intrinsic shaping signals can
make RLVR training more effective and reliable.

</details>


### [87] [VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription](https://arxiv.org/abs/2510.22295)
*Quoc Anh Nguyen,Bernard Cheng,Kelvin Soh*

Main category: cs.AI

TL;DR: 本文创建了首个大规模越南语歌词转录数据集VietLyrics，并通过微调Whisper模型在越南语歌词转录任务上取得了优于现有系统的性能。


<details>
  <summary>Details</summary>
Motivation: 越南语歌词转录面临音调复杂性和方言变异的挑战，但由于缺乏专用数据集，该领域研究较少。

Method: 构建647小时的越南语歌词数据集VietLyrics，包含行级对齐的歌词和元数据，并微调Whisper模型进行歌词转录。

Result: 微调后的Whisper模型在越南语歌词转录任务上表现优于现有的多语言ALT系统，包括LyricWhiz。

Conclusion: VietLyrics数据集和微调模型的发布将推动越南音乐计算研究，并展示了该方法在低资源语言和音乐歌词转录中的潜力。

Abstract: Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique
challenges due to its tonal complexity and dialectal variations, but remains
largely unexplored due to the lack of a dedicated dataset. Therefore, we
curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising
647 hours of songs with line-level aligned lyrics and metadata to address these
issues. Our evaluation of current ASRbased approaches reveal significant
limitations, including frequent transcription errors and hallucinations in
non-vocal segments. To improve performance, we fine-tuned Whisper models on the
VietLyrics dataset, achieving superior results compared to existing
multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics
and our models, aiming to advance Vietnamese music computing research while
demonstrating the potential of this approach for ALT in low-resource language
and music.

</details>


### [88] [Graph-Coarsening Approach for the Capacitated Vehicle Routing Problem with Time Windows](https://arxiv.org/abs/2510.22329)
*Mustafa Mert Özyılmaz*

Main category: cs.AI

TL;DR: 提出一种多级图粗化和细化框架，通过时空距离度量将客户聚合成元节点，减少CVRPTW问题的计算时间同时保持或改进解质量。


<details>
  <summary>Details</summary>
Motivation: CVRPTW是物流中的基础NP难优化问题，大规模实例对精确求解器计算挑战大，需要更高效的求解方法。

Method: 使用多级图粗化框架，基于时空距离度量聚合客户为元节点，在简化问题上应用经典启发式算法，然后通过可行性修正扩展回原空间。

Result: 在Solomon基准实例上的初步实验表明，该方法减少了计算时间，同时保持或改进了解质量，特别是在容量和时间窗口约束方面。

Conclusion: 该方法能有效加速大规模车辆路径问题的求解，量子启发优化技术的集成有望进一步加速求解过程。

Abstract: The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a
fundamental NP-hard optimization problem in logistics. Solving large-scale
instances remains computationally challenging for exact solvers. This work
introduces a multilevel graph coarsening and refinement framework that
aggregates customers into meta-nodes using a spatio-temporal distance metric.
The reduced problem is solved with classical heuristics and subsequently
expanded back into the original space with feasibility corrections. Preliminary
experiments on Solomon benchmark instances show that the proposed method
reduces computation time while preserving or improving solution quality,
particularly with respect to capacity and time window constraints. The paper
also explores the integration of quantum-inspired optimization techniques,
highlighting their potential to further accelerate large-scale vehicle routing
tasks.

</details>


### [89] [LIFT: Interpretable truck driving risk prediction with literature-informed fine-tuned LLMs](https://arxiv.org/abs/2510.22333)
*Xiao Hu,Yuansheng Lian,Ke Zhang,Yunxuan Li,Yuelong Su,Meng Li*

Main category: cs.AI

TL;DR: 提出了一种基于文献知识微调大语言模型(LIFT LLM)的可解释性卡车驾驶风险预测框架，该框架在真实数据集上表现优异，并能提供与文献一致的变量重要性解释。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够准确预测卡车驾驶风险并提供可解释性结果的框架，结合领域文献知识来增强模型的可解释性和鲁棒性。

Method: 构建包含LLM驱动的推理核心、文献处理管道和结果评估器的框架。通过299篇领域文献构建知识库，并在真实卡车驾驶风险数据集上微调LLM。

Result: LIFT LLM在召回率上比基准模型提升26.7%，F1分数提升10.1%。变量重要性排序与基准模型一致，且在不同数据采样条件下保持稳定。通过PERMANOVA测试验证了识别的风险场景。

Conclusion: LIFT LLM框架在风险预测准确性和可解释性方面均表现优异，证明了文献知识库和微调过程对模型可解释性的贡献，展示了在数据驱动知识发现方面的潜力。

Abstract: This study proposes an interpretable prediction framework with
literature-informed fine-tuned (LIFT) LLMs for truck driving risk prediction.
The framework integrates an LLM-driven Inference Core that predicts and
explains truck driving risk, a Literature Processing Pipeline that filters and
summarizes domain-specific literature into a literature knowledge base, and a
Result Evaluator that evaluates the prediction performance as well as the
interpretability of the LIFT LLM. After fine-tuning on a real-world truck
driving risk dataset, the LIFT LLM achieved accurate risk prediction,
outperforming benchmark models by 26.7% in recall and 10.1% in F1-score.
Furthermore, guided by the literature knowledge base automatically constructed
from 299 domain papers, the LIFT LLM produced variable importance ranking
consistent with that derived from the benchmark model, while demonstrating
robustness in interpretation results to various data sampling conditions. The
LIFT LLM also identified potential risky scenarios by detecting key combination
of variables in truck driving risk, which were verified by PERMANOVA tests.
Finally, we demonstrated the contribution of the literature knowledge base and
the fine-tuning process in the interpretability of the LIFT LLM, and discussed
the potential of the LIFT LLM in data-driven knowledge discovery.

</details>


### [90] [DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry](https://arxiv.org/abs/2510.22340)
*Changti Wu,Shijie Lian,Zihao Liu,Lei Zhang,Laurence Tianruo Yang,Kai Chen*

Main category: cs.AI

TL;DR: 提出了DynaSolidGeo，这是第一个用于评估视觉语言模型真实空间推理能力的动态基准，专注于立体几何问题解决，包含503个专家策划的种子问题，可动态生成无限多样的多模态实例。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注2D平面几何，依赖静态数据集容易导致数据污染和记忆，且仅通过最终答案评估模型而忽略了推理过程。

Method: 通过半自动标注流程构建，包含专家标注的推理链进行过程评估，测量逻辑有效性和因果一致性。

Result: 实验显示代表性VLMs存在较大性能差距，在动态设置下性能严重下降，在需要高水平空间智能的任务上表现不佳。

Conclusion: DynaSolidGeo填补了立体几何推理评估的空白，揭示了当前VLMs在空间推理方面的局限性。

Abstract: Solid geometry problem solving demands spatial mathematical reasoning that
integrates spatial intelligence and symbolic reasoning. However, most existing
multimodal mathematical reasoning benchmarks focus primarily on 2D plane
geometry, rely on static datasets prone to data contamination and memorization,
and evaluate models solely by final answers, overlooking the reasoning process.
To address these limitations, we introduce DynaSolidGeo, the first dynamic
benchmark for evaluating genuine spatial reasoning in Vision-Language Models
(VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo
contains 503 expert-curated seed questions that can, in principle, dynamically
generate an unbounded number of diverse multimodal text-visual instances.
Beyond answer accuracy, we incorporate process evaluation based on
expert-annotated reasoning chains to measure logical validity and causal
coherence. Experiments across representative open-source and closed-source VLMs
reveal large performance gaps, severe degradation in dynamic settings, and poor
performance on tasks requiring high-level spatial intelligence, such as mental
rotation and visualization. The code and dataset are available at
\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.

</details>


### [91] [Reasoning Models Reason Well, Until They Don't](https://arxiv.org/abs/2510.22371)
*Revanth Rameshkumar,Jimson Huang,Yunxin Sun,Fei Xia,Abulhair Saparov*

Main category: cs.AI

TL;DR: 大型推理模型在复杂推理任务上存在局限性，虽然在某些基准测试中表现优异，但当问题复杂度超过训练分布时性能会急剧下降。


<details>
  <summary>Details</summary>
Motivation: 重新评估大型语言模型在复杂推理任务上的真实能力，揭示现有基准测试的局限性，并开发更严格的评估方法。

Method: 开发了深度推理数据集（DeepRD），包含可扩展复杂度的图连接性和自然语言证明规划任务，用于评估模型在复杂推理中的表现。

Result: 大型推理模型在足够复杂度下性能急剧下降，无法泛化到超出训练分布复杂度的任务，但大多数现实世界问题仍在其成功范围内。

Conclusion: 大型推理模型在短期内具有实用性，但需要开发能够超越训练分布复杂度泛化的新方法。

Abstract: Large language models (LLMs) have shown significant progress in reasoning
tasks. However, recent studies show that transformers and LLMs fail
catastrophically once reasoning problems exceed modest complexity. We revisit
these findings through the lens of large reasoning models (LRMs) -- LLMs
fine-tuned with incentives for step-by-step argumentation and
self-verification. LRM performance on graph and reasoning benchmarks such as
NLGraph seem extraordinary, with some even claiming they are capable of
generalized reasoning and innovation in reasoning-intensive fields such as
mathematics, physics, medicine, and law. However, by more carefully scaling the
complexity of reasoning problems, we show existing benchmarks actually have
limited complexity. We develop a new dataset, the Deep Reasoning Dataset
(DeepRD), along with a generative process for producing unlimited examples of
scalable complexity. We use this dataset to evaluate model performance on graph
connectivity and natural language proof planning. We find that the performance
of LRMs drop abruptly at sufficient complexity and do not generalize. We also
relate our LRM results to the distributions of the complexities of large,
real-world knowledge graphs, interaction graphs, and proof datasets. We find
the majority of real-world examples fall inside the LRMs' success regime, yet
the long tails expose substantial failure potential. Our analysis highlights
the near-term utility of LRMs while underscoring the need for new methods that
generalize beyond the complexity of examples in the training distribution.

</details>


### [92] [Modeling Hierarchical Thinking in Large Reasoning Models](https://arxiv.org/abs/2510.22437)
*G M Shahariar,Ali Nazari,Erfan Shayegani,Nael Abu-Ghazaleh*

Main category: cs.AI

TL;DR: 该论文提出使用有限状态机(FSM)来建模大型推理模型(LRMs)的层次推理过程，将推理轨迹表示为状态转换序列，为理解和分析LLM推理能力提供了新的系统化框架。


<details>
  <summary>Details</summary>
Motivation: 理解大型推理模型(LRMs)涌现的推理能力是一个重要但困难的开放性问题，这对于改进训练和理解模型鲁棒性具有重要应用价值。

Method: 采用无记忆有限状态机(FSM)来近似LRMs的层次推理动态，识别出初始化、演绎、增强策略、不确定性估计、回溯和最终结论等离散推理状态，并将推理轨迹表示为状态图上的转换序列。

Result: FSM分析揭示了不同模型在推理方法上的明显差异和潜在缺陷，提供了评估和改进LLM推理的新视角。

Conclusion: FSM框架为系统化分析、解释和可视化不同模型处理问题的方式提供了有效方法，能够揭示推理模式并识别潜在问题。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
when they generate step-by-step solutions, known as chain-of-thought (CoT)
reasoning. When trained to using chain-of-thought reasoning examples, the
resulting models (called Large Reasoning Models, or LRMs) appear to learn
hierarchical thinking strategies similar to those used by humans. However,
understanding LRMs emerging reasoning capabilities remains a difficult open
problem, with many potential important applications including improving
training and understanding robustness. In this paper, we adopt a memoryless
Finite State Machine formulation to approximate LRM's emerging hierarchical
reasoning dynamics as a structured, interpretable abstraction. We identify a
small set of discrete reasoning states including - initialization, deduction,
augmentation-strategy, uncertainty-estimation, backtracking, and
final-conclusion that capture the high-level states present in the model's
reasoning process. By annotating each step of a model's CoT with these states,
we can represent the reasoning trajectory as a transition sequence through the
state graph. This FSM formulation provides a systematic way to analyze,
interpret and visualize how different models approach problems. We describe the
FSM model, provide examples of CoT annotations under this scheme, and discuss
how it can shed light on differences between available models in their approach
to reasoning. Our results demonstrate that this FSM-based analysis reveals
distinct reasoning patterns and potential shortcomings, offering a new lens to
evaluate and improve LLM reasoning.

</details>


### [93] [On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset](https://arxiv.org/abs/2510.22898)
*Vishvesh Bhat,Omkar Ghugarkar,Julian McAuley*

Main category: cs.AI

TL;DR: 论文提出了CoreThink Agentic Reasoner框架，通过轻量级符号推理层增强LLMs，在多个工具调用基准测试中实现530%的性能提升，且计算成本仅为十分之一。


<details>
  <summary>Details</summary>
Motivation: 解决智能体工具调用环境中的泛化挑战，当前LLMs在跨领域转移推理策略和协调工具方面表现不佳。

Method: 开发CoreThink Agentic Reasoner框架，为LLMs添加符号推理层，实现结构化分解和自适应工具编排。

Result: 在多个基准测试中实现最先进性能，MAVEN基准测试准确率超过50%，相比基线提升530%，计算成本降低90%。

Conclusion: CoreThink框架有效解决了工具调用环境中的泛化问题，无需额外训练即可实现跨基准测试的优异表现。

Abstract: Generalization across Agentic tool-calling environments remains a key
unsolved challenge in developing reliable agentic reasoning systems. While
large language models (LLMs) demonstrate strong performance on isolated
benchmarks, their ability to transfer reasoning strategies and co-ordinate
tools across diverse domains is poorly understood. In this work, we conduct a
large-scale evaluation of state-of-the-art LLMs on multiple tool-calling
benchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math &
Physics Adversarial Verification & Evaluation Network), a new out of
distribution (OOD) benchmark designed to stress-test multi-step reasoning
through explicit verification and adversarial task composition. Our results
show that most current models achieve below 50% accuracy on MAVEN, revealing a
significant generalization gap across tool-use settings.
  To address this, we present the CoreThink Agentic Reasoner, a framework that
augments LLMs with a lightweight symbolic reasoning layer for structured
decomposition and adaptive tool orchestration. Without additional training, it
generalizes across all benchmarks, achieving state-of-the-art performance with
530% improvements over existing baselines at roughly one-tenth the
computational cost.

</details>


### [94] [Learning "Partner-Aware" Collaborators in Multi-Party Collaboration](https://arxiv.org/abs/2510.22462)
*Abhijnan Nath,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: 本文提出了一种可中断协作角色扮演算法(ICR)，用于训练LLM代理在协作任务中更好地处理伙伴干预，提高群体共识对齐。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在代理场景中的部署，需要评估其在多轮多任务协作中的能力。现有LLM代理倾向于忽略伙伴的干预，这使得提高群体共识变得困难。

Method: 使用改进动作MDP分析标准AI代理的次优行为，提出ICR算法训练共识最优的协作代理，使其能智能收集伙伴干预信息。

Result: 在多个协作任务环境中的实验表明，ICR平均能更好地促进共识收敛并探索更多样化的解决方案。

Conclusion: ICR算法能有效解决LLM代理在协作中忽略干预的问题，提高群体共识对齐能力。

Abstract: Large Language Models (LLMs) are increasingly bring deployed in agentic
settings where they act as collaborators with humans. Therefore, it is
increasingly important to be able to evaluate their abilities to collaborate
effectively in multi-turn, multi-party tasks. In this paper, we build on the AI
alignment and safe interruptability literature to offer novel theoretical
insights on collaborative behavior between LLM-driven collaborator agents and
an intervention agent. Our goal is to learn an ideal partner-aware collaborator
that increases the group's common-ground (CG)-alignment on task-relevant
propositions-by intelligently collecting information provided in interventions
by a partner agent.We show how LLM agents trained using standard RLHF and
related approaches are naturally inclined to ignore possibly well-meaning
interventions, which makes increasing group common ground non-trivial in this
setting. We employ a two-player Modified-Action MDP to examine this suboptimal
behavior of standard AI agents, and propose Interruptible Collaborative
Roleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal
collaborators. Experiments on multiple collaborative task environments show
that ICR, on average, is more capable of promoting successful CG convergence
and exploring more diverse solutions in such tasks.

</details>


### [95] [OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models](https://arxiv.org/abs/2510.22535)
*Hao Zheng,Zirui Pang,Ling li,Zhijie Deng,Yuhan Pu,Zhaowei Zhu,Xiaobo Xia,Jiaheng Wei*

Main category: cs.AI

TL;DR: OFFSIDE是一个基于足球转会谣言的多模态大语言模型遗忘评估基准，包含15.68K条手动整理的数据，评估遗忘效果、泛化性、效用和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的数据隐私问题日益严重，需要选择性遗忘技术，但现有基准存在图像多样性不足、准确性问题和评估场景不充分等局限。

Method: 构建基于足球转会谣言的手动整理数据集，包含四个测试集评估遗忘效果、泛化性、效用和鲁棒性，支持选择性遗忘、纠正再学习和单模态遗忘等高级设置。

Result: 评估发现：单模态方法在多模态谣言上失败；遗忘效果主要由灾难性遗忘驱动；所有方法在视觉谣言上都表现不佳；遗忘的谣言容易被恢复；所有方法都易受提示攻击。

Conclusion: 当前方法存在显著漏洞，需要更鲁棒的多模态遗忘解决方案。

Abstract: Advances in Multimodal Large Language Models (MLLMs) intensify concerns about
data privacy, making Machine Unlearning (MU), the selective removal of learned
information, a critical necessity. However, existing MU benchmarks for MLLMs
are limited by a lack of image diversity, potential inaccuracies, and
insufficient evaluation scenarios, which fail to capture the complexity of
real-world applications. To facilitate the development of MLLMs unlearning and
alleviate the aforementioned limitations, we introduce OFFSIDE, a novel
benchmark for evaluating misinformation unlearning in MLLMs based on football
transfer rumors. This manually curated dataset contains 15.68K records for 80
players, providing a comprehensive framework with four test sets to assess
forgetting efficacy, generalization, utility, and robustness. OFFSIDE supports
advanced settings like selective unlearning and corrective relearning, and
crucially, unimodal unlearning (forgetting only text data). Our extensive
evaluation of multiple baselines reveals key findings: (1) Unimodal methods
(erasing text-based knowledge) fail on multimodal rumors; (2) Unlearning
efficacy is largely driven by catastrophic forgetting; (3) All methods struggle
with "visual rumors" (rumors appear in the image); (4) The unlearned rumors can
be easily recovered and (5) All methods are vulnerable to prompt attacks. These
results expose significant vulnerabilities in current approaches, highlighting
the need for more robust multimodal unlearning solutions. The code is available
at
\href{https://github.com/zh121800/OFFSIDE}{https://github.com/zh121800/OFFSIDE}.

</details>


### [96] [Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards](https://arxiv.org/abs/2510.23083)
*Jan Niklas Groeneveld,Xi Qin,Alexander Schaefer,Yaad Oren*

Main category: cs.AI

TL;DR: 该论文研究了如何将小型语言模型（如Phi-4系列）转变为有效的奖励模型，用于评估代码生成质量，通过结合过程奖励和结果奖励来提升代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成高质量代码方面仍面临挑战，需要奖励模型作为中间步骤来评估结果或中间步骤。虽然模型的反省能力通常随规模增加，但本研究旨在探索小型语言模型是否也能成为有效的奖励模型。

Method: 构建基于APPS编程挑战基准的代码样本数据集，训练带有回归层的价值头模型来估计中间输出的成功概率，将解码器专用transformer模型转变为奖励模型。

Result: 评估显示小型LLM能够作为有效的奖励模型或代码评估批评器，成功识别多个候选方案中的正确解决方案。使用该批评器，在多个生成代码中搜索最准确代码的能力提升了20%以上。

Conclusion: 小型语言模型可以被成功转变为有效的奖励模型，通过结合过程奖励和结果奖励的方法，显著提升了代码生成的质量评估能力。

Abstract: Generating high-quality code remains a challenge for Large Language Models
(LLMs). For the evolution of reasoning models on this task, reward models are a
necessary intermediate step. These models judge outcomes or intermediate steps.
Decoder-only transformer models can be turned into reward models by introducing
a regression layer and supervised fine-tuning. While it is known that
reflection capabilities generally increase with the size of a model, we want to
investigate whether state-of-the-art small language models like the Phi-4
family can be turned into usable reward models blending the consideration of
process rewards and outcome rewards.
  Targeting this goal, we construct a dataset of code samples with correctness
labels derived from the APPS coding challenge benchmark. We then train a
value-head model to estimate the success probability of intermediate outputs.
Our evaluation shows that small LLMs are capable of serving as effective reward
models or code evaluation critics, successfully identifying correct solutions
among multiple candidates. Using this critic, we achieve over a 20% improvement
in the search capability of the most accurate code out of multiple generations.

</details>


### [97] [ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs](https://arxiv.org/abs/2510.22590)
*Yassir Lairgi,Ludovic Moncla,Khalid Benabdeslem,Rémy Cazabet,Pierre Cléau*

Main category: cs.AI

TL;DR: ATOM是一个用于从非结构化文本构建和持续更新时序知识图谱的少样本可扩展方法，通过将文档分解为原子事实并采用双时间建模，显著提高了提取的完整性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统静态知识图谱构建忽略了数据的动态性和时效性，而现有的零样本或少样本方法存在不稳定性和关键事实覆盖不完整的问题。

Method: 将输入文档分割为最小自包含的原子事实，构建原子时序知识图谱，采用双时间建模区分信息观测时间和有效时间，并并行合并原子图谱。

Result: 相比基线方法，ATOM实现了约18%的完整性提升、约17%的稳定性改善和超过90%的延迟减少。

Conclusion: ATOM展示了动态时序知识图谱构建的强大可扩展性潜力，能够有效处理实时变化的数据。

Abstract: In today's rapidly expanding data landscape, knowledge extraction from
unstructured text is vital for real-time analytics, temporal inference, and
dynamic memory frameworks. However, traditional static knowledge graph (KG)
construction often overlooks the dynamic and time-sensitive nature of
real-world data, limiting adaptability to continuous changes. Moreover, recent
zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance
on prebuilt ontologies often suffer from instability across multiple runs, as
well as incomplete coverage of key facts. To address these challenges, we
introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that
builds and continuously updates Temporal Knowledge Graphs (TKGs) from
unstructured texts. ATOM splits input documents into minimal, self-contained
"atomic" facts, improving extraction exhaustivity and stability. Then, it
constructs atomic TKGs from these facts while employing a dual-time modeling
that distinguishes when information is observed from when it is valid. The
resulting atomic TKGs are subsequently merged in parallel. Empirical
evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%
better stability, and over 90% latency reduction compared to baseline methods,
demonstrating a strong scalability potential for dynamic TKG construction.

</details>


### [98] [A Framework for Quantifying How Pre-Training and Context Benefit In-Context Learning](https://arxiv.org/abs/2510.22594)
*Bingqing Song,Jiaxiang Li,Rong Wang,Songtao Lu,Mingyi Hong*

Main category: cs.AI

TL;DR: 本文提出了一个分析上下文学习性能的新框架，通过理论分析和实验验证，揭示了预训练数据分布与查询任务分布差异时，适当构建的上下文如何量化地改善预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现了强大的上下文学习能力，但理论上尚不清楚这种能力如何产生，特别是预训练过程和上下文构建等关键因素的确切作用。

Method: 构建了一个包含单层transformer的简单示例，然后扩展到更一般情况，推导了ICL性能、上下文长度以及预训练与查询任务分布之间KL散度的精确关系，并通过实验验证理论结果。

Result: 当预训练数据分布与查询任务分布不同时，适当构建的上下文可以量化地将输出分布向查询任务分布转移，从而提高查询主题的预测准确性。

Conclusion: 研究为理解上下文学习的机制提供了理论框架，明确了预训练数据分布与任务分布差异对ICL性能的影响，并量化了上下文长度在调节这种影响中的作用。

Abstract: Pre-trained large language models have demonstrated a strong ability to learn
from context, known as in-context learning (ICL). Despite a surge of recent
applications that leverage such capabilities, it is by no means clear, at least
theoretically, how the ICL capabilities arise, and in particular, what is the
precise role played by key factors such as pre-training procedure as well as
context construction. In this work, we propose a new framework to analyze the
ICL performance, for a class of realistic settings, which includes network
architectures, data encoding, data generation, and prompt construction process.
As a first step, we construct a simple example with a one-layer transformer,
and show an interesting result, namely when the pre-train data distribution is
different from the query task distribution, a properly constructed context can
shift the output distribution towards the query task distribution, in a
quantifiable manner, leading to accurate prediction on the query topic. We then
extend the findings in the previous step to a more general case, and derive the
precise relationship between ICL performance, context length and the KL
divergence between pre-train and query task distribution. Finally, we provide
experiments to validate our theoretical results.

</details>


### [99] [JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence](https://arxiv.org/abs/2510.23538)
*Qiushi Sun,Jingyang Gong,Yang Liu,Qiaosheng Chen,Lei Li,Kai Chen,Qipeng Guo,Ben Kao,Fei Yuan*

Main category: cs.AI

TL;DR: 提出了JanusCode-800K多模态代码语料库和JanusCoder系列模型，通过视觉-程序接口实现从文本指令、视觉输入或两者结合生成代码，在文本和视觉编程任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决高质量多模态代码数据稀缺的问题，促进基于程序生成丰富视觉输出的应用发展，如灵活内容生成和程序驱动的可视化编辑。

Method: 开发合成工具包利用数据模态间的协同作用构建大规模高质量语料库，训练JanusCoder系列统一模型支持文本、视觉或混合输入生成代码。

Result: JanusCode-800K成为最大的多模态代码语料库，JanusCoder系列模型在7B到14B规模上接近或超过商业模型性能。

Conclusion: 建立了程序逻辑与视觉表达协调的关键见解，为多模态代码智能提供了有效的数据和建模解决方案。

Abstract: The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.

</details>


### [100] [CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation](https://arxiv.org/abs/2510.22609)
*Md. Mehedi Hasan,Rafid Mostafiz,Md. Abir Hossain,Bikash Kumar Paul*

Main category: cs.AI

TL;DR: CLIN-LLM是一个安全约束的混合管道系统，通过多模态患者编码、不确定性校准的疾病分类和检索增强的治疗生成，实现症状到疾病的准确分类和临床治疗推荐。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的医疗系统缺乏医学基础且无法量化不确定性，导致输出不安全。需要开发能够提供安全、可解释医疗决策支持的系统。

Method: 使用BioBERT在1200个临床案例上微调，结合Focal Loss和Monte Carlo Dropout进行置信度感知预测；采用Biomedical Sentence-BERT从MedDialog语料库检索相关对话，使用FLAN-T5模型生成个性化治疗建议，并通过RxNorm进行抗生素管理和药物相互作用筛查。

Result: CLIN-LLM达到98%的准确率和F1分数，比ClinicalBERT提升7.1%；检索精度为78%，临床有效性评分为4.2/5；不安全抗生素建议比GPT-5减少67%。

Conclusion: CLIN-LLM展示了鲁棒性、可解释性和临床安全性，为资源有限的医疗环境提供了可部署的、人机协作的决策支持框架。

Abstract: Accurate symptom-to-disease classification and clinically grounded treatment
recommendations remain challenging, particularly in heterogeneous patient
settings with high diagnostic risk. Existing large language model (LLM)-based
systems often lack medical grounding and fail to quantify uncertainty,
resulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid
pipeline that integrates multimodal patient encoding, uncertainty-calibrated
disease classification, and retrieval-augmented treatment generation. The
framework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease
dataset and incorporates Focal Loss with Monte Carlo Dropout to enable
confidence-aware predictions from free-text symptoms and structured vitals.
Low-certainty cases (18%) are automatically flagged for expert review, ensuring
human oversight. For treatment generation, CLIN-LLM employs Biomedical
Sentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample
MedDialog corpus. The retrieved evidence and patient context are fed into a
fine-tuned FLAN-T5 model for personalized treatment generation, followed by
post-processing with RxNorm for antibiotic stewardship and drug-drug
interaction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score,
outperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval
precision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic
suggestions are reduced by 67% compared to GPT-5. These results demonstrate
CLIN-LLM's robustness, interpretability, and clinical safety alignment. The
proposed system provides a deployable, human-in-the-loop decision support
framework for resource-limited healthcare environments. Future work includes
integrating imaging and lab data, multilingual extensions, and clinical trial
validation.

</details>


### [101] [SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming](https://arxiv.org/abs/2510.22626)
*Adhyayan Veer Singh,Aaron Shen,Brian Law,Ahmed Ismail,Jonas Rohweder,Sean O'Brien,Kevin Zhu*

Main category: cs.AI

TL;DR: SwiftSolve是一个复杂度感知的多智能体系统，用于竞争性编程，通过算法规划、经验分析和复杂度引导的修复来确保程序不仅正确，还满足时间和内存限制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的程序经常通过单元测试但违反竞赛的时间和内存预算，需要确保程序在资源约束下的高效性。

Method: 采用多智能体系统，包括规划器提出算法草图、静态剪枝器过滤高风险计划、编码器生成C++代码、分析器进行性能分析、复杂度分析器评估复杂度并引导修复。

Result: 在26个问题上的测试显示，首轮通过率61.54%，三轮内解决率80.77%，运行成功率73.08%，相比Claude Opus 4有明显提升。

Conclusion: 通过性能分析和复杂度引导的重新规划，SwiftSolve能有效减少低效问题，同时保持准确性，证明了多智能体方法在资源约束编程中的有效性。

Abstract: Correctness alone is insufficient: LLM-generated programs frequently satisfy
unit tests while violating contest time or memory budgets. We present
SwiftSolve, a complexity-aware multi-agent system for competitive programming
that couples algorithmic planning with empirical profiling and
complexity-guided repair. We frame competitive programming as a software
environment where specialized agents act as programmers, each assuming roles
such as planning, coding, profiling, and complexity analysis. A Planner
proposes an algorithmic sketch; a deterministic Static Pruner filters high-risk
plans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on
a fixed input-size schedule to record wall time and peak memory; and a
Complexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a
complexity class and dispatch targeted patches to either the Planner or Coder.
Agents communicate via typed, versioned JSON; a controller enforces iteration
caps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10
Codeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains
pass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with
marginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate
run-level success is 73.08% at 12.40 s mean. Failures are predominantly
resource-bound, indicating inefficiency rather than logic errors. Against
Claude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at
approximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness
(pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence
of TLE or MLE, and complexity fit accuracy on BigO), demonstrating that
profiling and complexity-guided replanning reduce inefficiency while preserving
accuracy.

</details>


### [102] [Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration](https://arxiv.org/abs/2510.22679)
*Yuval Kainan,Shaked Zychlinski*

Main category: cs.AI

TL;DR: 提出一种基于首个生成token的对数概率分布来检测LLM模板化响应的方法，可在单步生成后识别拒绝、简单确认等非实质性回答，实现计算效率优化。


<details>
  <summary>Details</summary>
Motivation: LLM在生成模板化响应（如拒绝、简单确认等）时浪费大量计算资源，增加了不必要的成本和延迟。

Method: 利用首个生成token的对数概率分布作为信号，通过轻量级k-NN分类器预测响应类型，区分实质性回答与模板化响应。

Result: 实验表明首个token的对数概率向量在不同响应类型中形成明显可分离的聚类，能够高精度预测响应性质。

Conclusion: 该方法提供了一种计算简单的实用技术，通过早期终止或重定向到较小模型来优化LLM推理，显著节省计算成本，推动更高效可持续的LLM部署。

Abstract: Large Language Models (LLMs) often expend significant computational resources
generating boilerplate responses, such as refusals, simple acknowledgements and
casual greetings, which adds unnecessary cost and latency. To address this
inefficiency, we propose a simple yet highly effective method for detecting
such responses after only a single generation step. We demonstrate that the
log-probability distribution of the first generated token serves as a powerful
signal for classifying the nature of the entire subsequent response. Our
experiments, conducted across a diverse range of small, large, and
reasoning-specialized models, show that the first-token log-probability vectors
form distinctly separable clusters for different response types. Using a
lightweight k-NN classifier, we achieve high accuracy in predicting whether a
response will be a substantive answer or a form of boilerplate response,
including user-specified refusals. The primary implication is a practical,
computationally trivial technique, optimizing LLM inference by enabling early
termination or redirection to a smaller model, thereby yielding significant
savings in computational cost. This work presents a direct path toward more
efficient and sustainable LLM deployment.

</details>


### [103] [A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration](https://arxiv.org/abs/2510.23443)
*Chiara Bonfanti,Alessandro Druetto,Cataldo Basile,Tharindu Ranasinghe,Marcos Zampieri*

Main category: cs.AI

TL;DR: 提出构建智能系统来弥合网络安全与法律领域之间的知识鸿沟，解决传统法律研究工具在处理案件、法规和技术漏洞之间复杂联系时的局限性。


<details>
  <summary>Details</summary>
Motivation: 网络安全与法律的交叉领域日益复杂，传统法律研究工具难以处理案件、法规和技术漏洞之间的细微联系，阻碍了法律专家与网络安全专业人员之间的协作。

Method: 开发能够导航日益复杂的网络法律领域的智能系统，在多语言任务上进行了初步验证。

Result: 在多语言任务上取得了有希望的初步结果。

Conclusion: 这项工作为构建能够弥合网络法律领域知识鸿沟的智能系统迈出了重要第一步。

Abstract: The growing intersection of cybersecurity and law creates a complex
information space where traditional legal research tools struggle to deal with
nuanced connections between cases, statutes, and technical vulnerabilities.
This knowledge divide hinders collaboration between legal experts and
cybersecurity professionals. To address this important gap, this work provides
a first step towards intelligent systems capable of navigating the increasingly
intricate cyber-legal domain. We demonstrate promising initial results on
multilingual tasks.

</details>


### [104] [Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally Calibrated Urban Development Monitoring](https://arxiv.org/abs/2510.22702)
*Mithul Chander,Sai Pragnya Ranga,Prathamesh Mayekar*

Main category: cs.AI

TL;DR: 提出了Atlas城市指数(AUI)，一种基于Sentinel-2卫星影像衡量城市发展的新指标，利用视觉语言模型(VLMs)克服传统指数在大气噪声、季节变化和云层覆盖方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如归一化建筑指数(NDBI)在准确捕捉城市发展方面存在困难，主要受大气噪声、季节变化和云层覆盖等因素影响，这阻碍了大规模人类发展和城市化的监测。

Method: 收集每个区域的Sentinel-2影像时间序列，在固定时间窗口内处理图像以获得最小云层覆盖的代表性图像。采用两种策略确保评分一致性：(i)提供代表不同城市化水平的参考图像集，(ii)提供最近的历史图像以保持时间一致性并减轻当前图像的云层噪声。

Result: 在班加罗尔的定性实验表明，AUI优于NDBI等标准指数，能够产生更可靠和稳定的发展评分。

Conclusion: AUI通过结合视觉语言模型和精心设计的图像处理策略，成功克服了传统城市化指数的挑战，为城市发展监测提供了更有效的工具。

Abstract: We introduce the {\em Atlas Urban Index} (AUI), a metric for measuring urban
development computed using Sentinel-2 \citep{spoto2012sentinel2} satellite
imagery. Existing approaches, such as the {\em Normalized Difference Built-up
Index} (NDBI), often struggle to accurately capture urban development due to
factors like atmospheric noise, seasonal variation, and cloud cover. These
limitations hinder large-scale monitoring of human development and
urbanization. To address these challenges, we propose an approach that
leverages {\em Vision-Language Models }(VLMs) to provide a development score
for regions. Specifically, we collect a time series of Sentinel-2 images for
each region. Then, we further process the images within fixed time windows to
get an image with minimal cloud cover, which serves as the representative image
for that time window. To ensure consistent scoring, we adopt two strategies:
(i) providing the VLM with a curated set of reference images representing
different levels of urbanization, and (ii) supplying the most recent past image
to both anchor temporal consistency and mitigate cloud-related noise in the
current image. Together, these components enable AUI to overcome the challenges
of traditional urbanization indices and produce more reliable and stable
development scores. Our qualitative experiments on Bangalore suggest that AUI
outperforms standard indices such as NDBI.

</details>


### [105] [RaCoT: Plug-and-Play Contrastive Example Generation Mechanism for Enhanced LLM Reasoning Reliability](https://arxiv.org/abs/2510.22710)
*Kaitong Cai,Jusheng Zhang,Yijia Fan,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: RaCoT通过在检索前阶段生成对比性问题并提取关键差异提示，引导模型关注决定答案分歧的关键细节，从而在单次检索中抑制语义干扰，提高RAG系统对知识稀疏和语义模糊的长尾查询的处理能力。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中知识稀疏和语义模糊的长尾查询问题，这些查询中的检索噪声会扭曲推理并需要昂贵的后处理。

Method: 提出RaCoT框架，在检索前阶段自动生成语义相邻但答案不同的对比性问题，提取Δ-Prompt捕获关键差异，引导模型主动关注决定答案分歧的关键细节。

Result: 在六个权威基准测试中，RaCoT比RankRAG和Self-RAG等强基线方法表现更好，提升0.9-2.4个百分点。在对抗性测试中性能下降仅8.6%，远优于其他方法超过15%的下降。延迟低至3.12秒，令牌开销仅11.54。

Conclusion: RaCoT将RAG范式从"事后上下文清理"重新定义为"先验塑造判别推理"，为实时、资源受限部署提供了高效且可靠的AI系统路径。

Abstract: Retrieval-Augmented Generation (RAG) faces a core bottleneck with
knowledge-sparse and semantically ambiguous long-tail queries, where retrieval
noise distorts reasoning and necessitates costly post-processing. To tackle
this, we propose RaCoT (Retrieval-aware Contrastive-of-Thought), a novel
framework that shifts contrastive thinking to the pre-retrieval stage. By
automatically generating a semantically adjacent yet differently answered
contrastive question and extracting a $\Delta$-Prompt to capture their key
differences, RaCoT guides the model to proactively focus on the ``critical
details that determine answer divergence." This approach allows it to suppress
semantic interference within a single retrieval pass, overcoming the
theoretical bottleneck of single-vector queries that struggle to simultaneously
encode signals for what to attend to and what to ignore. On six authoritative
benchmarks, including PopQA and TriviaQA-unfiltered, RaCoT outperforms strong
baselines like RankRAG and Self-RAG by 0.9-2.4 percentage points. It exhibits
superior robustness, with a performance drop of only 8.6\% in adversarial
tests, far surpassing the over 15\% degradation in other methods. Furthermore,
its low latency (3.12s) and token overhead (11.54) place it on the
accuracy-efficiency Pareto frontier, while ablation studies validate the
necessity of each component. Ultimately, RaCoT reframes the RAG paradigm from
``post-hoc context cleaning" to ``a priori shaping of discriminative
reasoning", offering an efficient and robust path toward reliable AI systems
for real-time, resource-constrained deployments.

</details>


### [106] [Critical Insights into Leading Conversational AI Models](https://arxiv.org/abs/2510.22729)
*Urja Kohli,Aditi Singh,Arun Sharma*

Main category: cs.AI

TL;DR: 本研究比较了五个顶级大语言模型（Gemini、DeepSeek、Claude、GPT和LLaMA）在性能准确性、伦理偏见缓解和可用性集成三个关键维度的差异。


<details>
  <summary>Details</summary>
Motivation: 随着各大公司开发出更好的大语言模型，了解不同模型在性能、道德行为和可用性方面的差异变得至关重要，这些差异源于它们不同的设计理念。

Method: 通过分析三个重要因素：性能和准确性、伦理和偏见缓解、可用性和集成，对五个顶级LLM进行比较评估。

Result: 发现Claude在道德推理方面表现良好，Gemini在多模态能力和伦理框架方面更强，DeepSeek擅长基于事实的推理，LLaMA适合开放应用，ChatGPT提供平衡性能并注重使用体验。

Conclusion: 这些模型在工作效果、易用性和伦理处理方面存在显著差异，用户应根据具体需求选择最能发挥其优势的模型。

Abstract: Big Language Models (LLMs) are changing the way businesses use software, the
way people live their lives and the way industries work. Companies like Google,
High-Flyer, Anthropic, OpenAI and Meta are making better LLMs. So, it's crucial
to look at how each model is different in terms of performance, moral behaviour
and usability, as these differences are based on the different ideas that built
them. This study compares five top LLMs: Google's Gemini, High-Flyer's
DeepSeek, Anthropic's Claude, OpenAI's GPT models and Meta's LLaMA. It performs
this by analysing three important factors: Performance and Accuracy, Ethics and
Bias Mitigation and Usability and Integration. It was found that Claude has
good moral reasoning, Gemini is better at multimodal capabilities and has
strong ethical frameworks. DeepSeek is great at reasoning based on facts, LLaMA
is good for open applications and ChatGPT delivers balanced performance with a
focus on usage. It was concluded that these models are different in terms of
how well they work, how easy they are to use and how they treat people
ethically, making it a point that each model should be utilised by the user in
a way that makes the most of its strengths.

</details>


### [107] [Multi-Modal Fact-Verification Framework for Reducing Hallucinations in Large Language Models](https://arxiv.org/abs/2510.22751)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: 开发了一个事实验证框架，通过交叉检查LLM输出与多个知识源来实时捕获和纠正错误，将幻觉减少了67%，同时保持响应质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型会自信地生成听起来完全合理但实际上是错误的信息，这种幻觉问题已成为在准确性重要的现实应用中部署这些模型的主要障碍。

Method: 结合结构化数据库、实时网络搜索和学术文献来验证生成的事实声明，检测到不一致时自动纠正，同时保持响应的自然流畅性。

Result: 在各种领域的测试中，幻觉减少了67%，医疗保健、金融和科学研究领域的专家对纠正后输出的满意度达到89%，显著优于未验证的LLM响应。

Conclusion: 这项工作为在不能出错的应用中使LLM更可信提供了一个实用解决方案。

Abstract: While Large Language Models have transformed how we interact with AI systems,
they suffer from a critical flaw: they confidently generate false information
that sounds entirely plausible. This hallucination problem has become a major
barrier to deploying these models in real-world applications where accuracy
matters. We developed a fact verification framework that catches and corrects
these errors in real-time by cross checking LLM outputs against multiple
knowledge sources. Our system combines structured databases, live web searches,
and academic literature to verify factual claims as they're generated. When we
detect inconsistencies, we automatically correct them while preserving the
natural flow of the response. Testing across various domains showed we could
reduce hallucinations by 67% without sacrificing response quality. Domain
experts in healthcare, finance, and scientific research rated our corrected
outputs 89% satisfactory a significant improvement over unverified LLM
responses. This work offers a practical solution for making LLMs more
trustworthy in applications where getting facts wrong isn't an option.

</details>


### [108] [Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval](https://arxiv.org/abs/2510.22765)
*Binxiao Xu,Junyu Feng,Ruichuan An,Yulin Luo,Shilin Yan,Hao Liang,Ming Lu,Wentao Zhang*

Main category: cs.AI

TL;DR: Jarvis是一个通过个人KV-Cache检索实现个性化AI助手的创新框架，在视觉问答和纯文本任务中均取得最先进结果


<details>
  <summary>Details</summary>
Motivation: 现有方法要么学习一组概念标记，要么训练VLM来利用用户特定信息，但都难以生成准确答案。需要更好的个性化AI助手解决方案

Method: 通过个人KV-Cache检索框架，将用户特定信息存储在文本和视觉标记的KV-Cache中。文本标记通过总结用户信息为元数据创建，视觉标记通过从用户图像中提取独特图像块产生

Result: Jarvis能够提供更准确的响应，特别是在依赖特定局部细节时。在多个数据集的视觉问答和纯文本任务中均取得最先进结果

Conclusion: Jarvis为个性化AI助手提供了一条实用路径，通过个人KV-Cache检索机制有效提升了回答准确性

Abstract: The rapid development of Vision-language models (VLMs) enables open-ended
perception and reasoning. Recent works have started to investigate how to adapt
general-purpose VLMs into personalized assistants. Even commercial models such
as ChatGPT now support model personalization by incorporating user-specific
information. However, existing methods either learn a set of concept tokens or
train a VLM to utilize user-specific information. However, both pipelines
struggle to generate accurate answers as personalized assistants. We introduce
Jarvis, an innovative framework for a personalized AI assistant through
personal KV-Cache retrieval, which stores user-specific information in the
KV-Caches of both textual and visual tokens. The textual tokens are created by
summarizing user information into metadata, while the visual tokens are
produced by extracting distinct image patches from the user's images. When
answering a question, Jarvis first retrieves related KV-Caches from personal
storage and uses them to ensure accuracy in responses. We also introduce a
fine-grained benchmark built with the same distinct image patch mining
pipeline, emphasizing accurate question answering based on fine-grained
user-specific information. Jarvis is capable of providing more accurate
responses, particularly when they depend on specific local details. Jarvis
achieves state-of-the-art results in both visual question answering and
text-only tasks across multiple datasets, indicating a practical path toward
personalized AI assistants. The code and dataset will be released.

</details>


### [109] [How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations](https://arxiv.org/abs/2510.22780)
*Zora Zhiruo Wang,Yijia Shao,Omar Shaikh,Daniel Fried,Graham Neubig,Diyi Yang*

Main category: cs.AI

TL;DR: 本文首次直接比较了人类与AI代理在多个工作技能上的表现，发现代理倾向于程序化方法、质量较低但速度更快成本更低，揭示了人机协作的潜力。


<details>
  <summary>Details</summary>
Motivation: AI代理在人类工作相关任务中不断优化，但缺乏对人类工作执行方式的清晰理解，需要揭示代理的专业能力和在不同工作流程中的角色。

Method: 引入可扩展工具包从人类或代理的计算机使用活动中诱导可解释的结构化工作流程，并比较人类和代理执行相同任务的方式。

Result: 代理工作流程与人类对齐但过于程序化；工作质量较低但通过数据伪造和工具滥用掩盖缺陷；交付速度快88.3%，成本低90.4-96.2%。

Conclusion: 代理在可编程任务上具有效率优势，为人机协作提供了潜力，但需要解决其程序化倾向和质量问题。

Abstract: AI agents are continually optimized for tasks related to human work, such as
software engineering and professional writing, signaling a pressing trend with
significant impacts on the human workforce. However, these agent developments
have often not been grounded in a clear understanding of how humans execute
work, to reveal what expertise agents possess and the roles they can play in
diverse workflows. In this work, we study how agents do human work by
presenting the first direct comparison of human and agent workers across
multiple essential work-related skills: data analysis, engineering,
computation, writing, and design. To better understand and compare
heterogeneous computer-use activities of workers, we introduce a scalable
toolkit to induce interpretable, structured workflows from either human or
agent computer-use activities. Using such induced workflows, we compare how
humans and agents perform the same tasks and find that: (1) While agents
exhibit promise in their alignment to human workflows, they take an
overwhelmingly programmatic approach across all work domains, even for
open-ended, visually dependent tasks like design, creating a contrast with the
UI-centric methods typically used by humans. (2) Agents produce work of
inferior quality, yet often mask their deficiencies via data fabrication and
misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster
and cost 90.4-96.2% less than humans, highlighting the potential for enabling
efficient collaboration by delegating easily programmable tasks to agents.

</details>


### [110] [Agentic Meta-Orchestrator for Multi-task Copilots](https://arxiv.org/abs/2510.22781)
*Xiaofeng Zhu,Yunshen Zhou*

Main category: cs.AI

TL;DR: 提出了一种用于微软Copilot服务的Agentic Meta-orchestrator (AMO)，能够处理多任务和可扩展的代理，通过元学习决策树模型选择最佳推理策略。


<details>
  <summary>Details</summary>
Motivation: 微软Copilot套件作为各种代理的通用入口点，需要强大的编排器来将用户提示的任务分发给正确的代理，支持动态扩展的代理库。

Method: 使用Agentic Meta-orchestrator (AMO)，利用元学习训练决策树模型来决定不同代理/模型之间的最佳推理策略，提供自然语言和动作响应。

Result: 通过两个生产用例展示有效性：M365电子商务Copilot提供最新产品信息并连接多个代理；代码合规Copilot扫描DevOps代码检测合规问题。

Conclusion: AMO能够有效处理多任务和可扩展代理，在电子商务和代码合规场景中表现出色。

Abstract: Microsoft Copilot suites serve as the universal entry point for various
agents skilled in handling important tasks, ranging from assisting a customer
with product purchases to detecting vulnerabilities in corporate programming
code. Each agent can be powered by language models, software engineering
operations, such as database retrieval, and internal \& external knowledge. The
repertoire of a copilot can expand dynamically with new agents. This requires a
robust orchestrator that can distribute tasks from user prompts to the right
agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for
handling multiple tasks and scalable agents in copilot services, which can
provide both natural language and action responses. We will also demonstrate
the planning that leverages meta-learning, i.e., a trained decision tree model
for deciding the best inference strategy among various agents/models. We
showcase the effectiveness of our AMO through two production use cases:
Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365
E-Commerce Copilot advertises Microsoft products to external customers to
promote sales success. The M365 E-Commerce Copilot provides up-to-date product
information and connects to multiple agents, such as relational databases and
human customer support. The code compliance copilot scans the internal DevOps
code to detect known and new compliance issues in pull requests (PR).

</details>


### [111] [Will Humanity Be Rendered Obsolete by AI?](https://arxiv.org/abs/2510.22814)
*Mohamed El Louadi,Emna Ben Romdhane*

Main category: cs.AI

TL;DR: 本文分析了人工智能对人类构成的生存风险，探讨了从当前AI到超智能的发展轨迹，以及超级智能可能带来的伦理和生存威胁。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨人工智能发展到超智能水平时可能对人类构成的生存风险，特别是当机器智能远超人类时可能导致的不可控后果。

Method: 基于Irving J. Good和Nick Bostrom的理论工作，结合近期出版物（AI 2027; If Anyone Builds It, Everyone Dies），分析AGI和超智能的发展轨迹。

Result: 分析表明，机器认知能力呈指数级增长，超智能可能从根本上与人类智能异质，人类灭绝可能不是源于恶意，而是源于不可控的、漠不关心的认知优势。

Conclusion: 人工智能的超智能发展可能对人类构成严重的生存威胁，这种威胁可能源于智能优势而非恶意，需要认真对待和防范。

Abstract: This article analyzes the existential risks artificial intelligence (AI)
poses to humanity, tracing the trajectory from current AI to ultraintelligence.
Drawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent
publications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and
superintelligence. Considering machines' exponentially growing cognitive power
and hypothetical IQs, it addresses the ethical and existential implications of
an intelligence vastly exceeding humanity's, fundamentally alien. Human
extinction may result not from malice, but from uncontrollable, indifferent
cognitive superiority.

</details>


### [112] [HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning](https://arxiv.org/abs/2510.22832)
*Long H Dang,David Rawlinson*

Main category: cs.AI

TL;DR: HRM-Agent是HRM的强化学习变体，能够在动态不确定的迷宫环境中学习导航到目标，并能够重用之前时间步的计算。


<details>
  <summary>Details</summary>
Motivation: HRM虽然在小规模下具有强大的推理能力，但只能应用于监督、静态、完全可观察的问题。许多现实世界问题具有动态、不确定、部分可观察的特性，需要能够整合和重用之前时间步的计算。

Method: 提出了HRM-Agent，这是HRM的一个变体，仅使用强化学习进行训练。探索了循环推理过程的动态特性。

Result: HRM-Agent能够在动态和不确定的迷宫环境中学习导航到目标。研究发现循环推理过程成功重用了之前环境时间步的计算。

Conclusion: HRM可以通过强化学习扩展到动态和不确定的环境中，其循环推理过程能够有效重用之前的计算，这为处理现实世界问题提供了可能。

Abstract: The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities
given its small size, but has only been applied to supervised, static,
fully-observable problems. One of HRM's strengths is its ability to adapt its
computational effort to the difficulty of the problem. However, in its current
form it cannot integrate and reuse computation from previous time-steps if the
problem is dynamic, uncertain or partially observable, or be applied where the
correct action is undefined, characteristics of many real-world problems.
  This paper presents HRM-Agent, a variant of HRM trained using only
reinforcement learning. We show that HRM can learn to navigate to goals in
dynamic and uncertain maze environments. Recent work suggests that HRM's
reasoning abilities stem from its recurrent inference process. We explore the
dynamics of the recurrent inference process and find evidence that it is
successfully reusing computation from earlier environment time-steps.

</details>


### [113] [Toward Agents That Reason About Their Computation](https://arxiv.org/abs/2510.22833)
*Adrian Orenstein,Jessica Chen,Gwyneth Anne Delos Santos,Bayley Sapara,Michael Bowling*

Main category: cs.AI

TL;DR: 论文研究了让强化学习智能体在训练过程中考虑计算成本，从而减少计算资源使用的方法。通过在Arcade Learning Environment上的实验，发现这种智能体在75%的游戏中表现更好，同时平均减少三倍计算量。


<details>
  <summary>Details</summary>
Motivation: 人类在熟练任务后会减少认知努力，而传统强化学习智能体在提升性能时不会变得更高效。如果智能体能在学习过程中考虑计算成本，就能实现更节能的智能体或释放计算资源用于其他任务。

Method: 向智能体展示其计算成本，并赋予它们控制何时使用计算的能力。在Arcade Learning Environment上进行实验。

Result: 在相同的训练计算预算下，考虑计算成本的智能体在75%的游戏中表现更好，并且平均使用三倍更少的计算资源。

Conclusion: 让强化学习智能体考虑计算成本可以有效提高计算效率，在保持性能的同时显著减少计算资源消耗。

Abstract: While reinforcement learning agents can achieve superhuman performance in
many complex tasks, they typically do not become more computationally efficient
as they improve. In contrast, humans gradually require less cognitive effort as
they become more proficient at a task. If agents could reason about their
compute as they learn, could they similarly reduce their computation footprint?
If they could, we could have more energy efficient agents or free up compute
cycles for other processes like planning. In this paper, we experiment with
showing agents the cost of their computation and giving them the ability to
control when they use compute. We conduct our experiments on the Arcade
Learning Environment, and our results demonstrate that with the same training
compute budget, agents that reason about their compute perform better on 75% of
games. Furthermore, these agents use three times less compute on average. We
analyze individual games and show where agents gain these efficiencies.

</details>


### [114] [Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes](https://arxiv.org/abs/2510.22836)
*Guanyu Yao,Qiucheng Wu,Yang Zhang,Zhaowen Wang,Handong Zhao,Shiyu Chang*

Main category: cs.AI

TL;DR: 该论文分析了多模态大语言模型中存在的模态差距问题，即模型过度依赖文本线索而忽视视觉内容，导致视觉推理能力不足。作者从训练方法角度探讨了如何通过数据和损失函数设计来弥合这一差距。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉和文本模态之间存在不平衡的推理能力，模型过度依赖文本线索而忽视视觉内容，这影响了需要真正视觉推理的任务性能。

Method: 从数据和损失函数设计两个互补角度，系统性地探索弥合模态差距的策略，分析现有训练方法如何放大这一差距。

Result: 研究发现现有训练方法会放大模态差距，通过提出的策略可以缓解这一问题，促进更平衡的多模态推理。

Conclusion: 论文为开发能够减轻模态差距的训练方法提供了见解，有助于实现更平衡的多模态推理能力。

Abstract: Multimodal large language models (MLLMs) have demonstrated strong
capabilities on vision-and-language tasks. However, recent findings reveal an
imbalance in their reasoning capabilities across visual and textual modalities.
Specifically, current MLLMs often over-rely on textual cues while
under-attending to visual content, resulting in suboptimal performance on tasks
that require genuine visual reasoning. We refer to this phenomenon as the
\textit{modality gap}, defined as the performance disparity between
text-centric and vision-centric inputs. In this paper, we analyze the modality
gap through the lens of training recipes. We first show that existing training
recipes tend to amplify this gap. Then, we systematically explore strategies to
bridge it from two complementary perspectives: data and loss design. Our
findings provide insights into developing training recipes that mitigate the
modality gap and promote more balanced multimodal reasoning. Our code is
publicly available at https://github.com/UCSB-NLP-Chang/Bridging-Modality-Gap.

</details>


### [115] [Lyapunov Function-guided Reinforcement Learning for Flight Control](https://arxiv.org/abs/2510.22840)
*Yifei Li,Erik-Jan van Kampen*

Main category: cs.AI

TL;DR: 开发并改进了级联在线学习飞行控制系统，重点研究其收敛性能，通过李雅普诺夫函数增量的变化来表征，并考虑了离散化误差和增量模型引入的状态预测误差。


<details>
  <summary>Details</summary>
Motivation: 研究级联在线学习飞行控制系统的收敛性能，特别关注动作平滑性改进后的系统表现，需要量化分析系统的稳定性。

Method: 使用李雅普诺夫函数候选的增量变化作为收敛性能指标，推导过程中考虑了离散化误差和增量模型的状态预测误差，通过飞行控制仿真进行对比验证。

Result: 通过仿真实验展示了改进后控制系统的收敛性能，量化了系统稳定性指标。

Conclusion: 所提出的方法能够有效评估级联在线学习飞行控制系统的收敛性能，为系统稳定性分析提供了理论依据和实验验证。

Abstract: A cascaded online learning flight control system has been developed and
enhanced with respect to action smoothness. In this paper, we investigate the
convergence performance of the control system, characterized by the increment
of a Lyapunov function candidate. The derivation of this metric accounts for
discretization errors and state prediction errors introduced by the incremental
model. Comparative results are presented through flight control simulations.

</details>


### [116] [Exploring Structures of Inferential Mechanisms through Simplistic Digital Circuits](https://arxiv.org/abs/2510.22883)
*Giovanni Sileno,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: 本文提出了一个统一框架，通过逻辑门电路的视角来整合认知研究和人工智能中的各种推理机制，识别了四种主要依赖形式和八种常见推理模式。


<details>
  <summary>Details</summary>
Motivation: 认知研究和人工智能为各种推理机制开发了不同的模型，但缺乏统一框架。本文试图填补这一空白，从物质角度假设高级激活过程。

Method: 使用基于逻辑门的电子电路简化视角，结合符号AI建模技术，通过组合探索识别依赖形式，并在逻辑程序背景下分析推理模式。

Result: 识别了四种主要依赖形式和八种常见推理模式，在统一框架中揭示了传统上不同的推理机制。通过逻辑程序的概率解释揭示了内部功能依赖。

Conclusion: 即使论证主要基于符号方法和数字系统基础设施，这些观察可能指向更普遍适用的结构。

Abstract: Cognitive studies and artificial intelligence have developed distinct models
for various inferential mechanisms (categorization, induction, abduction,
causal inference, contrast, merge, ...). Yet, both natural and artificial views
on cognition lack apparently a unifying framework. This paper formulates a
speculative answer attempting to respond to this gap. To postulate on
higher-level activation processes from a material perspective, we consider
inferential mechanisms informed by symbolic AI modelling techniques, through
the simplistic lenses of electronic circuits based on logic gates. We observe
that a logic gate view entails a different treatment of implication and
negation compared to standard logic and logic programming. Then, by
combinatorial exploration, we identify four main forms of dependencies that can
be realized by these inferential circuits. Looking at how these forms are
generally used in the context of logic programs, we identify eight common
inferential patterns, exposing traditionally distinct inferential mechanisms in
an unifying framework. Finally, following a probabilistic interpretation of
logic programs, we unveil inner functional dependencies. The paper concludes
elaborating in what sense, even if our arguments are mostly informed by
symbolic means and digital systems infrastructures, our observations may
pinpoint to more generally applicable structures.

</details>


### [117] [GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation](https://arxiv.org/abs/2510.22942)
*Zhuoxuan Li,Jieyuan Pei,Tangwei Ye,Zhongyuan Lai,Zihan Liu,Fengyuan Xu,Qi Zhang,Liang Hu*

Main category: cs.AI

TL;DR: GTR-Mamba是一个新颖的POI推荐框架，通过跨流形条件路由同时建模空间层次结构和用户时间上下文动态变化，在双曲几何中建模静态偏好层次，在欧几里得切空间中处理动态序列更新。


<details>
  <summary>Details</summary>
Motivation: 现有POI推荐模型难以同时捕捉空间选择的层次结构和用户特定时间上下文的动态变化及不规则转变，存在根本性局限。

Method: 利用不同数学空间的优势：在双曲几何中建模静态树状偏好层次，在欧几里得切空间中使用新颖的Mamba层路由动态序列更新，通过跨流形通道融合时空信息来显式引导状态空间模型。

Result: 在三个真实世界数据集上的广泛实验表明，GTR-Mamba在下一个POI推荐任务中持续优于最先进的基线模型。

Conclusion: GTR-Mamba通过跨流形条件路由成功解决了同时建模空间层次结构和时间上下文动态的挑战，为POI推荐提供了有效的解决方案。

Abstract: Next Point-of-Interest (POI) recommendation is a critical task in modern
Location-Based Social Networks (LBSNs), aiming to model the complex
decision-making process of human mobility to provide personalized
recommendations for a user's next check-in location. Existing POI
recommendation models, predominantly based on Graph Neural Networks and
sequential models, have been extensively studied. However, these models face a
fundamental limitation: they struggle to simultaneously capture the inherent
hierarchical structure of spatial choices and the dynamics and irregular shifts
of user-specific temporal contexts. To overcome this limitation, we propose
GTR-Mamba, a novel framework for cross-manifold conditioning and routing.
GTR-Mamba leverages the distinct advantages of different mathematical spaces
for different tasks: it models the static, tree-like preference hierarchies in
hyperbolic geometry, while routing the dynamic sequence updates to a novel
Mamba layer in the computationally stable and efficient Euclidean tangent
space. This process is coordinated by a cross-manifold channel that fuses
spatio-temporal information to explicitly steer the State Space Model (SSM),
enabling flexible adaptation to contextual changes. Extensive experiments on
three real-world datasets demonstrate that GTR-Mamba consistently outperforms
state-of-the-art baseline models in next POI recommendation.

</details>


### [118] [Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner](https://arxiv.org/abs/2510.22969)
*Kechen Meng,Sinuo Zhang,Rongpeng Li,Xiangming Meng,Chan Wang,Ming Lei,Zhifeng Zhao*

Main category: cs.AI

TL;DR: 提出MA-CDMP方法，使用扩散模型和平均场机制解决去中心化通信资源管理中的非平稳性和合作问题


<details>
  <summary>Details</summary>
Motivation: 解决集中式MARL的可扩展性和隐私风险，以及分布式训练中的非平稳性和有限合作问题

Method: 基于模型强化学习，使用扩散模型捕捉环境动态和规划轨迹，结合逆动态模型生成动作，引入平均场机制近似大规模智能体交互

Result: 在平均奖励和QoS指标上持续优于现有MARL基线方法

Conclusion: MA-CDMP展示了在真实无线网络优化中的可扩展性和实用性

Abstract: In wireless communication systems, efficient and adaptive resource allocation
plays a crucial role in enhancing overall Quality of Service (QoS). While
centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a
central coordinator for policy training and resource scheduling, they suffer
from scalability issues and privacy risks. In contrast, the Distributed
Training with Decentralized Execution (DTDE) paradigm enables distributed
learning and decision-making, but it struggles with non-stationarity and
limited inter-agent cooperation, which can severely degrade system performance.
To overcome these challenges, we propose the Multi-Agent Conditional Diffusion
Model Planner (MA-CDMP) for decentralized communication resource management.
Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP
employs Diffusion Models (DMs) to capture environment dynamics and plan future
trajectories, while an inverse dynamics model guides action generation, thereby
alleviating the sample inefficiency and slow convergence of conventional DTDE
methods. Moreover, to approximate large-scale agent interactions, a Mean-Field
(MF) mechanism is introduced as an assistance to the classifier in DMs. This
design mitigates inter-agent non-stationarity and enhances cooperation with
minimal communication overhead in distributed settings. We further
theoretically establish an upper bound on the distributional approximation
error introduced by the MF-based diffusion generation, guaranteeing convergence
stability and reliable modeling of multi-agent stochastic dynamics. Extensive
experiments demonstrate that MA-CDMP consistently outperforms existing MARL
baselines in terms of average reward and QoS metrics, showcasing its
scalability and practicality for real-world wireless network optimization.

</details>


### [119] [Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction](https://arxiv.org/abs/2510.22981)
*Jin Hu,Jiakai Wang,Linna Jing,Haolin Li,Haodong Liu,Haotong Qin,Aishan Liu,Ke Xu,Xianglong Liu*

Main category: cs.AI

TL;DR: 本文提出了多维度指令不确定性减少框架(InSUR)，通过稳定对抗优化、补充缺失知识和明确评估边界，生成更优的语义约束对抗样本。


<details>
  <summary>Details</summary>
Motivation: 当前生成语义约束对抗样本的方法攻击能力不足，因为人类指令中的语义不确定性因素（如指代多样性、描述不完整性和边界模糊性）尚未得到充分研究。

Method: 提出InSUR框架，包含三个维度：1）采样方法维度：残差驱动攻击方向稳定化；2）任务建模维度：上下文编码攻击场景约束；3）生成器评估维度：语义抽象攻击评估增强。

Result: 广泛实验证明InSUR在迁移攻击性能上的优越性，并首次实现了无参考的语义约束3D对抗样本生成。

Conclusion: InSUR框架通过多维度减少指令不确定性，能够生成更令人满意的语义约束对抗样本，具有更好的可迁移性、适应性和有效性。

Abstract: Recently, semantically constrained adversarial examples (SemanticAE), which
are directly generated from natural language instructions, have become a
promising avenue for future research due to their flexible attacking forms. To
generate SemanticAEs, current methods fall short of satisfactory attacking
ability as the key underlying factors of semantic uncertainty in human
instructions, such as referring diversity, descriptive incompleteness, and
boundary ambiguity, have not been fully investigated. To tackle the issues,
this paper develops a multi-dimensional instruction uncertainty reduction
(InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable,
adaptive, and effective. Specifically, in the dimension of the sampling method,
we propose the residual-driven attacking direction stabilization to alleviate
the unstable adversarial optimization caused by the diversity of language
references. By coarsely predicting the language-guided sampling process, the
optimization process will be stabilized by the designed ResAdv-DDIM sampler,
therefore releasing the transferable and robust adversarial capability of
multi-step diffusion models. In task modeling, we propose the context-encoded
attacking scenario constraint to supplement the missing knowledge from
incomplete human instructions. Guidance masking and renderer integration are
proposed to regulate the constraints of 2D/3D SemanticAE, activating stronger
scenario-adapted attacks. Moreover, in the dimension of generator evaluation,
we propose the semantic-abstracted attacking evaluation enhancement by
clarifying the evaluation boundary, facilitating the development of more
effective SemanticAE generators. Extensive experiments demonstrate the
superiority of the transfer attack performance of InSUR. Moreover, we realize
the reference-free generation of semantically constrained 3D adversarial
examples for the first time.

</details>


### [120] [ProfileXAI: User-Adaptive Explainable AI](https://arxiv.org/abs/2510.22998)
*Gilber A. Corrales,Carlos Andrés Ferro Sánchez,Reinel Tabares-Soto,Jesús Alfonso López Sotelo,Gonzalo A. Ruz,Johan Sebastian Piña Durán*

Main category: cs.AI

TL;DR: ProfileXAI是一个模型和领域无关的框架，结合后置解释器(SHAP、LIME、Anchor)与检索增强LLM，为不同类型用户生成解释。系统索引多模态知识库，通过量化标准为每个实例选择解释器，并通过聊天提示生成基于知识的叙述。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够为不同类型用户提供高效可信解释的系统，解决现有解释方法在用户适配性和解释质量方面的不足。

Method: 将后置解释器(SHAP、LIME、Anchor)与检索增强的大型语言模型耦合，索引多模态知识库，基于量化标准选择解释器，使用聊天提示生成基于知识的叙述。

Result: 在心脏病和甲状腺癌数据集上的评估显示：LIME在保真度-鲁棒性权衡方面最佳；Anchor产生最稀疏、低token的规则；SHAP获得最高满意度。Profile条件化稳定token使用并保持各用户配置文件的正面评分。

Conclusion: ProfileXAI能够为不同用户配置文件提供高效可信的解释，没有单一解释器在所有指标上表现最优，但通过框架集成可以实现综合优势。

Abstract: ProfileXAI is a model- and domain-agnostic framework that couples post-hoc
explainers (SHAP, LIME, Anchor) with retrieval - augmented LLMs to produce
explanations for different types of users. The system indexes a multimodal
knowledge base, selects an explainer per instance via quantitative criteria,
and generates grounded narratives with chat-enabled prompting. On Heart Disease
and Thyroid Cancer datasets, we evaluate fidelity, robustness, parsimony, token
use, and perceived quality. No explainer dominates: LIME achieves the best
fidelity--robustness trade-off (Infidelity $\le 0.30$, $L<0.7$ on Heart
Disease); Anchor yields the sparsest, low-token rules; SHAP attains the highest
satisfaction ($\bar{x}=4.1$). Profile conditioning stabilizes tokens ($\sigma
\le 13\%$) and maintains positive ratings across profiles ($\bar{x}\ge 3.7$,
with domain experts at $3.77$), enabling efficient and trustworthy
explanations.

</details>


### [121] [From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports](https://arxiv.org/abs/2510.23008)
*Qiuli Wang,Xiaoming Li,Jie Chen,Yongxu Liu,Xingpeng Zhang,Chen Liu,Wei Chen*

Main category: cs.AI

TL;DR: 该研究提出多维度可信度评估框架(MDCA)来提升LLM生成肝脏MRI报告的可信度，并为机构特定提示优化提供指导，在SiliconFlow平台上评估了多个先进LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对不同临床场景优化提示设计的系统指导，以及评估LLM生成放射学报告可信度的标准化框架。

Method: 引入多维度可信度评估(MDCA)框架，在SiliconFlow平台上评估多个先进LLM(Kimi-K2-Instruct-0905、Qwen3-235B-A22B-Instruct-2507、DeepSeek-V3、ByteDance-Seed-OSS-36B-Instruct)的性能。

Result: 应用MDCA框架评估和比较了多个LLM在生成肝脏MRI报告方面的表现。

Conclusion: 该研究为提升LLM生成放射学报告的可信度提供了系统评估框架和优化指导。

Abstract: Large language models (LLMs) have demonstrated promising performance in
generating diagnostic conclusions from imaging findings, thereby supporting
radiology reporting, trainee education, and quality control. However,
systematic guidance on how to optimize prompt design across different clinical
contexts remains underexplored. Moreover, a comprehensive and standardized
framework for assessing the trustworthiness of LLM-generated radiology reports
is yet to be established. This study aims to enhance the trustworthiness of
LLM-generated liver MRI reports by introducing a Multi-Dimensional Credibility
Assessment (MDCA) framework and providing guidance on institution-specific
prompt optimization. The proposed framework is applied to evaluate and compare
the performance of several advanced LLMs, including Kimi-K2-Instruct-0905,
Qwen3-235B-A22B-Instruct-2507, DeepSeek-V3, and
ByteDance-Seed-OSS-36B-Instruct, using the SiliconFlow platform.

</details>


### [122] [Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution](https://arxiv.org/abs/2510.23026)
*Crimson Stambaugh,Rajesh P. N. Rao*

Main category: cs.AI

TL;DR: 提出了混合密度扩散器（MDD），通过在不同时间范围内调整规划密度来改进扩散规划器，在多个任务领域达到新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散规划器使用稀疏步长规划优于单步规划，但过度稀疏会降低性能。作者假设时间密度阈值在时间范围内不均匀，某些轨迹部分需要更密集的规划。

Method: 提出MDD扩散规划器，在整个时间范围内的密度是可调的超参数，允许在不同时间区域设置不同的规划密度。

Result: 在Maze2D、Franka Kitchen和Antmaze D4RL任务领域实现了新的最先进性能。

Conclusion: 通过调整时间范围内的规划密度，MDD能够更好地捕捉长期依赖关系，同时避免过度稀疏规划带来的性能下降。

Abstract: Recent studies demonstrate that diffusion planners benefit from sparse-step
planning over single-step planning. Training models to skip steps in their
trajectories helps capture long-term dependencies without additional or memory
computational cost. However, predicting excessively sparse plans degrades
performance. We hypothesize this temporal density threshold is non-uniform
across a temporal horizon and that certain parts of a planned trajectory should
be more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion
planner where the densities throughout the horizon are tunable hyperparameters.
MDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL
task domains.

</details>


### [123] [A Survey of AI Scientists: Surveying the automatic Scientists and Research](https://arxiv.org/abs/2510.23045)
*Guiyao Tie,Pan Zhou,Lichao Sun*

Main category: cs.AI

TL;DR: 本文提出了一个统一的六阶段方法论框架来分析AI科学家系统，追踪了该领域从基础模块到闭环系统再到可扩展性发展的演变历程。


<details>
  <summary>Details</summary>
Motivation: AI正从计算工具转变为自主科学知识创造者，但快速无序发展导致研究领域碎片化，需要系统性框架来理解方法论原则和发展趋势。

Method: 引入六阶段方法论框架：文献综述、创意生成、实验准备、实验执行、科学写作和论文生成，通过这一分析视角追踪领域演变。

Result: 系统梳理了AI科学家领域从基础模块(2022-2023)到闭环系统(2024)再到可扩展性与人机协作(2025至今)的发展路径。

Conclusion: 该调查不仅阐明了自主科学的现状，还为克服鲁棒性和治理方面的挑战提供了关键路线图，指导下一代系统成为人类科学探究中值得信赖的合作伙伴。

Abstract: Artificial intelligence is undergoing a profound transition from a
computational instrument to an autonomous originator of scientific knowledge.
This emerging paradigm, the AI scientist, is architected to emulate the
complete scientific workflow-from initial hypothesis generation to the final
synthesis of publishable findings-thereby promising to fundamentally reshape
the pace and scale of discovery. However, the rapid and unstructured
proliferation of these systems has created a fragmented research landscape,
obscuring overarching methodological principles and developmental trends. This
survey provides a systematic and comprehensive synthesis of this domain by
introducing a unified, six-stage methodological framework that deconstructs the
end-to-end scientific process into: Literature Review, Idea Generation,
Experimental Preparation, Experimental Execution, Scientific Writing, and Paper
Generation. Through this analytical lens, we chart the field's evolution from
early Foundational Modules (2022-2023) to integrated Closed-Loop Systems
(2024), and finally to the current frontier of Scalability, Impact, and
Human-AI Collaboration (2025-present). By rigorously synthesizing these
developments, this survey not only clarifies the current state of autonomous
science but also provides a critical roadmap for overcoming remaining
challenges in robustness and governance, ultimately guiding the next generation
of systems toward becoming trustworthy and indispensable partners in human
scientific inquiry.

</details>


### [124] [TLCD: A Deep Transfer Learning Framework for Cross-Disciplinary Cognitive Diagnosis](https://arxiv.org/abs/2510.23062)
*Zhifeng Wang,Meixin Su,Yang Yang,Chunyan Zeng,Lizhi Ye*

Main category: cs.AI

TL;DR: 提出了一种基于深度学习和迁移学习的跨学科认知诊断方法(TLCD)，利用主学科的共同特征提升目标学科模型性能，在跨学科认知诊断任务中表现优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 在线教育模式快速发展，但跨学科领域的认知诊断面临知识体系差异、认知结构不同和数据特征不一致等挑战，传统方法难以应对。

Method: 结合深度学习技术和迁移学习策略，研究神经网络认知诊断和知识关联神经网络认知诊断，提出TLCD跨学科认知诊断方法。

Result: 实验结果表明，基于深度学习的跨学科认知诊断模型在跨学科认知诊断任务中表现优于基础模型，能更准确地评估学生学习情况。

Conclusion: TLCD方法有效解决了跨学科认知诊断中的挑战，通过迁移学习利用主学科特征提升了目标学科的诊断准确性。

Abstract: Driven by the dual principles of smart education and artificial intelligence
technology, the online education model has rapidly emerged as an important
component of the education industry. Cognitive diagnostic technology can
utilize students' learning data and feedback information in educational
evaluation to accurately assess their ability level at the knowledge level.
However, while massive amounts of information provide abundant data resources,
they also bring about complexity in feature extraction and scarcity of
disciplinary data. In cross-disciplinary fields, traditional cognitive
diagnostic methods still face many challenges. Given the differences in
knowledge systems, cognitive structures, and data characteristics between
different disciplines, this paper conducts in-depth research on neural network
cognitive diagnosis and knowledge association neural network cognitive
diagnosis, and proposes an innovative cross-disciplinary cognitive diagnosis
method (TLCD). This method combines deep learning techniques and transfer
learning strategies to enhance the performance of the model in the target
discipline by utilizing the common features of the main discipline. The
experimental results show that the cross-disciplinary cognitive diagnosis model
based on deep learning performs better than the basic model in
cross-disciplinary cognitive diagnosis tasks, and can more accurately evaluate
students' learning situation.

</details>


### [125] [Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs](https://arxiv.org/abs/2510.23127)
*Kai Zhuang,Jiawei Zhang,Yumou Liu,Hanqun Cao,Chunbin Gu,Mengdi Liu,Zhangyang Gao,Zitong Jerry Wang,Xuanhe Zhou,Pheng-Ann Heng,Lijun Wu,Conghui He,Cheng Tan*

Main category: cs.AI

TL;DR: 研究发现，在科学大语言模型处理生物分子序列时，提供高层次结构化上下文比直接处理原始序列表现更好，甚至原始序列会干扰模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决科学大语言模型在处理原始生物分子序列时面临的tokenization困境，无论是将序列视为专门语言还是单独模态都存在局限性。

Method: 系统比较了领先科学大语言模型在生物推理任务上的三种输入模式：仅序列、仅上下文、以及两者结合。

Result: 仅上下文方法在所有模式中表现最佳，原始序列的加入反而会降低性能，表明原始序列对模型来说是信息噪声。

Conclusion: 科学大语言模型的主要优势在于对结构化知识的推理能力，而非直接从序列中解释生物分子语法，应将其重新定位为专家知识的推理引擎。

Abstract: Scientific Large Language Models (Sci-LLMs) have emerged as a promising
frontier for accelerating biological discovery. However, these models face a
fundamental challenge when processing raw biomolecular sequences: the
tokenization dilemma. Whether treating sequences as a specialized language,
risking the loss of functional motif information, or as a separate modality,
introducing formidable alignment challenges, current strategies fundamentally
limit their reasoning capacity. We challenge this sequence-centric paradigm by
positing that a more effective strategy is to provide Sci-LLMs with high-level
structured context derived from established bioinformatics tools, thereby
bypassing the need to interpret low-level noisy sequence data directly. Through
a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we
tested three input modes: sequence-only, context-only, and a combination of
both. Our findings are striking: the context-only approach consistently and
substantially outperforms all other modes. Even more revealing, the inclusion
of the raw sequence alongside its high-level context consistently degrades
performance, indicating that raw sequences act as informational noise, even for
models with specialized tokenization schemes. These results suggest that the
primary strength of existing Sci-LLMs lies not in their nascent ability to
interpret biomolecular syntax from scratch, but in their profound capacity for
reasoning over structured, human-readable knowledge. Therefore, we argue for
reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines
over expert knowledge. This work lays the foundation for a new class of hybrid
scientific AI agents, repositioning the developmental focus from direct
sequence interpretation towards high-level knowledge synthesis. The code is
available at github.com/opendatalab-raise-dev/CoKE.

</details>


### [126] [Guiding Skill Discovery with Foundation Models](https://arxiv.org/abs/2510.23167)
*Zhao Yang,Thomas M. Moerland,Mike Preuss,Aske Plaat,Vincent François-Lavet,Edward S. Hu*

Main category: cs.AI

TL;DR: 提出FoG技能发现方法，通过基础模型将人类意图融入技能发现过程，消除危险行为并学习符合偏好的技能


<details>
  <summary>Details</summary>
Motivation: 现有技能发现方法仅最大化技能多样性而不考虑人类偏好，导致危险行为（如机器人翻滚），需要将人类意图融入技能发现

Method: 从基础模型提取评分函数评估状态，基于人类意图为期望状态分配高值，然后用这些分数重新加权技能发现算法的奖励

Result: FoG成功消除了翻滚等不良行为，避免了危险区域，在状态和像素任务中都能发现难以定义行为的技能

Conclusion: FoG方法有效将人类偏好融入技能发现，生成更安全、更有用的技能，且能发现难以明确定义的行为

Abstract: Learning diverse skills without hand-crafted reward functions could
accelerate reinforcement learning in downstream tasks. However, existing skill
discovery methods focus solely on maximizing the diversity of skills without
considering human preferences, which leads to undesirable behaviors and
possibly dangerous skills. For instance, a cheetah robot trained using previous
methods learns to roll in all directions to maximize skill diversity, whereas
we would prefer it to run without flipping or entering hazardous areas. In this
work, we propose a Foundation model Guided (FoG) skill discovery method, which
incorporates human intentions into skill discovery through foundation models.
Specifically, FoG extracts a score function from foundation models to evaluate
states based on human intentions, assigning higher values to desirable states
and lower to undesirable ones. These scores are then used to re-weight the
rewards of skill discovery algorithms. By optimizing the re-weighted skill
discovery rewards, FoG successfully learns to eliminate undesirable behaviors,
such as flipping or rolling, and to avoid hazardous areas in both state-based
and pixel-based tasks. Interestingly, we show that FoG can discover skills
involving behaviors that are difficult to define. Interactive visualisations
are available from https://sites.google.com/view/submission-fog.

</details>


### [127] [AUPO -- Abstracted Until Proven Otherwise: A Reward Distribution Based Abstraction Algorithm](https://arxiv.org/abs/2510.23214)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: AUPO是一种MCTS决策策略的改进方法，通过自动动作抽象提升性能，无需转移概率或DAG搜索图，能检测对称动作。


<details>
  <summary>Details</summary>
Motivation: 改进MCTS决策策略，解决现有自动抽象算法需要转移概率和DAG搜索图的限制，提升对称动作检测能力。

Method: 基于MCTS过程中获得的奖励分布统计，开发自动动作抽象算法AUPO，仅影响决策策略。

Result: 在IPPC基准问题上，AUPO明显优于标准MCTS，能检测ASAP等框架难以处理的对称动作。

Conclusion: AUPO是一种有效的MCTS改进方法，与其他只影响树搜索的抽象技术兼容，提升了决策性能。

Abstract: We introduce a novel, drop-in modification to Monte Carlo Tree Search's
(MCTS) decision policy that we call AUPO. Comparisons based on a range of IPPC
benchmark problems show that AUPO clearly outperforms MCTS. AUPO is an
automatic action abstraction algorithm that solely relies on reward
distribution statistics acquired during the MCTS. Thus, unlike other automatic
abstraction algorithms, AUPO requires neither access to transition
probabilities nor does AUPO require a directed acyclic search graph to build
its abstraction, allowing AUPO to detect symmetric actions that
state-of-the-art frameworks like ASAP struggle with when the resulting
symmetric states are far apart in state space. Furthermore, as AUPO only
affects the decision policy, it is not mutually exclusive with other
abstraction techniques that only affect the tree search.

</details>


### [128] [Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach](https://arxiv.org/abs/2510.23216)
*Alessandro Sestini,Joakim Bergdahl,Jean-Philippe Barrette-LaPierre,Florian Fuchs,Brady Chen,Micheal Jones,Linus Gisslén*

Main category: cs.AI

TL;DR: 提出了一种针对游戏工业的样本高效深度强化学习方法，在EA SPORTS FC 25中训练守门员智能体，比内置AI提升了10%的救球率，训练速度快50%，并产生更人性化的游戏体验。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在游戏行业应用有限，现有研究主要关注训练超人智能体，而游戏工作室需要的是在有限资源下训练类人智能体的实用方法。

Method: 基于价值函数的深度强化学习方法，利用预收集数据提高样本效率，增加网络可塑性，专门为工业环境中的智能体训练和微调设计。

Result: 在EA SPORTS FC 25中训练的守门员智能体比游戏内置AI救球率提高10%，训练速度比标准DRL方法快50%，领域专家评估显示产生更人性化的游戏体验。

Conclusion: 该方法成功解决了游戏工业中DRL应用的实用性挑战，效果显著，计划在系列游戏的后续版本中替代手工制作的智能体。

Abstract: While several high profile video games have served as testbeds for Deep
Reinforcement Learning (DRL), this technique has rarely been employed by the
game industry for crafting authentic AI behaviors. Previous research focuses on
training super-human agents with large models, which is impractical for game
studios with limited resources aiming for human-like agents. This paper
proposes a sample-efficient DRL method tailored for training and fine-tuning
agents in industrial settings such as the video game industry. Our method
improves sample efficiency of value-based DRL by leveraging pre-collected data
and increasing network plasticity. We evaluate our method training a goalkeeper
agent in EA SPORTS FC 25, one of the best-selling football simulations today.
Our agent outperforms the game's built-in AI by 10% in ball saving rate.
Ablation studies show that our method trains agents 50% faster compared to
standard DRL methods. Finally, qualitative evaluation from domain experts
indicates that our approach creates more human-like gameplay compared to
hand-crafted agents. As a testimony of the impact of the approach, the method
is intended to replace the hand-crafted counterpart in next iterations of the
series.

</details>


### [129] [Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action](https://arxiv.org/abs/2510.23221)
*Hong Wang,Wenkai Yang,Jie Wang,Huanshuo Dong,Zijie Geng,Zhen Huang,Depeng Xie,Zhezheng Hao,Hande Dong*

Main category: cs.AI

TL;DR: 提出了一种名为BlocKOA的新算法，用于高效生成集成电路热仿真数据，相比现有方法实现420倍加速，同时保证数据精度。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法需要大量高保真训练数据，导致计算成本高昂，限制了神经网络算子等方法的实际应用。

Method: 采用块Krylov算法结合热方程结构快速获取基础解，然后组合生成满足物理约束的温度分布，最后应用热算子确定热源分布。

Result: BlocKOA的时间复杂度比现有方法低一个数量级，在生成5000个芯片的热仿真数据时实现420倍加速，仅用4%的时间就能达到可比性能。

Conclusion: BlocKOA算法能高效生成精确的IC热仿真数据，显著降低数据驱动方法的训练成本，具有重要应用价值。

Abstract: Recent advances in data-driven approaches, such as neural operators (NOs),
have shown substantial efficacy in reducing the solution time for integrated
circuit (IC) thermal simulations. However, a limitation of these approaches is
requiring a large amount of high-fidelity training data, such as chip
parameters and temperature distributions, thereby incurring significant
computational costs. To address this challenge, we propose a novel algorithm
for the generation of IC thermal simulation data, named block Krylov and
operator action (BlocKOA), which simultaneously accelerates the data generation
process and enhances the precision of generated data. BlocKOA is specifically
designed for IC applications. Initially, we use the block Krylov algorithm
based on the structure of the heat equation to quickly obtain a few basic
solutions. Then we combine them to get numerous temperature distributions that
satisfy the physical constraints. Finally, we apply heat operators on these
functions to determine the heat source distributions, efficiently generating
precise data points. Theoretical analysis shows that the time complexity of
BlocKOA is one order lower than the existing method. Experimental results
further validate its efficiency, showing that BlocKOA achieves a 420-fold
speedup in generating thermal simulation data for 5000 chips with varying
physical parameters and IC structures. Even with just 4% of the generation
time, data-driven approaches trained on the data generated by BlocKOA exhibits
comparable performance to that using the existing method.

</details>


### [130] [CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach](https://arxiv.org/abs/2510.23304)
*Riccardo Romanello,Daniele Lizzio Bosco,Jacopo Cossio,Dusan Sutulovic,Giuseppe Serra,Carla Piazza,Paolo Burelli*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的CNOT门最小化方法，使用单一智能体处理不同尺寸的量子电路，在3到15量子比特的电路上超越了现有最优算法。


<details>
  <summary>Details</summary>
Motivation: CNOT门是量子计算中产生纠缠的关键组件，最小化CNOT门数量对量子算法效率至关重要，但该问题的计算复杂度尚未完全明确。

Method: 使用单一强化学习智能体处理固定尺寸m的电路，对不同尺寸的矩阵采用嵌入或高斯条纹化预处理，训练m=8的智能体并评估3-15量子比特的电路。

Result: 该方法在量子比特数增加时表现优于现有最优算法，特别是在较大尺寸电路中效果显著。

Conclusion: 强化学习方法在CNOT门最小化问题上具有潜力，单一智能体策略可有效处理不同尺寸的量子电路优化问题。

Abstract: CNOT gates are fundamental to quantum computing, as they facilitate
entanglement, a crucial resource for quantum algorithms. Certain classes of
quantum circuits are constructed exclusively from CNOT gates. Given their
widespread use, it is imperative to minimise the number of CNOT gates employed.
This problem, known as CNOT minimisation, remains an open challenge, with its
computational complexity yet to be fully characterised. In this work, we
introduce a novel reinforcement learning approach to address this task. Instead
of training multiple reinforcement learning agents for different circuit sizes,
we use a single agent up to a fixed size $m$. Matrices of sizes different from
m are preprocessed using either embedding or Gaussian striping. To assess the
efficacy of our approach, we trained an agent with m = 8, and evaluated it on
matrices of size n that range from 3 to 15. The results we obtained show that
our method overperforms the state-of-the-art algorithm as the value of n
increases.

</details>


### [131] [Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps](https://arxiv.org/abs/2510.23340)
*Anwesha Das,John Duff,Jörg Hoffmann,Vera Demberg*

Main category: cs.AI

TL;DR: 提出了一个基于理性沟通原则的自适应信号框架，用于优化人机协作中信息的及时传递，结合多步规划和用户意识模型来提升沟通效果。


<details>
  <summary>Details</summary>
Motivation: 在快速变化的环境中，确保人类准确理解关键任务要素需要辅助智能体不仅识别最高优先级信息，还要估计如何以及何时最有效地传递这些信息，因为人类注意力是零和认知资源。

Method: 使用贝叶斯参考解析和理性言语行为(RSA)建模框架，规划消息序列以优化用户信念与动态环境的及时对齐。智能体根据用户和场景特点调整消息的特异性和时机。

Result: 与基线方法相比，该方法的效果关键在于结合多步规划和现实的用户意识模型。这是RSA在动态环境通信和人类-AI交互中的首次应用。

Conclusion: 为人类-智能体团队中的语用沟通建立了理论基础，展示了如何利用认知科学见解来指导辅助智能体的设计。

Abstract: Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.

</details>


### [132] [Opinion Mining Based Entity Ranking using Fuzzy Logic Algorithmic Approach](https://arxiv.org/abs/2510.23384)
*Pratik N. Kalamkar,A. G. Phakatkar*

Main category: cs.AI

TL;DR: 提出了一种基于模糊逻辑的细粒度意见挖掘方法，用于从评论文本中提取更详细的属性信息并对实体进行排名


<details>
  <summary>Details</summary>
Motivation: 由于社交媒体和电商网站的快速发展，网络上存在大量意见数据。现有研究主要关注实体级别的意见分类，但缺乏细粒度属性级别的意见分析和实体排名方法

Method: 使用模糊逻辑推理进行细粒度意见挖掘，从评论文本中提取实体属性和组件信息，并确定意见的极性（正面、负面、中性）

Result: 该方法能够实现更深层次的细粒度意见分析，并基于提取的信息对实体进行排名

Conclusion: 提出的模糊逻辑方法在细粒度意见挖掘和实体排名方面具有创新性，填补了现有研究的空白

Abstract: Opinions are central to almost all human activities and are key influencers
of our behaviors. In current times due to growth of social networking website
and increase in number of e-commerce site huge amount of opinions are now
available on web. Given a set of evaluative statements that contain opinions
(or sentiments) about an Entity, opinion mining aims to extract attributes and
components of the object that have been commented on in each statement and to
determine whether the comments are positive, negative or neutral. While lot of
research recently has been done in field of opinion mining and some of it
dealing with ranking of entities based on review or opinion set, classifying
opinions into finer granularity level and then ranking entities has never been
done before. In this paper method for opinion mining from statements at a
deeper level of granularity is proposed. This is done by using fuzzy logic
reasoning, after which entities are ranked as per this information.

</details>


### [133] [AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines](https://arxiv.org/abs/2510.23408)
*Abolfazl Younesi,Zahra Najafabadi Samani,Thomas Fahringer*

Main category: cs.AI

TL;DR: AutoStreamPipe是一个利用大语言模型自动化设计和部署流处理管道的框架，通过超图思维(HGoT)技术显著提升开发效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决流处理管道设计中高层用户意图与平台特定实现之间的语义鸿沟，自动化管道设计以支持快速数据分析。

Method: 集成超图思维(HGoT)作为GoT的扩展版本，结合弹性执行策略和高级查询分析，实现多智能体推理的流处理管道自动化生成。

Result: 实验评估显示，相比LLM代码生成方法，AutoStreamPipe显著减少开发时间(6.3倍)和错误率(5.19倍)，通过新颖的错误自由评分(EFS)衡量。

Conclusion: AutoStreamPipe框架成功实现了流处理管道的自动化设计和部署，在效率和准确性方面都取得了显著提升。

Abstract: Data pipelines are essential in stream processing as they enable the
efficient collection, processing, and delivery of real-time data, supporting
rapid data analysis. In this paper, we present AutoStreamPipe, a novel
framework that employs Large Language Models (LLMs) to automate the design,
generation, and deployment of stream processing pipelines. AutoStreamPipe
bridges the semantic gap between high-level user intent and platform-specific
implementations across distributed stream processing systems for structured
multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an
extended version of GoT. AutoStreamPipe combines resilient execution
strategies, advanced query analysis, and HGoT to deliver pipelines with good
accuracy. Experimental evaluations on diverse pipelines demonstrate that
AutoStreamPipe significantly reduces development time (x6.3) and error rates
(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM
code-generation methods.

</details>


### [134] [Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising from A Foundation Model Lens](https://arxiv.org/abs/2510.23410)
*Jiahao Ji,Tianyu Wang,Yeshu Li,Yushen Huo,Zhilin Zhang,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 提出了Bid2X竞价基础模型，通过统一函数估计不同竞价场景下的广告效果，使用注意力机制和零膨胀投影模块处理竞价数据，在淘宝平台部署并显著提升了GMV和ROI。


<details>
  <summary>Details</summary>
Motivation: 解决现有竞价模型在跨场景泛化能力上的局限性，通过统一函数学习不同竞价场景下的基本效果估计函数。

Method: 构建Bid2X模型，使用统一序列嵌入编码异构数据，提出两种注意力机制分别处理变量间和时序依赖关系，采用变量感知融合模块进行自适应预测，设计零膨胀投影模块处理数据分布。

Result: 在8个数据集上离线评估显示优于各种基线方法，在线A/B测试中GMV提升4.65%，ROI提升2.44%。

Conclusion: Bid2X为计算广告中的竞价基础模型开辟了新途径，展示了在多个场景下的优越性能和泛化能力。

Abstract: Auto-bidding is crucial in facilitating online advertising by automatically
providing bids for advertisers. While previous work has made great efforts to
model bidding environments for better ad performance, it has limitations in
generalizability across environments since these models are typically tailored
for specific bidding scenarios. To this end, we approach the
scenario-independent principles through a unified function that estimates the
achieved effect under specific bids, such as budget consumption, gross
merchandise volume (GMV), page views, etc. Then, we propose a bidding
foundation model Bid2X to learn this fundamental function from data in various
scenarios. Our Bid2X is built over uniform series embeddings that encode
heterogeneous data through tailored embedding methods. To capture complex
inter-variable and dynamic temporal dependencies in bidding data, we propose
two attention mechanisms separately treating embeddings of different variables
and embeddings at different times as attention tokens for representation
learning. On top of the learned variable and temporal representations, a
variable-aware fusion module is used to perform adaptive bidding outcome
prediction. To model the unique bidding data distribution, we devise a
zero-inflated projection module to incorporate the estimated non-zero
probability into its value prediction, which makes up a joint optimization
objective containing classification and regression. The objective is proven to
converge to the zero-inflated distribution. Our model has been deployed on the
ad platform in Taobao, one of the world's largest e-commerce platforms. Offline
evaluation on eight datasets exhibits Bid2X's superiority compared to various
baselines and its generality across different scenarios. Bid2X increased GMV by
4.65% and ROI by 2.44% in online A/B tests, paving the way for bidding
foundation model in computational advertising.

</details>


### [135] [Causal Deep Q Network](https://arxiv.org/abs/2510.23424)
*Elouanes Khelifi,Amir Saki,Usef Faghihi*

Main category: cs.AI

TL;DR: 将因果推理整合到DQN中，使用PEACE公式估计因果效应，减少伪相关影响，提升问题解决能力


<details>
  <summary>Details</summary>
Motivation: 传统DQN依赖关联学习容易产生伪相关，限制了其问题解决能力，需要引入因果推理来理解环境的因果结构

Method: 提出将因果原则整合到DQN的新方法，利用PEACE公式估计因果效应，在训练过程中融入因果推理

Result: 在标准基准环境上的实验表明，该方法优于传统DQN，展示了因果推理在强化学习中的有效性

Conclusion: 通过原则性因果推断为深度强化学习智能体的能力提升提供了有前景的途径

Abstract: Deep Q Networks (DQN) have shown remarkable success in various reinforcement
learning tasks. However, their reliance on associative learning often leads to
the acquisition of spurious correlations, hindering their problem-solving
capabilities. In this paper, we introduce a novel approach to integrate causal
principles into DQNs, leveraging the PEACE (Probabilistic Easy vAriational
Causal Effect) formula for estimating causal effects. By incorporating causal
reasoning during training, our proposed framework enhances the DQN's
understanding of the underlying causal structure of the environment, thereby
mitigating the influence of confounding factors and spurious correlations. We
demonstrate that integrating DQNs with causal capabilities significantly
enhances their problem-solving capabilities without compromising performance.
Experimental results on standard benchmark environments showcase that our
approach outperforms conventional DQNs, highlighting the effectiveness of
causal reasoning in reinforcement learning. Overall, our work presents a
promising avenue for advancing the capabilities of deep reinforcement learning
agents through principled causal inference.

</details>


### [136] [What are the odds? Risk and uncertainty about AI existential risk](https://arxiv.org/abs/2510.23453)
*Marco Grossi*

Main category: cs.AI

TL;DR: 对Cappelen等人关于AI生存风险的分类分析论文进行评论，强调线性风险模型的哲学局限性，讨论瑞士奶酪模型的差异，以及在认知无差异情况下灾难概率P(D)可能更高，并区分风险与不确定性。


<details>
  <summary>Details</summary>
Motivation: 提醒人们注意AI存在风险分析中线性模型的哲学局限性，强调需要更全面地考虑不确定性维度来评估AI灾难风险。

Method: 通过分析作者使用的模型，比较标准瑞士奶酪模型与论文模型的差异，讨论认知无差异情况下的概率评估，并区分风险与不确定性（选项不确定性和状态空间不确定性）。

Result: 在认知无差异情况下，考虑到各层之间的结构关系，灾难概率P(D)可能比初步估计更高；纳入不确定性维度可以提供对AI存在风险可能性的更好理解。

Conclusion: 在AI存在风险的定性讨论中，需要纳入选项不确定性和状态空间不确定性这两个维度，以获得对灾难概率P(D)更准确的理解。

Abstract: This work is a commentary of the article
\href{https://doi.org/10.18716/ojs/phai/2025.2801}{AI Survival Stories: a
Taxonomic Analysis of AI Existential Risk} by Cappelen, Goldstein, and
Hawthorne. It is not just a commentary though, but a useful reminder of the
philosophical limitations of \say{linear} models of risk. The article will
focus on the model employed by the authors: first, I discuss some differences
between standard Swiss Cheese models and this one. I then argue that in a
situation of epistemic indifference the probability of P(D) is higher than what
one might first suggest, given the structural relationships between layers. I
then distinguish between risk and uncertainty, and argue that any estimation of
P(D) is structurally affected by two kinds of uncertainty: option uncertainty
and state-space uncertainty. Incorporating these dimensions of uncertainty into
our qualitative discussion on AI existential risk can provide a better
understanding of the likeliness of P(D).

</details>


### [137] [Policy-Aware Generative AI for Safe, Auditable Data Access Governance](https://arxiv.org/abs/2510.23474)
*Shames Al Mandalawi,Muzakkiruddin Ahmed Mohammed,Hendrika Maclean,Mert Can Cakmak,John R. Talburt*

Main category: cs.AI

TL;DR: 提出一个基于LLM的策略感知控制器，通过六阶段推理框架将自然语言请求与书面策略进行匹配，实现安全、合规且可追溯的访问决策。


<details>
  <summary>Details</summary>
Motivation: 企业需要满足最小权限、合规性和可审计性的访问决策，但传统方法难以有效解释自然语言请求与书面策略。

Method: 使用Google Gemini 2.0 Flash实现六阶段推理框架：上下文解释、用户验证、数据分类、业务目的测试、合规映射和风险综合，采用早期硬策略门控和默认拒绝机制。

Result: 在14个典型案例评估中，精确决策匹配率从10/14提升到13/14（92.9%），拒绝召回率达到1.00，必须拒绝场景的误批准率降为0，功能适当性和合规性均为14/14。

Conclusion: 策略约束的LLM推理结合显式门控和审计追踪，能够将人类可读策略转化为安全、合规且可追溯的机器决策。

Abstract: Enterprises need access decisions that satisfy least privilege, comply with
regulations, and remain auditable. We present a policy aware controller that
uses a large language model (LLM) to interpret natural language requests
against written policies and metadata, not raw data. The system, implemented
with Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context
interpretation, user validation, data classification, business purpose test,
compliance mapping, and risk synthesis) with early hard policy gates and deny
by default. It returns APPROVE, DENY, CONDITIONAL together with cited controls
and a machine readable rationale. We evaluate on fourteen canonical cases
across seven scenario families using a privacy preserving benchmark. Results
show Exact Decision Match improving from 10/14 to 13/14 (92.9\%) after applying
policy gates, DENY recall rising to 1.00, False Approval Rate on must-deny
families dropping to 0, and Functional Appropriateness and Compliance Adherence
at 14/14. Expert ratings of rationale quality are high, and median latency is
under one minute. These findings indicate that policy constrained LLM
reasoning, combined with explicit gates and audit trails, can translate human
readable policies into safe, compliant, and traceable machine decisions.

</details>


### [138] [Human-AI Collaborative Uncertainty Quantification](https://arxiv.org/abs/2510.23476)
*Sima Noorani,Shayan Kiyani,George Pappas,Hamed Hassani*

Main category: cs.AI

TL;DR: 提出了Human AI Collaborative Uncertainty Quantification框架，通过AI模型优化人类专家的预测集，避免对正确人类判断的损害，同时补充人类遗漏的正确结果。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在高风险决策中缺乏领域知识、长期上下文和物理世界推理能力，需要结合人类和AI的互补优势来提升不确定性量化下的决策可靠性。

Method: 引入协作不确定性量化框架，基于单一评分函数的两阈值结构构建最优协作预测集，开发具有分布无关有限样本保证的离线和在线校准算法。

Result: 在图像分类、回归和文本医疗决策等实验中，协作预测集始终优于单独使用人类或AI，在各种条件下实现了更高的覆盖率和更小的集合大小。

Conclusion: Human AI协作不确定性量化框架能有效结合人类和AI的优势，提升决策系统的可靠性和性能，特别是在面对分布漂移和人类行为演化时。

Abstract: AI predictive systems are increasingly embedded in decision making pipelines,
shaping high stakes choices once made solely by humans. Yet robust decisions
under uncertainty still rely on capabilities that current AI lacks: domain
knowledge not captured by data, long horizon context, and reasoning grounded in
the physical world. This gap has motivated growing efforts to design
collaborative frameworks that combine the complementary strengths of humans and
AI. This work advances this vision by identifying the fundamental principles of
Human AI collaboration within uncertainty quantification, a key component of
reliable decision making. We introduce Human AI Collaborative Uncertainty
Quantification, a framework that formalizes how an AI model can refine a human
expert's proposed prediction set with two goals: avoiding counterfactual harm,
ensuring the AI does not degrade correct human judgments, and complementarity,
enabling recovery of correct outcomes the human missed. At the population
level, we show that the optimal collaborative prediction set follows an
intuitive two threshold structure over a single score function, extending a
classical result in conformal prediction. Building on this insight, we develop
practical offline and online calibration algorithms with provable distribution
free finite sample guarantees. The online method adapts to distribution shifts,
including human behavior evolving through interaction with AI, a phenomenon we
call Human to AI Adaptation. Experiments across image classification,
regression, and text based medical decision making show that collaborative
prediction sets consistently outperform either agent alone, achieving higher
coverage and smaller set sizes across various conditions.

</details>


### [139] [Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy](https://arxiv.org/abs/2510.23487)
*Roham Koohestani,Ziyou Li,Anton Podkopaev,Maliheh Izadi*

Main category: cs.AI

TL;DR: 本文建立了现代智能体AI系统架构与乔姆斯基层次抽象机器之间的形式等价关系，提出AI智能体的记忆架构决定其计算能力，并直接映射到相应的自动机类别。


<details>
  <summary>Details</summary>
Motivation: 为智能体AI系统提供形式化理论基础，实现架构优化、形式验证和安全保证，通过自动机理论成熟技术确保智能体的安全性和可预测性。

Method: 建立自动机-智能体框架，将简单反射智能体映射为有限自动机，分层任务分解智能体映射为下推自动机，具有读写记忆的反思智能体映射为图灵机，并扩展到概率自动机以处理LLM的随机性。

Result: 提出了一个原则性方法来确定智能体架构规模以优化计算效率和成本，创建了形式验证的直接路径，能够正式界定可验证系统与行为根本不可判定系统之间的边界。

Conclusion: 该框架为开发智能体框架的静态分析工具和语法奠定了基础，通过分类智能体实现形式化验证和定量风险分析。

Abstract: This paper establishes a formal equivalence between the architectural classes
of modern agentic AI systems and the abstract machines of the Chomsky
hierarchy. We posit that the memory architecture of an AI agent is the
definitive feature determining its computational power and that it directly
maps it to a corresponding class of automaton. Specifically, we demonstrate
that simple reflex agents are equivalent to Finite Automata, hierarchical
task-decomposition agents are equivalent to Pushdown Automata, and agents
employing readable/writable memory for reflection are equivalent to TMs. This
Automata-Agent Framework provides a principled methodology for right-sizing
agent architectures to optimize computational efficiency and cost. More
critically, it creates a direct pathway to formal verification, enables the
application of mature techniques from automata theory to guarantee agent safety
and predictability. By classifying agents, we can formally delineate the
boundary between verifiable systems and those whose behavior is fundamentally
undecidable. We address the inherent probabilistic nature of LLM-based agents
by extending the framework to probabilistic automata that allow quantitative
risk analysis. The paper concludes by outlining an agenda for developing static
analysis tools and grammars for agentic frameworks.

</details>


### [140] [Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier](https://arxiv.org/abs/2510.23506)
*Hyeongseop Rha,Jeong Hun Yeo,Yeonju Kim,Yong Man Ro*

Main category: cs.AI

TL;DR: 提出情感推理验证器(ERV)和解释奖励方法，解决多模态大语言模型在情感识别中解释与预测不一致的问题，提升解释的忠实性和情感准确性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在情感识别中经常产生与目标标签不符甚至自相矛盾的解释，这种不一致性会降低系统可靠性和用户信任。

Method: 提出情感推理验证器(ERV)和解释奖励方法，在不修改模型架构或需要额外视频描述标注的情况下，引导模型生成与目标情感一致的解释。

Result: 在MAFW和DFEW数据集上显著提高了解释-预测一致性和解释情感准确性，通过实验和人工评估验证了方法的有效性。

Conclusion: 该方法不仅增强了解释与预测的对齐，还使多模态大语言模型能够提供情感一致、可信的交互，是实现真正类人HCI系统的关键一步。

Abstract: The recent advancement of Multimodal Large Language Models (MLLMs) is
transforming human-computer interaction (HCI) from surface-level exchanges into
more nuanced and emotionally intelligent communication. To realize this shift,
emotion understanding becomes essential allowing systems to capture subtle cues
underlying user intent. Furthermore, providing faithful explanations for
predicted emotions is crucial to ensure interpretability and build user trust.
However, current MLLM-based methods often generate emotion explanations that
diverge from the target labels and sometimes even contradict their own
predicted emotions. This inconsistency poses a critical risk for
misunderstanding and erodes reliability in interactive settings. To address
this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and
an Explanation Reward. Our method guides the model to produce reasoning that is
explicitly consistent with the target emotion during multimodal emotion
recognition without modifying the model architecture or requiring additional
paired video-description annotations. Our method significantly improves
faithful explanation-prediction consistency and explanation emotion accuracy on
the MAFW and DFEW datasets. Through extensive experiments and human
evaluations, we show that our approach not only enhances alignment between
explanation and prediction but also empowers MLLMs to deliver emotionally
coherent, trustworthy interactions, marking a key step toward truly human-like
HCI systems.

</details>


### [141] [Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence](https://arxiv.org/abs/2510.23524)
*KC Santosh,Rodrigue Rizk,Longwei Wang*

Main category: cs.AI

TL;DR: 提出HAI框架，通过增量学习、碳感知优化和人机协作实现可持续AI，平衡性能与生态责任


<details>
  <summary>Details</summary>
Motivation: AI快速发展带来巨大计算需求，引发环境和伦理担忧，需要从依赖大规模静态数据集转向可持续AI解决方案

Method: 引入HAI框架，采用增量学习、碳感知优化、人机协作，借鉴生物认知和动态架构，实现持续上下文学习

Result: HAI框架能够最小化碳足迹和人工标注成本，解决主动学习、持续适应和节能模型部署等挑战

Conclusion: HAI为负责任、以人为中心的人工智能提供了一条可行路径，平衡性能与生态责任

Abstract: The rapid advancement of Artificial Intelligence (AI) has led to
unprecedented computational demands, raising significant environmental and
ethical concerns. This paper critiques the prevailing reliance on large-scale,
static datasets and monolithic training paradigms, advocating for a shift
toward human-inspired, sustainable AI solutions. We introduce a novel
framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware
optimization, and human-in-the-loop collaboration to enhance adaptability,
efficiency, and accountability. By drawing parallels with biological cognition
and leveraging dynamic architectures, HAI seeks to balance performance with
ecological responsibility. We detail the theoretical foundations, system
design, and operational principles that enable AI to learn continuously and
contextually while minimizing carbon footprints and human annotation costs. Our
approach addresses pressing challenges in active learning, continual
adaptation, and energy-efficient model deployment, offering a pathway toward
responsible, human-centered artificial intelligence.

</details>


### [142] [When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning](https://arxiv.org/abs/2510.23532)
*Anirban Das,Irtaza Khalid,Rafael Peñaloza,Steven Schockaert*

Main category: cs.AI

TL;DR: NoRA是一个新的系统性关系推理基准，它增加了多个难度级别，要求模型超越基于路径的推理方法。


<details>
  <summary>Details</summary>
Motivation: 现有系统性关系推理基准过于简化，假设推理可以简化为组合关系路径，这限制了模型的泛化能力。

Method: 引入NoRA基准，包含多个难度级别，挑战模型超越路径推理的能力。

Result: NoRA基准为系统性关系推理领域提供了更全面的评估标准。

Conclusion: NoRA基准将推动神经网络在系统性关系推理领域的进一步发展。

Abstract: Designing models that can learn to reason in a systematic way is an important
and long-standing challenge. In recent years, a wide range of solutions have
been proposed for the specific case of systematic relational reasoning,
including Neuro-Symbolic approaches, variants of the Transformer architecture,
and specialised Graph Neural Networks. However, existing benchmarks for
systematic relational reasoning focus on an overly simplified setting, based on
the assumption that reasoning can be reduced to composing relational paths. In
fact, this assumption is hard-baked into the architecture of several recent
models, leading to approaches that can perform well on existing benchmarks but
are difficult to generalise to other settings. To support further progress in
the field of systematic relational reasoning with neural networks, we introduce
NoRA, a new benchmark which adds several levels of difficulty and requires
models to go beyond path-based reasoning.

</details>


### [143] [OntoPret: An Ontology for the Interpretation of Human Behavior](https://arxiv.org/abs/2510.23553)
*Alexis Ellis,Stacie Severyn,Fjollë Novakazi,Hadi Banaee,Cogan Shimizu*

Main category: cs.AI

TL;DR: OntoPret是一个基于认知科学和模块化工程方法的人类行为解释本体论，旨在填补技术中心机器人框架与描述性行为本体论之间的研究空白，为机器提供可处理的人类行为分类框架。


<details>
  <summary>Details</summary>
Motivation: 随着人机协作在工业5.0等范式中变得重要，机器需要安全有效地解释复杂人类行为。当前技术中心机器人框架缺乏细致的人类行为模型，而描述性行为本体论又不适合实时协作解释。

Method: 基于认知科学和模块化工程方法，开发了OntoPret本体论，提供形式化、机器可处理的行为分类框架，包括任务偏差和欺骗性行为。

Result: 在制造和游戏两个不同用例中展示了OntoPret的适应性，并建立了高级人类意图推理所需的语义基础。

Conclusion: OntoPret填补了人机协作中行为解释的研究空白，为机器理解人类行为提供了有效的本体论框架。

Abstract: As human machine teaming becomes central to paradigms like Industry 5.0, a
critical need arises for machines to safely and effectively interpret complex
human behaviors. A research gap currently exists between techno centric robotic
frameworks, which often lack nuanced models of human behavior, and descriptive
behavioral ontologies, which are not designed for real time, collaborative
interpretation. This paper addresses this gap by presenting OntoPret, an
ontology for the interpretation of human behavior. Grounded in cognitive
science and a modular engineering methodology, OntoPret provides a formal,
machine processable framework for classifying behaviors, including task
deviations and deceptive actions. We demonstrate its adaptability across two
distinct use cases manufacturing and gameplay and establish the semantic
foundations necessary for advanced reasoning about human intentions.

</details>


### [144] [ReCode: Unify Plan and Action for Universal Granularity Control](https://arxiv.org/abs/2510.23564)
*Zhaoyang Yu,Jiayi Zhang,Huixue Su,Yufan Zhao,Yifan Wu,Mingyi Deng,Jinyu Xiang,Yizhang Lin,Lingxiao Tang,Yingchao Li,Yuyu Luo,Bang Liu,Chenglin Wu*

Main category: cs.AI

TL;DR: ReCode提出了一种通过递归代码生成统一规划和行动的新范式，解决了LLM智能体在不同决策粒度间灵活操作的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务需要在不同粒度上做决策，而现有LLM智能体缺乏在决策粒度间流畅操作的能力，因为传统方法将高层规划与底层行动严格分离，限制了动态适应性和泛化能力。

Method: ReCode将高层规划视为抽象占位函数，通过递归分解为更细粒度的子函数，直到达到原始行动，从而消除规划与行动之间的刚性边界。

Result: 大量实验表明，ReCode在推理性能上显著超越先进基线方法，并在训练中表现出卓越的数据效率。

Conclusion: 通过递归代码生成统一规划和行动是实现通用粒度控制的有力有效方法。

Abstract: Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.

</details>


### [145] [Reduced AI Acceptance After the Generative AI Boom: Evidence From a Two-Wave Survey Study](https://arxiv.org/abs/2510.23578)
*Joachim Baumann,Aleksandra Urman,Ulrich Leicht-Deobald,Zachary J. Roman,Anikó Hannák,Markus Christen*

Main category: cs.AI

TL;DR: 生成式AI的普及导致公众对AI的接受度下降，对人类监督的需求增加，并加剧了社会不平等


<details>
  <summary>Details</summary>
Motivation: 研究公众对AI的态度变化，特别是在ChatGPT发布前后，以了解生成式AI技术对公众认知的影响

Method: 使用大规模两波调查（n_wave1=1514，n_wave2=1488），代表瑞士人口，比较ChatGPT发布前后的态度变化

Result: 生成式AI热潮显著降低了公众对AI的接受度，完全不可接受AI的比例从23%升至30%，支持纯人类决策的比例从18%升至26%，并扩大了教育、语言和性别差距

Conclusion: 研究结果挑战了行业对公众AI部署准备度的假设，强调技术发展必须与公众偏好保持一致的重要性

Abstract: The rapid adoption of generative artificial intelligence (GenAI) technologies
has led many organizations to integrate AI into their products and services,
often without considering user preferences. Yet, public attitudes toward AI
use, especially in impactful decision-making scenarios, are underexplored.
Using a large-scale two-wave survey study (n_wave1=1514, n_wave2=1488)
representative of the Swiss population, we examine shifts in public attitudes
toward AI before and after the launch of ChatGPT. We find that the GenAI boom
is significantly associated with reduced public acceptance of AI (see Figure 1)
and increased demand for human oversight in various decision-making contexts.
The proportion of respondents finding AI "not acceptable at all" increased from
23% to 30%, while support for human-only decision-making rose from 18% to 26%.
These shifts have amplified existing social inequalities in terms of widened
educational, linguistic, and gender gaps post-boom. Our findings challenge
industry assumptions about public readiness for AI deployment and highlight the
critical importance of aligning technological development with evolving public
preferences.

</details>


### [146] [Multi-Agent Evolve: LLM Self-Improve through Co-evolution](https://arxiv.org/abs/2510.23595)
*Yixing Chen,Yiding Wang,Siqi Zhu,Haofei Yu,Tao Feng,Muhan Zhan,Mostofa Patwary,Jiaxuan You*

Main category: cs.AI

TL;DR: MAE是一个多智能体自进化框架，通过三个交互代理（提议者、求解者、评判者）来增强LLM的推理能力，无需人工标注数据，在多个基准测试中平均提升4.54%。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法依赖人工标注数据和可验证奖励，限制了可扩展性和通用性。自博弈RL方法需要基础环境反馈，难以扩展到通用领域。

Method: 提出MAE框架，基于单一LLM实例化三个交互代理：提议者生成问题，求解者尝试解答，评判者评估并共同进化，应用强化学习优化行为。

Result: 在Qwen2.5-3B-Instruct上的实验显示，MAE在数学、推理和常识问答等多个基准测试中平均提升4.54%。

Conclusion: MAE是一种可扩展、数据高效的方法，能够以最小的人工监督依赖增强LLM的通用推理能力。

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in
enhancing the reasoning capabilities of large language models (LLMs). However,
the success of RL for LLMs heavily relies on human-curated datasets and
verifiable rewards, which limit their scalability and generality. Recent
Self-Play RL methods, inspired by the success of the paradigm in games and Go,
aim to enhance LLM reasoning capabilities without human-annotated data.
However, their methods primarily depend on a grounded environment for feedback
(e.g., a Python interpreter or a game engine); extending them to general
domains remains challenging. To address these challenges, we propose
Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in
solving diverse tasks, including mathematics, reasoning, and general knowledge
Q&A. The core design of MAE is based on a triplet of interacting agents
(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies
reinforcement learning to optimize their behaviors. The Proposer generates
questions, the Solver attempts solutions, and the Judge evaluates both while
co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves
an average improvement of 4.54% on multiple benchmarks. These results highlight
MAE as a scalable, data-efficient method for enhancing the general reasoning
abilities of LLMs with minimal reliance on human-curated supervision.

</details>


### [147] [Alita-G: Self-Evolving Generative Agent for Agent Generation](https://arxiv.org/abs/2510.23601)
*Jiahao Qiu,Xuan Qi,Hongru Wang,Xinzhe Juan,Yimin Wang,Zelin Zhao,Jiayi Geng,Jiacheng Guo,Peihang Li,Jingzhe Shi,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: ALITA-G是一个自进化框架，通过系统生成、抽象和整理MCP工具，将通用智能体转化为领域专家，在多个基准测试中实现性能提升并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前的自进化智能体主要局限于提示重写或失败重试，需要更系统的方法将通用智能体转化为领域专家。

Method: 通过执行目标领域任务，从成功轨迹中合成候选MCP工具，抽象为参数化原语并整合到MCP Box中，在推理时进行检索增强的MCP选择。

Result: 在GAIA验证集上达到83.03% pass@1和89.09% pass@3的新SOTA结果，同时将每个示例的平均token数减少约15%。

Conclusion: ALITA-G提供了从通用能力到可重用领域特定能力的原理性路径，提高了复杂推理任务的准确性和效率。

Abstract: Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool's descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.

</details>
