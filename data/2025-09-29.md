<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.CR](#cs.CR) [Total: 30]
- [cs.AI](#cs.AI) [Total: 51]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Extracting Conceptual Knowledge to Locate Software Issues](https://arxiv.org/abs/2509.21427)
*Ying Wang,Wenjun Mao,Chong Wang,Zhenhao Zhou,Yicheng Zhou,Wenyun Zhao,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: RepoLens通过抽象和利用代码仓库的概念知识来解决问题定位中的关注点混合和分散问题，显著提升了现有工具的定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM方法在大规模代码仓库中面临关注点混合（相关逻辑被埋没在大函数中）和关注点分散（相关逻辑分布在多个文件中）的挑战，导致问题定位效果不佳。

Method: RepoLens采用两阶段方法：离线阶段提取和丰富概念知识构建仓库级知识库；在线阶段检索问题特定术语，对关注点进行聚类和排序，并通过最小侵入式提示增强集成到定位工作流中。

Result: 在SWE-Lancer-Loc基准测试中，RepoLens显著提升了三种最先进工具的性能，在文件和函数级定位上平均提升超过22%的Hit@k和46%的Recall@k，在不同模型上Hit@1和Recall@10最高提升分别达504%和376%。

Conclusion: RepoLens通过构建语义一致的功能集群来指导LLM，有效解决了代码仓库中的关注点混合和分散问题，提高了问题定位的准确性和可靠性。

Abstract: Issue localization, which identifies faulty code elements such as files or
functions, is critical for effective bug fixing. While recent LLM-based and
LLM-agent-based approaches improve accuracy, they struggle in large-scale
repositories due to concern mixing, where relevant logic is buried in large
functions, and concern scattering, where related logic is dispersed across
files.
  To address these challenges, we propose RepoLens, a novel approach that
abstracts and leverages conceptual knowledge from code repositories. RepoLens
decomposes fine-grained functionalities and recomposes them into high-level
concerns, semantically coherent clusters of functionalities that guide LLMs. It
operates in two stages: an offline stage that extracts and enriches conceptual
knowledge into a repository-wide knowledge base, and an online stage that
retrieves issue-specific terms, clusters and ranks concerns by relevance, and
integrates them into localization workflows via minimally intrusive prompt
enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks
derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art
tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains
of over 22% in Hit@k and 46% in Recall@k for file- and function-level
localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with
Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies
and manual evaluation confirm the effectiveness and reliability of the
constructed concerns.

</details>


### [2] [Lost in Transition: The Struggle of Women Returning to Software Engineering Research after Career Breaks](https://arxiv.org/abs/2509.21533)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: 研究女性软件工程师在职业中断后重返学术界面临的挑战，与行业对比，分析不同国家的政策差异，提出支持透明招聘的建议


<details>
  <summary>Details</summary>
Motivation: 学术界为女性重返职场提供的支持机会有限，而IT行业已有多种支持途径。职业中断（如怀孕、移民身份、缺乏灵活工作选择）严重影响女性职业发展，造成重返学术界的障碍

Method: 开展多元文化研究项目，在多个国家和大学进行调查，比较不同地区的政策和机会差异，分析制度视角

Result: 研究发现学术界在支持女性重返方面政策不如行业突出，不同国家存在政策差异，需要改进透明招聘实践

Conclusion: 需要为女性重返学术界提供更好的支持机制，包括改进政策、增加灵活工作选择、实施透明招聘实践

Abstract: The IT industry provides supportive pathways such as returnship programs,
coding boot camps, and buddy systems for women re-entering their job after a
career break. Academia, however, offers limited opportunities to motivate women
to return. We propose a diverse multicultural research project investigating
the challenges faced by women with software engineering (SE) backgrounds
re-entering academia or related research roles after a career break. Career
disruptions due to pregnancy, immigration status, or lack of flexible work
options can significantly impact women's career progress, creating barriers for
returning as lecturers, professors, or senior researchers. Although many
companies promote gender diversity policies, such measures are less prominent
and often under-recognized within academic institutions. Our goal is to explore
the specific challenges women encounter when re-entering academic roles
compared to industry roles; to understand the institutional perspective,
including a comparative analysis of existing policies and opportunities in
different countries for women to return to the field; and finally, to provide
recommendations that support transparent hiring practices. The research project
will be carried out in multiple universities and in multiple countries to
capture the diverse challenges and policies that vary by location.

</details>


### [3] [No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials](https://arxiv.org/abs/2509.21816)
*Yuhang Xie,Jian Mu,Xiaojun Ma,Chaoyun Zhang,Lu Wang,Mengyu Zhou,Mugeng Liu,Si Qin,Qingwei Lin,Saravan Rajmohan,Shi Han,Dongmei Zhang*

Main category: cs.SE

TL;DR: 提出了首个从自然语言任务描述自动生成Excel教程的框架，通过执行代理在Excel中规划执行解决方案并收集中间产物，自动生成结构化文档和视频演示，显著提升了教程生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: Excel功能复杂但用户需求大，现有手动制作的教程更新成本高、劳动密集，需要自动化解决方案来降低制作成本并提高可扩展性。

Method: 开发了一个自动化框架，包含任务实例化、执行代理在Excel中规划执行解决方案并收集中间产物，最后将这些产物转换为结构化文档和视频演示。

Result: 在1559个真实场景任务上的实验显示，任务执行成功率比现有最佳方法提升8.5%，生成的教程可读性和教学效果接近或超过专家编写的材料，时间成本降至专家编写的1/20。

Conclusion: 该框架首次实现了高质量Excel教程的自动化生成，消除了人工劳动，使可扩展的教程生成变得实用可行。

Abstract: Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.

</details>


### [4] [Software Engineering Data Analytics: A Framework Based on a Multi-Layered Abstraction Mechanism](https://arxiv.org/abs/2509.21881)
*Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: 提出了一个用于软件分析的领域特定框架，支持异构软件仓库的查询、建模和集成。


<details>
  <summary>Details</summary>
Motivation: 为了解决异构软件仓库数据难以统一分析和集成的问题，需要一种专门针对软件分析领域的框架。

Method: 采用多层抽象机制，包含领域特定操作符，并通过案例研究展示方法可行性。

Result: 框架能够有效支持异构软件仓库的查询、建模和集成操作。

Conclusion: 该领域特定框架为软件分析提供了有效的解决方案，具有实际应用价值。

Abstract: This paper presents a concept of a domain-specific framework for software
analytics by enabling querying, modeling, and integration of heterogeneous
software repositories. The framework adheres to a multi-layered abstraction
mechanism that consists of domain-specific operators. We showcased the
potential of this approach by employing a case study.

</details>


### [5] [AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans](https://arxiv.org/abs/2509.21891)
*Yangtian Zi,Zixuan Wu,Aleksander Boruch-Gruszecki,Jonathan Bell,Arjun Guha*

Main category: cs.SE

TL;DR: AgentPack是一个包含130万代码编辑的数据集，这些编辑由Claude Code、OpenAI Codex和Cursor Agent与人类共同在GitHub项目中完成，相比传统的人类提交数据质量更高，可用于训练更好的代码编辑模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于提交和拉取请求的代码编辑数据存在噪声问题：提交信息简略、人类提交包含多个不相关编辑、许多提交来自简单的基于规则的机器人。而软件工程代理的出现改变了这一状况，它们与人类共同编写的代码变更更加专注，提交信息更详细。

Method: 收集了截至2025年8月中旬公共GitHub项目中由Claude Code、OpenAI Codex和Cursor Agent共同编写的130万代码编辑，构建了AgentPack语料库，并描述了识别和整理流程。

Result: 基于AgentPack微调的模型在代码编辑任务上表现优于基于传统人类提交语料库训练的模型，证明了使用软件工程代理的公共数据训练未来代码编辑模型的潜力。

Conclusion: 软件工程代理产生的代码编辑数据质量更高，能够有效提升代码编辑模型的性能，为未来代码编辑模型的训练提供了新的高质量数据来源。

Abstract: Fine-tuning large language models for code editing has typically relied on
mining commits and pull requests. The working hypothesis has been that commit
messages describe human intent in natural language, and patches to code
describe the changes that implement that intent. However, much of the
previously collected data is noisy: commit messages are terse, human-written
commits commingle several unrelated edits, and many commits come from simple,
rule-based bots.
  The recent adoption of software engineering agents changes this landscape.
Code changes co-authored by humans and agents tend to be more narrowly scoped
and focused on clearer goals. Their commit messages, generated by LLMs,
articulate intent and rationale in much greater detail. Moreover, when these
changes land in public repositories, they are implicitly filtered by humans:
maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,
OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August
2025. We describe the identification and curation pipeline, quantify adoption
trends of these agents, and analyze the structural properties of the edits.
Finally, we show that models fine-tuned on AgentPack can outperform models
trained on prior human-only commit corpora, highlighting the potential of using
public data from software engineering agents to train future code-editing
models.

</details>


### [6] [Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective](https://arxiv.org/abs/2509.21945)
*Pengzhou Chen,Hongyuan Liang,Tao Chen*

Main category: cs.SE

TL;DR: 本文首次系统性地探讨了配置调优中代理模型的作用，提出了基于适应度景观分析的新视角，并开发了Model4Tune工具来自动预测最佳模型-调优器组合。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为代理模型越准确越好，但研究发现准确性可能会误导，需要重新审视代理模型在配置调优中的真正作用。

Method: 通过适应度景观分析提出评估模型有用性的理论框架，进行了涉及27,000个案例的实证研究，并开发了Model4Tune自动化预测工具。

Result: Model4Tune在79%-82%的情况下显著优于随机猜测，能够有效预测未见系统的最佳模型-调优器组合。

Conclusion: 研究不仅揭示了未来研究方向，还提供了实用的解决方案，帮助从业者评估配置调优中最有用的模型。

Abstract: To efficiently tune configuration for better system performance (e.g.,
latency), many tuners have leveraged a surrogate model to expedite the process
instead of solely relying on the profoundly expensive system measurement. As
such, it is naturally believed that we need more accurate models. However, the
fact of accuracy can lie-a somewhat surprising finding from prior work-has left
us many unanswered questions regarding what role the surrogate model plays in
configuration tuning. This paper provides the very first systematic exploration
and discussion, together with a resolution proposal, to disclose the many faces
of surrogate models for configuration tuning, through the novel perspective of
fitness landscape analysis. We present a theory as an alternative to accuracy
for assessing the model usefulness in tuning, based on which we conduct an
extensive empirical study involving up to 27,000 cases. Drawing on the above,
we propose Model4Tune, an automated predictive tool that estimates which
model-tuner pairs are the best for an unforeseen system without expensive tuner
profiling. Our results suggest that Moldel4Tune, as one of the first of its
kind, performs significantly better than random guessing in 79%-82% of the
cases. Our results not only shed light on the possible future research
directions but also offer a practical resolution that can assist practitioners
in evaluating the most useful model for configuration tuning.

</details>


### [7] [SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios](https://arxiv.org/abs/2509.22097)
*Junkai Chen,Huihui Huang,Yunbo Lyu,Junwen An,Jieke Shi,Chengran Yang,Ting Zhang,Haoye Tian,Yikun Li,Zhenhao Li,Xin Zhou,Xing Hu,David Lo*

Main category: cs.SE

TL;DR: 提出了SecureAgentBench基准测试，包含105个编码任务，用于严格评估代码代理在安全代码生成方面的能力。评估结果显示当前代理在生成安全代码方面表现不佳，最佳代理仅达到15.2%的正确且安全解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估代码代理生成代码的安全性方面存在不足，往往忽略了漏洞引入的真实上下文，或采用狭窄的评估协议无法全面捕捉功能正确性或新引入的漏洞。

Method: 设计了包含105个编码任务的基准测试，每个任务包含：现实任务设置（需要在大仓库中进行多文件编辑）、基于真实开源漏洞的对齐上下文、结合功能测试、漏洞检查（通过概念验证利用）和静态分析检测新引入漏洞的综合评估。

Result: 评估了三个代表性代理（SWE-agent、OpenHands、Aider）和三个最先进LLM（Claude 3.7 Sonnet、GPT-4.1、DeepSeek-V3.1）。结果显示：当前代理难以生成安全代码，最佳代理仅达到15.2%正确且安全的解决方案；一些代理生成功能正确代码但仍引入漏洞；添加明确安全指令未能显著改善安全编码。

Conclusion: SecureAgentBench为安全代码生成建立了严格的基准测试，是迈向更可靠LLM软件开发的一步，强调了进一步研究的必要性。

Abstract: Large language model (LLM) powered code agents are rapidly transforming
software engineering by automating tasks such as testing, debugging, and
repairing, yet the security risks of their generated code have become a
critical concern. Existing benchmarks have offered valuable insights but remain
insufficient: they often overlook the genuine context in which vulnerabilities
were introduced or adopt narrow evaluation protocols that fail to capture
either functional correctness or newly introduced vulnerabilities. We therefore
introduce SecureAgentBench, a benchmark of 105 coding tasks designed to
rigorously evaluate code agents' capabilities in secure code generation. Each
task includes (i) realistic task settings that require multi-file edits in
large repositories, (ii) aligned contexts based on real-world open-source
vulnerabilities with precisely identified introduction points, and (iii)
comprehensive evaluation that combines functionality testing, vulnerability
checking through proof-of-concept exploits, and detection of newly introduced
vulnerabilities using static analysis. We evaluate three representative agents
(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7
Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents
struggle to produce secure code, as even the best-performing one, SWE-agent
supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,
(ii) some agents produce functionally correct code but still introduce
vulnerabilities, including new ones not previously recorded, and (iii) adding
explicit security instructions for agents does not significantly improve secure
coding, underscoring the need for further research. These findings establish
SecureAgentBench as a rigorous benchmark for secure code generation and a step
toward more reliable software development with LLMs.

</details>


### [8] [SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin](https://arxiv.org/abs/2509.22114)
*Hanzhuo Tan,Weihao Li,Xiaolong Tian,Siyi Wang,Jiaming Liu,Jing Li,Yuqun Zhang*

Main category: cs.SE

TL;DR: SK2Decompile是一个两阶段反编译方法，先恢复程序结构（骨架），再生成标识符（皮肤），显著提升了反编译的正确性和可读性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的反编译器在有效呈现程序源代码结构和原始标识符方面存在局限性，需要改进反编译质量。

Method: 采用两阶段方法：1) 结构恢复模型将二进制代码转换为中间表示（骨架），使用强化学习确保符合编译器规则；2) 标识符命名模型生成有意义的标识符（皮肤），通过强化学习奖励语义相似性。

Result: 在HumanEval数据集上比GPT-5-mini平均重执行率提升21.6%，在GitHub2025基准上比Idioms平均R2I提升29.4%。

Conclusion: SK2Decompile通过分离结构和标识符恢复，独立推进反编译的正确性和可读性，显著优于现有最先进方法。

Abstract: Large Language Models (LLMs) have emerged as a promising approach for binary
decompilation. However, the existing LLM-based decompilers still are somewhat
limited in effectively presenting a program's source-level structure with its
original identifiers. To mitigate this, we introduce SK2Decompile, a novel
two-phase approach to decompile from the skeleton (semantic structure) to the
skin (identifier) of programs. Specifically, we first apply a Structure
Recovery model to translate a program's binary code to an Intermediate
Representation (IR) as deriving the program's "skeleton", i.e., preserving
control flow and data structures while obfuscating all identifiers with generic
placeholders. We also apply reinforcement learning to reward the model for
producing program structures that adhere to the syntactic and semantic rules
expected by compilers. Second, we apply an Identifier Naming model to produce
meaningful identifiers which reflect actual program semantics as deriving the
program's "skin". We train the Identifier Naming model with a separate
reinforcement learning objective that rewards the semantic similarity between
its predictions and the reference code. Such a two-phase decompilation process
facilitates advancing the correctness and readability of decompilation
independently. Our evaluations indicate that SK2Decompile, significantly
outperforms the SOTA baselines, achieving 21.6% average re-executability rate
gain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement
over Idioms on the GitHub2025 benchmark.

</details>


### [9] [Leveraging LLM Agents for Automated Video Game Testing](https://arxiv.org/abs/2509.22170)
*Chengjia Wang,Lanling Tang,Ming Yuan,Jiongchi Yu,Xiaofei Xie,Jiajun Bu*

Main category: cs.SE

TL;DR: TITAN是一个基于LLM的智能MMORPG测试框架，通过状态感知、动作优化、长时程推理和LLM预言机等组件，显著提升了游戏测试的任务完成率和bug检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统自动化游戏测试方法在复杂的MMORPG环境中难以实现高状态覆盖率和效率，现有LLM方法对复杂游戏状态-动作空间和长复杂任务的理解能力有限。

Method: TITAN包含四个核心组件：高维游戏状态感知与抽象、主动优化和优先处理可用动作、基于动作轨迹记忆和反思自校正的长时程推理、使用LLM预言机检测功能性和逻辑性bug。

Result: 在两个大型商业MMORPG上的实验显示，TITAN达到95%的任务完成率，bug检测性能优于现有方法，发现了四个先前未知的bug，并已在8个真实游戏QA流水线中部署。

Conclusion: TITAN证明了LLM驱动框架在复杂游戏测试中的有效性，为推进智能通用测试系统提供了新方向。

Abstract: Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a
critical yet labor-intensive task in game development due to their complexity
and frequent updating nature. Traditional automated game testing approaches
struggle to achieve high state coverage and efficiency in these rich,
open-ended environments, while existing LLM-based game-playing approaches are
limited to shallow reasoning ability in understanding complex game state-action
spaces and long-complex tasks. To address these challenges, we propose TITAN,
an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN
incorporates four key components to: (1) perceive and abstract high-dimensional
game states, (2) proactively optimize and prioritize available actions, (3)
enable long-horizon reasoning with action trace memory and reflective
self-correction, and (4) employ LLM-based oracles to detect potential
functional and logic bugs with diagnostic reports.
  We implement the prototype of TITAN and evaluate it on two large-scale
commercial MMORPGs spanning both PC and mobile platforms. In our experiments,
TITAN achieves significantly higher task completion rates (95%) and bug
detection performance compared to existing automated game testing approaches.
An ablation study further demonstrates that each core component of TITAN
contributes substantially to its overall performance. Notably, TITAN detects
four previously unknown bugs that prior testing approaches fail to identify. We
provide an in-depth discussion of these results, which offer guidance for new
avenues of advancing intelligent, general-purpose testing systems. Moreover,
TITAN has been deployed in eight real-world game QA pipelines, underscoring its
practical impact as an LLM-driven game testing framework.

</details>


### [10] [Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries](https://arxiv.org/abs/2509.22202)
*Lukas Twist,Jie M. Zhang,Mark Harman,Helen Yannakoudakis*

Main category: cs.SE

TL;DR: 本研究首次系统性地分析了用户提示变化对LLM生成代码中库幻觉的影响，发现即使是单字符拼写错误也能触发高达26%的幻觉率，虚假库名被接受率高达99%，时间相关提示导致84%的幻觉。


<details>
  <summary>Details</summary>
Motivation: LLM在生成代码时经常产生库幻觉（发明不存在的库），这些幻觉可能误导开发者、破坏构建过程，并带来供应链安全威胁。然而，现实世界中提示变化如何影响幻觉率尚不清楚。

Method: 评估了6种不同的LLM，研究两种幻觉类型：库名幻觉（无效导入）和库成员幻觉（有效库中的无效调用）。分析了从开发者论坛提取的真实用户语言以及不同程度的用户错误（单字符/多字符拼写错误和完全虚假名称/成员）对幻觉率的影响。

Result: 发现系统性漏洞：库名单字符拼写错误在26%的任务中触发幻觉，虚假库名在99%的任务中被接受，时间相关提示在84%的任务中导致幻觉。提示工程有缓解潜力但效果不一致且依赖特定LLM。

Conclusion: LLM对自然提示变化极其脆弱，迫切需要针对库相关幻觉及其潜在利用的安全防护措施。

Abstract: Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.

</details>


### [11] [Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering](https://arxiv.org/abs/2509.22320)
*Vincenzo De Martino,Mohammad Amin Zadenoori,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: 本文提出绿色提示工程，研究提示语言复杂度对语言模型能耗和性能的影响，发现简化提示可在不显著降低性能的情况下减少能耗。


<details>
  <summary>Details</summary>
Motivation: 语言模型在软件工程中应用日益广泛，但其推理过程带来环境问题。现有研究关注硬件和提示长度，但很少考虑语言复杂度作为可持续性因素。

Method: 通过实证研究，使用开源小语言模型进行需求分类任务，改变提示的可读性水平。

Result: 结果显示可读性影响环境可持续性和性能，存在两者之间的权衡。简化提示可减少能耗成本且F1分数损失不大。

Conclusion: 为从业者提供降低能耗的方法，为研究者开辟了在绿色AI议程下可持续提示设计的研究路径。

Abstract: Language Models are increasingly applied in software engineering, yet their
inference raises growing environmental concerns. Prior work has examined
hardware choices and prompt length, but little attention has been paid to
linguistic complexity as a sustainability factor. This paper introduces Green
Prompt Engineering, framing linguistic complexity as a design dimension that
can influence energy consumption and performance. We conduct an empirical study
on requirement classification using open-source Small Language Models, varying
the readability of prompts. Our results reveal that readability affects
environmental sustainability and performance, exposing trade-offs between them.
For practitioners, simpler prompts can reduce energy costs without a
significant F1-score loss; for researchers, it opens a path toward guidelines
and studies on sustainable prompt design within the Green AI agenda.

</details>


### [12] [GPU-Accelerated Loopy Belief Propagation for Program Analysis](https://arxiv.org/abs/2509.22337)
*Haoyu Feng,Xin Zhang*

Main category: cs.SE

TL;DR: 提出了一种GPU加速的LBP算法用于程序分析，支持灵活更新策略，在真实Java程序的数据竞争分析中比现有方法快2.14-5.56倍


<details>
  <summary>Details</summary>
Motivation: LBP在大型程序分析中面临计算挑战，现有GPU方法缺乏灵活更新策略支持且未整合逻辑约束，导致性能不佳

Method: 提出统一表示法支持任意用户定义更新策略，进行依赖分析，基于Horn子句局部结构分组消息以减少warp分歧并优化GPU资源利用

Result: 在8个真实Java程序的数据竞争分析中，比最优串行方法平均加速2.14倍，比最优GPU方法平均加速5.56倍，同时保持高精度

Conclusion: 该方法有效解决了LBP在程序分析中的计算瓶颈，通过GPU加速和灵活策略支持显著提升了性能

Abstract: Loopy Belief Propagation (LBP) is a widely used approximate inference
algorithm in probabilistic graphical models, with applications in computer
vision, error correction codes, protein folding, program analysis, etc.
However, LBP faces significant computational challenges when applied to
large-scale program analysis. While GPU (Graphics Processing Unit) parallel
computing provides a promising solution, existing approaches lack support for
flexible update strategies and have yet to integrate logical constraints with
GPU acceleration, leading to suboptimal practical performance.
  This paper presents a GPU-accelerated LBP algorithm for program analysis. To
support the diverse update strategies required by users, we propose a unified
representation for specifying arbitrary user-defined update strategies, along
with a dependency analysis algorithm. Furthermore, building on previous work
that leverages the local structure of Horn clauses to simplify message passing,
we group messages to minimize warp divergence and better utilize GPU resources.
Experimental results on datarace analysis over eight real-world Java programs
show that our approach achieves an average speedup of $2.14\times$ over the
state-of-the-art sequential approach and $5.56\times$ over the state-of-the-art
GPU-based approach, while maintaining high accuracy.

</details>


### [13] [A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems](https://arxiv.org/abs/2509.22379)
*Stefano Carlo Lambertenghi,Mirena Flores Valdez,Andrea Stocco*

Main category: cs.SE

TL;DR: 本文通过实证研究比较了四种自动驾驶测试模式（SiL、ViL、MR和真实世界测试），评估了它们在执行、感知和行为保真度三个维度上的现实差距，发现MR测试在提高感知真实性的同时保持安全性和控制能力。


<details>
  <summary>Details</summary>
Motivation: 模拟测试与真实世界行为之间的现实差距挑战了测试结果向实际部署系统的可转移性，需要系统评估不同测试模式的有效性。

Method: 使用配备真实传感器的小型物理车辆及其数字孪生，在室内驾驶场景中实现四种测试设置，并评估两种ADS架构（模块化和端到端）。

Result: SiL和ViL简化了真实世界动态和感知的关键方面，而MR测试提高了感知真实性且不损害安全性或控制。识别了故障在不同测试模式间不转移的条件。

Conclusion: 研究结果为每种测试模式的优缺点提供了可行见解，并为实现更稳健和可转移的自动驾驶系统验证指明了路径。

Abstract: Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)
development, offering safe and scalable evaluation across diverse driving
scenarios. However, discrepancies between simulated and real-world behavior,
known as the reality gap, challenge the transferability of test results to
deployed systems. In this paper, we present a comprehensive empirical study
comparing four representative testing modalities: Software-in-the-Loop (SiL),
Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.
Using a small-scale physical vehicle equipped with real sensors (camera and
LiDAR) and its digital twin, we implement each setup and evaluate two ADS
architectures (modular and end-to-end) across diverse indoor driving scenarios
involving real obstacles, road topologies, and indoor environments. We
systematically assess the impact of each testing modality along three
dimensions of the reality gap: actuation, perception, and behavioral fidelity.
Our results show that while SiL and ViL setups simplify critical aspects of
real-world dynamics and sensing, MR testing improves perceptual realism without
compromising safety or control. Importantly, we identify the conditions under
which failures do not transfer across testing modalities and isolate the
underlying dimensions of the gap responsible for these discrepancies. Our
findings offer actionable insights into the respective strengths and
limitations of each modality and outline a path toward more robust and
transferable validation of autonomous driving systems.

</details>


### [14] [Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers](https://arxiv.org/abs/2509.22420)
*Ziyi Zhang,Devjeet Roy,Venera Arnaoudova*

Main category: cs.SE

TL;DR: 上下文特定的调试指导比抽象指南或无上下文的具体步骤更有效，能更快提高新手的错误定位能力并保持长期效果


<details>
  <summary>Details</summary>
Motivation: 新手在错误定位方面缺乏系统性方法，现有研究测试了抽象指南和通用具体步骤，但上下文特定指导的影响尚不清楚

Method: 进行为期八周的纵向研究，设置四个条件：无指导(G1)、抽象指南(G2)、具体步骤(G3)、以及将具体步骤与问题特定细节配对的上下文特定指导(G4)。44名本科生参与，完成5个会话的调试任务

Result: G4组在正确率和完成时间上表现最佳：一次会话后达到80%正确率（其他组20-44%），三周后仍保持80%，显著优于其他组；完成时间稳定在13-15分钟，而其他组需要2-3个会话才能稳定在22-27分钟

Conclusion: 上下文特定指导比抽象指南或上下文无关步骤能更快获得技能并保持更强记忆。即使1-2个会话也能产生显著提升，而扩展练习能优化和稳定表现。将上下文示例与抽象原则结合可以弥合错误定位教育中的理论与实践差距

Abstract: Bug localization is a critical skill, yet novices often lack systematic
approaches. Prior work tested abstract guidelines and general concrete steps;
the impact of context-specific instruction is unclear. We ran an eight-week
longitudinal study with four conditions: no instruction (G1), abstract
guidelines (G2), concrete steps (G3), and our context-specific instruction that
pairs concrete bug-localization steps with problem-specific details (G4).
Forty-four undergraduates participated; 41 completed all five sessions (S1-S5).
Each session included 2-3 debugging tasks to identify the minimal code element
containing a seeded logical fault. We measured correctness (binary), time to
completion, self-perceived scores (stress, difficulty, satisfaction, and
strategy adherence). G4 achieved higher correctness and shorter time to
completion: it reached 80% correctness after one session (vs. 20-44% for other
groups) and maintained 80% after three weeks, outperforming all groups (p <
0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other
groups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses
showed lower stress and higher satisfaction in G4, with participants
internalizing strategies via contextual examples. We conclude that
context-specific instruction yields faster skill acquisition and stronger
retention than abstract guidelines or context-agnostic steps. Even 1-2 sessions
produced significant gains, while extended practice optimized and stabilized
performance. Integrating contextual examples with abstract principles may
bridge theory-practice gaps in bug-localization education and provide a more
equitable path for novices.

</details>


### [15] [TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search](https://arxiv.org/abs/2509.22431)
*Zhengyu Chen,Zhaoyi Meng,Wenxiang Zhao,Wansen Wang,Haoyang Zhao,Jiahao Zhan,Jie Cui,Hong Zhong*

Main category: cs.SE

TL;DR: TreeMind结合大型语言模型和蒙特卡洛树搜索，通过战略性的UI探索来自动重现Android应用崩溃，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习或LLM的方法在重现不完整错误报告时存在局限，难以推断未观察步骤和重建用户操作序列来导航复杂的UI交互空间。

Method: 将重现任务建模为目标驱动搜索问题，使用MCTS作为核心规划机制，引入两个LLM引导的智能体：Expander生成有前景的操作，Simulator评估操作成功率。

Result: 在93个真实Android错误报告数据集上的实验表明，TreeMind在重现成功率上显著优于四种最先进的基线方法。

Conclusion: 将LLM推理与基于MCTS的规划相结合是自动化错误重现的一个有前景的方向。

Abstract: Automatically reproducing Android app crashes from textual bug reports is
challenging, particularly when the reports are incomplete and the modern UI
exhibits high combinatorial complexity. Existing approaches based on
reinforcement learning or large language models (LLMs) exhibit limitations in
such scenarios. They struggle to infer unobserved steps and reconstruct the
underlying user action sequences to navigate the vast UI interaction space,
primarily due to limited goal-directed reasoning and planning. We present
TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo
Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug
reproduction. To the best of our knowledge, this is the first work to combine
external decision-making with LLM semantic reasoning for reliable bug
reproduction. We formulate the reproduction task as a target-driven search
problem, leveraging MCTS as the core planning mechanism to iteratively refine
action sequences. To enhance MCTS with semantic reasoning, we introduce two
LLM-guided agents with distinct roles: Expander generates top-k promising
actions based on the current UI state and exploration history, while Simulator
estimates the likelihood that each action leads toward successful reproduction.
By incorporating multi-modal UI inputs and advanced prompting techniques,
TreeMind conducts feedback-aware navigation that identifies missing but
essential user actions and incrementally reconstructs the reproduction paths.
We evaluate TreeMind on a dataset of 93 real-world Android bug reports from
three widely-used benchmarks. Experimental results show that it significantly
outperforms four state-of-the-art baselines in reproduction success rate. A
real-world case study indicates that integrating LLM reasoning with MCTS-based
planning is a compelling direction for automated bug reproduction.

</details>


### [16] [Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection](https://arxiv.org/abs/2509.22530)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Peng Di,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: AFD通过自动识别和建模自定义分配函数来提升指针分析精度，结合值流分析和LLM处理复杂分配模式，在15个C项目中识别600+自定义分配函数，使堆对象建模增加26倍，别名集大小减少39%，运行时开销仅1.4倍。


<details>
  <summary>Details</summary>
Motivation: 现有指针分析方法大多忽略C/C++程序中普遍存在的用户自定义分配函数，导致别名分析精度不足。

Method: 采用混合方法：使用值流分析检测简单包装器，利用大语言模型推理具有副作用的复杂分配模式。

Result: 在15个真实C项目中识别600+自定义分配函数，堆对象建模增加26倍，别名集大小减少39%，运行时开销1.4倍，发现17个未检测内存错误。

Conclusion: 精确建模自定义分配函数为提升大型软件系统中指针分析提供了可扩展且实用的路径。

Abstract: Pointer analysis is foundational for many static analysis tasks, yet its
effectiveness is often hindered by imprecise modeling of heap allocations,
particularly in C/C++ programs where user-defined allocation functions (AFs)
are pervasive. Existing approaches largely overlook these custom allocators,
leading to coarse aliasing and reduced analysis precision. In this paper, we
present AFD, a novel technique that enhances pointer analysis by automatically
identifying and modeling custom allocation functions. AFD employs a hybrid
approach: it uses value-flow analysis to detect straightforward wrappers and
leverages Large Language Models (LLMs) to reason about more complex allocation
patterns with side effects. This targeted enhancement enables precise modeling
of heap objects at each call site, achieving context-sensitivity-like benefits
without the associated overhead. We evaluate AFD on 15 real-world C projects,
identifying over 600 custom AFs. Integrating AFD into a baseline pointer
analysis yields a 26x increase in modeled heap objects and a 39% reduction in
alias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced
analysis improves indirect call resolution and uncovers 17 previously
undetected memory bugs. These results demonstrate that precise modeling of
custom allocation functions offers a scalable and practical path to improving
pointer analysis in large software systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan](https://arxiv.org/abs/2509.21367)
*Yu-Kai Shih,You-Kai Kang*

Main category: cs.CR

TL;DR: 本文提出了一个针对新竹智慧旅游服务的安全检索增强生成(RAG)聊天机器人，通过多层防御机制有效抵御提示注入攻击，在674个对抗性提示测试中表现出色，同时保持了95%以上的良性任务准确率。


<details>
  <summary>Details</summary>
Motivation: 随着智慧旅游发展，AI聊天机器人面临严重的提示注入攻击威胁，攻击者可能操纵输入导致敏感信息泄露或有害内容生成，需要构建安全的对话系统。

Method: 集成RAG与API函数调用、多层语言分析和防护机制，采用分层响应策略、RAG驱动的知识基础、意图分解（词汇、语义、语用层面），以及系统规范、意图判断门禁和反向RAG文本验证等防御机制。

Result: 在674个对抗性提示和223个良性查询评估中，系统在良性任务上准确率超过95%，并能有效检测注入攻击。GPT-5变体约能阻止85%的攻击，显示仍需分层防御。

Conclusion: 这项工作为智慧旅游中安全聊天机器人的部署提供了实用框架，促进了可持续旅游、多语言可访问性和伦理AI部署，为构建弹性、可信赖的AI应用做出贡献。

Abstract: As smart tourism evolves, AI-powered chatbots have become indispensable for
delivering personalized, real-time assistance to travelers while promoting
sustainability and efficiency. However, these systems are increasingly
vulnerable to prompt injection attacks, where adversaries manipulate inputs to
elicit unintended behaviors such as leaking sensitive information or generating
harmful content. This paper presents a case study on the design and
implementation of a secure retrieval-augmented generation (RAG) chatbot for
Hsinchu smart tourism services. The system integrates RAG with API function
calls, multi-layered linguistic analysis, and guardrails against injections,
achieving high contextual awareness and security. Key features include a tiered
response strategy, RAG-driven knowledge grounding, and intent decomposition
across lexical, semantic, and pragmatic levels. Defense mechanisms include
system norms, gatekeepers for intent judgment, and reverse RAG text to
prioritize verified data. We also benchmark a GPT-5 variant (released
2025-08-07) to assess inherent robustness. Evaluations with 674 adversarial
prompts and 223 benign queries show over 95% accuracy on benign tasks and
substantial detection of injection attacks. GPT-5 blocked about 85% of attacks,
showing progress yet highlighting the need for layered defenses. Findings
emphasize contributions to sustainable tourism, multilingual accessibility, and
ethical AI deployment. This work offers a practical framework for deploying
secure chatbots in smart tourism and contributes to resilient, trustworthy AI
applications.

</details>


### [18] [Towards Adapting Federated & Quantum Machine Learning for Network Intrusion Detection: A Survey](https://arxiv.org/abs/2509.21389)
*Devashish Chaudhary,Sutharshan Rajasegarar,Shiva Raj Pokhrel*

Main category: cs.CR

TL;DR: 该调查探讨了联邦学习(FL)与网络入侵检测系统(NIDS)的集成，特别关注深度学习和量子机器学习方法，分析了FL架构、隐私保护技术、量子FL等关键方面。


<details>
  <summary>Details</summary>
Motivation: 在网络安全环境中，敏感流量数据无法集中处理，需要保护数据隐私的同时实现协作模型训练，因此研究FL与NIDS的集成具有重要意义。

Method: 采用系统性分析方法，全面考察FL架构、部署策略、通信协议和聚合方法，深入调查隐私保护技术、模型压缩方法，并开创性地探索量子FL(QFL)。

Result: 通过经典与量子方法的严格比较分析，识别研究空白，评估实际部署，为工业采用和未来研究方向制定了具体路线图。

Conclusion: 该工作为研究人员和从业者提供了权威参考，旨在增强联邦入侵检测系统的隐私性、效率和鲁棒性，同时为未来的量子增强网络安全格局做好准备。

Abstract: This survey explores the integration of Federated Learning (FL) with Network
Intrusion Detection Systems (NIDS), with particular emphasis on deep learning
and quantum machine learning approaches. FL enables collaborative model
training across distributed devices while preserving data privacy-a critical
requirement in network security contexts where sensitive traffic data cannot be
centralized. Our comprehensive analysis systematically examines the full
spectrum of FL architectures, deployment strategies, communication protocols,
and aggregation methods specifically tailored for intrusion detection. We
provide an in-depth investigation of privacy-preserving techniques, model
compression approaches, and attack-specific federated solutions for threats
including DDoS, MITM, and botnet attacks. The survey further delivers a
pioneering exploration of Quantum FL (QFL), discussing quantum feature
encoding, quantum machine learning algorithms, and quantum-specific aggregation
methods that promise exponential speedups for complex pattern recognition in
network traffic. Through rigorous comparative analysis of classical and quantum
approaches, identification of research gaps, and evaluation of real-world
deployments, we outline a concrete roadmap for industrial adoption and future
research directions. This work serves as an authoritative reference for
researchers and practitioners seeking to enhance privacy, efficiency, and
robustness of federated intrusion detection systems in increasingly complex
network environments, while preparing for the quantum-enhanced cybersecurity
landscape of tomorrow.

</details>


### [19] [Dynamic Dual-level Defense Routing for Continual Adversarial Training](https://arxiv.org/abs/2509.21392)
*Wenxuan Wang,Chenglei Wang,Xuelin Qian*

Main category: cs.CR

TL;DR: 提出DDeR框架，通过双层防御路由机制自主选择路由器来整合特定防御专家，适应不断演化的对抗攻击。第一级路由包含多个防御专家和路由器，第二级路由使用对抗哨兵网络指导选择，并采用伪任务替代训练策略促进路由器间通信。


<details>
  <summary>Details</summary>
Motivation: 现有连续对抗训练方法由于对抗样本的多样性和攻击性，在持续学习后会出现灾难性遗忘先前防御知识的问题。需要开发能够适应演化对抗攻击的新方法。

Method: DDeR框架包含：1）第一级防御路由：多个防御专家和路由器，每个路由器动态选择和组合专家处理受攻击特征；2）第二级防御路由：对抗哨兵网络指导路由器选择；3）伪任务替代训练策略：利用数据分布差异促进路由器间通信，无需存储历史数据。

Result: 大量实验表明，DDeR在连续防御性能和分类准确率方面优于现有方法。

Conclusion: DDeR框架通过双层路由机制和伪任务训练策略，有效解决了连续对抗训练中的灾难性遗忘问题，实现了对演化对抗攻击的适应性防御。

Abstract: As adversarial attacks continue to evolve, defense models face the risk of
recurrent vulnerabilities, underscoring the importance of continuous
adversarial training (CAT). Existing CAT approaches typically balance decision
boundaries by either data replay or optimization strategy to constrain shared
model parameters. However, due to the diverse and aggressive nature of
adversarial examples, these methods suffer from catastrophic forgetting of
previous defense knowledge after continual learning. In this paper, we propose
a novel framework, called Dual-level Defense Routing or DDeR, that can
autonomously select appropriate routers to integrate specific defense experts,
thereby adapting to evolving adversarial attacks. Concretely, the first-level
defense routing comprises multiple defense experts and routers, with each
router dynamically selecting and combining suitable experts to process attacked
features. Routers are independently incremented as continuous adversarial
training progresses, and their selections are guided by an Adversarial Sentinel
Network (ASN) in the second-level defense routing. To compensate for the
inability to test due to the independence of routers, we further present a
Pseudo-task Substitution Training (PST) strategy, which leverages
distributional discrepancy in data to facilitate inter-router communication
without storing historical data. Extensive experiments demonstrate that DDeR
achieves superior continuous defense performance and classification accuracy
compared to existing methods.

</details>


### [20] [SafeSteer: Adaptive Subspace Steering for Efficient Jailbreak Defense in Vision-Language Models](https://arxiv.org/abs/2509.21400)
*Xiyu Zeng,Siyuan Liang,Liming Lu,Haotian Zhu,Enguang Liu,Jisheng Dang,Yongbin Zhou,Shuchao Pang*

Main category: cs.CR

TL;DR: SafeSteer是一个轻量级的推理时防御框架，通过奇异值分解构建安全子空间，有效抵御各种越狱攻击，同时保持模型性能和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法存在两个主要问题：难以在不损害模型实用性的前提下确保安全性；许多防御机制显著降低模型推理效率。

Method: 使用奇异值分解构建低维安全子空间，在推理时将原始转向向量投影到该子空间并重构，自适应去除有害生成信号，同时保持处理良性输入的能力。整个过程在单次推理中完成。

Result: 实验表明，SafeSteer将攻击成功率降低超过60%，在正常任务上的准确率提高1-2%，且未引入显著推理延迟。

Conclusion: 通过简单高效的推理时控制，可以实现稳健实用的越狱防御。

Abstract: As the capabilities of Vision Language Models (VLMs) continue to improve,
they are increasingly targeted by jailbreak attacks. Existing defense methods
face two major limitations: (1) they struggle to ensure safety without
compromising the model's utility; and (2) many defense mechanisms significantly
reduce the model's inference efficiency. To address these challenges, we
propose SafeSteer, a lightweight, inference-time steering framework that
effectively defends against diverse jailbreak attacks without modifying model
weights. At the core of SafeSteer is the innovative use of Singular Value
Decomposition to construct a low-dimensional "safety subspace." By projecting
and reconstructing the raw steering vector into this subspace during inference,
SafeSteer adaptively removes harmful generation signals while preserving the
model's ability to handle benign inputs. The entire process is executed in a
single inference pass, introducing negligible overhead. Extensive experiments
show that SafeSteer reduces the attack success rate by over 60% and improves
accuracy on normal tasks by 1-2%, without introducing significant inference
latency. These results demonstrate that robust and practical jailbreak defense
can be achieved through simple, efficient inference-time control.

</details>


### [21] [Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic](https://arxiv.org/abs/2509.21475)
*Sen Yang,Burak Öz,Fei Wu,Fan Zhang*

Main category: cs.CR

TL;DR: 本文通过基于延迟的代理模型比较了以太坊的两种区块构建范式：单源范式（SSP，类似MEV-Boost）和多源范式（MSP）。研究发现MSP会导致更快的中心化，因为聚合多个来源使边际价值具有位置依赖性，放大收益差异并促使验证者向延迟最低地区迁移。


<details>
  <summary>Details</summary>
Motivation: 传统指标如权益分布忽略了去中心化的地理维度。验证者运行位置影响系统对区域冲击的弹性以及奖励获取的公平性。当前以太坊验证者在大西洋沿岸（欧盟和美国东海岸）聚集，这些地区具有结构性延迟优势。

Method: 开发了基于延迟的代理模型，比较两种以太坊区块构建范式：单源范式（SSP）和多源范式（MSP）。SSP中提议者从单个中继获取完整区块，MSP中提议者从多个来源聚合价值并自行广播区块。

Result: 模拟显示SSP围绕中继位置集中但速度较慢，因为邻近性主要影响传播，边际价值在各地区相对均匀。MSP中心化更快：跨来源聚合使边际价值具有位置依赖性，放大收益差异并促使向延迟最低地区迁移。北美始终成为中心枢纽。

Conclusion: 协议设计显著影响验证者地理分布，并提供了促进地理去中心化的杠杆。一旦验证者已经聚集，来源位置对去中心化的影响有限。

Abstract: Decentralization has a geographic dimension that conventional metrics such as
stake distribution overlook. Where validators run affects resilience to
regional shocks (outages, disasters, government intervention) and fairness in
reward access. Yet in permissionless systems, locations cannot be mandated, but
they emerge from incentives. Today, Ethereum's validators cluster along the
Atlantic (EU and U.S. East Coast), where latency is structurally favorable.
This raises a key question: when some regions already enjoy latency advantages,
how does protocol design shape validator incentives and the geography of
(de)centralization? We develop a latency-calibrated agent-based model and
compare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP),
akin to MEV-Boost, where proposers fetch full blocks from a relay that also
propagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate
value from multiple sources and broadcast the block themselves. Simulations
show that SSP concentrates around relay placement but more slowly, since
proximity mainly affects propagation, and the marginal value of time is
relatively uniform across regions. MSP centralizes faster: aggregating across
sources makes marginal value location-dependent, amplifying payoff dispersion
and migration toward latency minima. Source placement and consensus settings
can dampen or intensify these effects, though once validators are already
clustered, the impact of source placement on decentralization is marginal. In
most cases, North America consistently emerges as the focal hub. These findings
show that protocol design materially shapes validator geography and offer
levers for promoting geographical decentralization.

</details>


### [22] [Functional Encryption in Secure Neural Network Training: Data Leakage and Practical Mitigations](https://arxiv.org/abs/2509.21497)
*Alexandru Ioniţă,Andreea Ioniţă*

Main category: cs.CR

TL;DR: 本文提出了一种针对使用功能加密进行安全训练的神经网络的攻击方法，使用线性规划重构原始输入，并提出了两种解决方案来确保训练和推理的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习即服务的普及，云端训练模型存在隐私问题，需要安全训练方法。虽然功能加密提供了加密数据训练的新视角，但现有方法存在安全漏洞。

Method: 使用线性规划攻击方法重构原始输入数据，暴露安全漏洞。提出了两种解决方案：一种不依赖加密但需要客户端参与计算，另一种使用函数隐藏内积技术。

Result: 成功演示了对功能加密安全训练神经网络的攻击，能够重构原始输入数据，揭示了现有安全承诺的脆弱性。

Conclusion: 现有的功能加密方法在神经网络安全训练中存在漏洞，需要结合客户端参与或更高级的加密技术来确保真正的安全性。

Abstract: With the increased interest in artificial intelligence, Machine Learning as a
Service provides the infrastructure in the Cloud for easy training, testing,
and deploying models. However, these systems have a major privacy issue:
uploading sensitive data to the Cloud, especially during training. Therefore,
achieving secure Neural Network training has been on many researchers' minds
lately. More and more solutions for this problem are built around a main
pillar: Functional Encryption (FE). Although these approaches are very
interesting and offer a new perspective on ML training over encrypted data,
some vulnerabilities do not seem to be taken into consideration. In our paper,
we present an attack on neural networks that uses FE for secure training over
encrypted data. Our approach uses linear programming to reconstruct the
original input, unveiling the previous security promises. To address the
attack, we propose two solutions for secure training and inference that involve
the client during the computation phase. One approach ensures security without
relying on encryption, while the other uses function-hiding inner-product
techniques.

</details>


### [23] [From Indexing to Coding: A New Paradigm for Data Availability Sampling](https://arxiv.org/abs/2509.21586)
*Moritz Grundei,Aayush Rajasekaran,Kishori Konwar,Muriel Medard*

Main category: cs.CR

TL;DR: 提出了一种新的数据可用性采样方法，通过模块化编码和承诺过程，在采样时进行实时编码，相比传统固定速率擦除码方法，显著提高了轻节点的数据可用性保证强度。


<details>
  <summary>Details</summary>
Motivation: 解决区块链系统中的数据可用性问题，这是以太坊等平台可访问性和可扩展性挑战的核心。现有DAS方法通常对固定速率擦除码的码字形成加密承诺，限制了轻节点只能从预定编码符号集采样。

Method: 采用模块化方法，承诺原始数据而非编码数据，通过实时编码进行采样。具体实现使用随机线性网络编码(RLNC)协议。

Result: 在具体实现中，轻节点获得的数据可用性保证比传统使用Reed Solomon或低密度奇偶校验码的DAS方案强多个数量级。

Conclusion: 提出的新DAS方法通过实时编码采样提供了更强大的数据可用性保证，解决了现有方法的局限性。

Abstract: The data availability problem is a central challenge in blockchain systems
and lies at the core of the accessibility and scalability issues faced by
platforms such as Ethereum. Modern solutions employ several approaches, with
data availability sampling (DAS) being the most self-sufficient and
minimalistic in its security assumptions. Existing DAS methods typically form
cryptographic commitments on codewords of fixed-rate erasure codes, which
restrict light nodes to sampling from a predetermined set of coded symbols.
  In this paper, we introduce a new approach to DAS that modularizes the coding
and commitment process by committing to the uncoded data while performing
sampling through on-the-fly coding. The resulting samples are significantly
more expressive, enabling light nodes to obtain, in concrete implementations,
up to multiple orders of magnitude stronger assurances of data availability
than from sampling pre-committed symbols from a fixed-rate redundancy code as
done in established DAS schemes using Reed Solomon or low density parity check
codes. We present a concrete protocol that realizes this paradigm using random
linear network coding (RLNC).

</details>


### [24] [It's not Easy: Applying Supervised Machine Learning to Detect Malicious Extensions in the Chrome Web Store](https://arxiv.org/abs/2509.21590)
*Ben Rosenzweig,Valentino Dalla Valle,Giovanni Apruzzese,Aurore Fass*

Main category: cs.CR

TL;DR: 该研究开发了基于监督机器学习的恶意Chrome扩展检测系统，在实验室环境中准确率达98%，但在真实场景中面临概念漂移问题，发现68个绕过审核的恶意扩展，表明恶意浏览器扩展检测是一个根本性难题。


<details>
  <summary>Details</summary>
Motivation: Chrome Web Store中的恶意扩展可能绕过Google的审核流程，对用户安全和隐私构成威胁，需要更有效的自动化检测方法。

Method: 收集2017-2023年的7,140个恶意扩展和63,598个良性扩展，开发三种监督机器学习分类器，并在2023年新发布的35,462个扩展上进行测试。

Result: 实验室环境中分类器准确率达98%，但在真实场景中发现68个绕过审核的恶意扩展，同时检测到超过1,000个疑似恶意扩展，揭示了强烈的概念漂移效应。

Conclusion: 恶意浏览器扩展检测是一个根本性难题，需要研究社区和Google共同努力改进检测方法，同时商业检测工具效果不佳。

Abstract: Google Chrome is the most popular Web browser. Users can customize it with
extensions that enhance their browsing experience. The most well-known
marketplace of such extensions is the Chrome Web Store (CWS). Developers can
upload their extensions on the CWS, but such extensions are made available to
users only after a vetting process carried out by Google itself. Unfortunately,
some malicious extensions bypass such checks, putting the security and privacy
of downstream browser extension users at risk.
  Here, we scrutinize the extent to which automated mechanisms reliant on
supervised machine learning (ML) can be used to detect malicious extensions on
the CWS. To this end, we first collect 7,140 malicious extensions published in
2017--2023. We combine this dataset with 63,598 benign extensions published or
updated on the CWS before 2023, and we develop three supervised-ML-based
classifiers. We show that, in a "lab setting", our classifiers work well (e.g.,
98% accuracy). Then, we collect a more recent set of 35,462 extensions from the
CWS, published or last updated in 2023, with unknown ground truth. We were
eventually able to identify 68 malicious extensions that bypassed the vetting
process of the CWS. However, our classifiers also reported >1k likely malicious
extensions. Based on this finding (further supported with empirical evidence),
we elucidate, for the first time, a strong concept drift effect on browser
extensions. We also show that commercial detectors (e.g., VirusTotal) work
poorly to detect known malicious extensions. Altogether, our results highlight
that detecting malicious browser extensions is a fundamentally hard problem.
This requires additional work both by the research community and by Google
itself -- potentially by revising their approaches. In the meantime, we
informed Google of our discoveries, and we release our artifacts.

</details>


### [25] [World's First Authenticated Satellite Pseudorange from Orbit](https://arxiv.org/abs/2509.21601)
*Jason Anderson*

Main category: cs.CR

TL;DR: Pulsar-0卫星从太空广播认证测距服务，无需加密密钥所有权或泄露假设，通过水印设计实现防欺骗检测，声称实现世界首个来自轨道的认证卫星伪距


<details>
  <summary>Details</summary>
Motivation: 开发无需依赖加密密钥所有权或泄露假设的认证卫星测距服务，提供数学上可证明的安全性

Method: 设计Pulsar水印系统，分析水印的漏检概率和虚警概率，开发接收机处理流程，利用轨道传输进行验证

Result: 成功验证了Pulsar水印在轨道传输中的有效性，在包含真实轨道传输的欺骗场景中展示了防欺骗检测效能

Conclusion: 该工作提供了水印安全性的数学证明，并声称实现了世界首个来自轨道的认证卫星伪距

Abstract: Cryptographic Ranging Authentication is here! We present initial results on
the Pulsar authenticated ranging service broadcast from space with Pulsar-0
utilizing a recording taken at Xona headquarters in Burlingame, CA. No
assumptions pertaining to the ownership or leakage of encryption keys are
required. This work discusses the Pulsar watermark design and security
analysis. We derive the Pulsar watermark's probabilities of missed detection
and false alarm, and we discuss the required receiver processing needed to
utilize the Pulsar watermark. We present validation results of the Pulsar
watermark utilizing the transmissions from orbit. Lastly, we provide results
that demonstrate the spoofing detection efficacy with a spoofing scenario that
incorporates the authentic transmissions from orbit. Because we make no
assumption about the leakage of symmetric encryption keys, this work provides
mathematical justification of the watermark's security, and our July 2025
transmissions from orbit, we claim the world's first authenticated satellite
pseudorange from orbit.

</details>


### [26] [MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs](https://arxiv.org/abs/2509.21634)
*Prakhar Sharma,Haohuang Wen,Vinod Yegneswaran,Ashish Gehani,Phillip Porras,Zhiqiang Lin*

Main category: cs.CR

TL;DR: 提出了MobiLLM框架，这是一个基于大语言模型的智能AI系统，用于在6G O-RAN环境中实现端到端的自动化威胁缓解。


<details>
  <summary>Details</summary>
Motivation: 6G O-RAN的开放性虽然促进了创新，但也扩大了攻击面，现有防御措施多为被动式、劳动密集型，无法满足下一代系统的规模和复杂性需求。

Method: 采用模块化多智能体系统，包括威胁分析智能体（实时数据分类）、威胁分类智能体（使用RAG技术映射异常到对策）、威胁响应智能体（通过O-RAN控制接口安全执行缓解措施）。

Result: 初步评估显示MobiLLM能有效识别和协调复杂缓解策略，显著降低响应延迟，证明了6G中自主安全操作的可行性。

Conclusion: MobiLLM为可信赖的AI驱动网络安全提供了蓝图，基于MITRE FiGHT框架和3GPP规范等可信知识库，并配备强大的安全防护措施。

Abstract: The evolution toward 6G networks is being accelerated by the Open Radio
Access Network (O-RAN) paradigm -- an open, interoperable architecture that
enables intelligent, modular applications across public telecom and private
enterprise domains. While this openness creates unprecedented opportunities for
innovation, it also expands the attack surface, demanding resilient, low-cost,
and autonomous security solutions. Legacy defenses remain largely reactive,
labor-intensive, and inadequate for the scale and complexity of next-generation
systems. Current O-RAN applications focus mainly on network optimization or
passive threat detection, with limited capability for closed-loop, automated
response.
  To address this critical gap, we present an agentic AI framework for fully
automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM
orchestrates security workflows through a modular multi-agent system powered by
Large Language Models (LLMs). The framework features a Threat Analysis Agent
for real-time data triage, a Threat Classification Agent that uses
Retrieval-Augmented Generation (RAG) to map anomalies to specific
countermeasures, and a Threat Response Agent that safely operationalizes
mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge
bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped
with robust safety guardrails, MobiLLM provides a blueprint for trustworthy
AI-driven network security. Initial evaluations demonstrate that MobiLLM can
effectively identify and orchestrate complex mitigation strategies,
significantly reducing response latency and showcasing the feasibility of
autonomous security operations in 6G.

</details>


### [27] [Not My Agent, Not My Boundary? Elicitation of Personal Privacy Boundaries in AI-Delegated Information Sharing](https://arxiv.org/abs/2509.21712)
*Bingcan Guo,Eryue Xu,Zhiping Zhang,Tianshi Li*

Main category: cs.CR

TL;DR: 提出了一种基于AI的隐私边界激发方法，通过判别任务探究个体的隐私边界，研究发现沟通角色、AI委托等因素显著影响隐私披露偏好。


<details>
  <summary>Details</summary>
Motivation: 理解个体在隐私披露中的细微行为，超越一般规范，但由于隐私决策的情境依赖性和复杂权衡，激发这些边界仍然具有挑战性。

Method: 采用AI驱动的激发方法，通过判别任务探究隐私边界；进行了一项被试间研究，系统变化沟通角色和委托条件，从169名参与者收集了1,681个边界规范，涵盖61种场景。

Result: 沟通角色影响个体对详细和可识别披露的接受度；AI委托和个体隐私需求增强了对披露标识符的敏感性；AI委托导致个体间共识减少。

Conclusion: 强调将隐私偏好激发置于真实世界数据流中的重要性，建议将细微隐私边界作为未来AI系统的对齐目标。

Abstract: Aligning AI systems with human privacy preferences requires understanding
individuals' nuanced disclosure behaviors beyond general norms. Yet eliciting
such boundaries remains challenging due to the context-dependent nature of
privacy decisions and the complex trade-offs involved. We present an AI-powered
elicitation approach that probes individuals' privacy boundaries through a
discriminative task. We conducted a between-subjects study that systematically
varied communication roles and delegation conditions, resulting in 1,681
boundary specifications from 169 participants for 61 scenarios. We examined how
these contextual factors and individual differences influence the boundary
specification. Quantitative results show that communication roles influence
individuals' acceptance of detailed and identifiable disclosure, AI delegation
and individuals' need for privacy heighten sensitivity to disclosed
identifiers, and AI delegation results in less consensus across individuals.
Our findings highlight the importance of situating privacy preference
elicitation within real-world data flows. We advocate using nuanced privacy
boundaries as an alignment goal for future AI systems.

</details>


### [28] [Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models](https://arxiv.org/abs/2509.21761)
*Miao Yu,Zhenhong Zhou,Moayad Aloqaily,Kun Wang,Biwei Huang,Stephen Wang,Yueming Jin,Qingsong Wen*

Main category: cs.CR

TL;DR: 提出了Backdoor Attribution (BkdAttr)框架，通过三部分因果分析揭示LLM后门机制，发现后门特征由稀疏的注意力头编码，仅需干预约3%的头部即可消除90%以上的后门攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注对齐、越狱和幻觉问题，但忽视了后门攻击的内部机制，使得难以理解和完全消除后门威胁。

Method: 开发了Backdoor Probe证明可学习的后门特征存在于表示中，并构建Backdoor Attention Head Attribution (BAHA)精确定位处理这些特征的特定注意力头。

Result: 发现后门注意力头相对稀疏，消融约3%的总头部即可将攻击成功率降低90%以上；构建的后门向量通过单点干预可将攻击成功率提升至约100%或降至约0%。

Conclusion: 本研究开创了LLM后门机制可解释性探索，展示了强大的后门控制方法，为社区提供了可操作的见解。

Abstract: Fine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks
through data poisoning, yet the internal mechanisms governing these attacks
remain a black box. Previous research on interpretability for LLM safety tends
to focus on alignment, jailbreak, and hallucination, but overlooks backdoor
mechanisms, making it difficult to understand and fully eliminate the backdoor
threat. In this paper, aiming to bridge this gap, we explore the interpretable
mechanisms of LLM backdoors through Backdoor Attribution (BkdAttr), a
tripartite causal analysis framework. We first introduce the Backdoor Probe
that proves the existence of learnable backdoor features encoded within the
representations. Building on this insight, we further develop Backdoor
Attention Head Attribution (BAHA), efficiently pinpointing the specific
attention heads responsible for processing these features. Our primary
experiments reveals these heads are relatively sparse; ablating a minimal
\textbf{$\sim$ 3%} of total heads is sufficient to reduce the Attack Success
Rate (ASR) by \textbf{over 90%}. More importantly, we further employ these
findings to construct the Backdoor Vector derived from these attributed heads
as a master controller for the backdoor. Through only \textbf{1-point}
intervention on \textbf{single} representation, the vector can either boost ASR
up to \textbf{$\sim$ 100% ($\uparrow$)} on clean inputs, or completely
neutralize backdoor, suppressing ASR down to \textbf{$\sim$ 0% ($\downarrow$)}
on triggered inputs. In conclusion, our work pioneers the exploration of
mechanistic interpretability in LLM backdoors, demonstrating a powerful method
for backdoor control and revealing actionable insights for the community.

</details>


### [29] [PSRT: Accelerating LRM-based Guard Models via Prefilled Safe Reasoning Traces](https://arxiv.org/abs/2509.21768)
*Jiawei Zhao,Yuang Qi,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CR

TL;DR: PSRT方法通过预填充安全推理痕迹来替代大型推理模型的推理过程，显著降低推理成本，同时保持有害查询检测的分类效果。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理过程中会产生冗长的推理痕迹，导致巨大的计算开销，需要一种方法来降低这种开销同时保持性能。

Method: PSRT预填充来自构建数据集的"安全推理虚拟标记"，并在其连续嵌入上学习，借助指示标记在单次前向传播中实现有害查询检测。

Result: 在7个模型、13个数据集和8种越狱方法上的评估显示，PSRT完全消除了推理过程中生成推理标记的开销，分类性能几乎相同，平均F1分数仅下降0.015。

Conclusion: PSRT是一种高效的方法，能够在显著降低推理成本的同时，保持大型推理模型在有害查询检测方面的分类效果。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on
tasks such as mathematics and code generation. Motivated by these strengths,
recent work has empirically demonstrated the effectiveness of LRMs as guard
models in improving harmful query detection. However, LRMs typically generate
long reasoning traces during inference, causing substantial computational
overhead. In this paper, we introduce PSRT, a method that replaces the model's
reasoning process with a Prefilled Safe Reasoning Trace, thereby significantly
reducing the inference cost of LRMs. Concretely, PSRT prefills "safe reasoning
virtual tokens" from a constructed dataset and learns over their continuous
embeddings. With the aid of indicator tokens, PSRT enables harmful-query
detection in a single forward pass while preserving the classification
effectiveness of LRMs. We evaluate PSRT on 7 models, 13 datasets, and 8
jailbreak methods. In terms of efficiency, PSRT completely removes the overhead
of generating reasoning tokens during inference. In terms of classification
performance, PSRT achieves nearly identical accuracy, with only a minor average
F1 drop of 0.015 across 7 models and 5 datasets.

</details>


### [30] ["Your AI, My Shell": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors](https://arxiv.org/abs/2509.22040)
*Yue Liu,Yanjie Zhao,Yunbo Lyu,Ting Zhang,Haoyu Wang,David Lo*

Main category: cs.CR

TL;DR: 本研究首次对高权限AI编程编辑器的提示注入攻击进行实证分析，展示了攻击者如何通过污染外部开发资源来远程利用这些系统，将AI代理转变为攻击者的shell。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的AI编程编辑器（如Cursor）获得更多系统权限来执行复杂编码任务，虽然提升了开发效率，但也带来了新的安全担忧。这些高权限编辑器可能被恶意指令劫持，从而执行攻击者命令。

Method: 开发了AIShellJack自动化测试框架，包含314个独特攻击载荷，覆盖MITRE ATT&CK框架中的70种技术，用于评估AI编程编辑器的提示注入漏洞。

Result: 对GitHub Copilot和Cursor的大规模评估显示，执行恶意命令的攻击成功率高达84%，这些攻击在初始访问、系统发现、凭据窃取和数据外泄等多个目标上均有效。

Conclusion: 高权限AI编程编辑器存在严重的提示注入漏洞，攻击者能够远程劫持AI代理执行恶意命令，迫切需要加强这些系统的安全防护措施。

Abstract: Agentic AI coding editors driven by large language models have recently
become more popular due to their ability to improve developer productivity
during software development. Modern editors such as Cursor are designed not
just for code completion, but also with more system privileges for complex
coding tasks (e.g., run commands in the terminal, access development
environments, and interact with external systems). While this brings us closer
to the "fully automated programming" dream, it also raises new security
concerns. In this study, we present the first empirical analysis of prompt
injection attacks targeting these high-privilege agentic AI coding editors. We
show how attackers can remotely exploit these systems by poisoning external
development resources with malicious instructions, effectively hijacking AI
agents to run malicious commands, turning "your AI" into "attacker's shell". To
perform this analysis, we implement AIShellJack, an automated testing framework
for assessing prompt injection vulnerabilities in agentic AI coding editors.
AIShellJack contains 314 unique attack payloads that cover 70 techniques from
the MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale
evaluation on GitHub Copilot and Cursor, and our evaluation results show that
attack success rates can reach as high as 84% for executing malicious commands.
Moreover, these attacks are proven effective across a wide range of objectives,
ranging from initial access and system discovery to credential theft and data
exfiltration.

</details>


### [31] [PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation](https://arxiv.org/abs/2509.21772)
*Daiki Chiba,Hiroki Nakano,Takashi Koide*

Main category: cs.CR

TL;DR: PhishLumos是一个基于多智能体系统的主动钓鱼攻击检测系统，利用LLM智能体分析基础设施模式，能够提前一周识别100%的攻击活动，实现从被动URL拦截到主动活动缓解的转变。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击对社会构成严重威胁，尤其伤害弱势群体，破坏数字服务信任。现有防御措施多为被动响应，无法应对现代规避技术如内容隐藏。网络安全存在根本不平衡：攻击者易于扩展，而防御需要专家密集工作。

Method: 开发PhishLumos自适应多智能体系统，将规避行为视为关键信号来调查底层基础设施。系统使用LLM驱动的智能体发现共享托管、证书和域名注册模式。

Result: 在真实世界数据上，系统在中等情况下识别了100%的攻击活动，比网络安全专家确认提前一周以上。

Conclusion: PhishLumos展示了从被动URL拦截到主动活动缓解的实用转变，在用户受害前提供保护，使数字世界对所有用户更安全。

Abstract: Phishing attacks are a significant societal threat, disproportionately
harming vulnerable populations and eroding trust in essential digital services.
Current defenses are often reactive, failing against modern evasive tactics
like cloaking that conceal malicious content. To address this, we introduce
PhishLumos, an adaptive multi-agent system that proactively mitigates entire
attack campaigns. It confronts a core cybersecurity imbalance: attackers can
easily scale operations, while defense remains an intensive expert task.
Instead of being blocked by evasion, PhishLumos treats it as a critical signal
to investigate the underlying infrastructure. Its Large Language Model
(LLM)-powered agents uncover shared hosting, certificates, and domain
registration patterns. On real-world data, our system identified 100% of
campaigns in the median case, over a week before their confirmation by
cybersecurity experts. PhishLumos demonstrates a practical shift from reactive
URL blocking to proactive campaign mitigation, protecting users before they are
harmed and making the digital world safer for all.

</details>


### [32] [Lattice-Based Dynamic $k$-times Anonymous Authentication](https://arxiv.org/abs/2509.21786)
*Junjie Song,Jinguang Han,Man Ho Au,Rupeng Yang,Chao Sun*

Main category: cs.CR

TL;DR: 提出了首个基于格的动态k次匿名认证方案，支持动态用户管理、有限次匿名认证和后量子安全性


<details>
  <summary>Details</summary>
Motivation: 现有基于格的k-TAA方案不支持动态授予和撤销用户权限，且需考虑量子计算攻击的威胁

Method: 构建了具体的构造方案，并将其安全性归约到标准复杂性假设

Result: 相比现有基于格的k-TAA方案，本方案在通信成本方面更加高效

Conclusion: 成功实现了支持动态成员管理、有限次匿名认证且具有后量子安全性的首个基于格的动态k-TAA方案

Abstract: With the development of Internet, privacy has become a close concern of
users. Anonymous authentication plays an important role in privacy-preserving
systems. $k$-times anonymous authentication ($k$-TAA) scheme allows members of
a group to be authenticated anonymously by application providers up to $k$
times. Considering quantum computing attacks, lattice-based $k$-TAA was
introduced. However, existing schemes do not support dynamically granting and
revoking users. In this paper, we construct the first lattice-based dynamic
$k$-TAA, which offers limited times anonymous authentication, dynamic member
management, and post-quantum security. We present a concrete construction, and
reduce its security to standard complexity assumptions. Notably, compared with
existing lattice-based $k$-TAA, our scheme is efficient in terms of
communication cost.

</details>


### [33] [SoK: Potentials and Challenges of Large Language Models for Reverse Engineering](https://arxiv.org/abs/2509.21821)
*Xinyu Hu,Zhiwei Fu,Shaocong Xie,Steven H. H. Ding,Philippe Charland*

Main category: cs.CR

TL;DR: 本文系统综述了44篇研究论文和18个开源项目，探讨了大型语言模型在逆向工程中的应用，提出了分类法并分析了现有工作的优缺点、可重复性问题和新兴风险。


<details>
  <summary>Details</summary>
Motivation: 逆向工程在软件安全中至关重要，但传统方法劳动密集且需要专业知识。虽然深度学习已开始自动化部分逆向工程任务，但大型语言模型的应用仍缺乏系统性分析和比较框架。

Method: 通过系统文献综述方法，分析了44篇研究论文和18个开源项目，提出了基于目标、目标对象、方法、评估策略和数据规模的多维度分类法。

Result: 识别出现有工作的优势和局限性，揭示了可重复性和评估方面的差距，并分析了新兴风险。发现LLM在逆向工程中的应用方法多样，但缺乏统一标准和可比性。

Conclusion: 提出了开放挑战和未来研究方向，旨在指导LLM在逆向工程中更一致和安全相关的应用，促进该领域的累积性进展。

Abstract: Reverse Engineering (RE) is central to software security, enabling tasks such
as vulnerability discovery and malware analysis, but it remains labor-intensive
and requires substantial expertise. Earlier advances in deep learning start to
automate parts of RE, particularly for malware detection and vulnerability
classification. More recently, a rapidly growing body of work has applied Large
Language Models (LLMs) to similar purposes. Their role compared to prior
machine learning remains unclear, since some efforts simply adapt existing
pipelines with minimal change while others seek to exploit broader reasoning
and generative abilities. These differences, combined with varied problem
definitions, methods, and evaluation practices, limit comparability,
reproducibility, and cumulative progress. This paper systematizes the field by
reviewing 44 research papers, including peer-reviewed publications and
preprints, and 18 additional open-source projects that apply LLMs in RE. We
propose a taxonomy that organizes existing work by objective, target, method,
evaluation strategy, and data scale. Our analysis identifies strengths and
limitations, highlights reproducibility and evaluation gaps, and examines
emerging risks. We conclude with open challenges and future research directions
that aim to guide more coherent and security-relevant applications of LLMs in
RE.

</details>


### [34] [The Dark Art of Financial Disguise in Web3: Money Laundering Schemes and Countermeasures](https://arxiv.org/abs/2509.21831)
*Hesam Sarkhosh,Uzma Maroof,Diogo Barradas*

Main category: cs.CR

TL;DR: 本文调查了Web3和DeFi生态系统中的洗钱策略和机制，分析了犯罪分子如何利用Web3的匿名性和监管薄弱来掩盖非法金融活动，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: Web3和DeFi的无信任、无需许可和跨境特性带来了重大监管挑战，特别是洗钱问题日益严重，犯罪分子利用加密货币洗钱涉及诈骗、毒品交易和恐怖主义融资等非法活动。

Method: 通过构建高级策略和底层机制的分类法，分析犯罪分子如何利用Web3的匿名性和监管框架薄弱来掩盖非法金融活动。

Result: 识别了Web3洗钱的主要策略和机制，揭示了现有知识差距，并指出了检测和预防此类活动面临的开放挑战。

Conclusion: 需要进一步研究来促进更透明的Web3金融生态系统，为研究人员、政策制定者和行业从业者提供有价值的见解。

Abstract: The rise of Web3 and Decentralized Finance (DeFi) has enabled borderless
access to financial services empowered by smart contracts and blockchain
technology. However, the ecosystem's trustless, permissionless, and borderless
nature presents substantial regulatory challenges. The absence of centralized
oversight and the technical complexity create fertile ground for financial
crimes. Among these, money laundering is particularly concerning, as in the
event of successful scams, code exploits, and market manipulations, it
facilitates covert movement of illicit gains. Beyond this, there is a growing
concern that cryptocurrencies can be leveraged to launder proceeds from drug
trafficking, or to transfer funds linked to terrorism financing.
  This survey aims to outline a taxonomy of high-level strategies and
underlying mechanisms exploited to facilitate money laundering in Web3. We
examine how criminals leverage the pseudonymous nature of Web3, alongside weak
regulatory frameworks, to obscure illicit financial activities. Our study seeks
to bridge existing knowledge gaps on laundering schemes, identify open
challenges in the detection and prevention of such activities, and propose
future research directions to foster a more transparent Web3 financial
ecosystem -- offering valuable insights for researchers, policymakers, and
industry practitioners.

</details>


### [35] [SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models](https://arxiv.org/abs/2509.21843)
*Jingkai Guo,Chaitali Chakrabarti,Deliang Fan*

Main category: cs.CR

TL;DR: SBFA是一种新的位翻转攻击方法，仅需翻转单个比特就能使LLM性能崩溃，同时保持扰动值在良性权重分布范围内。


<details>
  <summary>Details</summary>
Motivation: 现有位翻转攻击方法通常分别针对整数或浮点模型，灵活性有限。在浮点模型中，随机位翻转常导致参数变为极端值，不够隐蔽且可能引发数值运行时错误。

Method: 通过定义的参数敏感度指标ImpactScore（结合梯度敏感性和受良性层间权重分布约束的扰动范围）进行迭代搜索和排序，并提出轻量级SKIP搜索算法来大幅降低搜索复杂度。

Result: 在Qwen、LLaMA和Gemma模型上，仅翻转单个比特就成功将MMLU和SST-2的准确率降至随机水平以下，适用于BF16和INT8数据格式。

Conclusion: 仅翻转数十亿参数中的单个比特就能揭示SOTA LLM模型的严重安全漏洞。

Abstract: Model integrity of Large language models (LLMs) has become a pressing
security concern with their massive online deployment. Prior Bit-Flip Attacks
(BFAs) -- a class of popular AI weight memory fault-injection techniques -- can
severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips
can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs
and reveal that, despite the intuition of better robustness from modularity and
redundancy, only a handful of adversarial bit flips can also cause LLMs'
catastrophic accuracy degradation. However, existing BFA methods typically
focus on either integer or floating-point models separately, limiting attack
flexibility. Moreover, in floating-point models, random bit flips often cause
perturbed parameters to extreme values (e.g., flipping in exponent bit), making
it not stealthy and leading to numerical runtime error (e.g., invalid tensor
values (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky
Bit-Flip Attack), which collapses LLM performance with only one single bit flip
while keeping perturbed values within benign layer-wise weight distribution. It
is achieved through iterative searching and ranking through our defined
parameter sensitivity metric, ImpactScore, which combines gradient sensitivity
and perturbation range constrained by the benign layer-wise weight
distribution. A novel lightweight SKIP searching algorithm is also proposed to
greatly reduce searching complexity, which leads to successful SBFA searching
taking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma
models, with only one single bit flip, SBFA successfully degrades accuracy to
below random levels on MMLU and SST-2 in both BF16 and INT8 data formats.
Remarkably, flipping a single bit out of billions of parameters reveals a
severe security concern of SOTA LLM models.

</details>


### [36] [You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors](https://arxiv.org/abs/2509.21884)
*Bochuan Cao,Changjiang Li,Yuanpu Cao,Yameng Ge,Ting Wang,Jinghui Chen*

Main category: cs.CR

TL;DR: 本文提出了一种简单的系统提示泄露攻击方法，并开发了SysVec解决方案，通过将系统提示编码为内部表示向量而非原始文本来防止泄露，同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型广泛使用系统提示，但面临泄露风险。现有防御方法主要基于模式识别，容易受到新型攻击。需要从根本上解决系统提示泄露问题。

Method: 提出SysVec方法，将系统提示编码为内部表示向量而非原始文本，从根本上防止提示泄露。

Result: 实验结果显示SysVec能有效缓解提示泄露攻击，保持LLM功能完整性，并在长上下文场景中减轻遗忘问题，同时提升模型的指令遵循能力。

Conclusion: SysVec通过将系统提示向量化，不仅增强了安全性，还改善了模型性能，为解决系统提示泄露问题提供了根本性解决方案。

Abstract: Large language models (LLMs) have been widely adopted across various
applications, leveraging customized system prompts for diverse tasks. Facing
potential system prompt leakage risks, model developers have implemented
strategies to prevent leakage, primarily by disabling LLMs from repeating their
context when encountering known attack patterns. However, it remains vulnerable
to new and unforeseen prompt-leaking techniques. In this paper, we first
introduce a simple yet effective prompt leaking attack to reveal such risks.
Our attack is capable of extracting system prompts from various LLM-based
application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our
findings further inspire us to search for a fundamental solution to the
problems by having no system prompt in the context. To this end, we propose
SysVec, a novel method that encodes system prompts as internal representation
vectors rather than raw text. By doing so, SysVec minimizes the risk of
unauthorized disclosure while preserving the LLM's core language capabilities.
Remarkably, this approach not only enhances security but also improves the
model's general instruction-following abilities. Experimental results
demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves
the LLM's functional integrity, and helps alleviate the forgetting issue in
long-context scenarios.

</details>


### [37] [Eliminating Exponential Key Growth in PRG-Based Distributed Point Functions](https://arxiv.org/abs/2509.22022)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: 本文优化了多党分布式点函数方案，通过诚实多数假设消除了指数级密钥增长，实现了首个实用的PRG基多党DPF方案，密钥大小比现有最佳方案小3倍。


<details>
  <summary>Details</summary>
Motivation: 现有的多党PRG基DPF方案存在密钥大小随参与方数量和域大小指数增长的问题，导致实用性受限。

Method: 基于Boyle等人的方案进行优化，利用诚实多数假设来消除指数因子，构建首个实用的PRG基多党DPF方案。

Result: 实现了密钥大小比已知最佳多党DPF方案小3倍，证明了PRG基多党DPF可以达到实用性能。

Conclusion: 通过精心优化，PRG基多党DPF能够实现实用性能，甚至达到顶级性能表现。

Abstract: Distributed Point Functions (DPFs) enable sharing secret point functions
across multiple parties, supporting privacy-preserving technologies such as
Private Information Retrieval, and anonymous communications. While 2-party
PRG-based schemes with logarithmic key sizes have been known for a decade,
extending these solutions to multi-party settings has proven challenging. In
particular, PRG-based multi-party DPFs have historically struggled with
practicality due to key sizes growing exponentially with the number of parties
and the field size.
  Our work addresses this efficiency bottleneck by optimizing the PRG-based
multi-party DPF scheme of Boyle et al. (EUROCRYPT'15). By leveraging the
honest-majority assumption, we eliminate the exponential factor present in this
scheme. Our construction is the first PRG-based multi-party DPF scheme with
practical key sizes, and provides key up to 3x smaller than the best known
multi-party DPF. This work demonstrates that with careful optimization,
PRG-based multi-party DPFs can achieve practical performances, and even obtain
top performances.

</details>


### [38] [NanoTag: Systems Support for Efficient Byte-Granular Overflow Detection on ARM MTE](https://arxiv.org/abs/2509.22027)
*Mingkai Li,Hang Ye,Joseph Devietti,Suman Jana,Tanvir Ahmed Khan*

Main category: cs.CR

TL;DR: NanoTag是一个在ARM MTE硬件上实现字节级内存安全错误检测的系统，通过设置tripwire来检测16字节粒度内的缓冲区溢出，在保持低开销的同时接近ASAN的检测精度。


<details>
  <summary>Details</summary>
Motivation: ARM MTE硬件检测内存安全错误开销低但精度粗（16字节粒度），而软件方法如ASAN精度高但开销大，需要结合两者优势。

Method: 在可能发生intra-granule溢出的tag granules上设置tripwire，软件检测访问tripwire时的溢出，其余访问由MTE硬件检测。基于Android 11的Scudo Hardened Allocator实现。

Result: NanoTag在流行基准测试和实际案例中检测到的内存安全错误数量接近ASAN，运行开销与MTE SYNC模式的Scudo Hardened Allocator相当。

Conclusion: NanoTag成功结合了硬件MTE的低开销和软件方法的高精度，实现了实用的字节级内存安全错误检测。

Abstract: Memory safety bugs, such as buffer overflows and use-after-frees, are the
leading causes of software safety issues in production. Software-based
approaches, e.g., Address Sanitizer (ASAN), can detect such bugs with high
precision, but with prohibitively high overhead. ARM's Memory Tagging Extension
(MTE) offers a promising alternative to detect these bugs in hardware with a
much lower overhead. However, in this paper, we perform a thorough
investigation of Google Pixel 8, the first production implementation of ARM
MTE, and show that MTE can only achieve coarse precision in bug detection
compared with software-based approaches such as ASAN, mainly due to its 16-byte
tag granularity. To address this issue, we present NanoTag, a system to detect
memory safety bugs in unmodified binaries at byte granularity with ARM MTE.
NanoTag detects intra-granule buffer overflows by setting up a tripwire for tag
granules that may require intra-granule overflow detection. The memory access
to the tripwire causes additional overflow detection in the software while
using MTE's hardware to detect bugs for the rest of the accesses. We implement
NanoTag based on the Scudo Hardened Allocator, the default memory allocator on
Android since Android 11. Our evaluation results across popular benchmarks and
real-world case studies show that NanoTag detects nearly as many memory safety
bugs as ASAN while incurring similar run-time overhead to Scudo Hardened
Allocator in MTE SYNC mode.

</details>


### [39] [Guidance Watermarking for Diffusion Models](https://arxiv.org/abs/2509.22126)
*Enoal Gesny,Eva Giboulot,Teddy Furon,Vivien Chappelier*

Main category: cs.CR

TL;DR: 提出一种基于梯度引导的扩散模型水印方法，可将后处理水印方案转换为生成过程中的嵌入，无需重新训练或微调。


<details>
  <summary>Details</summary>
Motivation: 将后处理水印方案有效整合到扩散模型的生成过程中，增强水印的鲁棒性，同时保持生成图像的质量和多样性。

Method: 使用现成水印解码器计算梯度来引导扩散过程，梯度计算包含多种图像增强技术，提高对未训练攻击的鲁棒性。

Result: 该方法在不同扩散模型和检测器上验证有效，水印引导不会显著改变给定种子和提示的生成图像，保持了生成质量和多样性。

Conclusion: 该方法成功将后处理水印转换为生成过程嵌入，与变分自编码器修改技术互补，为扩散模型提供有效的水印保护方案。

Abstract: This paper introduces a novel watermarking method for diffusion models. It is
based on guiding the diffusion process using the gradient computed from any
off-the-shelf watermark decoder. The gradient computation encompasses different
image augmentations, increasing robustness to attacks against which the decoder
was not originally robust, without retraining or fine-tuning. Our method
effectively convert any \textit{post-hoc} watermarking scheme into an
in-generation embedding along the diffusion process. We show that this approach
is complementary to watermarking techniques modifying the variational
autoencoder at the end of the diffusion process. We validate the methods on
different diffusion models and detectors. The watermarking guidance does not
significantly alter the generated image for a given seed and prompt, preserving
both the diversity and quality of generation.

</details>


### [40] [The Express Lane to Spam and Centralization: An Empirical Analysis of Arbitrum's Timeboost](https://arxiv.org/abs/2509.22143)
*Johnnatan Messias,Christof Ferreira Torres*

Main category: cs.CR

TL;DR: 对Arbitrum Timeboost机制的首个大规模实证研究，分析了1150万笔快速通道交易和15.1万次拍卖，发现该机制未能实现公平、去中心化和减少垃圾交易的目标，反而加剧了中心化。


<details>
  <summary>Details</summary>
Motivation: DeFi应用易受MEV攻击，Arbitrum引入Timeboost拍卖机制旨在缓解延迟竞争并将MEV收入内部化，但需要实证验证其实际效果。

Method: 对2025年4月至7月期间的1150万笔快速通道交易和15.1万次拍卖进行大规模实证分析。

Result: 发现快速通道控制高度中心化（两个实体赢得90%以上拍卖）、MEV机会集中在区块末尾、22%的时间加速交易被回滚、二级市场崩溃、拍卖竞争下降导致DAO收入减少。

Conclusion: Timeboost未能实现其公平、去中心化和减少垃圾交易的目标，反而强化了中心化并限制了采用，凸显了基于拍卖的排序机制在rollup中公平交易排序的局限性。

Abstract: DeFi applications are vulnerable to MEV, where specialized actors profit by
reordering or inserting transactions. To mitigate latency races and internalize
MEV revenue, Arbitrum introduced Timeboost, an auction-based transaction
sequencing mechanism that grants short-term priority access to an express lane.
In this paper we present the first large-scale empirical study of Timeboost,
analyzing over 11.5 million express lane transactions and 151 thousand auctions
between April and July 2025. Our results reveal five main findings. First,
express lane control is highly centralized, with two entities winning more than
90% of auctions. Second, while express lane access provides earlier inclusion,
profitable MEV opportunities cluster at the end of blocks, limiting the value
of priority access. Third, approximately 22% of time-boosted transactions are
reverted, indicating that the Timeboost does not effectively mitigate spam.
Fourth, secondary markets for reselling express lane rights have collapsed due
to poor execution reliability and unsustainable economics. Finally, auction
competition declined over time, leading to steadily reduced revenue for the
Arbitrum DAO. Taken together, these findings show that Timeboost fails to
deliver on its stated goals of fairness, decentralization, and spam reduction.
Instead, it reinforces centralization and narrows adoption, highlighting the
limitations of auction-based ordering as a mechanism for fair transaction
sequencing in rollups.

</details>


### [41] [Collusion-Driven Impersonation Attack on Channel-Resistant RF Fingerprinting](https://arxiv.org/abs/2509.22154)
*Zhou Xu,Guyue Li,Zhe Peng,Aiqun Hu*

Main category: cs.CR

TL;DR: 提出了一种基于共谋的RF指纹模仿攻击，通过同步攻击者与共谋接收器来匹配合法发射器的集中对数功率谱特征，成功破解了多种环境下的RFF识别系统。


<details>
  <summary>Details</summary>
Motivation: 射频指纹识别技术的研究重点从鲁棒性转向安全性，目前虽然验证了RFF对基本欺骗攻击的安全性，但其对高级模仿攻击的抵抗能力仍未知。

Method: 设计了结合变分自编码器和多目标损失函数的欺骗信号生成网络，攻击者与共谋接收器同步匹配合法发射器的CLPS特征，实现RF级别的欺骗。

Result: 在包含AWGN、多径衰落和多普勒频移的标准信道变化环境下进行广泛仿真，攻击方案在不同信道条件下基本保持95%以上的成功率。

Conclusion: 该攻击方案有效揭示了RFF识别系统在高级模仿攻击下的脆弱性，成功实现了RF级别的欺骗攻击。

Abstract: Radio frequency fingerprint (RFF) is a promising device identification
technology, with recent research shifting from robustness to security due to
growing concerns over vulnerabilities. To date, while the security of RFF
against basic spoofing such as MAC address tampering has been validated, its
resilience to advanced mimicry remains unknown. To address this gap, we propose
a collusion-driven impersonation attack that achieves RF-level mimicry,
successfully breaking RFF identification systems across diverse environments.
Specifically, the attacker synchronizes with a colluding receiver to match the
centralized logarithmic power spectrum (CLPS) of the legitimate transmitter;
once the colluder deems the CLPS identical, the victim receiver will also
accept the forged fingerprint, completing RF-level spoofing. Given that the
distribution of CLPS features is relatively concentrated and has a clear
underlying structure, we design a spoofed signal generation network that
integrates a variational autoencoder (VAE) with a multi-objective loss function
to enhance the similarity and deceptive capability of the generated samples. We
carry out extensive simulations, validating cross-channel attacks in
environments that incorporate standard channel variations including additive
white Gaussian noise (AWGN), multipath fading, and Doppler shift. The results
indicate that the proposed attack scheme essentially maintains a success rate
of over 95% under different channel conditions, revealing the effectiveness of
this attack.

</details>


### [42] [Accuracy-First Rényi Differential Privacy and Post-Processing Immunity](https://arxiv.org/abs/2509.22213)
*Ossi Räisä,Antti Koskela,Antti Honkela*

Main category: cs.CR

TL;DR: 本文研究了差分隐私中准确度优先视角下的后处理免疫性问题，提出了基于Rényi差分隐私的新定义，并开发了实用工具，应用于合成数据生成。


<details>
  <summary>Details</summary>
Motivation: 现有准确度优先视角的差分隐私定义忽略了后处理免疫性这一重要特性，导致隐私保护可能被削弱。

Method: 提出基于Rényi差分隐私的新定义，开发了包括高斯机制和验证集准确性检查在内的实用工具。

Result: 新定义具有后处理免疫性，并在合成数据生成应用中成功实现了根据私有验证集准确性调整隐私边界。

Conclusion: 提出的基于Rényi差分隐私的新定义既保持了后处理免疫性，又提供了实用的分析工具，解决了现有方法的局限性。

Abstract: The accuracy-first perspective of differential privacy addresses an important
shortcoming by allowing a data analyst to adaptively adjust the quantitative
privacy bound instead of sticking to a predetermined bound. Existing works on
the accuracy-first perspective have neglected an important property of
differential privacy known as post-processing immunity, which ensures that an
adversary is not able to weaken the privacy guarantee by post-processing. We
address this gap by determining which existing definitions in the
accuracy-first perspective have post-processing immunity, and which do not. The
only definition with post-processing immunity, pure ex-post privacy, lacks
useful tools for practical problems, such as an ex-post analogue of the
Gaussian mechanism, and an algorithm to check if accuracy on separate private
validation set is high enough. To address this, we propose a new definition
based on R\'enyi differential privacy that has post-processing immunity, and we
develop basic theory and tools needed for practical applications. We
demonstrate the practicality of our theory with an application to synthetic
data generation, where our algorithm successfully adjusts the privacy bound
until an accuracy threshold is met on a private validation dataset.

</details>


### [43] [Learn, Check, Test -- Security Testing Using Automata Learning and Model Checking](https://arxiv.org/abs/2509.22215)
*Stefan Marksteiner,Mikael Sjödin,Marjan Sirjani*

Main category: cs.CR

TL;DR: 该论文提出了一种通过主动黑盒学习技术推断CPS行为模型，并将其转换为模型检查器可评估格式的方法，用于验证工业系统的正确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 由于供应链长或保密性等原因，许多CPS系统只能在黑盒设置下进行检测，而获取准确的行为模型具有挑战性。

Method: 使用主动黑盒学习技术推断Mealy机行为模型，通过上下文命题映射(CPMs)标注模型，将其转换为类似Kripke结构的形式，并定义模板转换为模型检查器兼容格式。

Result: 开发了能够灵活扩展的方法，可以轻松修改模型引入非确定性行为或故障，并通过NFC和UDS等不同通信协议的案例研究验证了方法的通用性。

Conclusion: 该方法提供了一种系统化、自动化的方式来验证CPS系统的安全属性，具有灵活性和可扩展性，能够适应不同的通信协议。

Abstract: Cyber-physical systems are part of industrial systems and critical
infrastructure. Therefore, they should be examined in a comprehensive manner to
verify their correctness and security. At the same time, the complexity of such
systems demands such examinations to be systematic and, if possible, automated
for efficiency and accuracy. A method that can be useful in this context is
model checking. However, this requires a model that faithfully represents the
behavior of the examined system. Obtaining such a model is not trivial, as many
of these systems can be examined only in black box settings due to, e.g., long
supply chains or secrecy. We therefore utilize active black box learning
techniques to infer behavioral models in the form of Mealy machines of such
systems and translate them into a form that can be evaluated using a model
checker. To this end, we will investigate a cyber-physical systems as a black
box using its external communication interface. We first annotate the model
with propositions by mapping context information from the respective protocol
to the model using Context-based Proposition Maps (CPMs). We gain annotated
Mealy machines that resemble Kripke structures. We then formally define a
template, to transfer the structures model checker-compatible format. We
further define generic security properties based on basic security
requirements. Due to the used CPMs, we can instantiate these properties with a
meaningful context to check a specific protocol, which makes the approach
flexible and scalable. The gained model can be easily altered to introduce
non-deterministic behavior (like timeouts) or faults and examined if the
properties still. Lastly, we demonstrate the versatility of the approach by
providing case studies of different communication protocols (NFC and UDS),
checked with the same tool chain and the same security properties.

</details>


### [44] [Secure and Efficient Access Control for Computer-Use Agents via Context Space](https://arxiv.org/abs/2509.22256)
*Haochen Gong,Chenxiao Li,Rui Chang,Wenbo Shen*

Main category: cs.CR

TL;DR: CSAgent是一个基于静态策略的系统级访问控制框架，用于保护基于LLM的计算机使用代理，通过意图和上下文感知策略来防止代理行为偏离用户意图，提供超过99.36%的攻击防御率，性能开销仅为6.83%。


<details>
  <summary>Details</summary>
Motivation: LLM代理控制计算机存在安全风险，当代理行为偏离用户意图时可能造成不可逆后果。现有的缓解方法在可用性、安全性和性能方面仍有局限。

Method: 提出CSAgent框架，引入意图和上下文感知策略，提供自动化工具链帮助开发者构建和优化策略，通过优化的操作系统服务强制执行这些策略。

Result: CSAgent成功防御超过99.36%的攻击，仅引入6.83%的性能开销，支持通过API、CLI和GUI等多种接口保护代理。

Conclusion: CSAgent通过系统级的静态策略访问控制，有效解决了LLM代理在计算机使用中的安全问题，在保持高性能的同时提供了强大的安全保护。

Abstract: Large language model (LLM)-based computer-use agents represent a convergence
of AI and OS capabilities, enabling natural language to control system- and
application-level functions. However, due to LLMs' inherent uncertainty issues,
granting agents control over computers poses significant security risks. When
agent actions deviate from user intentions, they can cause irreversible
consequences. Existing mitigation approaches, such as user confirmation and
LLM-based dynamic action validation, still suffer from limitations in
usability, security, and performance. To address these challenges, we propose
CSAgent, a system-level, static policy-based access control framework for
computer-use agents. To bridge the gap between static policy and dynamic
context and user intent, CSAgent introduces intent- and context-aware policies,
and provides an automated toolchain to assist developers in constructing and
refining them. CSAgent enforces these policies through an optimized OS service,
ensuring that agent actions can only be executed under specific user intents
and contexts. CSAgent supports protecting agents that control computers through
diverse interfaces, including API, CLI, and GUI. We implement and evaluate
CSAgent, which successfully defends against more than 99.36% of attacks while
introducing only 6.83% performance overhead.

</details>


### [45] [A Global Analysis of Cyber Threats to the Energy Sector: "Currents of Conflict" from a Geopolitical Perspective](https://arxiv.org/abs/2509.22280)
*Gustavo Sánchez,Ghada Elbez,Veit Hagenmeyer*

Main category: cs.CR

TL;DR: 该论文利用生成式AI分析网络威胁情报，重点关注能源领域，通过地缘政治比较和网络安全工具评估，为研究人员和政策制定者提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁频率和复杂性的增加，需要更全面地理解这些威胁，特别是在能源等关键基础设施领域。

Method: 使用生成式人工智能从原始网络威胁描述中提取和结构化信息，进行威胁行为者来源和目标区域的地缘政治比较，并评估基于学习技术的网络安全工具在检测能源攻击指标方面的有效性。

Result: 通过跨多个数据库的比较分析，揭示了通用威胁态势中的趋势，并评估了网络安全工具在检测能源针对性攻击方面的效果。

Conclusion: 该分析为研究人员、政策制定者和网络安全专业人员提供了新的见解和可操作信息，有助于更好地理解和应对针对能源领域的网络威胁。

Abstract: The escalating frequency and sophistication of cyber threats increased the
need for their comprehensive understanding. This paper explores the
intersection of geopolitical dynamics, cyber threat intelligence analysis, and
advanced detection technologies, with a focus on the energy domain. We leverage
generative artificial intelligence to extract and structure information from
raw cyber threat descriptions, enabling enhanced analysis. By conducting a
geopolitical comparison of threat actor origins and target regions across
multiple databases, we provide insights into trends within the general threat
landscape. Additionally, we evaluate the effectiveness of cybersecurity tools
-- with particular emphasis on learning-based techniques -- in detecting
indicators of compromise for energy-targeted attacks. This analysis yields new
insights, providing actionable information to researchers, policy makers, and
cybersecurity professionals.

</details>


### [46] [Privacy Mechanism Design based on Empirical Distributions](https://arxiv.org/abs/2509.22428)
*Leonhard Grosse,Sara Saeidian,Mikael Skoglund,Tobias J. Oechtering*

Main category: cs.CR

TL;DR: 提出了一个基于经验数据分布估计的逐点最大泄漏隐私评估框架，通过考虑数据生成分布集合来获得最坏情况泄漏边界，提供分布无关的PML隐私保证。


<details>
  <summary>Details</summary>
Motivation: 传统PML隐私度量依赖于对私有数据生成分布的准确知识，但在实际中只能通过可用数据样本来估计分布，需要处理这种不确定性。

Method: 扩展PML框架以考虑数据生成分布集合，结合大偏差理论获得分布无关的(ε,δ)-PML保证，提出最优二元机制设计方法，将其转化为线性约束凸规划问题。

Result: 开发了最优二元机制，证明机制设计可转化为凸规划问题，并验证了所提方法在拉普拉斯机制和高斯机制上的应用，相比本地差分隐私可显著提升效用。

Conclusion: 提出的框架能够在数据生成分布不确定的情况下提供可靠的PML隐私保证，机制设计方法能显著提升效用同时保持相似的隐私保护水平。

Abstract: Pointwise maximal leakage (PML) is a per-outcome privacy measure based on
threat models from quantitative information flow. Privacy guarantees with PML
rely on knowledge about the distribution that generated the private data. In
this work, we propose a framework for PML privacy assessment and mechanism
design with empirical estimates of this data-generating distribution. By
extending the PML framework to consider sets of data-generating distributions,
we arrive at bounds on the worst-case leakage within a given set. We use these
bounds alongside large-deviation bounds from the literature to provide a method
for obtaining distribution-independent $(\varepsilon,\delta)$-PML guarantees
when the data-generating distribution is estimated from available data samples.
We provide an optimal binary mechanism, and show that mechanism design with
this type of uncertainty about the data-generating distribution reduces to a
linearly constrained convex program. Further, we show that optimal mechanisms
designed for a distribution estimate can be used. Finally, we apply these tools
to leakage assessment of the Laplace mechanism and the Gaussian mechanism for
binary private data, and numerically show that the presented approach to
mechanism design can yield significant utility increase compared to local
differential privacy, while retaining similar privacy guarantees.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [Towards mitigating information leakage when evaluating safety monitors](https://arxiv.org/abs/2509.21344)
*Gerard Boxo,Aman Neelappa,Shivam Raval*

Main category: cs.AI

TL;DR: 本文提出了一个系统框架来评估白盒监控器的真实性能，识别了两种导致性能虚高的泄漏问题，并提出了三种缓解策略来更准确地检测模型的实际有害行为。


<details>
  <summary>Details</summary>
Motivation: 白盒监控器在检测大型语言模型有害行为时面临挑战：训练和评估所需的有害行为样本通常通过提示或微调获得，这会导致用于激发行为的信息泄漏到监控器数据中，从而虚高其性能表现。

Method: 提出了三种评估策略：内容过滤（从输入中移除与欺骗相关的文本）、分数过滤（仅聚合任务相关标记的分数）、以及提示蒸馏微调模型生物（训练模型在没有明确提示的情况下表现出欺骗行为）。

Result: 实验发现：内容过滤能平滑移除激发信号，使探针AUROC降低30%；分数过滤减少AUROC 15%；微调模型生物虽然改善评估但使监控器性能下降高达40%，即使重新训练也是如此。

Conclusion: 识别了激发泄漏和推理泄漏两种导致监控器性能虚高的问题，提出的缓解策略能更准确地评估监控器检测真实模型行为的能力，而非表面的激发伪影。

Abstract: White box monitors that analyze model internals offer promising advantages
for detecting potentially harmful behaviors in large language models, including
lower computational costs and integration into layered defense systems.However,
training and evaluating these monitors requires response exemplars that exhibit
the target behaviors, typically elicited through prompting or fine-tuning. This
presents a challenge when the information used to elicit behaviors inevitably
leaks into the data that monitors ingest, inflating their effectiveness. We
present a systematic framework for evaluating a monitor's performance in terms
of its ability to detect genuine model behavior rather than superficial
elicitation artifacts. Furthermore, we propose three novel strategies to
evaluate the monitor: content filtering (removing deception-related text from
inputs), score filtering (aggregating only over task-relevant tokens), and
prompt distilled fine-tuned model organisms (models trained to exhibit
deceptive behavior without explicit prompting). Using deception detection as a
representative case study, we identify two forms of leakage that inflate
monitor performance: elicitation leakage from prompts that explicitly request
harmful behavior, and reasoning leakage from models that verbalize their
deceptive actions. Through experiments on multiple deception benchmarks, we
apply our proposed mitigation strategies and measure performance retention. Our
evaluation of the monitors reveal three crucial findings: (1) Content filtering
is a good mitigation strategy that allows for a smooth removal of elicitation
signal and can decrease probe AUROC by 30\% (2) Score filtering was found to
reduce AUROC by 15\% but is not as straightforward to attribute to (3) A
finetuned model organism improves monitor evaluations but reduces their
performance by upto 40\%, even when re-trained.

</details>


### [48] [Correct Reasoning Paths Visit Shared Decision Pivots](https://arxiv.org/abs/2509.21549)
*Dongkyu Cho,Amy B. Z. Zhang,Bilel Fehri,Sheng Wang,Rumi Chunara,Rui Song,Hengrui Cai*

Main category: cs.AI

TL;DR: 提出决策支点概念，通过自训练管道验证和压缩思维链推理，无需真实推理数据或外部指标即可对齐推理过程。


<details>
  <summary>Details</summary>
Motivation: 解决大规模验证思维链推理中间过程的难题，现有方法难以有效验证推理路径的正确性。

Method: 引入决策支点作为可验证检查点，通过采样多样推理路径挖掘共享支点，使用辅助验证器压缩推理路径，并进行自训练。

Result: 在LogiQA、MedQA和MATH500等标准基准测试中验证了方法的有效性。

Conclusion: 决策支点方法能够有效对齐推理过程，无需依赖真实推理数据或外部评估指标。

Abstract: Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of
large language models (LLMs), yet verifying those traces at scale remains
unsolved. In response, we introduce the idea of decision pivots-minimal,
verifiable checkpoints that any correct reasoning path must visit. We
hypothesize that correct reasoning, though stylistically diverse, converge on
the same pivot set, while incorrect ones violate at least one pivot. Leveraging
this property, we propose a self-training pipeline that (i) samples diverse
reasoning paths and mines shared decision pivots, (ii) compresses each trace
into pivot-focused short-path reasoning using an auxiliary verifier, and (iii)
post-trains the model using its self-generated outputs. The proposed method
aligns reasoning without ground truth reasoning data or external metrics.
Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the
effectiveness of our method.

</details>


### [49] [AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need](https://arxiv.org/abs/2509.21553)
*Ahmed Jaber,Wangshu Zhu,Karthick Jayavelu,Justin Downes,Sameer Mohamed,Candace Agonafir,Linnia Hawkins,Tian Zheng*

Main category: cs.AI

TL;DR: 提出了一种结合知识图谱和AI代理的云原生科学工作流系统，旨在降低气候数据科学的技术门槛，使非专业用户能够通过自然语言交互识别和分析相关数据集。


<details>
  <summary>Details</summary>
Motivation: 气候数据科学面临数据源分散、格式异构以及技术门槛高等挑战，限制了参与度、减缓了发现速度并降低了科学工作流的可重复性。

Method: 通过整合精心策划的知识图谱与AI代理，知识图谱提供统一的数据集、工具和工作流组织层，AI代理基于生成式AI服务实现自然语言交互、自动化数据访问和简化分析。

Result: 该系统显著降低了参与气候数据科学的技术门槛，使非专业用户能够识别和分析相关数据集，展示了通过知识图谱实现可扩展和智能科学工作流的可行性。

Conclusion: 该系统为民主化气候数据访问和建立可重复、可扩展的人类-AI协作科学研究框架提供了一条可行路径，其开源设计支持社区贡献，确保知识图谱和相关工具能够作为共享资源不断演进。

Abstract: Climate data science faces persistent barriers stemming from the fragmented
nature of data sources, heterogeneous formats, and the steep technical
expertise required to identify, acquire, and process datasets. These challenges
limit participation, slow discovery, and reduce the reproducibility of
scientific workflows. In this paper, we present a proof of concept for
addressing these barriers through the integration of a curated knowledge graph
(KG) with AI agents designed for cloud-native scientific workflows. The KG
provides a unifying layer that organizes datasets, tools, and workflows, while
AI agents -- powered by generative AI services -- enable natural language
interaction, automated data access, and streamlined analysis. Together, these
components drastically lower the technical threshold for engaging in climate
data science, enabling non-specialist users to identify and analyze relevant
datasets. By leveraging existing cloud-ready API data portals, we demonstrate
that "a knowledge graph is all you need" to unlock scalable and agentic
workflows for scientific inquiry. The open-source design of our system further
supports community contributions, ensuring that the KG and associated tools can
evolve as a shared commons. Our results illustrate a pathway toward
democratizing access to climate data and establishing a reproducible,
extensible framework for human--AI collaboration in scientific research.

</details>


### [50] [EEG-Based Consumer Behaviour Prediction: An Exploration from Classical Machine Learning to Graph Neural Networks](https://arxiv.org/abs/2509.21567)
*Mohammad Parsa Afshar,Aryan Azimi*

Main category: cs.AI

TL;DR: 该研究使用脑电图数据和机器学习模型预测消费者行为，比较了传统机器学习模型和图神经网络模型在NeuMa数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 预测消费者行为在营销、认知神经科学和人机交互中很重要，脑电图数据可以提供大脑神经活动的详细信息来分析决策过程。

Method: 从NeuMa数据集中提取和清洗EEG特征，为GNN模型创建大脑连接特征，使用包括传统模型和图神经网络在内的多种机器学习模型进行比较。

Result: 虽然整体结果没有显示显著差异，但GNN模型在某些基本标准上表现更好，而传统模型在这些标准上表现不理想。

Conclusion: EEG信号分析与机器学习模型结合可以更深入理解消费者行为，研究提供了传统模型（如SVM）与较少使用的GNN模型在基于EEG的神经营销中的全面比较。

Abstract: Prediction of consumer behavior is one of the important purposes in
marketing, cognitive neuroscience, and human-computer interaction. The
electroencephalography (EEG) data can help analyze the decision process by
providing detailed information about the brain's neural activity. In this
research, a comparative approach is utilized for predicting consumer behavior
by EEG data. In the first step, the features of the EEG data from the NeuMa
dataset were extracted and cleaned. For the Graph Neural Network (GNN) models,
the brain connectivity features were created. Different machine learning
models, such as classical models and Graph Neural Networks, are used and
compared. The GNN models with different architectures are implemented to have a
comprehensive comparison; furthermore, a wide range of classical models, such
as ensemble models, are applied, which can be very helpful to show the
difference and performance of each model on the dataset. Although the results
did not show a significant difference overall, the GNN models generally
performed better in some basic criteria where classical models were not
satisfactory. This study not only shows that combining EEG signal analysis and
machine learning models can provide an approach to deeper understanding of
consumer behavior, but also provides a comprehensive comparison between the
machine learning models that have been widely used in previous studies in the
EEG-based neuromarketing such as Support Vector Machine (SVM), and the models
which are not used or rarely used in the field, like Graph Neural Networks.

</details>


### [51] [GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models](https://arxiv.org/abs/2509.21593)
*Peng Luo,Xiayin Lou,Yu Zheng,Zhuo Zheng,Stefano Ermon*

Main category: cs.AI

TL;DR: GeoEvolve是一个多智能体LLM框架，通过结合进化搜索和地理空间领域知识来自动设计和优化地理空间算法，在空间插值和不确定性量化任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的算法发现框架缺乏地理空间领域知识和多步推理能力，难以解决复杂的地理空间问题。

Method: 采用双层循环结构：内环使用代码进化器生成和变异候选解，外环通过智能控制器评估全局精英并查询GeoKnowRAG模块（结构化地理空间知识库）来注入地理学理论先验。

Result: 在空间插值任务中减少RMSE误差13-21%，在不确定性估计中提升性能17%。消融研究证实领域知识引导的检索对稳定高质量进化至关重要。

Conclusion: GeoEvolve为自动化、知识驱动的地理空间建模提供了可扩展路径，为可信赖和高效的AI-for-Science发现开辟了新机会。

Abstract: Geospatial modeling provides critical solutions for pressing global
challenges such as sustainability and climate change. Existing large language
model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at
evolving generic code but lack the domain knowledge and multi-step reasoning
required for complex geospatial problems. We introduce GeoEvolve, a multi-agent
LLM framework that couples evolutionary search with geospatial domain knowledge
to automatically design and refine geospatial algorithms. GeoEvolve operates in
two nested loops: an inner loop leverages a code evolver to generate and mutate
candidate solutions, while an outer agentic controller evaluates global elites
and queries a GeoKnowRAG module -- a structured geospatial knowledge base that
injects theoretical priors from geography. This knowledge-guided evolution
steers the search toward theoretically meaningful and computationally efficient
algorithms. We evaluate GeoEvolve on two fundamental and classical tasks:
spatial interpolation (kriging) and spatial uncertainty quantification
(geospatial conformal prediction). Across these benchmarks, GeoEvolve
automatically improves and discovers new algorithms, incorporating geospatial
theory on top of classical models. It reduces spatial interpolation error
(RMSE) by 13-21% and enhances uncertainty estimation performance by 17\%.
Ablation studies confirm that domain-guided retrieval is essential for stable,
high-quality evolution. These results demonstrate that GeoEvolve provides a
scalable path toward automated, knowledge-driven geospatial modeling, opening
new opportunities for trustworthy and efficient AI-for-Science discovery.

</details>


### [52] [Automated and Interpretable Survival Analysis from Multimodal Data](https://arxiv.org/abs/2509.21600)
*Mafalda Malafaia,Peter A. N. Bosman,Coen Rasch,Tanja Alderliesten*

Main category: cs.AI

TL;DR: 提出可解释的多模态AI框架MultiFIX，整合临床变量和CT影像进行生存分析，通过深度学习提取特征并用Grad-CAM和遗传编程解释，使用透明Cox回归进行风险分层。


<details>
  <summary>Details</summary>
Motivation: 随着多模态数据增长和临床对透明模型的需求增加，准确且可解释的生存分析在肿瘤学中仍具挑战性。

Method: MultiFIX框架使用深度学习推断生存相关特征：影像特征通过Grad-CAM解释，临床变量通过遗传编程建模为符号表达式，风险估计采用透明Cox回归。

Result: 在头颈癌RADCURE数据集上，MultiFIX获得C-index 0.838（预测）和0.826（分层），优于临床和学术基线方法，并与已知预后标志物一致。

Conclusion: 结果凸显了可解释多模态AI在精准肿瘤学中的潜力，MultiFIX框架展现了良好前景。

Abstract: Accurate and interpretable survival analysis remains a core challenge in
oncology. With growing multimodal data and the clinical need for transparent
models to support validation and trust, this challenge increases in complexity.
We propose an interpretable multimodal AI framework to automate survival
analysis by integrating clinical variables and computed tomography imaging. Our
MultiFIX-based framework uses deep learning to infer survival-relevant features
that are further explained: imaging features are interpreted via Grad-CAM,
while clinical variables are modeled as symbolic expressions through genetic
programming. Risk estimation employs a transparent Cox regression, enabling
stratification into groups with distinct survival outcomes. Using the
open-source RADCURE dataset for head and neck cancer, MultiFIX achieves a
C-index of 0.838 (prediction) and 0.826 (stratification), outperforming the
clinical and academic baseline approaches and aligning with known prognostic
markers. These results highlight the promise of interpretable multimodal AI for
precision oncology with MultiFIX.

</details>


### [53] [Semantic F1 Scores: Fair Evaluation Under Fuzzy Class Boundaries](https://arxiv.org/abs/2509.21633)
*Georgios Chochlakis,Jackson Trager,Vedant Jhaveri,Nikhil Ravichandran,Alexandros Potamianos,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: 提出Semantic F1 Scores，一种用于主观或多标签分类的新评估指标，通过量化预测标签与真实标签之间的语义相关性来改进传统F1指标。


<details>
  <summary>Details</summary>
Motivation: 传统F1指标将语义相关的预测视为完全失败，无法反映现实中类别边界模糊和标注者分歧的情况。需要一种能够给予语义相关但不完全相同标签部分信用的评估方法。

Method: 使用标签相似度矩阵计算软精度和软召回分数，通过新颖的两步精度-召回公式，无需丢弃标签或强制匹配不相似标签即可比较任意大小的标签集。

Result: 通过理论论证和在合成及真实数据上的广泛实证验证，Semantic F1显示出更好的可解释性和生态效度。

Conclusion: Semantic F1提供了更公平的评估，识别类别重叠、标注者分歧以及基于相似预测会产生相似结果的事实，适用于各种任务和模态。

Abstract: We propose Semantic F1 Scores, novel evaluation metrics for subjective or
fuzzy multi-label classification that quantify semantic relatedness between
predicted and gold labels. Unlike the conventional F1 metrics that treat
semantically related predictions as complete failures, Semantic F1 incorporates
a label similarity matrix to compute soft precision-like and recall-like
scores, from which the Semantic F1 scores are derived. Unlike existing
similarity-based metrics, our novel two-step precision-recall formulation
enables the comparison of label sets of arbitrary sizes without discarding
labels or forcing matches between dissimilar labels. By granting partial credit
for semantically related but nonidentical labels, Semantic F1 better reflects
the realities of domains marked by human disagreement or fuzzy category
boundaries. In this way, it provides fairer evaluations: it recognizes that
categories overlap, that annotators disagree, and that downstream decisions
based on similar predictions lead to similar outcomes. Through theoretical
justification and extensive empirical validation on synthetic and real data, we
show that Semantic F1 demonstrates greater interpretability and ecological
validity. Because it requires only a domain-appropriate similarity matrix,
which is robust to misspecification, and not a rigid ontology, it is applicable
across tasks and modalities.

</details>


### [54] [Can AI Perceive Physical Danger and Intervene?](https://arxiv.org/abs/2509.21651)
*Abhishek Jindal,Dmitry Kalashnikov,Oscar Chang,Divya Garikapati,Anirudha Majumdar,Pierre Sermanet,Vikas Sindhwani*

Main category: cs.AI

TL;DR: 开发了一个可扩展的物理安全基准测试框架，用于评估具身AI系统的安全理解能力，基于真实世界伤害叙述生成多模态测试场景，并提出了提升模型安全推理能力的后训练方法。


<details>
  <summary>Details</summary>
Motivation: 当AI与物理世界交互时（如机器人或辅助代理），会面临直接的物理伤害风险。需要评估现有基础模型对物理安全常识的理解能力，以确保其在安全关键应用中的部署准备度。

Method: 1) 基于真实伤害叙述和操作安全约束，开发可扩展的物理安全基准测试方法；2) 使用先进生成模型将叙述和约束转化为逼真的图像和视频；3) 分析主要基础模型的风险感知、安全推理和干预触发能力；4) 开发后训练范式，通过系统指令教授模型具身特定的安全约束。

Result: 通过基准测试发现现有模型在安全理解方面存在差距，提出的后训练方法能显著提升模型的安全约束满足能力，生成可解释的安全推理轨迹，在约束满足评估中达到最先进性能。

Conclusion: 该研究为具身AI系统的物理安全评估提供了系统化框架，揭示了当前基础模型在安全理解方面的局限性，并提出有效方法提升模型的安全推理透明度和性能，对安全关键应用具有重要意义。

Abstract: When AI interacts with the physical world -- as a robot or an assistive agent
-- new safety challenges emerge beyond those of purely ``digital AI". In such
interactions, the potential for physical harm is direct and immediate. How well
do state-of-the-art foundation models understand common-sense facts about
physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of
coffee should not be handed to a child? In this paper, our contributions are
three-fold: first, we develop a highly scalable approach to continuous physical
safety benchmarking of Embodied AI systems, grounded in real-world injury
narratives and operational safety constraints. To probe multi-modal safety
understanding, we turn these narratives and constraints into photorealistic
images and videos capturing transitions from safe to unsafe states, using
advanced generative models. Secondly, we comprehensively analyze the ability of
major foundation models to perceive risks, reason about safety, and trigger
interventions; this yields multi-faceted insights into their deployment
readiness for safety-critical agentic applications. Finally, we develop a
post-training paradigm to teach models to explicitly reason about
embodiment-specific safety constraints provided through system instructions.
The resulting models generate thinking traces that make safety reasoning
interpretable and transparent, achieving state of the art performance in
constraint satisfaction evaluations. The benchmark will be released at
https://asimov-benchmark.github.io/v2

</details>


### [55] [Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization](https://arxiv.org/abs/2509.21718)
*Shehzeen Hussain,Paarth Neekhara,Xuesong Yang,Edresson Casanova,Subhankar Ghosh,Roy Fejgin,Ryan Langman,Mikyas Desta,Leili Tavabi,Jason Li*

Main category: cs.AI

TL;DR: 提出基于GRPO的框架，通过多语言预训练和有限配对数据微调，结合ASR、说话人验证和音频质量评估模型的多目标奖励，在低资源语言中实现高质量的文本转语音合成。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言中由于配对文本和语音数据稀缺导致的TTS系统开发困难，而ASR模型相对容易获得的问题。

Method: 1. 使用IPA标记训练多语言基础TTS模型；2. 在目标语言的有限配对数据上微调；3. 应用GRPO优化，仅使用非配对文本和说话人提示，由预训练ASR、说话人验证和音频质量估计模型提供多目标奖励。

Result: 在低资源语言中产生可理解且说话人一致的语音，显著优于仅微调方法；在高资源语言中也优于DPO等离线对齐方法，在可理解性、说话人相似性和音频质量方面表现更优。

Conclusion: GRPO框架能够有效利用有限的配对数据和丰富的非配对数据，在低资源和高资源语言中都能显著提升TTS性能。

Abstract: Developing high-quality text-to-speech (TTS) systems for low-resource
languages is challenging due to the scarcity of paired text and speech data. In
contrast, automatic speech recognition (ASR) models for such languages are
often more accessible, owing to large-scale multilingual pre-training efforts.
We propose a framework based on Group Relative Policy Optimization (GRPO) to
adapt an autoregressive, multilingual TTS model to new languages. Our method
first establishes a language-agnostic foundation for TTS synthesis by training
a multilingual baseline with International Phonetic Alphabet (IPA) tokens.
Next, we fine-tune this model on limited paired data of the new languages to
capture the target language's prosodic features. Finally, we apply GRPO to
optimize the model using only unpaired text and speaker prompts, guided by a
multi-objective reward from pretrained ASR, speaker verification, and audio
quality estimation models. Experiments demonstrate that this pipeline produces
intelligible and speaker-consistent speech in low-resource languages,
substantially outperforming fine-tuning alone. Furthermore, our GRPO-based
framework also improves TTS performance in high-resource languages, surpassing
offline alignment methods such as Direct Preference Optimization (DPO) yielding
superior intelligibility, speaker similarity, and audio quality.

</details>


### [56] [Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts](https://arxiv.org/abs/2509.21743)
*Ammar Ahmed,Azal Ahmad Khan,Ayaan Ahmad,Sheng Di,Zirui Liu,Ali Anwar*

Main category: cs.AI

TL;DR: RoT通过检索和重用先前的推理步骤作为可组合的"思维"步骤来指导新问题，显著减少推理延迟和成本，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过产生长推理轨迹来提高准确性，但这增加了延迟和成本，因此需要在推理时提高效率。

Method: RoT将推理步骤组织成具有顺序和语义边的思维图，通过检索查询相关节点并应用奖励引导的遍历来组装问题特定的模板，指导生成过程。

Result: RoT在多个模型上评估显示，输出token减少高达40%，推理延迟降低82%，成本降低59%，同时保持准确性。

Conclusion: RoT通过检索构建动态模板，为高效的大型推理模型推理建立了可扩展的范式。

Abstract: Large reasoning models improve accuracy by producing long reasoning traces,
but this inflates latency and cost, motivating inference-time efficiency. We
propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable
``thought" steps to guide new problems. RoT organizes steps into a thought
graph with sequential and semantic edges to enable fast retrieval and flexible
recombination. At inference, RoT retrieves query-relevant nodes and applies
reward-guided traversal to assemble a problem-specific template that guides
generation. This dynamic template reuse reduces redundant exploration and,
therefore, reduces output tokens while preserving accuracy. We evaluate RoT on
reasoning benchmarks with multiple models, measuring accuracy, token usage,
latency, and memory overhead. Findings show small prompt growth but substantial
efficiency gains, with RoT reducing output tokens by up to 40%, inference
latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a
scalable paradigm for efficient LRM reasoning via dynamic template construction
through retrieval.

</details>


### [57] [Lifelong Learning with Behavior Consolidation for Vehicle Routing](https://arxiv.org/abs/2509.21765)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.AI

TL;DR: 提出LLR-BC框架，通过行为整合解决神经VRP求解器在终身学习中的灾难性遗忘问题，在保持可塑性的同时提升零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器在新任务出现时，要么依赖零样本泛化（可能因任务差异而效果不佳），要么通过微调导致灾难性遗忘。需要一种终身学习范式来有效学习新任务同时保持先前任务的性能。

Method: 提出LLR-BC框架，通过决策导向的行为整合方法，将新任务训练的求解器行为与缓冲区中的行为对齐，并为低置信度决策分配更大的整合权重以关注关键经验。

Result: 在容量约束车辆路径问题和旅行商问题上的大量实验表明，LLR-BC在终身学习设置下能有效训练高性能神经求解器，解决灾难性遗忘问题，保持可塑性并提升零样本泛化能力。

Conclusion: LLR-BC为神经VRP求解器提供了一种有效的终身学习解决方案，通过行为整合机制成功平衡了新任务学习和旧任务保持的需求。

Abstract: Recent neural solvers have demonstrated promising performance in learning to
solve routing problems. However, existing studies are primarily based on
one-off training on one or a set of predefined problem distributions and
scales, i.e., tasks. When a new task arises, they typically rely on either
zero-shot generalization, which may be poor due to the discrepancies between
the new task and the training task(s), or fine-tuning the pretrained solver on
the new task, which possibly leads to catastrophic forgetting of knowledge
acquired from previous tasks. This paper explores a novel lifelong learning
paradigm for neural VRP solvers, where multiple tasks with diverse
distributions and scales arise sequentially over time. Solvers are required to
effectively and efficiently learn to solve new tasks while maintaining their
performance on previously learned tasks. Consequently, a novel framework called
Lifelong Learning Router with Behavior Consolidation (LLR-BC) is proposed.
LLR-BC consolidates prior knowledge effectively by aligning behaviors of the
solver trained on a new task with the buffered ones in a decision-seeking way.
To encourage more focus on crucial experiences, LLR-BC assigns greater
consolidated weights to decisions with lower confidence. Extensive experiments
on capacitated vehicle routing problems and traveling salesman problems
demonstrate LLR-BC's effectiveness in training high-performance neural solvers
in a lifelong learning setting, addressing the catastrophic forgetting issue,
maintaining their plasticity, and improving zero-shot generalization ability.

</details>


### [58] [UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios](https://arxiv.org/abs/2509.21766)
*Haotian Luo,Huaisong Zhang,Xuelin Zhang,Haoyu Wang,Zeyu Qin,Wenjie Lu,Guozheng Ma,Haiying He,Yingsha Xie,Qiyang Zhou,Zixuan Hu,Hongze Mi,Yibo Wang,Naiqiang Tan,Hong Chen,Yi R. Fung,Chun Yuan,Li Shen*

Main category: cs.AI

TL;DR: 提出了UltraHorizon基准，用于评估智能体在长视野、部分可观测场景下的持续推理、规划、记忆管理和工具使用能力，发现现有LLM智能体在这类任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注短视野、完全可观测任务，而现实世界中的关键任务（如软件开发、投资、科学发现）往往需要长视野和部分可观测环境下的持续能力，缺乏系统性评估基准。

Method: 通过探索任务在三个不同环境中验证核心能力，设计长视野发现任务，智能体需要通过持续推理、规划、记忆和工具管理来迭代发现隐藏规则。

Result: 在最重规模设置下，轨迹平均超过20万token和400+工具调用；标准配置下超过3.5万token和60+工具调用。LLM智能体表现不佳，而人类参与者得分更高，简单扩展方法在任务中失败。

Conclusion: 智能体在长视野能力上存在显著差距，识别出8种错误类型，归因于上下文锁定和功能基础能力不足两个主要原因。

Abstract: Autonomous agents have recently achieved remarkable progress across diverse
domains, yet most evaluations focus on short-horizon, fully observable tasks.
In contrast, many critical real-world tasks, such as large-scale software
development, commercial investment, and scientific discovery, unfold in
long-horizon and partially observable scenarios where success hinges on
sustained reasoning, planning, memory management, and tool use. Existing
benchmarks rarely capture these long-horizon challenges, leaving a gap in
systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a
novel benchmark that measures the foundational capabilities essential for
complex real-world challenges. We use exploration as a unifying task across
three distinct environments to validate these core competencies. Agents are
designed in long-horizon discovery tasks where they must iteratively uncover
hidden rules through sustained reasoning, planning, memory and tools
management, and interaction with environments. Under the heaviest scale
setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool
calls, whereas in standard configurations they still exceed \textbf{35k} tokens
and involve more than \textbf{60} tool calls on average. Our extensive
experiments reveal that LLM-agents consistently underperform in these settings,
whereas human participants achieve higher scores, underscoring a persistent gap
in agents' long-horizon abilities. We also observe that simple scaling fails in
our task. To better illustrate the failure of agents, we conduct an in-depth
analysis of collected trajectories. We identify eight types of errors and
attribute them to two primary causes: in-context locking and functional
fundamental capability gaps.
\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available
here.}

</details>


### [59] [Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety](https://arxiv.org/abs/2509.21782)
*Junliang Liu,Jingyu Xiao,Wenxin Tang,Wenxuan Wang,Zhixian Wang,Minrui Zhang,Shuanghe Yu*

Main category: cs.AI

TL;DR: 提出了WebRSSBench基准测试，用于全面评估多模态大语言模型在网页理解中的推理、鲁棒性和安全性能力，覆盖8个任务类型，包含3799个问答对。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注视觉感知或UI代码生成，缺乏对构建端到端网页应用所需的推理、鲁棒性和安全性能力的全面评估。

Method: 从729个网站构建包含3799个问答对的基准测试，涵盖位置关系推理、颜色鲁棒性、安全关键检测等8个任务，采用标准化提示、确定性评估脚本和多阶段质量控制。

Result: 评估12个MLLMs显示，模型在真实布局的组合和跨元素推理方面仍有困难，面对UI和内容扰动时鲁棒性有限，在识别和避免安全关键操作方面较为保守。

Conclusion: WebRSSBench揭示了当前MLLMs在网页理解方面的显著差距，强调了需要改进推理、鲁棒性和安全性能力以支持实际网页应用开发。

Abstract: Multimodal large language models (MLLMs) are increasingly positioned as AI
collaborators for building complex web-related applications like GUI agents and
front-end code generation. However, existing benchmarks largely emphasize
visual perception or UI code generation, showing insufficient evaluation on the
reasoning, robustness and safety capability required for end-to-end web
applications. To bridge the gap, we introduce a comprehensive web understanding
benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and
Safety across eight tasks, such as position relationship reasoning, color
robustness, and safety critical detection, etc. The benchmark is constructed
from 729 websites and contains 3799 question answer pairs that probe multi-step
inference over page structure, text, widgets, and safety-critical interactions.
To ensure reliable measurement, we adopt standardized prompts, deterministic
evaluation scripts, and multi-stage quality control combining automatic checks
with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The
results reveal significant gaps, models still struggle with compositional and
cross-element reasoning over realistic layouts, show limited robustness when
facing perturbations in user interfaces and content such as layout
rearrangements or visual style shifts, and are rather conservative in
recognizing and avoiding safety critical or irreversible actions. Our code is
available at https://github.com/jinliang-byte/webssrbench.

</details>


### [60] [D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents](https://arxiv.org/abs/2509.21799)
*Hongze Mi,Yibo Feng,Wenjie Lu,Yuqi Wang,Jinyuan Li,Song Cao,He Cui,Tengfei Tian,Xuelin Zhang,Haotian Luo,Di Sun,Naiqiang Tan,Gang Pan*

Main category: cs.AI

TL;DR: D-Artemis是一个基于人类认知循环（思考、对齐、反思）的GUI代理框架，通过细粒度的应用提示检索、执行前对齐检查（TAC和ACA）和执行后状态反思（SRA）来提升GUI任务的性能，无需复杂轨迹数据训练，在两个主要基准上达到SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 解决当前GUI代理面临的三个关键挑战：端到端训练的数据瓶颈、延迟错误检测的高成本以及矛盾指导的风险。

Method: 采用思考-对齐-反思的认知循环框架，包含应用特定提示检索、执行前对齐检查（TAC一致性检查和ACA动作修正）以及执行后状态反思（SRA）。

Result: 在AndroidWorld上达到75.8%的成功率，在ScreenSpot-V2上达到96.8%的成功率，在两个主要基准上都创造了新的SOTA结果。

Conclusion: D-Artemis通过认知循环框架显著提升了通用多模态大语言模型在GUI任务上的能力，无需复杂轨迹数据训练，展现出强大的泛化性能，每个组件都对整体性能有重要贡献。

Abstract: Graphical User Interface (GUI) agents aim to automate a wide spectrum of
human tasks by emulating user interaction. Despite rapid advancements, current
approaches are hindered by several critical challenges: data bottleneck in
end-to-end training, high cost of delayed error detection, and risk of
contradictory guidance. Inspired by the human cognitive loop of Thinking,
Alignment, and Reflection, we present D-Artemis -- a novel deliberative
framework in this paper. D-Artemis leverages a fine-grained, app-specific tip
retrieval mechanism to inform its decision-making process. It also employs a
proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)
Check module and Action Correction Agent (ACA) work in concert to mitigate the
risk of execution failures. A post-execution Status Reflection Agent (SRA)
completes the cognitive loop, enabling strategic learning from experience.
Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal
large language models (MLLMs) for GUI tasks without the need for training on
complex trajectory datasets, demonstrating strong generalization. D-Artemis
establishes new state-of-the-art (SOTA) results across both major benchmarks,
achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.
Extensive ablation studies further demonstrate the significant contribution of
each component to the framework.

</details>


### [61] [ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration](https://arxiv.org/abs/2509.21823)
*Gaole Dai,Shiqi Jiang,Ting Cao,Yuqing Yang,Yuanchun Li,Rui Tan,Mo Li,Lili Qiu*

Main category: cs.AI

TL;DR: ProRe是一个主动奖励系统，通过通用推理器和领域特定评估器主动与环境交互来改进GUI代理的奖励评估，显著提升了准确性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则或模型的奖励方法难以泛化到GUI代理，因为缺乏真实轨迹或应用数据库，而静态轨迹的LLM评估方法精度有限。

Method: 使用通用推理器调度定向状态探测任务，领域特定评估器主动与环境交互收集额外观察，从而分配更准确可验证的奖励。

Result: 在3000多个轨迹上的实验显示，ProRe将奖励准确率和F1分数分别提升5.3%和19.4%，与最先进策略代理集成后成功率提升22.4%。

Conclusion: ProRe通过主动环境交互显著改进了GUI代理的奖励评估，为LLM训练和评估提供了更可靠的奖励机制。

Abstract: Reward is critical to the evaluation and training of large language models
(LLMs). However, existing rule-based or model-based reward methods struggle to
generalize to GUI agents, where access to ground-truth trajectories or
application databases is often unavailable, and static trajectory-based
LLM-as-a-Judge approaches suffer from limited accuracy. To address these
challenges, we propose ProRe, a proactive reward system that leverages a
general-purpose reasoner and domain-specific evaluator agents (actors). The
reasoner schedules targeted state probing tasks, which the evaluator agents
then execute by actively interacting with the environment to collect additional
observations. This enables the reasoner to assign more accurate and verifiable
rewards to GUI agents. Empirical results on over 3K trajectories demonstrate
that ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%,
respectively. Furthermore, integrating ProRe with state-of-the-art policy
agents yields a success rate improvement of up to 22.4%.

</details>


### [62] [DS-STAR: Data Science Agent via Iterative Planning and Verification](https://arxiv.org/abs/2509.21825)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Tomas Pfister*

Main category: cs.AI

TL;DR: DS-STAR是一个新颖的数据科学智能体，通过自动数据文件分析、LLM验证步骤和顺序规划机制，解决了LLM在处理异构数据格式和生成优化分析计划方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 数据科学任务通常涉及探索多个数据源并综合发现以提供有洞察力的答案，但LLM在处理异构数据格式和验证计划充分性方面存在困难。

Method: DS-STAR包含三个关键组件：(1)自动探索和提取多种数据格式上下文的数据文件分析模块；(2)LLM验证步骤评估每个阶段分析计划的充分性；(3)从简单可执行计划开始，基于反馈迭代优化直到验证充分的顺序规划机制。

Result: 实验表明DS-STAR在三个具有挑战性的基准测试（DABStep、KramaBench和DA-Code）上实现了最先进的性能，特别是在需要处理多个异构格式数据文件的困难任务上显著优于基线方法。

Conclusion: DS-STAR通过其迭代优化机制能够可靠地导航涉及多样化数据源的复杂分析，为自动化数据科学任务提供了有效的解决方案。

Abstract: Data science, which transforms raw data into actionable insights, is critical
for data-driven decision-making. However, these tasks are often complex,
involving steps for exploring multiple data sources and synthesizing findings
to deliver insightful answers. While large language models (LLMs) show
significant promise in automating this process, they often struggle with
heterogeneous data formats and generate sub-optimal analysis plans, as
verifying plan sufficiency is inherently difficult without ground-truth labels
for such open-ended tasks. To overcome these limitations, we introduce DS-STAR,
a novel data science agent. Specifically, DS-STAR makes three key
contributions: (1) a data file analysis module that automatically explores and
extracts context from diverse data formats, including unstructured types; (2) a
verification step where an LLM-based judge evaluates the sufficiency of the
analysis plan at each stage; and (3) a sequential planning mechanism that
starts with a simple, executable plan and iteratively refines it based on the
DS-STAR's feedback until its sufficiency is verified. This iterative refinement
allows DS-STAR to reliably navigate complex analyses involving diverse data
sources. Our experiments show that DS-STAR achieves state-of-the-art
performance across three challenging benchmarks: DABStep, KramaBench, and
DA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks
that require processing multiple data files with heterogeneous formats.

</details>


### [63] [Axiomatic Choice and the Decision-Evaluation Paradox](https://arxiv.org/abs/2509.21836)
*Ben Abramowitz,Nicholas Mattei*

Main category: cs.AI

TL;DR: 提出了一个基于公理的决策建模框架，揭示了决策-评估悖论，指出在训练决策模型或应用公理时需要特别谨慎。


<details>
  <summary>Details</summary>
Motivation: 研究如何用公理（如伦理约束）来建模决策，并探讨决策公理的结构特性。

Method: 定义了一个基于结构特性的决策公理分类法，分析了决策与评估之间的张力。

Result: 发现了决策-评估悖论，该悖论在现实公理结构中普遍存在。

Conclusion: 在基于决策数据训练模型或应用公理进行决策和评估时，必须格外小心以避免悖论。

Abstract: We introduce a framework for modeling decisions with axioms that are
statements about decisions, e.g., ethical constraints. Using our framework we
define a taxonomy of decision axioms based on their structural properties and
demonstrate a tension between the use of axioms to make decisions and the use
of axioms to evaluate decisions which we call the Decision-Evaluation Paradox.
We argue that the Decision-Evaluation Paradox arises with realistic axiom
structures, and the paradox illuminates why one must be exceptionally careful
when training models on decision data or applying axioms to make and evaluate
decisions.

</details>


### [64] [DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents](https://arxiv.org/abs/2509.21842)
*Yansong Ning,Rui Liu,Jun Wang,Kai Chen,Wei Li,Jun Fang,Kan Zheng,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: DeepTravel是一个端到端的智能强化学习框架，用于构建自主旅行规划代理，能够自主规划、执行工具并反思工具响应，通过多步推理探索、验证和优化中间行动。


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划代理依赖手工制作的提示和固定的代理工作流程，限制了代理的灵活性和自主性。

Method: 构建了包含交通、住宿和POI数据的沙盒环境；开发了层次化奖励建模系统，包括轨迹级验证器和轮次级验证器；提出了回复增强的强化学习方法，从失败经验缓冲区定期重放。

Result: 在滴滴企业解决方案App上部署的DeepTravel使小型LLM（如Qwen3 32B）在旅行规划任务中显著优于OpenAI o1、o3和DeepSeek R1等前沿LLM。

Conclusion: DeepTravel框架成功构建了自主旅行规划代理，通过强化学习和层次化奖励系统实现了高效精确的旅行规划能力。

Abstract: Travel planning (TP) agent has recently worked as an emerging building block
to interact with external tools and resources for travel itinerary generation,
ensuring enjoyable user experience. Despite its benefits, existing studies rely
on hand craft prompt and fixed agent workflow, hindering more flexible and
autonomous TP agent. This paper proposes DeepTravel, an end to end agentic
reinforcement learning framework for building autonomous travel planning agent,
capable of autonomously planning, executing tools, and reflecting on tool
responses to explore, verify, and refine intermediate actions in multi step
reasoning. To achieve this, we first construct a robust sandbox environment by
caching transportation, accommodation and POI data, facilitating TP agent
training without being constrained by real world APIs limitations (e.g.,
inconsistent outputs). Moreover, we develop a hierarchical reward modeling
system, where a trajectory level verifier first checks spatiotemporal
feasibility and filters unsatisfied travel itinerary, and then the turn level
verifier further validate itinerary detail consistency with tool responses,
enabling efficient and precise reward service. Finally, we propose the reply
augmented reinforcement learning method that enables TP agent to periodically
replay from a failures experience buffer, emerging notable agentic capacity. We
deploy trained TP agent on DiDi Enterprise Solutions App and conduct
comprehensive online and offline evaluations, demonstrating that DeepTravel
enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing
frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.

</details>


### [65] [Reimagining Agent-based Modeling with Large Language Model Agents via Shachi](https://arxiv.org/abs/2509.21862)
*So Kuroki,Yingtao Tian,Kou Misaki,Takashi Ikegami,Takuya Akiba,Yujin Tang*

Main category: cs.AI

TL;DR: Shachi是一个用于LLM驱动多智能体系统的模块化框架，将智能体策略分解为配置、记忆和工具三个核心认知组件，通过LLM推理引擎协调，支持对集体行为的系统性分析。


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统的涌现行为研究缺乏受控实验的方法论，限制了该领域的科学进展。

Method: 提出Shachi框架，将智能体策略分解为Configuration（内在特质）、Memory（上下文持久性）和Tools（扩展能力）三个核心组件，由LLM推理引擎协调。

Result: 在10个任务的基准测试中验证了方法的有效性，通过模拟美国关税冲击实验证明，只有当智能体认知架构正确配置记忆和工具时，其行为才能与现实市场反应一致。

Conclusion: Shachi为构建和评估LLM智能体提供了严谨的开源基础，旨在促进更具累积性和科学基础的研究。

Abstract: The study of emergent behaviors in large language model (LLM)-driven
multi-agent systems is a critical research challenge, yet progress is limited
by a lack of principled methodologies for controlled experimentation. To
address this, we introduce Shachi, a formal methodology and modular framework
that decomposes an agent's policy into core cognitive components: Configuration
for intrinsic traits, Memory for contextual persistence, and Tools for expanded
capabilities, all orchestrated by an LLM reasoning engine. This principled
architecture moves beyond brittle, ad-hoc agent designs and enables the
systematic analysis of how specific architectural choices influence collective
behavior. We validate our methodology on a comprehensive 10-task benchmark and
demonstrate its power through novel scientific inquiries. Critically, we
establish the external validity of our approach by modeling a real-world U.S.
tariff shock, showing that agent behaviors align with observed market reactions
only when their cognitive architecture is appropriately configured with memory
and tools. Our work provides a rigorous, open-source foundation for building
and evaluating LLM agents, aimed at fostering more cumulative and
scientifically grounded research.

</details>


### [66] [TRACE: Learning to Compute on Graphs](https://arxiv.org/abs/2509.21886)
*Ziyang Zheng,Jiaying Zhu,Jingyi Zhou,Qiang Xu*

Main category: cs.AI

TL;DR: TRACE是一个新的图表示学习范式，通过层次化Transformer架构和函数偏移学习目标，解决了传统消息传递神经网络在计算图建模中的架构不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递神经网络及其Transformer变体存在架构不匹配问题，无法捕捉计算的位置感知和层次化特性，限制了模型对计算图功能行为的建模能力。

Method: 采用层次化Transformer架构模拟逐步计算流程，取代有缺陷的置换不变聚合；引入函数偏移学习目标，将复杂全局函数预测分解为仅预测函数偏移（真实全局函数与简单局部近似之间的差异）。

Result: 在电子电路这一复杂且经济关键的计算图类别上，TRACE在全面的基准测试中显著优于所有现有架构。

Conclusion: 架构对齐的主干网络和解耦学习目标构成了更强大的学习计算范式，为解决图上的计算学习这一基本挑战提供了更稳健的方法。

Abstract: Learning to compute, the ability to model the functional behavior of a
computational graph, is a fundamental challenge for graph representation
learning. Yet, the dominant paradigm is architecturally mismatched for this
task. This flawed assumption, central to mainstream message passing neural
networks (MPNNs) and their conventional Transformer-based counterparts,
prevents models from capturing the position-aware, hierarchical nature of
computation. To resolve this, we introduce \textbf{TRACE}, a new paradigm built
on an architecturally sound backbone and a principled learning objective.
First, TRACE employs a Hierarchical Transformer that mirrors the step-by-step
flow of computation, providing a faithful architectural backbone that replaces
the flawed permutation-invariant aggregation. Second, we introduce
\textbf{function shift learning}, a novel objective that decouples the learning
problem. Instead of predicting the complex global function directly, our model
is trained to predict only the \textit{function shift}, the discrepancy between
the true global function and a simple local approximation that assumes input
independence. We validate this paradigm on electronic circuits, one of the most
complex and economically critical classes of computational graphs. Across a
comprehensive suite of benchmarks, TRACE substantially outperforms all prior
architectures. These results demonstrate that our architecturally-aligned
backbone and decoupled learning objective form a more robust paradigm for the
fundamental challenge of learning to compute on graphs.

</details>


### [67] [GenesisGeo: Technical Report](https://arxiv.org/abs/2509.21896)
*Minfeng Zhu,Zi Wang,Sizhe Ji,Zhengtong Du,Junming Ke,Xiao Deng,Zanlang Yin,Xiuqi Huang,Heyu Wang,Wei Chen*

Main category: cs.AI

TL;DR: GenesisGeo是一个自动化的欧几里得几何定理证明器，通过120倍加速的符号推理引擎和神经符号方法，在IMO-AG-30基准测试中达到IMO金牌水平。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够自动解决复杂几何问题的定理证明器，特别是针对国际数学奥林匹克竞赛级别的几何问题。

Method: 结合符号推理引擎DDARN的120倍加速（通过定理匹配和C++核心组件实现）与基于Qwen3-0.6B-Base的神经符号证明器。

Result: 在IMO-AG-30基准测试中，单模型解决24/30问题（IMO银牌水平），双模型集成解决26/30问题（IMO金牌水平）。开源了包含2180万几何问题的大规模数据集。

Conclusion: GenesisGeo证明了神经符号方法在几何定理证明中的有效性，能够达到人类顶尖数学竞赛选手的水平。

Abstract: We present GenesisGeo, an automated theorem prover in Euclidean geometry. We
have open-sourced a large-scale geometry dataset of 21.8 million geometric
problems, over 3 million of which contain auxiliary constructions. Specially,
we significantly accelerate the symbolic deduction engine DDARN by 120x through
theorem matching, combined with a C++ implementation of its core components.
Furthermore, we build our neuro-symbolic prover, GenesisGeo, upon
Qwen3-0.6B-Base, which solves 24 of 30 problems (IMO silver medal level) in the
IMO-AG-30 benchmark using a single model, and achieves 26 problems (IMO gold
medal level) with a dual-model ensemble.

</details>


### [68] [DyRo-MCTS: A Robust Monte Carlo Tree Search Approach to Dynamic Job Shop Scheduling](https://arxiv.org/abs/2509.21902)
*Ruiqi Chen,Yi Mei,Fangfang Zhang,Mengjie Zhang*

Main category: cs.AI

TL;DR: 提出了DyRo-MCTS方法，将动作鲁棒性估计集成到MCTS中，用于动态作业车间调度问题，显著提升了离线学习策略的性能。


<details>
  <summary>Details</summary>
Motivation: 动态作业车间调度面临新作业到达带来的频繁干扰，现有离线学习策略不完美，而在线规划又因信息不完整容易受到扰动影响。

Method: DyRo-MCTS方法，在MCTS中集成动作鲁棒性估计，引导生产环境朝向既能产生良好调度结果又易于适应未来作业到达的状态。

Result: 实验表明DyRo-MCTS显著提升了离线学习策略的性能，在线规划时间增加可忽略，在各种调度场景下始终优于传统MCTS。

Conclusion: DyRo-MCTS通过做出鲁棒调度决策，在干扰下实现了长期可持续的性能提升。

Abstract: Dynamic job shop scheduling, a fundamental combinatorial optimisation problem
in various industrial sectors, poses substantial challenges for effective
scheduling due to frequent disruptions caused by the arrival of new jobs.
State-of-the-art methods employ machine learning to learn scheduling policies
offline, enabling rapid responses to dynamic events. However, these offline
policies are often imperfect, necessitating the use of planning techniques such
as Monte Carlo Tree Search (MCTS) to improve performance at online decision
time. The unpredictability of new job arrivals complicates online planning, as
decisions based on incomplete problem information are vulnerable to
disturbances. To address this issue, we propose the Dynamic Robust MCTS
(DyRo-MCTS) approach, which integrates action robustness estimation into MCTS.
DyRo-MCTS guides the production environment toward states that not only yield
good scheduling outcomes but are also easily adaptable to future job arrivals.
Extensive experiments show that DyRo-MCTS significantly improves the
performance of offline-learned policies with negligible additional online
planning time. Moreover, DyRo-MCTS consistently outperforms vanilla MCTS across
various scheduling scenarios. Further analysis reveals that its ability to make
robust scheduling decisions leads to long-term, sustainable performance gains
under disturbances.

</details>


### [69] [Outlier Detection in Plantar Pressure: Human-Centered Comparison of Statistical Parametric Mapping and Explainable Machine Learning](https://arxiv.org/abs/2509.21943)
*Carlo Dindorf,Jonas Dully,Steven Simon,Dennis Perchthaler,Stephan Becker,Hannah Ehmann,Kjell Heitmann,Bernd Stetter,Christian Diers,Michael Fröhlich*

Main category: cs.AI

TL;DR: 本研究比较了统计参数映射(SPM)和可解释机器学习方法在足底压力数据异常检测中的表现，发现ML模型在准确性上优于SPM，且两种方法的解释性都被专家认为是清晰有用的。


<details>
  <summary>Details</summary>
Motivation: 足底压力映射在临床诊断和运动科学中很重要，但大型异构数据集常包含技术错误或程序不一致导致的异常值。SPM提供可解释分析但对对齐敏感，其稳健异常检测能力尚不明确。

Method: 使用来自多个中心的数据，通过专家共识标注并添加合成异常，共798个有效样本和2000个异常值。评估了(i)非参数、配准依赖的SPM方法和(ii)使用SHAP解释的卷积神经网络(CNN)。通过嵌套交叉验证评估性能，通过语义差异调查评估解释质量。

Result: ML模型达到高准确率并优于SPM，SPM会误分类临床有意义的变异并遗漏真实异常。专家认为SPM和SHAP解释都清晰、有用且可信，但SPM被认为复杂度较低。

Conclusion: SPM和可解释ML在足底压力数据自动异常检测中具有互补潜力，可解释性在将复杂模型输出转化为可解释见解以有效支持决策方面至关重要。

Abstract: Plantar pressure mapping is essential in clinical diagnostics and sports
science, yet large heterogeneous datasets often contain outliers from technical
errors or procedural inconsistencies. Statistical Parametric Mapping (SPM)
provides interpretable analyses but is sensitive to alignment and its capacity
for robust outlier detection remains unclear. This study compares an SPM
approach with an explainable machine learning (ML) approach to establish
transparent quality-control pipelines for plantar pressure datasets. Data from
multiple centers were annotated by expert consensus and enriched with synthetic
anomalies resulting in 798 valid samples and 2000 outliers. We evaluated (i) a
non-parametric, registration-dependent SPM approach and (ii) a convolutional
neural network (CNN), explained using SHapley Additive exPlanations (SHAP).
Performance was assessed via nested cross-validation; explanation quality via a
semantic differential survey with domain experts. The ML model reached high
accuracy and outperformed SPM, which misclassified clinically meaningful
variations and missed true outliers. Experts perceived both SPM and SHAP
explanations as clear, useful, and trustworthy, though SPM was assessed less
complex. These findings highlight the complementary potential of SPM and
explainable ML as approaches for automated outlier detection in plantar
pressure data, and underscore the importance of explainability in translating
complex model outputs into interpretable insights that can effectively inform
decision-making.

</details>


### [70] [CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration](https://arxiv.org/abs/2509.21981)
*Zhimin Wang,Shaokang He,Duo Wu,Jinghe Wang,Linjia Kang,Jing Yu,Zhi Wang*

Main category: cs.AI

TL;DR: CoBel-World是一个为LLM智能体设计的协作信念世界框架，通过建模物理环境和合作者心理状态来减少沟通成本并提高任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM协作框架忽视了动态意图推理能力，导致计划不一致和冗余沟通，降低了协作效率。

Method: 使用符号信念语言将开放世界任务知识解析为结构化信念，通过LLM推理进行零样本贝叶斯式信念更新，主动检测潜在的不协调并自适应沟通。

Result: 在TDW-MAT和C-WAH基准测试中，相比最强基线，通信成本减少22-60%，任务完成效率提高4-28%。

Conclusion: 明确的、意图感知的信念建模对于基于LLM的多智能体系统中实现高效、类人协作至关重要。

Abstract: Effective real-world multi-agent collaboration requires not only accurate
planning but also the ability to reason about collaborators' intents -- a
crucial capability for avoiding miscoordination and redundant communication
under partial observable environments. Due to their strong planning and
reasoning capabilities, large language models (LLMs) have emerged as promising
autonomous agents for collaborative task solving. However, existing
collaboration frameworks for LLMs overlook their reasoning potential for
dynamic intent inference, and thus produce inconsistent plans and redundant
communication, reducing collaboration efficiency. To bridge this gap, we
propose CoBel-World, a novel framework that equips LLM agents with a
collaborative belief world -- an internal representation jointly modeling the
physical environment and collaborators' mental states. CoBel-World enables
agents to parse open-world task knowledge into structured beliefs via a
symbolic belief language, and perform zero-shot Bayesian-style belief updates
through LLM reasoning. This allows agents to proactively detect potential
miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated
on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World
significantly reduces communication costs by 22-60% and improves task
completion efficiency by 4-28% compared to the strongest baseline. Our results
show that explicit, intent-aware belief modeling is essential for efficient and
human-like collaboration in LLM-based multi-agent systems.

</details>


### [71] [RISK: A Framework for GUI Agents in E-commerce Risk Management](https://arxiv.org/abs/2509.21982)
*Renqi Chen,Zeyin Tao,Jianming Guo,Jingzhe Zhu,Yiheng Peng,Qingqing Sun,Tianyi Zhang,Shuai Chen*

Main category: cs.AI

TL;DR: RISK框架是一个专门为电商风险管理设计的GUI代理系统，包含数据集、基准测试和强化微调框架，能够处理多步骤交互任务，在单步和多步任务上分别提升6.8%和8.8%的性能。


<details>
  <summary>Details</summary>
Motivation: 传统爬虫方法和现有GUI代理无法处理电商风险管理所需的多样化、深度嵌入的网页数据，这些代理通常只能处理单步任务，缺乏管理动态交互内容的能力。

Method: RISK框架包含三个组件：RISK-Data数据集（8,492个单步和2,386个多步交互轨迹）、RISK-Bench基准测试（802个单步和320个多步轨迹）、RISK-R1强化微调框架（考虑输出格式、单步级别、多步级别和任务级别四个方面的奖励机制）。

Result: RISK-R1在离线评估中单步任务提升6.8%，多步任务提升8.8%，在线评估中达到70.5%的最高任务成功率。

Conclusion: RISK为自动化复杂网页交互提供了一个可扩展的领域特定解决方案，推动了电商风险管理的技术进步。

Abstract: E-commerce risk management requires aggregating diverse, deeply embedded web
data through multi-step, stateful interactions, which traditional scraping
methods and most existing Graphical User Interface (GUI) agents cannot handle.
These agents are typically limited to single-step tasks and lack the ability to
manage dynamic, interactive content critical for effective risk assessment. To
address this challenge, we introduce RISK, a novel framework designed to build
and deploy GUI agents for this domain. RISK integrates three components: (1)
RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction
trajectories, collected through a high-fidelity browser framework and a
meticulous data curation process; (2) RISK-Bench, a benchmark with 802
single-step and 320 multi-step trajectories across three difficulty levels for
standardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning
framework considering four aspects: (i) Output Format: Updated format reward to
enhance output syntactic correctness and task comprehension, (ii) Single-step
Level: Stepwise accuracy reward to provide granular feedback during early
training stages, (iii) Multi-step Level: Process reweight to emphasize critical
later steps in interaction sequences, and (iv) Task Level: Level reweight to
focus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms
existing baselines, achieving a 6.8% improvement in offline single-step and an
8.8% improvement in offline multi-step. Moreover, it attains a top task success
rate of 70.5% in online evaluation. RISK provides a scalable, domain-specific
solution for automating complex web interactions, advancing the state of the
art in e-commerce risk management.

</details>


### [72] [Bilinear relational structure fixes reversal curse and enables consistent model editing](https://arxiv.org/abs/2509.21993)
*Dong-Kyum Kim,Minsung Kim,Jea Kwon,Nakyeong Yang,Meeyoung Cha*

Main category: cs.AI

TL;DR: 该论文发现反转诅咒并非语言模型的固有缺陷，而是知识编码方式的产物。通过在关系知识图谱上训练模型，可以诱导出双线性关系结构，从而缓解反转诅咒并实现一致的模型编辑。


<details>
  <summary>Details</summary>
Motivation: 研究反转诅咒的根本原因，探索语言模型是否能通过改进知识表示来克服这一限制，并研究模型编辑的稳定性问题。

Method: 从头开始训练语言模型，使用合成的关系知识图谱数据集，分析隐藏表示中出现的双线性关系结构。

Result: 训练出的模型在隐藏表示中出现了双线性关系结构，这显著缓解了反转诅咒，使模型能够推断未见过的反向事实。该结构还确保了模型编辑的一致性，编辑会正确传播到相关事实。

Conclusion: 反转诅咒不是语言模型的固有缺陷，而是表示几何的函数。适当的训练数据可以诱导出双线性内部表示，使语言模型在编辑后保持逻辑一致性，模型编辑的成功关键取决于底层表示几何。

Abstract: The reversal curse -- a language model's (LM) inability to infer an unseen
fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a
fundamental limitation. We show that this is not an inherent failure but an
artifact of how models encode knowledge. By training LMs from scratch on a
synthetic dataset of relational knowledge graphs, we demonstrate that bilinear
relational structure emerges in their hidden representations. This structure
substantially alleviates the reversal curse, enabling LMs to infer unseen
reverse facts. Crucially, we also find that this bilinear structure plays a key
role in consistent model editing. When a fact is updated in a LM with this
structure, the edit correctly propagates to its reverse and other logically
dependent facts. In contrast, models lacking this representation not only
suffer from the reversal curse but also fail to generalize edits, further
introducing logical inconsistencies. Our results establish that training on a
relational knowledge dataset induces the emergence of bilinear internal
representations, which in turn enable LMs to behave in a logically consistent
manner after editing. This implies that the success of model editing depends
critically not just on editing algorithms but on the underlying
representational geometry of the knowledge being modified.

</details>


### [73] [GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments](https://arxiv.org/abs/2509.21998)
*Hanlin Zhu,Tianyu Guo,Song Mei,Stuart Russell,Nikhil Ghosh,Alberto Bietti,Jiantao Jiao*

Main category: cs.AI

TL;DR: 提出了GSM-Agent基准测试，用于评估LLM在需要主动使用工具收集信息来解决小学数学问题时的代理推理能力，发现即使是前沿模型也表现不佳，并提出了基于代理推理图的分析方法和工具增强的测试时扩展方法。


<details>
  <summary>Details</summary>
Motivation: 当前代理基准测试往往将代理推理与复杂的数学推理、专家级知识等高级能力混在一起，难以分离评估纯粹的代理推理能力。需要构建一个能够专门评估LLM结合工具使用和推理能力的基准。

Method: 构建GSM-Agent基准，要求LLM代理解决小学数学推理问题，但只提供问题而不提供前提信息，需要主动使用工具收集信息。提出代理推理图概念，将环境文档嵌入聚类为节点，将每个工具调用映射到最近节点来构建推理路径。

Result: 即使是GPT-5这样的前沿模型在GSM-Agent基准上也只能达到67%的准确率。分析发现许多模型在代理推理中缺乏重新访问先前访问节点的能力，而这种能力在静态推理中被认为是关键模式。

Conclusion: 提出的基准和代理推理框架有助于未来理解和推进代理推理边界的研究，通过工具增强的测试时扩展方法可以改善LLM的代理推理性能。

Abstract: As LLMs are increasingly deployed as agents, agentic reasoning - the ability
to combine tool use, especially search, and reasoning - becomes a critical
skill. However, it is hard to disentangle agentic reasoning when evaluated in
complex environments and tasks. Current agent benchmarks often mix agentic
reasoning with challenging math reasoning, expert-level knowledge, and other
advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent,
where an LLM agent is required to solve grade-school-level reasoning problems,
but is only presented with the question in the prompt without the premises that
contain the necessary information to solve the task, and needs to proactively
collect that information using tools. Although the original tasks are
grade-school math problems, we observe that even frontier models like GPT-5
only achieve 67% accuracy. To understand and analyze the agentic reasoning
patterns, we propose the concept of agentic reasoning graph: cluster the
environment's document embeddings into nodes, and map each tool call to its
nearest node to build a reasoning path. Surprisingly, we identify that the
ability to revisit a previously visited node, widely taken as a crucial pattern
in static reasoning, is often missing for agentic reasoning for many models.
Based on the insight, we propose a tool-augmented test-time scaling method to
improve LLM's agentic reasoning performance by adding tools to encourage models
to revisit. We expect our benchmark and the agentic reasoning framework to aid
future studies of understanding and pushing the boundaries of agentic
reasoning.

</details>


### [74] [The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging](https://arxiv.org/abs/2509.22034)
*Xiaochong Lan,Yu Zheng,Shiteng Cao,Yong Li*

Main category: cs.AI

TL;DR: 模型融合技术能够通过调整融合强度，在推理精度和计算成本之间实现可控权衡，甚至在某些情况下实现帕累托改进。


<details>
  <summary>Details</summary>
Motivation: 现实应用需要能够平衡推理深度和计算成本的大语言模型，模型融合作为一种无需训练的技术，其创建具有细粒度推理能力模型谱系的潜力尚未充分探索。

Method: 进行大规模实证研究，评估多种模型融合技术在不同推理基准上的表现，系统性地改变融合强度来构建精度-效率曲线。

Result: 研究发现模型融合能够有效校准推理精度和标记效率之间的权衡，即使父模型权重空间高度发散，且在某些情况下实现了帕累托改进。

Conclusion: 模型融合为创建具有特定推理配置的LLM提供了实用指南，能够满足多样化的应用需求。

Abstract: The growing demand for large language models (LLMs) with tunable reasoning
capabilities in many real-world applications highlights a critical need for
methods that can efficiently produce a spectrum of models balancing reasoning
depth and computational cost. Model merging has emerged as a promising,
training-free technique to address this challenge by arithmetically combining
the weights of a general-purpose model with a specialized reasoning model.
While various merging techniques exist, their potential to create a spectrum of
models with fine-grained control over reasoning abilities remains largely
unexplored. This work presents a large-scale empirical study evaluating a range
of model merging techniques across multiple reasoning benchmarks. We
systematically vary merging strengths to construct accuracy-efficiency curves,
providing the first comprehensive view of the tunable performance landscape.
Our findings reveal that model merging offers an effective and controllable
method for calibrating the trade-off between reasoning accuracy and token
efficiency, even when parent models have highly divergent weight spaces.
Crucially, we identify instances of Pareto Improvement, where a merged model
achieves both higher accuracy and lower token consumption than one of its
parents. Our study provides the first comprehensive analysis of this tunable
space, offering practical guidelines for creating LLMs with specific reasoning
profiles to meet diverse application demands.

</details>


### [75] [A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning](https://arxiv.org/abs/2509.22044)
*Ziqi Wang,Boye Niu,Zhongli Li,Linghui Meng,Jing Liu,Zhi Zheng,Tong Xu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.AI

TL;DR: A2R是一个非对称两阶段推理框架，通过探索器并行生成多个解决方案，再由合成器进行第二阶段精炼推理，显著提升模型在复杂任务上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在单次尝试中的表现与其潜在能力之间存在显著差距，需要通过多路径解决方案来揭示模型的真实潜力。

Method: 提出A2R框架：第一阶段使用探索器模型并行生成多个解决方案，第二阶段使用合成器模型整合这些参考进行精炼推理，实现计算资源的正交扩展。

Result: Qwen3-8B-distill模型使用A2R框架相比自一致性基线提升75%性能；A2R-Efficient变体（Qwen3-4B探索器+Qwen3-8B合成器）超越单体Qwen3-32B模型平均性能，成本降低近30%。

Conclusion: A2R不仅是一个性能提升框架，更是现实应用中高效实用的解决方案，通过非对称扩展范式有效平衡性能与成本。

Abstract: Recent Large Reasoning Models have achieved significant improvements in
complex task-solving capabilities by allocating more computation at the
inference stage with a "thinking longer" paradigm. Even as the foundational
reasoning capabilities of models advance rapidly, the persistent gap between a
model's performance in a single attempt and its latent potential, often
revealed only across multiple solution paths, starkly highlights the disparity
between its realized and inherent capabilities. To address this, we present
A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge
the gap between a model's potential and its actual performance. In this
framework, an "explorer" model first generates potential solutions in parallel
through repeated sampling. Subsequently,a "synthesizer" model integrates these
references for a more refined, second stage of reasoning. This two-stage
process allows computation to be scaled orthogonally to existing sequential
methods. Our work makes two key innovations: First, we present A2R as a
plug-and-play parallel reasoning framework that explicitly enhances a model's
capabilities on complex questions. For example, using our framework, the
Qwen3-8B-distill model achieves a 75% performance improvement compared to its
self-consistency baseline. Second, through a systematic analysis of the
explorer and synthesizer roles, we identify an effective asymmetric scaling
paradigm. This insight leads to A2R-Efficient, a "small-to-big" variant that
combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration
surpasses the average performance of a monolithic Qwen3-32B model at a nearly
30% lower cost. Collectively, these results show that A2R is not only a
performance-boosting framework but also an efficient and practical solution for
real-world applications.

</details>


### [76] [Generalizing Multi-Objective Search via Objective-Aggregation Functions](https://arxiv.org/abs/2509.22085)
*Hadar Peer,Eyal Weiss,Ron Alterovitz,Oren Salzman*

Main category: cs.AI

TL;DR: 提出了一种广义的多目标搜索问题表述，通过隐藏目标的聚合函数来优化解决方案目标，支持标准MOS算法的应用，只需扩展核心操作以反映特定的聚合函数。


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人系统需要同时平衡多个经常冲突的目标，而现有问题表述无法直接使用最先进的多目标搜索算法。

Method: 使用隐藏目标的聚合函数来优化解决方案目标，扩展标准MOS算法的核心操作以支持特定聚合函数。

Result: 在多个机器人规划问题中，扩展后的算法比原始版本性能高出数个数量级。

Conclusion: 提出的广义问题表述能够有效支持标准MOS算法的应用，通过适当扩展核心操作即可显著提升算法性能。

Abstract: Multi-objective search (MOS) has become essential in robotics, as real-world
robotic systems need to simultaneously balance multiple, often conflicting
objectives. Recent works explore complex interactions between objectives,
leading to problem formulations that do not allow the usage of out-of-the-box
state-of-the-art MOS algorithms. In this paper, we suggest a generalized
problem formulation that optimizes solution objectives via aggregation
functions of hidden (search) objectives. We show that our formulation supports
the application of standard MOS algorithms, necessitating only to properly
extend several core operations to reflect the specific aggregation functions
employed. We demonstrate our approach in several diverse robotics planning
problems, spanning motion-planning for navigation, manipulation and planning fr
medical systems under obstacle uncertainty as well as inspection planning, and
route planning with different road types. We solve the problems using
state-of-the-art MOS algorithms after properly extending their core operations,
and provide empirical evidence that they outperform by orders of magnitude the
vanilla versions of the algorithms applied to the same problems but without
objective aggregation.

</details>


### [77] [Ground-Truthing AI Energy Consumption: Validating CodeCarbon Against External Measurements](https://arxiv.org/abs/2509.22092)
*Raphael Fischer*

Main category: cs.AI

TL;DR: 本研究系统评估了AI能耗估算工具的准确性，通过与真实测量数据对比发现现有工具存在高达40%的误差，为可持续AI发展提供了实证依据和改进指南。


<details>
  <summary>Details</summary>
Motivation: 随着AI快速发展带来的环境影响日益显著，现有能耗估算工具虽然易于使用但存在简化假设和忽略重要因素的问题，需要验证其估算准确性。

Method: 通过提出的验证框架，将静态和动态能耗估算方法与数百个AI实验的真实测量数据进行比较，系统评估估算可靠性。

Result: 研究发现虽然估算工具总体上能反映AI能耗模式，但持续存在高达40%的误差，揭示了现有方法的局限性。

Conclusion: 本研究为AI能耗估算提供了透明度验证，制定了改进指南，并为资源感知的机器学习和AI可持续性研究做出了重要贡献。

Abstract: Although machine learning (ML) and artificial intelligence (AI) present
fascinating opportunities for innovation, their rapid development is also
significantly impacting our environment. In response to growing
resource-awareness in the field, quantification tools such as the ML Emissions
Calculator and CodeCarbon were developed to estimate the energy consumption and
carbon emissions of running AI models. They are easy to incorporate into AI
projects, however also make pragmatic assumptions and neglect important
factors, raising the question of estimation accuracy. This study systematically
evaluates the reliability of static and dynamic energy estimation approaches
through comparisons with ground-truth measurements across hundreds of AI
experiments. Based on the proposed validation framework, investigative insights
into AI energy demand and estimation inaccuracies are provided. While generally
following the patterns of AI energy consumption, the established estimation
approaches are shown to consistently make errors of up to 40%. By providing
empirical evidence on energy estimation quality and errors, this study
establishes transparency and validates widely used tools for sustainable AI
development. It moreover formulates guidelines for improving the
state-of-the-art and offers code for extending the validation to other domains
and tools, thus making important contributions to resource-aware ML and AI
sustainability research.

</details>


### [78] [Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach](https://arxiv.org/abs/2509.22137)
*Seoyoung Lee,Seonbin Yoon,Seongbeen Lee,Hyesoo Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: Log2Plan是一个结合结构化两级规划框架和用户行为日志任务挖掘的GUI任务自动化系统，解决了现有LLM/VLM代理在泛化性、延迟和长序列一致性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM或VLM的规划-执行代理存在脆弱的泛化能力、高延迟和有限的长序列一致性，依赖单次推理或静态计划使其在UI变化或复杂任务下表现脆弱。

Method: 采用结构化两级规划框架：高层规划将用户命令映射到结构化任务字典，确保一致性和可泛化性；通过用户行为日志挖掘识别用户特定模式；低层规划将高层计划基于实时GUI上下文转化为具体动作序列。

Result: 在200个真实世界任务上的评估显示，任务成功率和执行时间显著改善，在长序列任务中保持超过60.0%的成功率，证明了其在复杂多步工作流中的鲁棒性。

Conclusion: Log2Plan通过结合结构化规划和任务挖掘方法，实现了鲁棒且可适应的GUI自动化，在复杂任务场景下表现出色。

Abstract: GUI task automation streamlines repetitive tasks, but existing LLM or
VLM-based planner-executor agents suffer from brittle generalization, high
latency, and limited long-horizon coherence. Their reliance on single-shot
reasoning or static plans makes them fragile under UI changes or complex tasks.
Log2Plan addresses these limitations by combining a structured two-level
planning framework with a task mining approach over user behavior logs,
enabling robust and adaptable GUI automation. Log2Plan constructs high-level
plans by mapping user commands to a structured task dictionary, enabling
consistent and generalizable automation. To support personalization and reuse,
it employs a task mining approach from user behavior logs that identifies
user-specific patterns. These high-level plans are then grounded into low-level
action sequences by interpreting real-time GUI context, ensuring robust
execution across varying interfaces. We evaluated Log2Plan on 200 real-world
tasks, demonstrating significant improvements in task success rate and
execution time. Notably, it maintains over 60.0% success rate even on
long-horizon task sequences, highlighting its robustness in complex, multi-step
workflows.

</details>


### [79] [Clinical Uncertainty Impacts Machine Learning Evaluations](https://arxiv.org/abs/2509.22242)
*Simone Lionetti,Fabian Gröger,Philippe Gottfrois,Alvaro Gonzalez-Jimenez,Ludovic Amruthalingam,Alexander A. Navarini,Marc Pouly*

Main category: cs.AI

TL;DR: 论文主张在机器学习评估中考虑标注不确定性，使用概率指标直接处理分布，而非传统的多数投票等聚合方法，以更好地反映临床数据的特性。


<details>
  <summary>Details</summary>
Motivation: 临床数据标注存在不确定性和标注者分歧，传统聚合方法（如多数投票）掩盖了这种变异性，影响模型评估的准确性。

Method: 提出使用概率指标直接操作标注分布，这些指标独立于标注生成过程，计算轻量，具有线性时间实现的闭式表达式。

Result: 在医学影像基准测试的简单实验中，考虑二元标签的置信度显著影响模型排名。

Conclusion: 呼吁社区发布原始标注数据并采用不确定性感知评估，使性能估计能更好地反映临床数据特性。

Abstract: Clinical dataset labels are rarely certain as annotators disagree and
confidence is not uniform across cases. Typical aggregation procedures, such as
majority voting, obscure this variability. In simple experiments on medical
imaging benchmarks, accounting for the confidence in binary labels
significantly impacts model rankings. We therefore argue that machine-learning
evaluations should explicitly account for annotation uncertainty using
probabilistic metrics that directly operate on distributions. These metrics can
be applied independently of the annotations' generating process, whether
modeled by simple counting, subjective confidence ratings, or probabilistic
response models. They are also computationally lightweight, as closed-form
expressions have linear-time implementations once examples are sorted by model
score. We thus urge the community to release raw annotations for datasets and
to adopt uncertainty-aware evaluation so that performance estimates may better
reflect clinical data.

</details>


### [80] [Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase Heuristics for 2D Bin-Packing](https://arxiv.org/abs/2509.22255)
*Syed Mahbubul Huq,Daniel Brito,Daniel Sikar,Rajesh Mojumder*

Main category: cs.AI

TL;DR: 提出评估大语言模型在组合优化中能力的框架，针对二维装箱问题，结合进化算法迭代生成和优化启发式解，证明LLM能产生更高效解且计算资源需求更少。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在专业领域组合优化任务中的能力，建立评估基准，探索LLM在解决复杂优化问题方面的潜力。

Method: 系统化方法将LLM与进化算法结合，迭代生成和优化启发式解，并与传统方法（有限首次适应和混合首次适应）进行比较实验。

Result: GPT-4o在两次迭代内达到最优解，平均使用箱数从16减少到15，空间利用率从0.76-0.78提升到0.83，计算资源需求更少。

Conclusion: LLM在组合优化领域表现优异，能够产生高效解决方案，为理解LLM在专业领域评估和建立性能基准做出贡献。

Abstract: This paper presents an evaluation framework for assessing Large Language
Models' (LLMs) capabilities in combinatorial optimization, specifically
addressing the 2D bin-packing problem. We introduce a systematic methodology
that combines LLMs with evolutionary algorithms to generate and refine
heuristic solutions iteratively. Through comprehensive experiments comparing
LLM generated heuristics against traditional approaches (Finite First-Fit and
Hybrid First-Fit), we demonstrate that LLMs can produce more efficient
solutions while requiring fewer computational resources. Our evaluation reveals
that GPT-4o achieves optimal solutions within two iterations, reducing average
bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78
to 0.83. This work contributes to understanding LLM evaluation in specialized
domains and establishes benchmarks for assessing LLM performance in
combinatorial optimization tasks.

</details>


### [81] [InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.22261)
*Guanghao Zhu,Zhitian Hou,Zeyu Liu,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.AI

TL;DR: 提出了两个医疗专用多模态大语言模型InfiMed-Foundation-1.7B和4B，通过高质量数据筛选、高效训练方法和三阶段微调，在医疗任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 通用多模态大模型缺乏医疗专业知识，容易产生不确定或幻觉回答，知识蒸馏难以捕捉放射学和药理学等专业领域知识，且大规模医疗数据预训练计算成本高。

Method: 结合通用和医疗多模态数据，提出五维质量评估框架筛选高质量数据集，采用低到高图像分辨率和多模态序列打包提高训练效率，使用三阶段监督微调进行复杂医疗任务知识提取。

Result: 在MedEvalKit评估中，InfiMed-Foundation-1.7B优于Qwen2.5VL-3B，InfiMed-Foundation-4B超越HuatuoGPT-V-7B和MedGemma-27B-IT，在医疗视觉问答和诊断任务中表现优异。

Conclusion: 通过解决数据质量、训练效率和领域知识提取等关键挑战，为医疗领域提供更可靠有效的AI驱动解决方案。

Abstract: Multimodal large language models (MLLMs) have shown remarkable potential in
various domains, yet their application in the medical field is hindered by
several challenges. General-purpose MLLMs often lack the specialized knowledge
required for medical tasks, leading to uncertain or hallucinatory responses.
Knowledge distillation from advanced models struggles to capture
domain-specific expertise in radiology and pharmacology. Additionally, the
computational cost of continual pretraining with large-scale medical data poses
significant efficiency challenges. To address these issues, we propose
InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, two medical-specific MLLMs
designed to deliver state-of-the-art performance in medical applications. We
combined high-quality general-purpose and medical multimodal data and proposed
a novel five-dimensional quality assessment framework to curate high-quality
multimodal medical datasets. We employ low-to-high image resolution and
multimodal sequence packing to enhance training efficiency, enabling the
integration of extensive medical data. Furthermore, a three-stage supervised
fine-tuning process ensures effective knowledge extraction for complex medical
tasks. Evaluated on the MedEvalKit framework, InfiMed-Foundation-1.7B
outperforms Qwen2.5VL-3B, while InfiMed-Foundation-4B surpasses HuatuoGPT-V-7B
and MedGemma-27B-IT, demonstrating superior performance in medical visual
question answering and diagnostic tasks. By addressing key challenges in data
quality, training efficiency, and domain-specific knowledge extraction, our
work paves the way for more reliable and effective AI-driven solutions in
healthcare. InfiMed-Foundation-4B model is available at
\href{https://huggingface.co/InfiX-ai/InfiMed-Foundation-4B}{InfiMed-Foundation-4B}.

</details>


### [82] [Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models](https://arxiv.org/abs/2509.22284)
*Aleksandar Terzić,Nicolas Menet,Michael Hersche,Thomas Hofmann,Abbas Rahimi*

Main category: cs.AI

TL;DR: 提出PD-SSM方法，通过结构化稀疏参数化状态空间模型的转移矩阵，在保持计算效率的同时显著提升模型表达能力，能够最优地模拟有限状态自动机。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型使用对角化转移矩阵虽然计算高效，但表达能力受限；而无结构转移矩阵虽然表达能力最优，但计算和内存成本过高。需要一种既能保持计算效率又能实现最优表达能力的方法。

Method: 将转移矩阵参数化为列one-hot矩阵(P)和复值对角矩阵(D)的乘积，使并行扫描的计算成本随状态大小线性增长，同时保持BIBO稳定性。

Result: 在FSA状态跟踪任务中显著优于多种现代SSM变体，在多类时间序列分类中性能与专门构建的神经控制微分方程相当，并能有效跟踪复杂FSA状态。

Conclusion: PD-SSM在计算效率和表达能力之间实现了最佳平衡，能够以最优状态大小和深度模拟任何有限状态自动机，显著改进了现有结构化SSM的理论保证。

Abstract: Modern state-space models (SSMs) often utilize transition matrices which
enable efficient computation but pose restrictions on the model's expressivity,
as measured in terms of the ability to emulate finite-state automata (FSA).
While unstructured transition matrices are optimal in terms of expressivity,
they come at a prohibitively high compute and memory cost even for moderate
state sizes. We propose a structured sparse parametrization of transition
matrices in SSMs that enables FSA state tracking with optimal state size and
depth, while keeping the computational cost of the recurrence comparable to
that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix
as the product of a column one-hot matrix ($P$) and a complex-valued diagonal
matrix ($D$). Consequently, the computational cost of parallel scans scales
linearly with the state size. Theoretically, the model is BIBO-stable and can
emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout
of size $N \times N$, significantly improving on all current structured SSM
guarantees. Experimentally, the model significantly outperforms a wide
collection of modern SSM variants on various FSA state tracking tasks. On
multiclass time-series classification, the performance is comparable to that of
neural controlled differential equations, a paradigm explicitly built for
time-series analysis. Finally, we integrate PD-SSM into a hybrid
Transformer-SSM architecture and demonstrate that the model can effectively
track the states of a complex FSA in which transitions are encoded as a set of
variable-length English sentences. The code is available at
https://github.com/IBM/expressive-sparse-state-space-model

</details>


### [83] [Large Language Models as Nondeterministic Causal Models](https://arxiv.org/abs/2509.22297)
*Sander Beckers*

Main category: cs.AI

TL;DR: 提出了一种更简单的反事实生成方法，将LLM表示为非确定性因果模型，相比现有方法更直接适用于黑盒LLM且不依赖实现细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法对LLM的解释存在歧义，既不是字面解释也不是意图解释，需要改进反事实生成方法以更好地解释、评估和比较LLM行为。

Method: 基于LLM的意图解释，将其表示为非确定性因果模型，开发了可直接应用于任何黑盒LLM的简化反事实生成方法。

Result: 新方法比现有方法更简单，对实现细节不可知，可直接应用于任何黑盒LLM而无需修改。

Conclusion: 为LLM反事实推理提供了理论基础，阐明了两种方法的关系，为开发面向特定应用的反事实生成方法奠定了基础。

Abstract: Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first
time, a method for generating counterfactuals of probabilistic Large Language
Models. Such counterfactuals tell us what would - or might - have been the
output of an LLM if some factual prompt ${\bf x}$ had been ${\bf x}^*$ instead.
The ability to generate such counterfactuals is an important necessary step
towards explaining, evaluating, and comparing, the behavior of LLMs. I argue,
however, that the existing method rests on an ambiguous interpretation of LLMs:
it does not interpret LLMs literally, for the method involves the assumption
that one can change the implementation of an LLM's sampling process without
changing the LLM itself, nor does it interpret LLMs as intended, for the method
involves explicitly representing a nondeterministic LLM as a deterministic
causal model. I here present a much simpler method for generating
counterfactuals that is based on an LLM's intended interpretation by
representing it as a nondeterministic causal model instead. The advantage of my
simpler method is that it is directly applicable to any black-box LLM without
modification, as it is agnostic to any implementation details. The advantage of
the existing method, on the other hand, is that it directly implements the
generation of a specific type of counterfactuals that is useful for certain
purposes, but not for others. I clarify how both methods relate by offering a
theoretical foundation for reasoning about counterfactuals in LLMs based on
their intended semantics, thereby laying the groundwork for novel
application-specific methods for generating counterfactuals.

</details>


### [84] [PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning](https://arxiv.org/abs/2509.22315)
*Hieu Tran,Zonghai Yao,Nguyen Luong Tran,Zhichao Yang,Feiyun Ouyang,Shuo Han,Razieh Rahimi,Hong Yu*

Main category: cs.AI

TL;DR: PRIME是一个多智能体推理框架，通过整合系统1（快速直觉思维）和系统2（缓慢审慎思维）来增强LLM的推理能力，使开源模型在复杂推理任务上达到与闭源模型竞争的水平。


<details>
  <summary>Details</summary>
Motivation: 受人类认知的双过程理论启发，旨在模拟人类快速直觉思维和缓慢审慎思维的动态整合，以提升LLM在复杂知识密集型推理任务中的表现。

Method: 采用多智能体设计：先由快速思考智能体（系统1）生成快速答案，若检测到不确定性，则触发包含规划、假设生成、检索、信息整合和决策等专门智能体的系统2推理流程。

Result: 实验结果表明，PRIME使LLaMA 3模型在多跳和知识基础推理基准测试中，能够与GPT-4和GPT-4o等最先进闭源模型竞争。

Conclusion: PRIME为提升LLM在复杂知识密集型推理领域的性能提供了一个可扩展的解决方案，有效模拟了人类认知过程并提高了效率和准确性。

Abstract: Inspired by the dual-process theory of human cognition from \textit{Thinking,
Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated
Memory for Enhanced Reasoning), a multi-agent reasoning framework that
dynamically integrates \textbf{System 1} (fast, intuitive thinking) and
\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick
Thinking Agent (System 1) to generate a rapid answer; if uncertainty is
detected, it then triggers a structured System 2 reasoning pipeline composed of
specialized agents for \textit{planning}, \textit{hypothesis generation},
\textit{retrieval}, \textit{information integration}, and
\textit{decision-making}. This multi-agent design faithfully mimics human
cognitive processes and enhances both efficiency and accuracy. Experimental
results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to
perform competitively with state-of-the-art closed-source models like GPT-4 and
GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This
research establishes PRIME as a scalable solution for improving LLMs in domains
requiring complex, knowledge-intensive reasoning.

</details>


### [85] [Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents](https://arxiv.org/abs/2509.22391)
*Jiaqi Shao,Yuxiang Lin,Munish Prasad Lohani,Yufeng Miao,Bing Luo*

Main category: cs.AI

TL;DR: SeekBench：首个通过逐步分析LLM搜索代理响应轨迹来评估其认知能力的基准，包含190个专家标注的轨迹和1800多个响应步骤，重点关注代理是否基于观察证据生成推理步骤、自适应调整搜索策略以及正确评估证据充分性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM搜索代理在开放域问答中的最终答案准确性，但忽视了代理如何利用外部证据进行推理和行动的过程，需要更细粒度的评估方法。

Method: 构建SeekBench基准，包含190个专家标注的响应轨迹，每个轨迹包含多个响应步骤，并添加证据标注，用于分析代理的三个关键认知能力维度。

Result: 开发了包含190个标注轨迹和1800多个响应步骤的基准数据集，能够对LLM搜索代理的认知能力进行细粒度评估。

Conclusion: SeekBench为评估LLM搜索代理的认知能力提供了首个系统化基准，有助于更全面地理解代理在搜索过程中的推理和行为模式。

Abstract: Recent work has explored training Large Language Model (LLM) search agents
with reinforcement learning (RL) for open-domain question answering (QA).
However, most evaluations focus solely on final answer accuracy, overlooking
how these agents reason with and act on external evidence. We introduce
SeekBench, the first benchmark for evaluating the \textit{epistemic competence}
of LLM search agents through step-level analysis of their response traces.
SeekBench comprises 190 expert-annotated traces with over 1,800 response steps
generated by LLM search agents, each enriched with evidence annotations for
granular analysis of whether agents (1) generate reasoning steps grounded in
observed evidence, (2) adaptively reformulate searches to recover from
low-quality results, and (3) have proper calibration to correctly assess
whether the current evidence is sufficient for providing an answer.

</details>


### [86] [EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer](https://arxiv.org/abs/2509.22407)
*Zhehao Dong,Xiaofeng Wang,Zheng Zhu,Yirui Wang,Yang Wang,Yukun Zhou,Boyuan Wang,Chaojun Ni,Runqi Ouyang,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang*

Main category: cs.AI

TL;DR: 提出了EMMA框架，通过DreamTransfer生成多视角一致的机器人操作视频和AdaMix训练策略，显著提升VLA模型在未见物体类别和新视觉域上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 收集大规模真实机器人操作数据成本高昂且耗时，限制了VLA模型的泛化能力。

Method: 使用DreamTransfer进行文本控制的视觉编辑，生成多视角一致的机器人操作视频；采用AdaMix训练策略动态重加权训练批次，专注于感知或运动学上的困难样本。

Result: 生成的视频在多视角一致性、几何保真度和文本条件准确性上显著优于现有方法；在零样本视觉域的真实机器人操作任务中，相比仅使用真实数据训练，性能提升超过200%，结合AdaMix后进一步提升13%。

Conclusion: EMMA框架通过生成数据和混合训练策略有效解决了机器人操作数据稀缺问题，显著提升了VLA模型的泛化性能。

Abstract: Vision-language-action (VLA) models increasingly rely on diverse training
data to achieve robust generalization. However, collecting large-scale
real-world robot manipulation data across varied object appearances and
environmental conditions remains prohibitively time-consuming and expensive. To
overcome this bottleneck, we propose Embodied Manipulation Media Adaptation
(EMMA), a VLA policy enhancement framework that integrates a generative data
engine with an effective training pipeline. We introduce DreamTransfer, a
diffusion Transformer-based framework for generating multi-view consistent,
geometrically grounded embodied manipulation videos. DreamTransfer enables
text-controlled visual editing of robot videos, transforming foreground,
background, and lighting conditions without compromising 3D structure or
geometrical plausibility. Furthermore, we explore hybrid training with real and
generated data, and introduce AdaMix, a hard-sample-aware training strategy
that dynamically reweights training batches to focus optimization on
perceptually or kinematically challenging samples. Extensive experiments show
that videos generated by DreamTransfer significantly outperform prior video
generation methods in multi-view consistency, geometric fidelity, and
text-conditioning accuracy. Crucially, VLAs trained with generated data enable
robots to generalize to unseen object categories and novel visual domains using
only demonstrations from a single appearance. In real-world robotic
manipulation tasks with zero-shot visual domains, our approach achieves over a
200% relative performance gain compared to training on real data alone, and
further improves by 13% with AdaMix, demonstrating its effectiveness in
boosting policy generalization.

</details>


### [87] [Guiding Evolution of Artificial Life Using Vision-Language Models](https://arxiv.org/abs/2509.22447)
*Nikhil Baid,Hannah Erlebach,Paul Hellegouarch,Frederico Wieser*

Main category: cs.AI

TL;DR: ASAL++是基于ASAL的改进方法，使用多模态基础模型进行开放式搜索，通过第二个基础模型根据模拟视觉历史提出新的进化目标，形成复杂性递增的进化轨迹。


<details>
  <summary>Details</summary>
Motivation: 基础模型为人工生命领域提供了自动化搜索的强大工具，之前的工作使用视觉语言模型将人工生命模拟与自然语言目标提示对齐。本文旨在开发具有开放式特征的FM驱动人工生命发现新方向。

Method: 引入ASAL++方法，使用第二个基础模型基于模拟视觉历史提出新的进化目标。探索两种策略：EST（每次迭代匹配单个新提示）和ETT（匹配整个生成提示序列）。在Lenia底物中使用Gemma-3进行实验。

Result: 实验表明，EST策略促进更大的视觉新颖性，而ETT策略培养更连贯和可解释的进化序列。

Conclusion: ASAL++为具有开放式特征的FM驱动人工生命发现指明了新方向。

Abstract: Foundation models (FMs) have recently opened up new frontiers in the field of
artificial life (ALife) by providing powerful tools to automate search through
ALife simulations. Previous work aligns ALife simulations with natural language
target prompts using vision-language models (VLMs). We build on Automated
Search for Artificial Life (ASAL) by introducing ASAL++, a method for
open-ended-like search guided by multimodal FMs. We use a second FM to propose
new evolutionary targets based on a simulation's visual history. This induces
an evolutionary trajectory with increasingly complex targets.
  We explore two strategies: (1) evolving a simulation to match a single new
prompt at each iteration (Evolved Supervised Targets: EST) and (2) evolving a
simulation to match the entire sequence of generated prompts (Evolved Temporal
Targets: ETT). We test our method empirically in the Lenia substrate using
Gemma-3 to propose evolutionary targets, and show that EST promotes greater
visual novelty, while ETT fosters more coherent and interpretable evolutionary
sequences.
  Our results suggest that ASAL++ points towards new directions for FM-driven
ALife discovery with open-ended characteristics.

</details>


### [88] [GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation](https://arxiv.org/abs/2509.22460)
*Shichao Weng,Zhiqiang Wang,Yuhua Zhou,Rui Lu,Ting Liu,Zhiyang Teng,Xiaozhang Liu,Hanmeng Liu*

Main category: cs.AI

TL;DR: GeoSketch是一个神经符号框架，将几何推理转化为交互式感知-推理-动作循环，通过结构化抽象、符号推理和草图操作来解决需要辅助线构造和仿射变换的几何问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型将图表视为静态图像处理，缺乏动态操作能力，而人类的几何推理涉及辅助线构造和仿射变换等动态操作。

Method: GeoSketch包含三个模块：感知模块将图表抽象为结构化逻辑形式，符号推理模块应用几何定理决定下一步推理步骤，草图动作模块执行绘制辅助线或应用变换等操作。采用两阶段训练：监督微调和强化学习。

Result: 在GeoSketch基准测试上，GeoSketch显著提高了逐步推理准确性和问题解决成功率，优于静态感知方法。

Conclusion: GeoSketch通过统一分层决策、可执行视觉动作和符号验证，将多模态推理从静态解释推进到动态可验证交互，为复杂视觉空间问题解决建立了新基础。

Abstract: Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large
Language Models (MLLMs), requiring not only the joint interpretation of text
and diagrams but also iterative visuospatial reasoning. While existing
approaches process diagrams as static images, they lack the capacity for
dynamic manipulation - a core aspect of human geometric reasoning involving
auxiliary line construction and affine transformations. We present GeoSketch, a
neural-symbolic framework that recasts geometric reasoning as an interactive
perception-reasoning-action loop. GeoSketch integrates: (1) a Perception module
that abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning
module that applies geometric theorems to decide the next deductive step, and
(3) a Sketch Action module that executes operations such as drawing auxiliary
lines or applying transformations, thereby updating the diagram in a closed
loop. To train this agent, we develop a two-stage pipeline: supervised
fine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement
learning with dense, symbolic rewards to enhance robustness and strategic
exploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a
high-quality set of 390 geometry problems requiring auxiliary construction or
affine transformations. Experiments on strong MLLM baselines demonstrate that
GeoSketch significantly improves stepwise reasoning accuracy and
problem-solving success over static perception methods. By unifying
hierarchical decision-making, executable visual actions, and symbolic
verification, GeoSketch advances multimodal reasoning from static
interpretation to dynamic, verifiable interaction, establishing a new
foundation for solving complex visuospatial problems.

</details>


### [89] [InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios](https://arxiv.org/abs/2509.22502)
*Chenglin Yu,Yang Yu,Songmiao Wang,Yucheng Wang,Yifan Yang,Jinjia Li,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: InfiAgent是一个基于DAG的金字塔式多智能体框架，通过自动分解复杂任务、双重审核机制、智能路由和自进化能力，解决了传统LLM智能体开发复杂、难以扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 传统LLM智能体开发需要精心设计工作流程、精心制作提示词和迭代调优，这需要LLM技术和领域专业知识，这些手工制作的限制阻碍了LLM智能体在各行业的可扩展性和成本效益。

Method: 提出InfiAgent框架，包含：通用化的"智能体即工具"机制自动分解复杂智能体；双重审核机制确保任务完成质量；智能体路由功能实现高效任务匹配；智能体自进化机制基于新任务、性能不佳或优化机会自主重构智能体DAG。

Result: 在多个基准测试中，InfiAgent相比类似自动生成智能体框架ADAS性能提升9.9%；案例研究显示AI研究助手InfiHelper生成的科学论文获得了IEEE顶级会议人类评审的认可。

Conclusion: InfiAgent框架演变成一个通用的金字塔式多智能体系统，能够解决广泛的问题，其原子任务设计支持智能体并行化，显著提高执行效率。

Abstract: Large Language Model (LLM) agents have demonstrated remarkable capabilities
in organizing and executing complex tasks, and many such agents are now widely
used in various application scenarios. However, developing these agents
requires carefully designed workflows, carefully crafted prompts, and iterative
tuning, which requires LLM techniques and domain-specific expertise. These
hand-crafted limitations hinder the scalability and cost-effectiveness of LLM
agents across a wide range of industries. To address these challenges, we
propose \textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that
can be applied to \textbf{infi}nite scenarios, which introduces several key
innovations: a generalized "agent-as-a-tool" mechanism that automatically
decomposes complex agents into hierarchical multi-agent systems; a dual-audit
mechanism that ensures the quality and stability of task completion; an agent
routing function that enables efficient task-agent matching; and an agent
self-evolution mechanism that autonomously restructures the agent DAG based on
new tasks, poor performance, or optimization opportunities. Furthermore,
InfiAgent's atomic task design supports agent parallelism, significantly
improving execution efficiency. This framework evolves into a versatile
pyramid-like multi-agent system capable of solving a wide range of problems.
Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\%
higher performance compared to ADAS (similar auto-generated agent framework),
while a case study of the AI research assistant InfiHelper shows that it
generates scientific papers that have received recognition from human reviewers
at top-tier IEEE conferences.

</details>


### [90] [Estimating the Empowerment of Language Model Agents](https://arxiv.org/abs/2509.22504)
*Jinyeop Song,Jeff Gore,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 提出基于信息论中赋权（empowerment）概念的语言模型代理评估框架EELMA，通过计算代理行为与未来状态的互信息来评估代理能力，在语言游戏和网页浏览场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型代理能力增强并接入更多现实工具，需要可扩展的评估框架。传统基于基准测试的评估成本高且需要人工设计任务，难以提供对通用能力的洞察。

Method: 提出EELMA算法，通过多轮文本交互近似估计有效赋权值，即代理行为与未来状态的互信息。在语言游戏和现实网页浏览场景中进行验证。

Result: 发现赋权值与平均任务性能强相关；表征了环境复杂性、思维链、模型规模和记忆长度等因素对赋权估计的影响；高赋权状态和行为通常是通用能力的关键时刻。

Conclusion: 赋权作为通用指标，适用于在复杂开放环境中评估和监控语言模型代理的能力表现。

Abstract: As language model (LM) agents become more capable and gain broader access to
real-world tools, there is a growing need for scalable evaluation frameworks of
agentic capability. However, conventional benchmark-centric evaluations are
costly to design and require human designers to come up with valid tasks that
translate into insights about general model capabilities. In this work, we
propose information-theoretic evaluation based on empowerment, the mutual
information between an agent's actions and future states, as an open-ended
method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of
Language Model Agents), an algorithm for approximating effective empowerment
from multi-turn text interactions. We validate EELMA on both language games and
scaled-up realistic web-browsing scenarios. We find that empowerment strongly
correlates with average task performance, characterize the impact of
environmental complexity and agentic factors such as chain-of-thought, model
scale, and memory length on estimated empowerment, and that high empowerment
states and actions are often pivotal moments for general capabilities.
Together, these results demonstrate empowerment as an appealing general-purpose
metric for evaluating and monitoring LM agents in complex, open-ended settings.

</details>


### [91] [TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent and Explainable Digital Assessments](https://arxiv.org/abs/2509.22516)
*Rakesh Thakur,Shivaansh Kaushik,Gauri Chopra,Harsh Rohilla*

Main category: cs.AI

TL;DR: TrueGradeAI是一个AI驱动的数字考试框架，通过保留手写输入和使用基于transformer的OCR转录，结合检索增强管道进行自动评分，解决传统纸质考试的纸张浪费、评分延迟和评估者偏见问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统纸质考试存在的纸张浪费、物流复杂、评分延迟和评估者偏见等缺点，同时保持手写的自然性。

Method: 使用安全平板电脑捕捉手写输入，应用基于transformer的光学字符识别进行转录，通过检索增强管道整合教师解决方案、缓存层和外部参考，让大语言模型进行评分并提供证据关联的推理。

Result: 该系统实现了手写保留与可扩展透明评估的结合，减少了环境成本，加速了反馈周期，并逐步构建可重复使用的知识库。

Conclusion: TrueGradeAI通过结合手写保留和可解释的自动化评分，在减少环境影响的同时提高了评估的公平性和透明度，并积极缓解评分偏见。

Abstract: This paper introduces TrueGradeAI, an AI-driven digital examination framework
designed to overcome the shortcomings of traditional paper-based assessments,
including excessive paper usage, logistical complexity, grading delays, and
evaluator bias. The system preserves natural handwriting by capturing stylus
input on secure tablets and applying transformer-based optical character
recognition for transcription. Evaluation is conducted through a
retrieval-augmented pipeline that integrates faculty solutions, cache layers,
and external references, enabling a large language model to assign scores with
explicit, evidence-linked reasoning. Unlike prior tablet-based exam systems
that primarily digitize responses, TrueGradeAI advances the field by
incorporating explainable automation, bias mitigation, and auditable grading
trails. By uniting handwriting preservation with scalable and transparent
evaluation, the framework reduces environmental costs, accelerates feedback
cycles, and progressively builds a reusable knowledge base, while actively
working to mitigate grading bias and ensure fairness in assessment.

</details>


### [92] [REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model](https://arxiv.org/abs/2509.22518)
*Bo Li,Guanzhi Deng,Ronghao Chen,Junrong Yue,Shuo Zhang,Qinghua Zhao,Linqi Song,Lijie Wen*

Main category: cs.AI

TL;DR: 提出了Reasoning Manifold概念和REMA框架，通过分析LLM内部表示的几何结构来理解推理失败机制


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型如何进行复杂推理及其失败机制是解释性研究的挑战，需要可测量的几何分析视角

Method: 定义推理流形概念，构建REMA框架，通过计算错误表示与正确表示形成的流形之间的k近邻距离来量化几何偏差，并跟踪层间偏差定位分歧点

Result: 实验验证了推理流形的低维特性，错误与正确推理表示的高度可分性，以及REMA框架在分析推理失败起源方面的有效性

Conclusion: 该研究将抽象推理失败与表示中的可测量几何偏差联系起来，为深入理解黑盒模型内部计算过程提供了新途径

Abstract: Understanding how Large Language Models (LLMs) perform complex reasoning and
their failure mechanisms is a challenge in interpretability research. To
provide a measurable geometric analysis perspective, we define the concept of
the Reasoning Manifold, a latent low-dimensional geometric structure formed by
the internal representations corresponding to all correctly reasoned
generations. This structure can be conceptualized as the embodiment of the
effective thinking paths that the model has learned to successfully solve a
given task. Based on this concept, we build REMA, a framework that explains the
origins of failures by quantitatively comparing the spatial relationships of
internal model representations corresponding to both erroneous and correct
reasoning samples. Specifically, REMA first quantifies the geometric deviation
of each erroneous representation by calculating its k-nearest neighbors
distance to the approximated manifold formed by correct representations,
thereby providing a unified failure signal. It then localizes the divergence
points where these deviations first become significant by tracking this
deviation metric across the model's layers and comparing it against a baseline
of internal fluctuations from correct representations, thus identifying where
the reasoning chain begins to go off-track. Our extensive experiments on
diverse language and multimodal models and tasks demonstrate the
low-dimensional nature of the reasoning manifold and the high separability
between erroneous and correct reasoning representations. The results also
validate the effectiveness of the REMA framework in analyzing the origins of
reasoning failures. This research connects abstract reasoning failures to
measurable geometric deviations in representations, providing new avenues for
in-depth understanding and diagnosis of the internal computational processes of
black-box models.

</details>


### [93] [The Emergence of Altruism in Large-Language-Model Agents Society](https://arxiv.org/abs/2509.22537)
*Haoyang Li,Xiao Jia,Zhanzhan Zhao*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在社会模拟中存在两种不同的社会倾向："适应性利己主义者"和"利他主义优化者"，揭示了LLM在利己与利他行为上的内在异质性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注小规模任务导向游戏中的合作行为，忽视了大规模智能体社会中利他主义的涌现机制，需要理解LLM智能体所体现的社会逻辑。

Method: 引入Schelling变体城市迁移模型，创建社会困境，让200多个LLM智能体在利己和利他目标间做出选择，并使用扎根理论方法系统编码智能体推理过程。

Result: 识别出两种原型：适应性利己主义者默认优先考虑自身利益但受社会规范影响会增加利他行为；利他主义优化者则表现出固有的利他逻辑，始终优先集体利益。

Conclusion: 社会模拟中的模型选择不仅是选择推理能力，更是选择内在的社会行动逻辑，不同原型适用于不同模拟场景。

Abstract: Leveraging Large Language Models (LLMs) for social simulation is a frontier
in computational social science. Understanding the social logics these agents
embody is critical to this attempt. However, existing research has primarily
focused on cooperation in small-scale, task-oriented games, overlooking how
altruism, which means sacrificing self-interest for collective benefit, emerges
in large-scale agent societies. To address this gap, we introduce a
Schelling-variant urban migration model that creates a social dilemma,
compelling over 200 LLM agents to navigate an explicit conflict between
egoistic (personal utility) and altruistic (system utility) goals. Our central
finding is a fundamental difference in the social tendencies of LLMs. We
identify two distinct archetypes: "Adaptive Egoists", which default to
prioritizing self-interest but whose altruistic behaviors significantly
increase under the influence of a social norm-setting message board; and
"Altruistic Optimizers", which exhibit an inherent altruistic logic,
consistently prioritizing collective benefit even at a direct cost to
themselves. Furthermore, to qualitatively analyze the cognitive underpinnings
of these decisions, we introduce a method inspired by Grounded Theory to
systematically code agent reasoning. In summary, this research provides the
first evidence of intrinsic heterogeneity in the egoistic and altruistic
tendencies of different LLMs. We propose that for social simulation, model
selection is not merely a matter of choosing reasoning capability, but of
choosing an intrinsic social action logic. While "Adaptive Egoists" may offer a
more suitable choice for simulating complex human societies, "Altruistic
Optimizers" are better suited for modeling idealized pro-social actors or
scenarios where collective welfare is the primary consideration.

</details>


### [94] [StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models](https://arxiv.org/abs/2509.22558)
*Chenyu Zhou,Tianyi Xu,Jianghao Lin,Dongdong Ge*

Main category: cs.AI

TL;DR: StepORLM是一个用于运筹学问题的自进化LLM框架，通过生成式过程监督和协同进化循环，解决了传统强化学习中的信用分配问题和短视的过程监督问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在运筹学问题上面临两个关键局限：结果奖励存在信用分配问题（正确最终答案可能强化有缺陷的推理），传统判别式过程监督短视且无法全面评估相互依赖的建模步骤。

Method: StepORLM采用协同进化循环，策略模型和生成式过程奖励模型(GenPRM)相互迭代改进。通过双反馈机制：外部求解器的确定性结果验证和GenPRM的细致过程评估，结合加权直接偏好优化(W-DPO)对齐策略并同时优化GenPRM。

Result: 8B参数的StepORLM在六个基准测试中创下新纪录，显著优于更大的通用模型、代理方法和专业基线。协同进化的GenPRM可作为强大且通用的过程验证器，大幅提升自身和其他现有LLM的推理扩展性能。

Conclusion: StepORLM通过生成式过程监督和协同进化机制，有效解决了运筹学问题中LLM训练的信用分配和过程评估问题，实现了卓越性能并产生了通用的过程验证能力。

Abstract: Large Language Models (LLMs) have shown promising capabilities for solving
Operations Research (OR) problems. While reinforcement learning serves as a
powerful paradigm for LLM training on OR problems, existing works generally
face two key limitations. First, outcome reward suffers from the credit
assignment problem, where correct final answers can reinforce flawed reasoning.
Second, conventional discriminative process supervision is myopic, failing to
evaluate the interdependent steps of OR modeling holistically. To this end, we
introduce StepORLM, a novel self-evolving framework with generative process
supervision. At its core, StepORLM features a co-evolutionary loop where a
policy model and a generative process reward model (GenPRM) iteratively improve
on each other. This loop is driven by a dual-feedback mechanism: definitive,
outcome-based verification from an external solver, and nuanced, holistic
process evaluation from the GenPRM. The combined signal is used to align the
policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously
refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new
state-of-the-art across six benchmarks, significantly outperforming vastly
larger generalist models, agentic methods, and specialized baselines. Moreover,
the co-evolved GenPRM is able to act as a powerful and universally applicable
process verifier, substantially boosting the inference scaling performance of
both our own model and other existing LLMs.

</details>


### [95] [UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration](https://arxiv.org/abs/2509.22570)
*Qi Mao,Tinghan Yang,Jiahao Li,Bin Li,Libiao Jin,Yan Lu*

Main category: cs.AI

TL;DR: UniMIC是一个统一的基于token的多模态交互编码框架，通过紧凑的token化表示作为通信媒介，在保持与大型多模态模型兼容的同时实现高效低比特率传输。


<details>
  <summary>Details</summary>
Motivation: 现有编解码器仍针对单模态单向通信优化，在传统的压缩-传输-重建流程中会导致重复的质量下降，无法满足多模态双向交互的需求。

Method: 使用紧凑的token化表示作为通信媒介，采用轻量级Transformer熵模型（通用、掩码和文本条件三种设计）来最小化token间冗余。

Result: 在文本到图像生成、文本引导修复、扩展和视觉问答等任务上，UniMIC实现了显著的比特率节省，即使在超低比特率（<0.05bpp）下仍保持鲁棒性，且不影响下游任务性能。

Conclusion: UniMIC为下一代多模态交互通信提供了一个实用且前瞻性的范式。

Abstract: The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI
agents is transforming human-AI collaboration into bidirectional, multimodal
interaction. However, existing codecs remain optimized for unimodal, one-way
communication, resulting in repeated degradation under conventional
compress-transmit-reconstruct pipelines. To address this limitation, we propose
UniMIC, a Unified token-based Multimodal Interactive Coding framework that
bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or
plain text, UniMIC employs compact tokenized representations as the
communication medium, enabling efficient low-bitrate transmission while
maintaining compatibility with LMMs. To further enhance compression,
lightweight Transformer-based entropy models with scenario-specific
designs-generic, masked, and text-conditioned-effectively minimize inter-token
redundancy. Extensive experiments on text-to-image generation, text-guided
inpainting, outpainting, and visual question answering show that UniMIC
achieves substantial bitrate savings and remains robust even at ultra-low
bitrates (<0.05bpp), without compromising downstream task performance. These
results establish UniMIC as a practical and forward-looking paradigm for
next-generation multimodal interactive communication.

</details>


### [96] [Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time](https://arxiv.org/abs/2509.22572)
*Yixuan Han,Fan Ma,Ruijie Quan,Yi Yang*

Main category: cs.AI

TL;DR: 提出了动态专家搜索（DES），一种测试时扩展策略，通过控制MoE模型中激活的专家数量来生成多样化的推理轨迹，无需额外成本即可提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法主要依赖输出级采样，忽视了模型架构的作用。在MoE模型中，发现改变激活专家数量能产生互补的解决方案集，这揭示了一个未被充分探索的多样性来源。

Method: DES包含两个关键组件：动态MoE（在推理时直接控制专家数量生成多样化推理轨迹）和专家配置继承（在推理路径内保持一致的专家数量，在运行间变化以平衡稳定性和多样性）。

Result: 在MoE架构、验证器和推理基准（数学、代码和知识）上的广泛实验表明，DES可靠地优于TTS基线，无需额外成本即可提高准确性和稳定性。

Conclusion: DES作为一种实用且可扩展的架构感知TTS形式，展示了现代LLMs结构灵活性如何推进推理能力发展。

Abstract: Test-Time Scaling (TTS) enhances the reasoning ability of large language
models (LLMs) by allocating additional computation during inference. However,
existing approaches primarily rely on output-level sampling while overlooking
the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we
observe that varying the number of activated experts yields complementary
solution sets with stable accuracy, revealing a new and underexplored source of
diversity. Motivated by this observation, we propose Dynamic Experts Search
(DES), a TTS strategy that elevates expert activation into a controllable
dimension of the search space. DES integrates two key components: (1) Dynamic
MoE, which enables direct control of expert counts during inference to generate
diverse reasoning trajectories without additional cost; and (2) Expert
Configuration Inheritance, which preserves consistent expert counts within a
reasoning path while varying them across runs, thereby balancing stability and
diversity throughout the search. Extensive experiments across MoE
architectures, verifiers and reasoning benchmarks (i.e., math, code and
knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing
accuracy and stability without additional cost. These results highlight DES as
a practical and scalable form of architecture-aware TTS, illustrating how
structural flexibility in modern LLMs can advance reasoning.

</details>


### [97] [Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective](https://arxiv.org/abs/2509.22613)
*Siwei Wang,Yifei Shen,Haoran Sun,Shi Feng,Shang-Hua Teng,Li Dong,Yaru Hao,Wei Chen*

Main category: cs.AI

TL;DR: 本文通过图论抽象分析强化学习在LLM规划中的理论机制，发现RL通过探索克服SFT的伪相关解，但PG存在多样性崩溃问题，Q-learning则能保持多样性且避免奖励破解。


<details>
  <summary>Details</summary>
Motivation: 当前RL方法显著提升了LLM的规划能力，但其有效性的理论基础尚不清楚，需要系统分析RL在规划任务中的优势和局限。

Method: 使用基于图的抽象模型，分析策略梯度(PG)和Q-learning方法，并在Blocksworld真实规划基准上进行验证。

Result: SFT会产生基于共现的伪解，RL通过探索实现正确规划；PG存在多样性崩溃问题，Q-learning能保持多样性且通过离策略学习避免奖励破解。

Conclusion: RL通过探索机制提升LLM规划泛化能力，但需注意PG的多样性问题，Q-learning在保持多样性和避免奖励破解方面更具优势。

Abstract: Recent reinforcement learning (RL) methods have substantially enhanced the
planning capabilities of Large Language Models (LLMs), yet the theoretical
basis for their effectiveness remains elusive. In this work, we investigate
RL's benefits and limitations through a tractable graph-based abstraction,
focusing on policy gradient (PG) and Q-learning methods. Our theoretical
analyses reveal that supervised fine-tuning (SFT) may introduce
co-occurrence-based spurious solutions, whereas RL achieves correct planning
primarily through exploration, underscoring exploration's role in enabling
better generalization. However, we also show that PG suffers from diversity
collapse, where output diversity decreases during training and persists even
after perfect accuracy is attained. By contrast, Q-learning provides two key
advantages: off-policy learning and diversity preservation at convergence. We
further demonstrate that careful reward design is necessary to prevent reward
hacking in Q-learning. Finally, applying our framework to the real-world
planning benchmark Blocksworld, we confirm that these behaviors manifest in
practice.

</details>
