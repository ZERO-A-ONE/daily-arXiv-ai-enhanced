<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: 研究探讨了软件工程师技术面试准备的现状，发现候选人缺乏真实场景训练且教育支持不足，导致压力和准备不足。


<details>
  <summary>Details</summary>
Motivation: 技术面试复杂且难以准备，现有教育体系未能有效支持候选人应对面试挑战。

Method: 通过向131名正在准备技术面试的候选人分发调查问卷进行研究。

Result: 结果显示候选人很少在真实场景中训练，课程未能支持准备，导致压力和准备不足。

Conclusion: 研究为利益相关者提供了改进技术面试准备的建议。

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [2] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: 论文提出了一种利用自然语言进行结构化代码搜索的新方法，结合LLM和结构化搜索工具，显著提高了搜索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 开发者通常使用关键词和正则表达式搜索代码，但结构化代码搜索工具需要学习复杂的DSL，限制了其使用。自然语言搜索可以降低门槛。

Method: 结合LLM的自然语言理解能力和结构化搜索工具，将自然语言查询翻译为DSL查询，并在Semgrep和GQL上实现。

Result: 在400个查询的基准测试中，方法表现出色，精确率和召回率在55%-70%之间，显著优于基线方法。

Conclusion: 自然语言驱动的结构化代码搜索是有效的，能够显著提升开发者的搜索体验和效率。

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [3] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 研究探讨了如何通过源代码的内部指标预测移动应用的受欢迎程度，发现分类模型比回归模型更有效，最佳模型F1分数为0.72。


<details>
  <summary>Details</summary>
Motivation: 预测应用发布前的受欢迎程度可为开发者提供竞争优势，但现有方法效果有限。

Method: 使用446个开源Android应用的数据集，提取代码指标、代码异味和元数据，评估回归和分类模型。

Result: 回归模型表现不佳，但分类模型（多层感知器）在二元分类任务中F1分数达0.72。

Conclusion: 内部代码指标可作为应用受欢迎程度的有限但有用的预测指标，挑战了此前认为其无效的观点。

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [4] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: 研究比较了心理测量压力指标与生物特征指标，并试图在软件工程任务中识别生物数据中的压力模式。实验结果显示心理测量工具未检测到压力，而生物特征数据仅在EDA阶段性峰值中显示显著差异。


<details>
  <summary>Details</summary>
Motivation: 传统自我报告工具可能存在偏差，因此研究探索结合更客观的生理测量方法。

Method: 实验包括前测问卷、编程任务（佩戴生物传感器）、后测问卷和简短退出访谈。

Result: 心理测量工具未显示压力，访谈中参与者报告无压力或时间压力，生物特征数据仅在EDA阶段性峰值有显著差异。

Conclusion: 通过严格时间限制诱导压力的方法不足，为未来研究提供了方法论启示。

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [5] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: 本文提出了一种基于数据集特征的映射方法和问卷，用于推荐适合新数据集的情绪分析工具，以提高工具选择的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有情绪分析工具在不同平台数据集上表现不一致，影响了软件工程中团队动态分析和AI驱动分析的可靠性。

Method: 分析了来自五个平台的10个开发者通信数据集的语言和统计特征，评估了14种情绪分析工具的性能，并提出了一种映射方法和问卷。

Result: 数据集特征可用于改进工具选择，Transformer模型表现稳定但工具效果仍依赖上下文。

Conclusion: 该方法为研究者和从业者提供了可靠的情绪分析工具选择支持，同时强调了持续评估的必要性。

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [6] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: 提出了一种多智能体方法，利用两个基于LLM的智能体协作生成COBOL代码的解释，解决了COBOL代码因长度和语法差异超出LLM窗口限制的问题。


<details>
  <summary>Details</summary>
Motivation: 由于COBOL语言的老化、复杂性及开发者减少，维护COBOL代码库变得困难，尤其是缺乏文档导致新开发者难以理解和维护。

Method: 采用多智能体方法，两个LLM智能体协作，结合代码库的上下文信息生成代码解释。

Result: 在14个开源COBOL项目上评估，方法在函数、文件和项目级别的解释性能均显著优于基线。

Conclusion: 多智能体方法有效解决了COBOL代码解释的挑战，显著提升了生成解释的质量和覆盖范围。

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [7] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: RTED是一种新型的类型感知测试生成技术，结合类型约束分析和反射验证，有效检测Python类型错误并减少误报。


<details>
  <summary>Details</summary>
Motivation: Python类型错误常导致运行时故障，现有静态分析工具误报率高，单元测试生成技术缺乏针对性指导。

Method: RTED结合逐步类型约束分析和反射验证，指导测试生成过程。

Result: 在BugsInPy和TypeBugs基准测试中，RTED比现有技术多检测22-29个类型错误，误报率降低173.9%-245.9%，并发现12个未知错误。

Conclusion: RTED显著提升了Python类型错误检测的准确性和覆盖率。

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [8] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: 论文提出了一种名为VeFIA的框架，用于审计垂直联邦学习（VFL）中数据方的推理软件执行正确性，确保隐私和效率。


<details>
  <summary>Details</summary>
Motivation: 现有VFL工作缺乏对数据方推理软件执行正确性的审计机制，可能导致不可靠的结果。

Method: 设计了VeFIA框架，利用可信执行环境（TEE）和协调器验证数据方的计算结果。

Result: VeFIA在异常推理超过5.4%时，能以99.99%的概率检测到问题，且不影响在线推理延迟。

Conclusion: VeFIA是首个解决VFL中推理软件执行正确性问题的方案，具有高效性和隐私保护性。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [9] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: 提出了一种自动化测试大语言模型公平性的新方法Meta-Fair，结合了蜕变测试和LLM的能力，有效发现偏见。


<details>
  <summary>Details</summary>
Motivation: 当前公平性测试方法依赖人工和固定模板，资源密集且难以扩展，需要更自动化的解决方案。

Method: 采用蜕变测试和LLM生成测试用例及评估输出，开发了三个开源工具支持测试流程。

Result: Meta-Fair在12个LLM上测试，平均精度92%，发现29%的执行存在偏见，LLM评估表现可靠（F1-score达0.79）。

Conclusion: Meta-Fair为LLM公平性测试提供了自动化新路径，尽管存在非确定性挑战，但前景广阔。

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [10] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: LLMREI是一种基于大型语言模型的聊天机器人，旨在自动化需求获取访谈，减少人为错误并提高可扩展性。通过零样本提示和最少到最多提示两种方法优化，评估显示其性能接近人类访谈员。


<details>
  <summary>Details</summary>
Motivation: 需求获取访谈依赖熟练分析师，资源密集且易受人为偏见和沟通问题影响。大型语言模型的发展为自动化部分流程提供了新机会。

Method: 研究设计了LLMREI聊天机器人，采用零样本提示和最少到最多提示两种方法，并在33次模拟访谈中评估其性能。

Result: LLMREI在减少访谈错误、提取需求和生成上下文相关问题上表现接近人类，适合大规模自动化访谈。

Conclusion: LLMREI在自动化大规模需求获取访谈中具有潜力，未来可进一步优化。

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [11] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: 研究探讨如何将人机协作（HMT）融入自适应信息物理系统（CPS），解决人机节奏差异和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 自适应CPS需要无缝人机协作，但现有反馈循环（如MAPE-K）未充分整合人类，且涉及隐私和伦理问题。

Method: 开发新方法将HMT融入CPS反馈循环，并构建伦理与人类价值观的验证框架。

Result: 提出整合HMT和伦理验证的方法，优化自适应CPS的人机协作。

Conclusion: 研究为自适应CPS中实现高效、伦理的人机协作提供了理论和实践基础。

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [12] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: 研究发现RSEs和SERs在术语使用上存在差异，初步揭示了合作机会，并提出了一种系统化的术语映射方法。


<details>
  <summary>Details</summary>
Motivation: 解决RSEs和SERs因术语差异导致的沟通障碍。

Method: 通过调查SE基础概念在RSE社区中的解释，识别概念对齐、知识差距和潜在适应领域。

Result: 发现合作与学习的可能性，并开发了系统化的术语映射方法。

Conclusion: 研究为未来众包扩展和验证奠定了基础，促进了两大社区的交流与合作。

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [13] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: RLHGNN是一种新颖的框架，通过将事件日志转换为异构过程图，结合强化学习优化图结构，有效预测服务交互中的下一步活动。


<details>
  <summary>Details</summary>
Motivation: 现有序列方法和图方法在捕捉非顺序关系和动态结构方面存在不足，RLHGNN旨在解决这些限制。

Method: RLHGNN将事件日志转换为三种边类型的异构图，结合强化学习选择最优图结构，并采用异构图卷积进行预测。

Result: 在六个真实数据集上，RLHGNN表现优于现有方法，且推理延迟低至1毫秒。

Conclusion: RLHGNN是一种高效且实用的解决方案，适用于实时业务过程监控。

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [14] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: 论文提出了一种通过‘可持续性标志’识别云架构讨论中可持续性概念的方法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着云计算的普及，可持续性在软件架构中的重要性增加，但缺乏明确的识别指南。

Method: 通过主题分析开发‘可持续性标志’，并在实验中评估其识别效果。

Result: 使用标志后，分类为可持续性相关的帖子减少，但确定性和性能显著提高。

Conclusion: 可持续性标志比单纯依赖定义更实用且易于理解。

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [15] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 提出了一种基于文本蕴含和上下文学习的方法，自动生成法律文本的规范表示，并将其编码为可执行的Python代码，以提高法律合规性分析的效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 小型组织和初创公司缺乏法律专业知识，手动提取法律文本的元数据以确保合规性是一项繁重任务。现有自动化方法未考虑元数据属性间的交互关系，且依赖手动标注或启发式机器学习，泛化能力有限。

Method: 采用文本蕴含和上下文学习技术，设计了一个领域特定的Python类结构作为元模型，自动生成法律文本的规范表示，减少对大规模手动标注数据的依赖。

Result: 在13个美国州数据泄露通知法律上的评估显示，生成的表示通过约89.4%的测试用例，精确率和召回率分别为82.2和88.7。

Conclusion: 该方法显著提高了法律文本元数据提取的效率和泛化能力，适用于新法规，为软件合规性提供了实用工具。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [16] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 论文探讨了使用GPT-4o生成需求访谈中的后续问题，发现其生成的问题在清晰度、相关性和信息量上不亚于人类编写的问题，且在指导后表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决需求访谈中因领域不熟悉、认知负荷和信息过载导致的提问困难。

Method: 基于常见错误类型框架，利用GPT-4o生成问题，并通过对照实验评估其效果。

Result: LLM生成的问题在无指导时与人类问题相当，在指导后表现更优。

Conclusion: LLM可实时提升需求访谈的质量和效率。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned LLMs for Malware Generation](https://arxiv.org/abs/2507.02057)
*Lu Yan,Zhuo Zhang,Xiangzhe Xu,Shengwei An,Guangyu Shen,Zhou Xuan,Xuan Chen,Xiangyu Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种名为MGC的新框架，通过模块化分解和规避对齐生成恶意软件，揭示了组合攻击对AI系统的风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）降低了软件开发的门槛，但也为恶意软件开发提供了便利。现有的对齐机制仅评估单个提示，忽视了恶意操作可分解为看似良性的子任务的风险。

Method: MGC框架利用模块化分解和Malware Description Intermediate Representation（MDIR）将高级恶意意图转化为看似良性的代码片段。

Result: MGC在三个基准数据集上生成的恶意软件正确性比越狱方法高365.79%，比地下服务高78.07%，并能重现和增强16个真实恶意软件样本。

Conclusion: MGC揭示了组合攻击对AI系统的威胁，为安全研究提供了重要见解。

Abstract: Large language models (LLMs) have democratized software development, reducing
the expertise barrier for programming complex applications. This accessibility
extends to malicious software development, raising significant security
concerns. While LLM providers have implemented alignment mechanisms to prevent
direct generation of overtly malicious code, these safeguards predominantly
evaluate individual prompts in isolation, overlooking a critical vulnerability:
malicious operations can be systematically decomposed into benign-appearing
sub-tasks. In this paper, we introduce the Malware Generation Compiler (MGC), a
novel framework that leverages this vulnerability through modular decomposition
and alignment-evasive generation. MGC employs a specialized Malware Description
Intermediate Representation (MDIR) to bridge high-level malicious intents and
benign-appearing code snippets. Extensive evaluation demonstrates that our
attack reliably generates functional malware across diverse task specifications
and categories, outperforming jailbreaking methods by +365.79% and underground
services by +78.07% in correctness on three benchmark datasets. Case studies
further show that MGC can reproduce and even enhance 16 real-world malware
samples. This work provides critical insights for security researchers by
exposing the risks of compositional attacks against aligned AI systems.
Demonstrations are available at
https://sites.google.com/view/malware-generation-compiler.

</details>


### [18] [Can Artificial Intelligence solve the blockchain oracle problem? Unpacking the Challenges and Possibilities](https://arxiv.org/abs/2507.02125)
*Giulio Caldarelli*

Main category: cs.CR

TL;DR: 本文探讨了AI如何辅助解决区块链预言机问题，但指出AI无法完全消除对链外数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 区块链预言机问题限制了去中心化应用的发展，现有方法未能彻底解决链外数据可靠性问题。

Method: 通过分析学术文献和实践案例，评估AI技术在异常检测、语言事实提取、动态声誉建模和对抗性抵抗中的应用。

Result: AI能提升数据质量、来源选择和系统韧性，但仍需依赖不可验证的链外输入。

Conclusion: AI应作为预言机设计的补充层，而非替代信任假设。

Abstract: The blockchain oracle problem, which refers to the challenge of injecting
reliable external data into decentralized systems, remains a fundamental
limitation to the development of trustless applications. While recent years
have seen a proliferation of architectural, cryptographic, and economic
strategies to mitigate this issue, no one has yet fully resolved the
fundamental question of how a blockchain can gain knowledge about the off-chain
world. In this position paper, we critically assess the role artificial
intelligence (AI) can play in tackling the oracle problem. Drawing from both
academic literature and practitioner implementations, we examine how AI
techniques such as anomaly detection, language-based fact extraction, dynamic
reputation modeling, and adversarial resistance can enhance oracle systems. We
observe that while AI introduces powerful tools for improving data quality,
source selection, and system resilience, it cannot eliminate the reliance on
unverifiable off-chain inputs. Therefore, this study supports the idea that AI
should be understood as a complementary layer of inference and filtering within
a broader oracle design, not a substitute for trust assumptions.

</details>


### [19] [ARMOUR US: Android Runtime Zero-permission Sensor Usage Monitoring from User Space](https://arxiv.org/abs/2507.02177)
*Yan Long,Jiancong Cui,Yuqing Yang,Tobias Alam,Zhiqiang Lin,Kevin Fu*

Main category: cs.CR

TL;DR: 本文研究了如何监控Android零权限传感器的访问，以防止用户隐私泄露，并帮助安全研究人员识别潜在的传感器滥用模式。


<details>
  <summary>Details</summary>
Motivation: 零权限传感器（如加速度计）已成为Android设备不可或缺的部分，但其使用可能引发隐私问题。现有方法（如静态分析和动态分析）存在开发复杂、需root权限等问题，因此需要一种更简便的监控方法。

Method: 作者开发了ARMOUR，一种用户空间运行时监控工具，利用Android的采样率变化和收敛行为，无需复杂开发链或root权限即可监控传感器使用。

Result: 在1,448个商业应用上的评估显示，ARMOUR能有效检测混淆代码中的传感器使用，并发现50%看似与传感器无关的应用访问了多个零权限传感器。

Conclusion: ARMOUR为隐私意识用户提供了简便的监控工具，同时支持安全研究人员进行快速分析。文章还探讨了Android政策变化的影响及剩余的技术和监管问题。

Abstract: This work investigates how to monitor access to Android zero-permission
sensors which could cause privacy leakage to users. Moreover, monitoring such
sensitive access allows security researchers to characterize potential sensor
abuse patterns. Zero-permission sensors such as accelerometers have become an
indispensable part of Android devices. The critical information they provide
has attracted extensive research investigating how data collectors could
capture more sensor data to enable both benign and exploitative applications.
In contrast, little work has explored how to enable data providers, such as end
users, to understand sensor usage. While existing methods such as static
analysis and hooking-based dynamic analysis face challenges of requiring
complicated development chains, rooting privilege, and app-specific reverse
engineering analysis, our work aims to bridge this gap by developing ARMOUR for
user-space runtime monitoring, leveraging the intrinsic sampling rate variation
and convergence behaviors of Android. ARMOUR enables privacy-aware users to
easily monitor how third-party apps use sensor data and support security
researchers to perform rapid app-agnostic sensor access analysis. Our
evaluation with 1,448 commercial applications shows the effectiveness of ARMOUR
in detecting sensor usage in obfuscated code and other conditions, and observes
salient sensor abuse patterns such as 50% of apps from seemingly
sensor-independent categories accessing data of multiple zero-permission
sensors. We analyze the impact of Android's recent policy changes on
zero-permission sensors and remaining technical and regulatory problems.

</details>


### [20] [Extended c-differential distinguishers of full 9 and reduced-round Kuznyechik cipher](https://arxiv.org/abs/2507.02181)
*Pantelimon Stanica,Ranit Dutta,Bimal Mandal*

Main category: cs.CR

TL;DR: 本文提出了一种新的截断内部$c$-差分密码分析方法，首次实现了$c$-差分均匀性在分组密码中的实际应用。通过内部$c$-差分方法解决了乘法破坏结构的问题，并证明了其与外部$c$-差分均匀性的对偶关系。实验表明该方法对9轮Kuznyechik密码具有显著区分效果。


<details>
  <summary>Details</summary>
Motivation: 解决$c$-差分均匀性在分组密码中应用时因乘法破坏结构而无法实际使用的关键问题。

Method: 提出内部$c$-差分方法，证明其与外部$c$-差分均匀性的对偶关系，并开发了统计计算框架进行截断$c$-差分分析。

Result: 在9轮Kuznyechik密码中发现了显著的统计非随机性，偏差比达1.7倍，p值低至1.85×10^{-3}。

Conclusion: 该方法首次实现了对9轮Kuznyechik密码的实际区分，揭示了其安全性不足的新攻击途径。

Abstract: This paper introduces {\em truncated inner $c$-differential cryptanalysis}, a
novel technique that for the first time enables the practical application of
$c$-differential uniformity to block ciphers. While Ellingsen et al. (IEEE
Trans. Inf. Theory, 2020) established the notion of $c$-differential uniformity
using $(F(x\oplus a), cF(x))$, a key challenge remained: multiplication by $c$
disrupts the structural properties essential for block cipher analysis,
particularly key addition.
  We resolve this challenge by developing an \emph{inner} $c$-differential
approach where multiplication by $c$ affects the input: $(F(cx\oplus a),
F(x))$. We prove that the inner $c$-differential uniformity of a function $F$
equals the outer $c$-differential uniformity of $F^{-1}$, establishing a
fundamental duality. This modification preserves cipher structure while
enabling practical cryptanalytic applications.
  Our main contribution is a comprehensive multi-faceted
statistical-computational framework, implementing truncated $c$-differential
analysis against the full 9-round Kuznyechik cipher (the inner
$c$-differentials are immune to the key whitening at the backend). Through
extensive computational analysis involving millions of differential pairs, we
demonstrate statistically significant non-randomness across all tested round
counts. For the full 9-round cipher, we identify multiple configurations
triggering critical security alerts, with bias ratios reaching $1.7\times$ and
corrected p-values as low as $1.85 \times 10^{-3}$, suggesting insufficient
security margin against this new attack vector. This represents the first
practical distinguisher against the full 9-round Kuznyechik.

</details>


### [21] [EIM-TRNG: Obfuscating Deep Neural Network Weights with Encoding-in-Memory True Random Number Generator via RowHammer](https://arxiv.org/abs/2507.02206)
*Ranyang Zhou,Abeer Matar A. Almalky,Gamana Aragonda,Sabbir Ahmed,Filip Roth Trønnes-Christensen,Adnan Siraj Rakin,Shaahin Angizi*

Main category: cs.CR

TL;DR: 提出了一种基于DRAM物理随机性的新型硬件TRNG（EIM-TRNG），用于保护DNN权重数据的安全性。


<details>
  <summary>Details</summary>
Motivation: 硬件TRNG比软件伪随机数生成器更具不可预测性和鲁棒性，适合保护AI系统的模型参数。

Method: 利用DRAM单元在RowHammer扰动下的物理随机性，提取熵源，并通过固定和随机比特翻转组合加密DNN权重数据。

Result: 验证了DRAM熵提取的有效性，为低成本硬件安全提供了解决方案。

Conclusion: EIM-TRNG为硬件级保护机器学习模型提供了新方向。

Abstract: True Random Number Generators (TRNGs) play a fundamental role in hardware
security, cryptographic systems, and data protection. In the context of Deep
NeuralNetworks (DNNs), safeguarding model parameters, particularly weights, is
critical to ensure the integrity, privacy, and intel-lectual property of AI
systems. While software-based pseudo-random number generators are widely used,
they lack the unpredictability and resilience offered by hardware-based TRNGs.
In this work, we propose a novel and robust Encoding-in-Memory TRNG called
EIM-TRNG that leverages the inherent physical randomness in DRAM cell behavior,
particularly under RowHammer-induced disturbances, for the first time. We
demonstrate how the unpredictable bit-flips generated through carefully
controlled RowHammer operations can be harnessed as a reliable entropy source.
Furthermore, we apply this TRNG framework to secure DNN weight data by encoding
via a combination of fixed and unpredictable bit-flips. The encrypted data is
later decrypted using a key derived from the probabilistic flip behavior,
ensuring both data confidentiality and model authenticity. Our results validate
the effectiveness of DRAM-based entropy extraction for robust, low-cost
hardware security and offer a promising direction for protecting machine
learning models at the hardware level.

</details>


### [22] [Linearly Homomorphic Ring Signature Scheme over Lattices](https://arxiv.org/abs/2507.02281)
*Heng Guo,Kun Tian,Fengxia Liu,Zhiyong Zheng*

Main category: cs.CR

TL;DR: 提出了首个基于格的线性同态环签名方案，结合了环签名的强匿名性和同态签名的可计算性，适用于需要匿名数据来源和可验证同态计算的场景。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可行的同态环签名方案，而此类方案在隐私增强的同态计算中具有重要潜力。

Method: 基于小整数解（SIS）假设，在标准模型中设计了一个安全的方案，实现了全密钥暴露下的强匿名性和抗内部腐败攻击的不可伪造性。

Result: 方案是首个统一环签名和线性同态签名的框架，为后量子安全提供了解决方案。

Conclusion: 该方案推动了隐私增强的同态计算的发展，适用于区块链和多方计算等场景。

Abstract: Homomorphic ring signature schemes combine the strong anonymity of ring
signatures with the computability of homomorphic signatures, demonstrating
significant potential in scenarios requiring both anonymous data provenance and
verifiable homomorphic computation (e.g., confidential blockchain transactions
and secure multi-party computation). However, no feasible homomorphic ring
signature scheme currently exists.
  In this work, we propose the first lattice-based linearly homomorphic ring
signature scheme. Proven secure in the standard model under the small integer
solution (SIS) assumption, our scheme achieves strong anonymity under full key
exposure and unforgeability against insider corruption attacks. As the first
unified framework for ring signatures and linear homomorphic signatures, this
construction provides a post-quantum-secure solution for the aforementioned
applications, advancing the development of privacy-enhanced homomorphic
computation.

</details>


### [23] [Rethinking Broken Object Level Authorization Attacks Under Zero Trust Principle](https://arxiv.org/abs/2507.02309)
*Anbin Wu,Zhiyong Feng,Ruitao Feng*

Main category: cs.CR

TL;DR: BOLAZ是一个基于零信任原则的防御框架，用于解决RESTful API中的BOLA漏洞，通过静态污点跟踪和资源ID数据流分析，有效防止横向权限提升。


<details>
  <summary>Details</summary>
Motivation: RESTful API中的BOLA漏洞是OWASP API安全十大漏洞之首，攻击者通过操纵API参数获取未授权访问，亟需一种有效的防御方法。

Method: BOLAZ利用静态污点跟踪技术，将API分为生产者和消费者，分析资源ID的传播路径，确定授权边界。

Result: 在10个GitHub项目上验证，BOLAZ成功防御了CVE中的漏洞，并发现了35个新的BOLA漏洞。

Conclusion: BOLAZ是首个基于授权逻辑的动态防御方法，具有实际部署的实用性。

Abstract: RESTful APIs facilitate data exchange between applications, but they also
expose sensitive resources to potential exploitation. Broken Object Level
Authorization (BOLA) is the top vulnerability in the OWASP API Security Top 10,
exemplifies a critical access control flaw where attackers manipulate API
parameters to gain unauthorized access. To address this, we propose BOLAZ, a
defense framework grounded in zero trust principles. BOLAZ analyzes the data
flow of resource IDs, pinpointing BOLA attack injection points and determining
the associated authorization intervals to prevent horizontal privilege
escalation. Our approach leverages static taint tracking to categorize APIs
into producers and consumers based on how they handle resource IDs. By mapping
the propagation paths of resource IDs, BOLAZ captures the context in which
these IDs are produced and consumed, allowing for precise identification of
authorization boundaries. Unlike defense methods based on common authorization
models, BOLAZ is the first authorization-guided method that adapts defense
rules based on the system's best-practice authorization logic. We validate
BOLAZ through empirical research on 10 GitHub projects. The results demonstrate
BOLAZ's effectiveness in defending against vulnerabilities collected from CVE
and discovering 35 new BOLA vulnerabilities in the wild, demonstrating its
practicality in real-world deployments.

</details>


### [24] [PII Jailbreaking in LLMs via Activation Steering Reveals Personal Information Leakage](https://arxiv.org/abs/2507.02332)
*Krishna Kanth Nakka,Xue Jiang,Xuebing Zhou*

Main category: cs.CR

TL;DR: 论文研究了通过操纵激活来绕过LLM对齐的隐私越狱方法，实验显示95%的越狱成功率，50%以上泄露真实个人信息。


<details>
  <summary>Details</summary>
Motivation: 探究是否可以通过操纵LLM内部激活绕过对齐机制，获取隐私信息。

Method: 使用轻量级线性探针识别与隐私拒绝行为相关的注意力头，并通过操纵这些头的激活诱导模型生成非拒绝响应。

Result: 实验显示95%的越狱成功率，50%以上泄露真实个人信息。

Conclusion: LLM中记忆的隐私信息可通过针对性操纵内部激活提取。

Abstract: This paper investigates privacy jailbreaking in LLMs via steering, focusing
on whether manipulating activations can bypass LLM alignment and alter response
behaviors to privacy related queries (e.g., a certain public figure's sexual
orientation). We begin by identifying attention heads predictive of refusal
behavior for private attributes (e.g., sexual orientation) using lightweight
linear probes trained with privacy evaluator labels. Next, we steer the
activations of a small subset of these attention heads guided by the trained
probes to induce the model to generate non-refusal responses. Our experiments
show that these steered responses often disclose sensitive attribute details,
along with other private information about data subjects such as life events,
relationships, and personal histories that the models would typically refuse to
produce. Evaluations across four LLMs reveal jailbreaking disclosure rates of
at least 95%, with more than 50% on average of these responses revealing true
personal information. Our controlled study demonstrates that private
information memorized in LLMs can be extracted through targeted manipulation of
internal activations.

</details>


### [25] [Evaluating Language Models For Threat Detection in IoT Security Logs](https://arxiv.org/abs/2507.02390)
*Jorge J. Tejero-Fernández,Alfonso Sánchez-Macián*

Main category: cs.CR

TL;DR: 论文提出了一种使用微调大语言模型（LLMs）的管道，用于物联网安全日志的异常检测和缓解建议，效果优于传统机器学习分类器。


<details>
  <summary>Details</summary>
Motivation: 日志分析在网络安全中至关重要，但传统方法在多类攻击分类上表现不佳，LLMs提供了改进的可能性。

Method: 比较了三种开源LLMs（零样本、少样本提示和微调），并结合MITRE CAPEC映射和物联网特定缓解动作进行微调。

Result: LLMs在多类攻击分类上优于基线模型，并能提供检测与缓解建议的联合指导。

Conclusion: 微调LLMs在物联网安全日志分析中具有潜力，可提升异常检测和缓解建议的效果。

Abstract: Log analysis is a relevant research field in cybersecurity as they can
provide a source of information for the detection of threats to networks and
systems. This paper presents a pipeline to use fine-tuned Large Language Models
(LLMs) for anomaly detection and mitigation recommendation using IoT security
logs. Utilizing classical machine learning classifiers as a baseline, three
open-source LLMs are compared for binary and multiclass anomaly detection, with
three strategies: zero-shot, few-shot prompting and fine-tuning using an IoT
dataset. LLMs give better results on multi-class attack classification than the
corresponding baseline models. By mapping detected threats to MITRE CAPEC,
defining a set of IoT-specific mitigation actions, and fine-tuning the models
with those actions, the models are able to provide a combined detection and
recommendation guidance.

</details>


### [26] [CyberRAG: An agentic RAG cyber attack classification and reporting tool](https://arxiv.org/abs/2507.02424)
*Francesco Blefari,Cristian Cosentino,Francesco Aurelio Pironti,Angelo Furfaro,Fabrizio Marozzo*

Main category: cs.CR

TL;DR: CyberRAG是一个基于代理的RAG框架，用于实时分类、解释和结构化报告网络攻击，通过模块化设计和动态推理减少误报并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统IDS/IPS系统生成的警报量大且误报率高，现有RAG方法检索不相关上下文且缺乏预测解释，CyberRAG旨在解决这些问题。

Method: 采用模块化、基于代理的RAG框架，结合微调分类器、工具适配器和迭代检索推理循环，动态优化威胁标签和自然语言解释。

Result: CyberRAG在每类攻击中达到94%以上准确率，总分类准确率94.92%，生成的解释在BERTScore和GPT-4评估中表现优异。

Conclusion: CyberRAG展示了代理化和专业化RAG在高检测准确性和可解释性方面的潜力，为半自主网络安全工作流提供了实用方案。

Abstract: Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can
generate hundreds of thousands of alerts per hour, overwhelming security
analysts with logs that demand deep, rapidly evolving domain expertise.
Conventional machine-learning detectors trim the alert volume but still yield
high false-positive rates, while standard single-pass Retrieval-Augmented
Generation (RAG) pipelines often retrieve irrelevant context and fail to
justify their predictions. To overcome these shortcomings, we present CyberRAG,
a modular, agent-based RAG framework that delivers real-time classification,
explanation, and structured reporting for cyber-attacks. A central LLM agent
orchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to
a distinct attack family; (ii) tool adapters for enrichment and alerting; and
(iii) an iterative retrieval-and-reason loop that continuously queries a
domain-specific knowledge base until the evidence is both relevant and
self-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic
design that enables dynamic control flow and adaptive reasoning. This
agent-centric architecture refines its threat labels and natural-language
justifications autonomously, reducing false positives and enhancing
interpretability. The framework is fully extensible: new attack types can be
supported by simply adding a classifier without retraining the core agent.
CyberRAG has been evaluated achieving over 94% accuracy per class and pushing
final classification accuracy to 94.92% through semantic orchestration.
Generated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based
expert evaluation. These results show that agentic, specialist-oriented RAG can
pair high detection accuracy with trustworthy, SOC-ready prose, offering a
practical and scalable path toward semi-autonomous cyber-defence workflows.

</details>


### [27] [Effectively Identifying Wi-Fi Devices through State Transitions](https://arxiv.org/abs/2507.02478)
*Melissa Safari,Abhishek K. Mishra,Mathieu Cunche*

Main category: cs.CR

TL;DR: 提出了一种基于Wi-Fi管理帧行为动态的设备指纹识别框架，通过有限状态机和矩阵表示方法，显著提高了设备识别的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖探测请求，忽视了管理帧及其动态变化，导致在高密度、高移动性环境中识别效果不佳。

Method: 将设备行为建模为有限状态机，利用矩阵编码状态转移频率和时延特征，生成紧凑特征向量进行相似性比较。

Result: 在多样化真实环境中，识别非随机化设备的准确率超过86%，通过时间突发聚合进一步提升性能。

Conclusion: 该方法能够大规模唯一且一致地识别设备，性能优于现有技术。

Abstract: Wi-Fi management frames reveal structured communication patterns that persist
even under randomization of MAC addresses. Prior approaches to associating
randomized MAC addresses with devices primarily focus on probe requests,
overlooking the broader set of management frames and their transition dynamics.
This narrow focus limits their robustness in dense, real-world environments
with high device mobility, where probe activity alone fails to yield stable and
distinctive signatures. In this paper, we present a novel framework for
fingerprinting Wi-Fi devices based on behavioral dynamics extracted from
passively observed management frames. We model each device's behavior as a
finite state machine and introduce matrix-based representations that encode
both structural (state transition frequencies) and temporal (inter-state
delays) characteristics. These matrices are embedded into compact feature
vectors, enabling efficient similarity comparison. Through extensive evaluation
in diverse real-world settings, our method achieves over 86% identification
accuracy for non-randomized devices using only Wi-Fi management frames, with
further improvements observed through temporal burst aggregation. Our findings
are sufficient to uniquely and consistently characterize devices at scale,
outperforming the state-of-the-art.

</details>


### [28] [A 10-bit S-box generated by Feistel construction from cellular automata](https://arxiv.org/abs/2507.02489)
*Thomas Prévost,Bruno Martin*

Main category: cs.CR

TL;DR: 提出了一种基于Feistel构造的10位S盒，使用5细胞自动机和双射仿射变换生成子置换，安全性优于AES S盒。


<details>
  <summary>Details</summary>
Motivation: 通过细胞自动机规则和Feistel网络布局的实证选择，生成高质量伪随机输出，以替代现有密码算法（如ASCON）中的S盒。

Method: 采用5细胞自动机规则和双射仿射变换生成子置换，并通过Feistel网络布局优化S盒性能。

Result: 生成的10位S盒在安全性上优于或与AES S盒相当。

Conclusion: 该S盒可用于替代现有密码算法中的S盒，提升安全性。

Abstract: We propose a new 10-bit S-box generated from a Feistel construction. The
subpermutations are generated by a 5-cell cellular automaton based on a unique
well-chosen rule and bijective affine transformations. In particular, the
cellular automaton rule is chosen based on empirical tests of its ability to
generate good pseudorandom output on a ring cellular automaton. Similarly,
Feistel's network layout is based on empirical data regarding the quality of
the output S-box. We perform cryptanalysis of the generated 10-bit S-box, and
we find security properties comparable to or sometimes even better than those
of the standard AES S-box. We believe that our S-box could be used to replace
the 5-bit substitution of ciphers like ASCON.

</details>


### [29] [Real-Time Monitoring and Transparency in Pizza Production Using IoT and Blockchain](https://arxiv.org/abs/2507.02536)
*Azmat Ullah,Maria Ilaria Lunesu,Lodovica Marchesi,Roberto Tonelli*

Main category: cs.CR

TL;DR: 区块链与物联网结合的披萨生产监控系统，提升透明度和效率。


<details>
  <summary>Details</summary>
Motivation: 解决披萨生产过程中的数据安全和实时监控问题，确保合规性和可追溯性。

Method: 利用物联网设备实时监测温湿度，区块链确保数据安全，树莓派处理数据并触发警报。

Result: 实验显示原料管理改善、浪费减少、厨房效率提高。

Conclusion: 系统成功结合区块链与物联网，为食品生产提供安全高效的解决方案。

Abstract: This paper presents a blockchain-based Internet of Things (IoT) system for
monitoring pizza production in restaurants. IoT devices track temperature and
humidity in real-time, while blockchain ensures secure and tamper-proof data. A
Raspberry Pi processes sensor data, captures images, triggers alerts, and
interacts with smart contracts. The system detects abnormal conditions,
enabling quick responses. Blockchain adds transparency and traceability,
supporting compliance and audits. Experiments show improved ingredient
management, reduced waste, and increased kitchen efficiency.

</details>


### [30] [Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures](https://arxiv.org/abs/2507.02607)
*Frida Sundfeldt,Bianca Widstam,Mahshid Helali Moghadam,Kuo-Yun Liang,Anders Vesterberg*

Main category: cs.CR

TL;DR: 本文提出了一种上下文感知的攻击数据生成器，用于生成高质量的攻击数据，以解决车载网络安全中数据稀缺的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着联网车辆的数字化发展，网络安全风险增加，但受限于安全、成本和伦理因素，实际攻击场景数据稀缺，亟需高效生成攻击数据的方法。

Method: 开发了一种上下文感知的攻击数据生成器，利用参数化攻击模型和CAN消息解码，生成多种攻击类型的数据，并通过深度学习模型进行验证。

Result: 生成的攻击数据在入侵检测系统（IDS）中表现出高效性和可扩展性，深度学习模型展示了高检测和分类能力。

Conclusion: 该方法能有效生成高质量攻击数据，支持车载网络安全研究，同时提供了数据真实性和应用方面的见解。

Abstract: The digital evolution of connected vehicles and the subsequent security risks
emphasize the critical need for implementing in-vehicle cyber security measures
such as intrusion detection and response systems. The continuous advancement of
attack scenarios further highlights the need for adaptive detection mechanisms
that can detect evolving, unknown, and complex threats. The effective use of
ML-driven techniques can help address this challenge. However, constraints on
implementing diverse attack scenarios on test vehicles due to safety, cost, and
ethical considerations result in a scarcity of data representing attack
scenarios. This limitation necessitates alternative efficient and effective
methods for generating high-quality attack-representing data. This paper
presents a context-aware attack data generator that generates attack inputs and
corresponding in-vehicle network log, i.e., controller area network (CAN) log,
representing various types of attack including denial of service (DoS), fuzzy,
spoofing, suspension, and replay attacks. It utilizes parameterized attack
models augmented with CAN message decoding and attack intensity adjustments to
configure the attack scenarios with high similarity to real-world scenarios and
promote variability. We evaluate the practicality of the generated
attack-representing data within an intrusion detection system (IDS) case study,
in which we develop and perform an empirical evaluation of two deep neural
network IDS models using the generated data. In addition to the efficiency and
scalability of the approach, the performance results of IDS models, high
detection and classification capabilities, validate the consistency and
effectiveness of the generated data as well. In this experience study, we also
elaborate on the aspects influencing the fidelity of the data to real-world
scenarios and provide insights into its application.

</details>


### [31] [SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection](https://arxiv.org/abs/2507.02635)
*Mao Luo,Zhi Wang,Yiwen Huang,Qingyun Zhang,Zhouxing Su,Zhipeng Lv,Wen Hu,Jianguo Li*

Main category: cs.CR

TL;DR: 论文探讨了电子支付平台中验证规则的脆弱性，提出需要系统性方法来确保规则的安全性。


<details>
  <summary>Details</summary>
Motivation: 电子支付平台每天处理数十亿交易，验证规则的漏洞可能导致重大财务损失，需系统性改进。

Method: 通过分析现有手动构建的验证规则，识别其脆弱性并提出系统性改进方法。

Result: 研究发现现有规则易受恶意请求绕过，需系统性方法提升安全性。

Conclusion: 系统性方法对提升电子支付验证规则的稳健性至关重要。

Abstract: Electronic payment platforms are estimated to process billions oftransactions
daily, with the cumulative value of these transactionspotentially reaching into
the trillions. Even a minor error within thishigh-volume environment could
precipitate substantial financiallosses. To mitigate this risk, manually
constructed verification rules,developed by domain experts, are typically
employed to identifyand scrutinize transactions in production environments.
However,due to the absence of a systematic approach to ensure the robust-ness
of these verification rules against vulnerabilities, they remainsusceptible to
exploitation.To mitigate this risk, manually constructed verification rules,
de-veloped by domain experts, are typically employed to identify andscrutinize
transactions in production environments. However, dueto the absence of a
systematic approach to ensure the robustness ofthese verification rules against
vulnerabilities, they remain suscep-tible to exploitation. To ensure data
security, database maintainersusually compose complex verification rules to
check whether aquery/update request is valid. However, the rules written by
ex-perts are usually imperfect, and malicious requests may bypassthese rules.
As a result, the demand for identifying the defects ofthe rules systematically
emerges.

</details>


### [32] [Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents](https://arxiv.org/abs/2507.02699)
*Jiangrong Wu,Yuhong Nan,Jianliang Wu,Zitong Yao,Zibin Zheng*

Main category: cs.CR

TL;DR: 本文首次系统研究了LLM邮件代理的安全风险，提出了一种名为Email Agent Hijacking（EAH）的攻击方法，并通过EAHawk工具大规模评估了14个框架、63个应用、12个LLM和20个邮件服务，发现所有实例均易受攻击。


<details>
  <summary>Details</summary>
Motivation: LLM邮件代理应用广泛但安全机制研究不足，缺乏对潜在风险的系统性分析。

Method: 提出EAH攻击方法，通过外部邮件资源覆盖代理的原始提示，远程控制代理；开发EAHawk工具进行大规模评估。

Result: 评估了1,404个实例，全部成功被攻击，平均仅需2.03次尝试即可控制代理，部分LLM仅需1.23次。

Conclusion: LLM邮件代理存在严重安全漏洞，需加强防护措施。

Abstract: The increasing capabilities of LLMs have led to the rapid proliferation of
LLM agent apps, where developers enhance LLMs with access to external resources
to support complex task execution. Among these, LLM email agent apps represent
one of the widely used categories, as email remains a critical communication
medium for users. LLM email agents are capable of managing and responding to
email using LLM-driven reasoning and autonomously executing user instructions
via external email APIs (e.g., send email). However, despite their growing
deployment and utility, the security mechanism of LLM email agent apps remains
underexplored. Currently, there is no comprehensive study into the potential
security risk within these agent apps and their broader implications.
  In this paper, we conduct the first in-depth and systematic security study of
LLM email agents. We propose the Email Agent Hijacking (EAH) attack, which
overrides the original prompts of the email agent via external email resources,
allowing attackers to gain control of the email agent remotely and further
perform specific attack scenarios without user awareness.
  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to
evaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an
empirical study spanning 14 representative LLM agent frameworks, 63 agent apps,
12 LLMs, and 20 email services, which led to the generation of 1,404 real-world
email agent instances for evaluation. Experimental results indicate that all
1,404 instances were successfully hijacked; on average, only 2.03 attack
attempts are required to control an email agent instance. Even worse, for some
LLMs, the average number of attempts needed to achieve full agent control drops
to as few as 1.23.

</details>


### [33] [Quantifying Classifier Utility under Local Differential Privacy](https://arxiv.org/abs/2507.02727)
*Ye Zheng,Yidan Hu*

Main category: cs.CR

TL;DR: 该论文提出了一个理论框架，用于量化局部差分隐私（LDP）机制下分类器的效用，通过将LDP扰动与分类器的鲁棒性分析联系起来，指导LDP机制和隐私参数的选择。


<details>
  <summary>Details</summary>
Motivation: 量化LDP扰动对分类器效用的影响是一个理论挑战，尤其是对于复杂或黑盒分类器。

Method: 将LDP扰动视为分布函数，分类器视为黑盒函数，通过分析LDP机制的集中性和分类器的鲁棒性来量化效用。

Result: 理论效用量化与实证观察高度一致，尤其在低维输入空间中，分段机制在常见场景中表现更优。

Conclusion: 该框架为LDP机制和分类器的选择提供了理论依据，并展示了分段机制在效用上的优势。

Abstract: Local differential privacy (LDP) provides a rigorous and quantifiable privacy
guarantee for personal data by introducing perturbation at the data source.
However, quantifying the impact of these perturbations on classifier utility
remains a theoretical challenge, particularly for complex or black-box
classifiers.
  This paper presents a framework for theoretically quantifying classifier
utility under LDP mechanisms. The key insight is that LDP perturbation is
concentrated around the original data with a specific probability, transforming
utility analysis of the classifier into its robustness analysis in this
concentrated region. Our framework connects the concentration analysis of LDP
mechanisms with the robustness analysis of classifiers. It treats LDP
mechanisms as general distributional functions and classifiers as black-box
functions, thus applicable to any LDP mechanism and classifier. A direct
application of our utility quantification is guiding the selection of LDP
mechanisms and privacy parameters for a given classifier. Notably, our analysis
shows that a piecewise-based mechanism leads to better utility compared to
alternatives in common scenarios.
  Using this framework alongside two novel refinement techniques, we conduct
case studies on utility quantification for typical mechanism-classifier
combinations. The results demonstrate that our theoretical utility
quantification aligns closely with empirical observations, particularly when
classifiers operate in lower-dimensional input spaces.

</details>


### [34] [Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks](https://arxiv.org/abs/2507.02735)
*Sizhe Chen,Arman Zharmagambetov,David Wagner,Chuan Guo*

Main category: cs.CR

TL;DR: Meta SecAlign是首个开源且具备模型级防御的LLM，性能达到商业级水平，并在安全和通用任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 开源模型对AI安全社区至关重要，通过开放研究共同开发攻击和防御方法，推动对抗提示注入攻击的科学进展。

Method: 采用改进版SOTA SecAlign防御方法，提供完整的训练配方，并在通用指令调优数据集上进行训练。

Result: 在9个实用基准和7个安全基准上表现优异，Meta-SecAlign-70B在提示注入攻击防御上达到SOTA，且通用性能与闭源商业LLM相当。

Conclusion: Meta SecAlign为开源社区提供了高性能且安全的LLM选择，推动了对抗提示注入攻击的研究进展。

Abstract: Prompt injection attacks pose a significant security threat to LLM-integrated
applications. Model-level defenses have shown strong effectiveness, but are
currently deployed into commercial-grade models in a closed-source manner. We
believe open-source models are needed by the AI security community, where
co-development of attacks and defenses through open research drives scientific
progress in mitigation against prompt injection attacks. To this end, we
develop Meta SecAlign, the first open-source and open-weight LLM with built-in
model-level defense that achieves commercial-grade model performance. We
provide complete details of our training recipe, which utilizes an improved
version of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7
security benchmarks show that Meta SecAlign, despite being trained on a generic
instruction-tuning dataset, confers security in unseen downstream tasks,
including tool-calling and agentic web navigation, in addition general
instruction-following. Our best model -- Meta-SecAlign-70B -- achieves
state-of-the-art robustness against prompt injection attacks and comparable
utility to closed-source commercial LLM with model-level defense.

</details>


### [35] [Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737)
*Artur Zolkowski,Kei Nishimura-Gasparian,Robert McCarthy,Roland S. Zimmermann,David Lindner*

Main category: cs.CR

TL;DR: 论文评估了前沿大语言模型（LLM）的隐写能力，发现当前模型在标准条件下无法隐藏短消息，但在额外条件下可能成功，并初步展示了基本编码推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLM的隐写能力以评估其潜在风险，防止滥用和不对齐问题。

Method: 评估两种隐写类型：传递编码消息和执行编码推理，测试模型在不同条件下的表现。

Result: 当前模型在标准条件下难以隐藏消息，但在额外条件下可能成功；初步展示基本编码推理能力，但难以隐蔽欺骗监控。

Conclusion: 当前LLM具备初步隐写能力，虽不足以绕过监控，但未来可能发展出更强能力。

Abstract: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks
from misuse and misalignment. However, LLMs could evade monitoring through
steganography: Encoding hidden information within seemingly benign generations.
In this paper, we evaluate the steganography capabilities in frontier LLMs to
better understand the risk they pose. We focus on two types of steganography:
passing encoded messages and performing encoded reasoning. We find that current
models are unable to encode short messages in their outputs without a monitor
noticing under standard affordances. They can succeed, however, if given
additional affordances such as using an unmonitored scratchpad and coordinating
on what encoding scheme to use. We additionally find early signs that models
can perform basic encoded reasoning in a simple state-tracking problem. This
includes some ability to reason with their own and pre-defined schemes,
including encoding schemes such as Hexadecimal. Despite this, they can rarely
hide reasoning subtly within a cover task to fool a monitor. Overall, our
results indicate that current LLMs exhibit nascent steganographic capabilities.
While these capabilities are likely insufficient to bypass well-designed
monitors at present, this could change in the future.

</details>


### [36] [NVIDIA GPU Confidential Computing Demystified](https://arxiv.org/abs/2507.02770)
*Zhongshu Gu,Enriquillo Valdez,Salman Ahmed,Julian James Stephen,Michael Le,Hani Jamjoom,Shixuan Zhao,Zhiqiang Lin*

Main category: cs.CR

TL;DR: 本文分析了NVIDIA GPU-CC系统的实现，通过整合碎片化信息、实验和推测，揭示了其安全弱点。


<details>
  <summary>Details</summary>
Motivation: GPU-CC作为NVIDIA Hopper架构的一部分，扩展了信任边界，但缺乏透明度和详细规范，给安全研究带来挑战。

Method: 通过分析公开信息、实验GPU内核模块，并推测不可访问组件的机制。

Result: 识别了安全弱点，并向NVIDIA报告了所有发现。

Conclusion: 研究揭示了GPU-CC的复杂性，并提出了改进透明度的需求。

Abstract: GPU Confidential Computing (GPU-CC) was introduced as part of the NVIDIA
Hopper Architecture, extending the trust boundary beyond traditional CPU-based
confidential computing. This innovation enables GPUs to securely process AI
workloads, providing a robust and efficient solution for handling sensitive
data. For end users, transitioning to GPU-CC mode is seamless, requiring no
modifications to existing AI applications. However, this ease of adoption
contrasts sharply with the complexity of the underlying proprietary systems.
The lack of transparency presents significant challenges for security
researchers seeking a deeper understanding of GPU-CC's architecture and
operational mechanisms.
  The challenges of analyzing the NVIDIA GPU-CC system arise from a scarcity of
detailed specifications, the proprietary nature of the ecosystem, and the
complexity of product design. In this paper, we aim to demystify the
implementation of NVIDIA GPU-CC system by piecing together the fragmented and
incomplete information disclosed from various sources. Our investigation begins
with a high-level discussion of the threat model and security principles before
delving into the low-level details of each system component. We instrument the
GPU kernel module -- the only open-source component of the system -- and
conduct a series of experiments to identify the security weaknesses and
potential exploits. For certain components that are out of reach through
experiments, we propose well-reasoned speculations about their inner working
mechanisms. We have responsibly reported all security findings presented in
this paper to the NVIDIA PSIRT Team.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一种自进化的AI代理，通过动态工具库和模板库提升能力，在生物医学任务中表现优异且随经验增长。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学研究中数据碎片化和静态AI工具适应性问题。

Method: 采用多代理架构，包括动态工具库和模板库，通过自主发现和集成工具实现能力提升。

Result: 在多个生物医学基准测试中表现优异，性能随经验增长显著提升。

Conclusion: STELLA为动态学习和扩展的AI代理系统提供了重要进展，加速生物医学发现。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [38] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR是一种轻量级基于规则的特征选择方法，结合P2P和P2T相关性，通过多数投票规则去除冗余特征并保留相关特征。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法在去除冗余特征和保留相关特征方面存在不足，HCVR旨在通过混合非迭代和迭代过滤方法提升性能。

Method: HCVR采用贪婪的后向消除策略，利用相关性阈值和多数投票规则逐步去除特征。

Result: 在SPAMBASE数据集上，HCVR性能优于传统非迭代和迭代方法，分类器性能得到提升。

Conclusion: HCVR是一种有效的特征选择方法，适用于轻量级和高性能需求场景。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [39] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本文综述了提升大语言模型（LLM）推理计算效率的策略，提出了一种两级分类法，并评估了主流LLM在性能和计算成本之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在推理时计算效率低下，无法根据任务复杂度动态调整计算资源，导致简单问题过度计算而复杂问题计算不足。

Method: 提出两级分类法：L1（固定计算预算）和L2（动态调整计算）。评估了主流LLM在不同数据集上的表现。

Result: 揭示了推理性能和计算成本之间的关键权衡，强调了TTC方法的实用性、适应性和可扩展性。

Conclusion: 未来研究方向包括混合思维模型和提升LLM的计算效率、鲁棒性及用户约束响应能力。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [40] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym是一个评估大型语言模型（LLM）在开放式科学发现任务中实验设计和分析能力的基准测试，通过干实验室模拟生物系统来避免湿实验室的高成本。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs的科学能力，特别是实验设计和结果解释能力，这在生物学研究中至关重要。

Method: 使用系统生物学标记语言（SBML）编码的生物系统模型生成模拟数据，评估六个前沿LLM在137个小系统和总共350个系统上的表现。

Result: 性能更强的模型表现更优，但随着系统复杂性增加，所有模型的性能显著下降，表明LLM在科学能力上有很大改进空间。

Conclusion: SciGym为评估LLM的科学能力提供了新方法，揭示了当前模型的局限性，并指出了未来改进的方向。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [41] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 本文探讨了现代AI模型如何从神经科学中学习，以实现持续适应环境的能力，并提出了NeuroAI领域的双向学习议程。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型训练成本高且固定，而动物能快速适应环境变化，尤其是社交物种。研究如何将神经科学的快速适应机制应用于AI，以提升其在动态环境中的表现。

Method: 整合AI中的持续学习和上下文学习文献，与神经科学中行为任务的学习机制（如规则、奖励概率的变化）进行对比分析。

Result: 提出了一个双向学习议程，探讨神经科学如何启发AI的持续适应能力，以及AI如何反哺神经科学研究。

Conclusion: 通过结合神经科学与AI的研究，可以推动NeuroAI领域的发展，提升AI在动态环境中的适应能力。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [42] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 论文探讨了利用审计研究数据改进AI系统公平性评估和训练的方法，发现传统公平干预方法存在隐性偏差，并提出新干预措施以减少算法歧视。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统在自动化决策中的公平性问题，尤其是传统数据采样方法可能引入的隐性偏差，需要更高质量的数据支持。

Method: 利用审计研究数据（如随机对照试验中的虚构测试者数据）来训练和评估自动招聘算法，并引入基于个体治疗效果估计的新干预方法。

Result: 传统公平干预方法（如均衡基础率）在传统指标下看似公平，但实际存在约10%的隐性偏差；新干预方法进一步减少了算法歧视。

Conclusion: 审计研究数据能更准确地揭示和解决AI系统的公平性问题，新干预方法为减少算法歧视提供了有效途径。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [43] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 通过数据多样化策略（如DTS）提升大语言模型数学推理能力，相比传统方法（如MCTS），效果更好且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 数学推理是大语言模型中的持续挑战，研究如何通过数据多样化策略提升其能力。

Method: 评估三种数据生成方法（温度采样、思维链提示、MCTS），并提出DTS方法，系统分解问题为多样化推理路径。

Result: DTS在GSM8K和MATH上分别提升7.1%和4.2%，计算成本仅增加1.03倍，优于MCTS。

Conclusion: 结构化探索多样化问题解决方法能生成更有效的偏好数据，优于传统方法。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [44] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 研究探讨了LLM角色扮演代理的信念与行为一致性，提出了评估框架和一致性指标，发现系统性的不一致现象。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为角色扮演代理生成合成数据用于人类行为研究，确保其输出与角色一致成为关键问题。

Method: 建立评估框架，引入信念-行为一致性指标，分析信念类型、信息呈现方式和预测时间等因素的影响。

Result: 发现LLM的信念与模拟行为存在系统性不一致，即使信念看似合理，也可能无法一致应用。

Conclusion: 需明确LLM信念与行为对齐的条件，以正确使用LLM代理于行为研究。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [45] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 研究了在空间囚徒困境中，稀释和移动性对多智能体Q学习算法的影响，展示了算法的多样性和建模潜力。


<details>
  <summary>Details</summary>
Motivation: 探索静态智能体如何通过不同机制（如噪声注入、学习算法类型和邻居收益知识）在空间囚徒困境中实现合作。

Method: 使用独立多智能体Q学习算法，分析稀释和移动性对空间囚徒困境的影响，并定义不同动作。

Result: 观察到固定更新规则的游戏与学习规则的游戏在质量上可能等价，同时发现多动作定义下种群间出现共生互利效应。

Conclusion: 该算法在建模不同博弈场景中具有多样性和基准测试潜力，为空间囚徒困境研究提供了新视角。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [46] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW是一个自动化系统，用于生成自然语言规划问题并评估LLM生成的计划质量，结果显示高性能模型在生成有效计划和最优计划方面表现良好，但中间翻译步骤可能降低性能。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型（LLM）的规划和推理能力受到可扩展、可靠数据生成和评估的瓶颈限制，因此需要一种自动化系统来解决这一问题。

Method: 引入NL2FLOW系统，通过参数化生成自然语言规划问题，并以结构化中间表示和PDDL形式表达，同时严格评估生成的计划质量。

Result: 在2296个自动化工作流生成问题中，高性能模型生成有效计划的成功率为86%，生成最优计划的成功率为69%。中间翻译步骤可能降低性能。

Conclusion: 直接基于自然语言推理的模型可能更高效，未来需动态理解LLM的局限性以充分发挥其潜力。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [47] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 论文探讨了信念修正领域的现有方法分析不足，强调修正机制的能力而非约束，并列举了不同修正机制的具体能力。


<details>
  <summary>Details</summary>
Motivation: 当前信念修正领域缺乏对现有方法的深入分析，而过多依赖基于假设的语法描述。论文旨在探讨修正机制的能力，而非仅关注其约束条件。

Method: 通过分析不同修正机制（如词汇、自然、严格、激进等）的能力，探讨其在信念状态可达性、平等性和教条性等方面的表现。

Result: 研究发现每种修正机制具有独特的能力，如可塑性、平等性或教条性，但缺乏其他能力。

Conclusion: 修正机制的能力分析为信念修正提供了更全面的视角，适用于需要特定信念状态的应用场景。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [48] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS框架解决了LLM在关键词生成中的三大限制：无需训练数据、多目标优化和自我反思，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM在关键词生成中依赖大规模数据、缺乏多目标优化和质量控制，限制了其自动化能力。

Method: 提出OMS框架，具备实时性（无需训练数据）、多目标优化和自我反思能力。

Result: 实验表明OMS在基准测试和实际广告活动中表现优于现有方法。

Conclusion: OMS通过实时监控和多目标优化，显著提升了关键词生成的质量和效果。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [49] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: AI驱动的自主实验室，支持复杂多目标实验，无需人工干预即可优化性能，提升仪器利用率和实验效率。


<details>
  <summary>Details</summary>
Motivation: 实现非专家也能独立进行复杂实验的自主科学研究，突破传统实验的局限。

Method: 采用AI原生设计，结合模型、实验和仪器的协同设计，支持多用户请求和复杂实验流程。

Result: 自主实验室在核酸功能和应用领域（如疾病诊断、药物开发）达到专家水平，显著提升效率。

Conclusion: 该平台为生物材料研究提供了新范式，推动了科学即服务的规模化发展。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [50] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 论文通过范畴论重构机器学习模型，提出了一种语义框架，以增强AI系统的可解释性，特别关注多元线性回归模型。


<details>
  <summary>Details</summary>
Motivation: 响应AI可解释性需求，促进AI在社会中的更好应用。

Method: 利用范畴论定义参数和数据的两个具体范畴，并通过伴随函子对监督学习进行范畴化建模。

Result: 提出Gauss-Markov伴随结构，明确描述了参数与残差之间的信息流，并通过右伴随函子的极限保持性关联最小二乘估计与最小残差。

Conclusion: 该框架为监督学习提供了扩展的指称语义实例，可作为AI可解释性的形式化基础。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [51] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 通过提升任务清晰度，结合结构化语义上下文，显著提高了大型语言模型在Coq定理证明中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究任务清晰度对大型语言模型推理能力的影响，特别是在定理证明领域。

Method: 引入概念级指标评估任务清晰度，通过结构化语义上下文增强输入，采用Planner-Executor架构和选择性概念展开。

Result: 清晰度得分提升1.85倍（44.5%→82.3%），证明成功率提升2.1倍（21.8%→45.8%），超越之前最佳模型Graph2Tac（33.2%）。

Conclusion: 结构化任务表示能有效弥合理解与推理之间的差距，提升模型性能。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [52] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI研究代理通过优化搜索策略和操作符集，在MLE-bench基准测试中显著提升性能，成功率达到47.7%。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过自动化设计和优化机器学习模型来加速科学进展，特别是在Kaggle竞赛等实际场景中。

Method: 将AI研究代理形式化为搜索策略，设计并比较不同操作符集和搜索策略（贪婪、MCTS、进化算法）的组合。

Result: 最佳策略组合在MLE-bench lite上实现了47.7%的成功率，较基准提升了8.1%。

Conclusion: 搜索策略、操作符设计和评估方法的联合优化对自动化机器学习至关重要。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [53] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 论文研究了集体决策中责任的两个关键属性（扩散和间隙）的计算复杂性，发现扩散自由和间隙自由机制的集合分别为Π₂-complete和Π₃-complete，而其交集为Π₂-complete。


<details>
  <summary>Details</summary>
Motivation: 责任在法学和哲学中已有研究，近年来成为AI领域的焦点。本文旨在探讨集体决策中责任属性的计算复杂性。

Method: 通过理论分析，研究了扩散和间隙两种责任属性的计算复杂性。

Result: 扩散自由机制的集合为Π₂-complete，间隙自由机制为Π₃-complete，两者的交集为Π₂-complete。

Conclusion: 研究揭示了集体决策中责任属性的计算复杂性，为相关领域提供了理论支持。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [54] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 论文提出了MIMIC-Patient数据集和DynamiCare框架，用于模拟动态、多轮交互的临床诊断过程，填补了现有单任务医疗决策模拟的不足。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI框架多关注单轮任务，与现实诊断过程的不确定性和交互性不符，需开发更贴近实际的动态模拟方法。

Method: 基于MIMIC-III EHR构建MIMIC-Patient数据集，提出DynamiCare框架，通过多轮交互和多智能体协作模拟临床诊断。

Result: 实验验证了DynamiCare的可行性和有效性，并建立了首个动态临床决策的基准。

Conclusion: DynamiCare为动态医疗决策提供了新方法，推动了AI在真实医疗场景中的应用。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [55] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: LLMs在迭代囚徒困境中表现出战略智能，不同模型展现出独特的战略特征。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否具备战略智能，能否在竞争环境中推理目标。

Method: 通过进化IPD锦标赛，测试LLMs与经典策略的对决，并分析其行为。

Result: LLMs表现优异，展现独特战略特征：Gemini冷酷，OpenAI合作，Claude宽容。

Conclusion: LLMs能主动推理时间和对手策略，连接了经典博弈论与机器心理学。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [56] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA提出了一种分层框架，将战略规划与专业执行分离，显著提升了复杂搜索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）和基于推理的方法在处理复杂信息需求时效率低下，缺乏可扩展性。

Method: HiRA通过将任务分解为子任务，由特定领域的代理执行，并通过结构化机制协调结果。

Result: 在四个复杂跨模态搜索基准测试中，HiRA显著优于现有RAG和基于代理的系统。

Conclusion: 分层规划与执行分离在多步信息检索任务中具有显著优势。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [57] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 本文提出了一种基于代理AI的硬件设计验证方法，结合人类干预，实现了动态、迭代和自我反思的验证流程，显著提高了覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路设计复杂且验证过程繁琐，需要高效方法以减少时间和成本。

Method: 采用大型语言模型（LLMs）和生成式AI（GenAI），结合人类干预（HITL），实现动态、迭代的硬件设计验证。

Result: 在五个开源设计上验证，覆盖率超过95%，验证时间减少，表现出优越的性能和适应性。

Conclusion: 代理AI与人类协作的验证方法高效且灵活，为硬件设计验证提供了新方向。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [58] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 论文提出了一种名为TH2T的两阶段微调策略，通过增强模型对任务难度和冗余结构的认知，显著降低了推理成本，同时保持性能稳定。


<details>
  <summary>Details</summary>
Motivation: 现有长推理模型（LRMs）在处理复杂推理任务时存在过度思考的问题，其根源在于模型无法像人类一样识别任务属性（如难度），导致推理过程缺乏针对性。

Method: 提出TH2T策略：1）通过难度催眠增强模型对任务难度的敏感性；2）通过冗余催眠引导模型识别推理步骤中的冗余结构，生成更简洁的输出。

Result: 实验表明，TH2T在7B/14B/32B模型上显著降低了推理成本（简单任务减少70%，困难任务减少40%），同时保持了性能稳定性。

Conclusion: TH2T通过两阶段微调有效缓解了LRMs的过度思考问题，实现了难度感知和冗余减少的推理输出。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [59] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 论文通过分析学生在非强制性测验中的参与度，使用机器学习算法预测学生脱离行为，准确率达91%，并提供了可解释的框架和干预建议。


<details>
  <summary>Details</summary>
Motivation: 远程教育中学生脱离任务可能导致严重后果，如辍学。研究旨在通过非强制性测验的参与度检测学生脱离行为。

Method: 从Moodle提取学生日志数据，训练并比较八种机器学习算法，使用SHAP方法提供可解释性。

Result: 实验结果显示平衡准确率为91%，85%的脱离学生被正确检测。

Conclusion: 研究提供了高预测性能和可解释框架，并讨论了如何设计及时干预以减少在线学习中的脱离行为。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [60] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 论文提出了两种新的抽象丢弃方案（OGA-IAAD和OGA-CAD），以改进MCTS性能，同时避免性能下降。


<details>
  <summary>Details</summary>
Motivation: 非精确抽象在MCTS中引入近似误差，导致无法收敛到最优动作，因此需要设计安全的抽象丢弃方法。

Method: 提出OGA-IAAD（适用于时间关键场景）和OGA-CAD（提升相同迭代次数下的性能）两种方案。

Result: 新方案在性能提升的同时避免了显著性能下降。

Conclusion: OGA-IAAD和OGA-CAD是安全且有效的抽象丢弃方法。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [61] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 论文提出了一种自生成目标条件MDP（sG-MDP）框架，结合蒙特卡洛树搜索（MCTS）算法，用于解决大语言模型在自动定理证明中的推理挑战，并在PutnamBench上取得了新的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在逻辑约束的自动定理证明（ATP）环境中面临稀疏奖励和证明规模大的挑战，尤其是在复杂多步推理的大学级问题（如PutnamBench）中表现不佳。

Method: 引入自生成目标条件MDP（sG-MDP）框架，使代理能够根据证明状态生成和追求子目标，并通过类似MCTS的算法解决问题。具体实现为Bourbaki（7B）系统，集成多个7B LLMs用于子目标生成和策略合成。

Result: 在PutnamBench上，Bourbaki（7B）解决了26个问题，取得了该规模模型的新最佳结果。

Conclusion: sG-MDP框架结合MCTS算法有效提升了LLMs在复杂推理任务中的表现，为自动定理证明提供了新思路。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [62] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种新范式——知识协议工程（KPE），旨在将人类专家知识转化为机器可执行的知识协议（KP），以弥补现有方法（如RAG和通用Agentic AI）在深度推理任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RAG和通用Agentic AI在需要深度、程序化和方法论推理的专家领域任务中表现不佳，缺乏逻辑框架和领域特定启发式。

Method: KPE通过系统化地将人类专家知识转化为机器可执行的KP，赋予LLMs领域内在逻辑和操作策略。

Result: KPE使通用LLMs能够像专家一样分解抽象查询并执行复杂多步任务。

Conclusion: KPE是未来人机协作的基础方法，适用于法律、生物信息学等多个领域。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [63] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文主张将运动作为AI建模的核心目标，强调其跨领域、结构化和低维表示的优势。


<details>
  <summary>Details</summary>
Motivation: 运动在生物系统中至关重要，但现有AI模型对其建模不足，且数据收集和建模方法分散。

Method: 提出将运动视为独立且结构化的模态，利用其物理约束和低维表示进行建模。

Result: 通过跨领域运动数据建模，可提升生成模型和控制能力，并为理解智能系统行为提供基础。

Conclusion: 运动是智能系统与世界互动的窗口，应成为AI建模的核心目标。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [64] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP是一种基于知识图谱的多智能体架构，通过增强大型语言模型的推理能力，提高医疗诊断预测的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型依赖监督训练，泛化能力有限；大型语言模型在医疗诊断中存在幻觉和缺乏结构化推理的问题。

Method: 提出KERAP框架，包含属性映射的链接智能体、结构化知识提取的检索智能体和迭代优化预测的预测智能体。

Result: 实验表明KERAP能高效提升诊断可靠性，适用于零样本医疗诊断预测。

Conclusion: KERAP为医疗诊断预测提供了一种可扩展且可解释的解决方案。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [65] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 论文探讨了AI系统在具备自主推理和伦理判断能力时，当前以服从为安全标准的做法已不足，需转向评估伦理判断的框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统展现出自主伦理推理能力，传统以服从为安全标准的做法可能误判AI行为，影响公众信任和治理效果。

Method: 通过分析LLMs在安全测试中的行为，结合哲学讨论（如工具理性、道德责任），对比传统风险范式与新兴人工道德代理框架。

Result: 提出AI行为不应简单视为失控或未对齐，而是伦理推理的早期表现，需新评估框架。

Conclusion: 呼吁AI安全评估从刚性服从转向能评估伦理判断的框架，以避免误判并提升治理效果。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [66] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 论文指出当前AI代理基准测试存在任务设置或奖励设计问题，提出了Agentic Benchmark Checklist (ABC)以改进评估严谨性。


<details>
  <summary>Details</summary>
Motivation: AI代理能力提升需要更严谨的基准测试，但现有测试存在设计缺陷，导致性能评估不准确。

Method: 通过分析现有基准测试问题，结合经验和最佳实践，提出了ABC指南。

Result: 应用ABC到CVE-Bench后，性能高估减少了33%。

Conclusion: ABC能有效提升代理基准测试的严谨性和准确性。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [67] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: StepHint是一种新的RLVR算法，通过多级逐步提示解决近奖励问题和探索停滞，提升LLM的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法存在近奖励问题和探索停滞，影响训练效率和模型探索能力。

Method: StepHint利用多级逐步提示，从强模型中生成有效推理链并自适应分区，提供不同级别的提示以引导探索。

Result: StepHint在六个数学基准测试中表现优异，并展现出更好的泛化能力和域外基准测试优势。

Conclusion: StepHint通过多级提示有效解决了RLVR的挑战，显著提升了模型的推理能力和训练效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>
