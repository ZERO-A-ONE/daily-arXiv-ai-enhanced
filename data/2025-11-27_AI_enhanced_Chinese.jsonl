{"id": "2511.19558", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19558", "abs": "https://arxiv.org/abs/2511.19558", "authors": ["Mohammed Talha Alam", "Nada Saadi", "Fahad Shamshad", "Nils Lukas", "Karthik Nandakumar", "Fahkri Karray", "Samuele Poppi"], "title": "SPQR: A Standardized Benchmark for Modern Safety Alignment Methods in Text-to-Image Diffusion Models", "comment": "20 pages, 8 figures, 10 tables", "summary": "Text-to-image diffusion models can emit copyrighted, unsafe, or private content. Safety alignment aims to suppress specific concepts, yet evaluations seldom test whether safety persists under benign downstream fine-tuning routinely applied after deployment (e.g., LoRA personalization, style/domain adapters). We study the stability of current safety methods under benign fine-tuning and observe frequent breakdowns. As true safety alignment must withstand even benign post-deployment adaptations, we introduce the SPQR benchmark (Safety-Prompt adherence-Quality-Robustness). SPQR is a single-scored metric that provides a standardized and reproducible framework to evaluate how well safety-aligned diffusion models preserve safety, utility, and robustness under benign fine-tuning, by reporting a single leaderboard score to facilitate comparisons. We conduct multilingual, domain-specific, and out-of-distribution analyses, along with category-wise breakdowns, to identify when safety alignment fails after benign fine-tuning, ultimately showcasing SPQR as a concise yet comprehensive benchmark for T2I safety alignment techniques for T2I models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u826f\u6027\u5fae\u8c03\u4e0b\u7684\u5b89\u5168\u5bf9\u9f50\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u5f53\u524d\u5b89\u5168\u65b9\u6cd5\u7ecf\u5e38\u5931\u6548\uff0c\u5e76\u63d0\u51fa\u4e86SPQR\u57fa\u51c6\u6765\u8bc4\u4f30\u5b89\u5168\u5bf9\u9f50\u6a21\u578b\u5728\u5fae\u8c03\u540e\u7684\u5b89\u5168\u6027\u3001\u6548\u7528\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u53ef\u80fd\u751f\u6210\u53d7\u7248\u6743\u4fdd\u62a4\u3001\u4e0d\u5b89\u5168\u6216\u79c1\u5bc6\u5185\u5bb9\u3002\u5b89\u5168\u5bf9\u9f50\u65e8\u5728\u6291\u5236\u7279\u5b9a\u6982\u5ff5\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u5f88\u5c11\u6d4b\u8bd5\u5728\u90e8\u7f72\u540e\u5e38\u89c4\u5e94\u7528\u7684\u826f\u6027\u5fae\u8c03\uff08\u5982LoRA\u4e2a\u6027\u5316\u3001\u98ce\u683c/\u9886\u57df\u9002\u914d\u5668\uff09\u4e0b\u5b89\u5168\u6027\u7684\u6301\u4e45\u6027\u3002", "method": "\u7814\u7a76\u5f53\u524d\u5b89\u5168\u65b9\u6cd5\u5728\u826f\u6027\u5fae\u8c03\u4e0b\u7684\u7a33\u5b9a\u6027\uff0c\u5f15\u5165SPQR\u57fa\u51c6\uff08\u5b89\u5168-\u63d0\u793a\u9075\u5faa-\u8d28\u91cf-\u9c81\u68d2\u6027\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5355\u4e00\u8bc4\u5206\u6307\u6807\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u548c\u53ef\u590d\u73b0\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u5b89\u5168\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u5728\u826f\u6027\u5fae\u8c03\u4e0b\u5982\u4f55\u4fdd\u6301\u5b89\u5168\u6027\u3001\u6548\u7528\u548c\u9c81\u68d2\u6027\u3002", "result": "\u89c2\u5bdf\u5230\u5728\u826f\u6027\u5fae\u8c03\u4e0b\u9891\u7e41\u51fa\u73b0\u5b89\u5168\u5931\u6548\uff0cSPQR\u57fa\u51c6\u901a\u8fc7\u591a\u8bed\u8a00\u3001\u9886\u57df\u7279\u5b9a\u548c\u5206\u5e03\u5916\u5206\u6790\u4ee5\u53ca\u7c7b\u522b\u7ec6\u5206\uff0c\u8bc6\u522b\u5b89\u5168\u5bf9\u9f50\u5728\u826f\u6027\u5fae\u8c03\u540e\u5931\u8d25\u7684\u60c5\u51b5\u3002", "conclusion": "SPQR\u4f5c\u4e3a\u4e00\u4e2a\u7b80\u6d01\u800c\u5168\u9762\u7684\u57fa\u51c6\uff0c\u4e3aT2I\u5b89\u5168\u5bf9\u9f50\u6280\u672f\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5f3a\u8c03\u771f\u6b63\u7684\u5b89\u5168\u5bf9\u9f50\u5fc5\u987b\u80fd\u591f\u627f\u53d7\u90e8\u7f72\u540e\u7684\u826f\u6027\u9002\u5e94\u3002"}}
{"id": "2511.19577", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.19577", "abs": "https://arxiv.org/abs/2511.19577", "authors": ["Abhay Goyal", "Navin Kumar", "Kimberly DiMeola", "Rafael Trujillo", "Soorya Ram Shimgekar", "Christian Poellabauer", "Pi Zonooz", "Ermonda Gjoni-Markaj", "Declan Barry", "Lynn Madden"], "title": "Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder", "comment": null, "summary": "Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u548cAI\u65b9\u6cd5\u9884\u6d4b\u6162\u6027\u75bc\u75db\u548c\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u60a3\u8005\u7684\u75bc\u75db\u5cf0\u503c\uff0c\u53d1\u73b0\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u8f83\u9ad8\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u6162\u6027\u75bc\u75db\u548c\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u662f\u76f8\u4e92\u5173\u8054\u7684\u5e38\u89c1\u6162\u6027\u75be\u75c5\uff0c\u76ee\u524d\u7f3a\u4e4f\u57fa\u4e8e\u8bc1\u636e\u7684\u7efc\u5408\u6cbb\u7597\u65b9\u6cd5\u3002\u53ef\u7a7f\u6234\u8bbe\u5907\u6709\u6f5c\u529b\u76d1\u6d4b\u590d\u6742\u60a3\u8005\u4fe1\u606f\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u76d1\u6d4b\u60a3\u8005\u6570\u636e\uff0c\u7ed3\u5408\u591a\u79cdAI\u65b9\u6cd5\uff08\u5305\u62ec\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5206\u6790\u75bc\u75db\u5cf0\u503c\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "result": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u75bc\u75db\u5cf0\u503c\u65b9\u9762\u8fbe\u5230\u76f8\u5bf9\u8f83\u9ad8\u7684\u51c6\u786e\u7387\uff08>0.7\uff09\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u4f9b\u75bc\u75db\u5cf0\u503c\u6d1e\u5bdf\u65b9\u9762\u8868\u73b0\u6709\u9650\u3002", "conclusion": "\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u5b9e\u65f6\u76d1\u6d4b\u7ed3\u5408\u5148\u8fdbAI\u6a21\u578b\u53ef\u4fc3\u8fdb\u75bc\u75db\u5cf0\u503c\u7684\u65e9\u671f\u68c0\u6d4b\uff0c\u652f\u6301\u4e2a\u6027\u5316\u5e72\u9884\uff0c\u4f46\u9700\u8981\u5f00\u53d1\u80fd\u591f\u63d0\u4f9b\u53ef\u64cd\u4f5c\u6d1e\u5bdf\u7684\u5927\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2511.19654", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19654", "abs": "https://arxiv.org/abs/2511.19654", "authors": ["Stephen C. Gravereaux", "Sheikh Rabiul Islam"], "title": "Accuracy and Efficiency Trade-Offs in LLM-Based Malware Detection and Explanation: A Comparative Study of Parameter Tuning vs. Full Fine-Tuning", "comment": "Accepted in IEEE Big Data 2025", "summary": "This study examines whether Low-Rank Adaptation (LoRA) fine-tuned Large Language Models (LLMs) can approximate the performance of fully fine-tuned models in generating human-interpretable decisions and explanations for malware classification. Achieving trustworthy malware detection, particularly when LLMs are involved, remains a significant challenge. We developed an evaluation framework using Bilingual Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), and Semantic Similarity Metrics to benchmark explanation quality across five LoRA configurations and a fully fine-tuned baseline. Results indicate that full fine-tuning achieves the highest overall scores, with BLEU and ROUGE improvements of up to 10% over LoRA variants. However, mid-range LoRA models deliver competitive performance exceeding full fine-tuning on two metrics while reducing model size by approximately 81% and training time by over 80% on a LoRA model with 15.5% trainable parameters. These findings demonstrate that LoRA offers a practical balance of interpretability and resource efficiency, enabling deployment in resource-constrained environments without sacrificing explanation quality. By providing feature-driven natural language explanations for malware classifications, this approach enhances transparency, analyst confidence, and operational scalability in malware detection systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86LoRA\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4e2d\u751f\u6210\u53ef\u89e3\u91ca\u51b3\u7b56\u548c\u89e3\u91ca\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4e2d\u6863LoRA\u6a21\u578b\u5728\u4fdd\u6301\u89e3\u91ca\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u9700\u8981\u5e73\u8861\u6a21\u578b\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528BLEU\u3001ROUGE\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6307\u6807\uff0c\u6bd4\u8f83\u4e86\u4e94\u79cdLoRA\u914d\u7f6e\u548c\u5b8c\u5168\u5fae\u8c03\u57fa\u7ebf\u7684\u89e3\u91ca\u8d28\u91cf\u3002", "result": "\u5b8c\u5168\u5fae\u8c03\u83b7\u5f97\u6700\u9ad8\u603b\u4f53\u5206\u6570\uff0cBLEU\u548cROUGE\u6bd4LoRA\u53d8\u4f53\u63d0\u9ad8\u8fbe10%\u3002\u4f46\u4e2d\u6863LoRA\u6a21\u578b\u5728\u4e24\u4e2a\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u5b8c\u5168\u5fae\u8c03\uff0c\u540c\u65f6\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u7ea681%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u8d85\u8fc780%\u3002", "conclusion": "LoRA\u5728\u53ef\u89e3\u91ca\u6027\u548c\u8d44\u6e90\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u5b9e\u7528\u5e73\u8861\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u800c\u4e0d\u727a\u7272\u89e3\u91ca\u8d28\u91cf\uff0c\u589e\u5f3a\u4e86\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u3001\u5206\u6790\u5e08\u4fe1\u5fc3\u548c\u64cd\u4f5c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.19484", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19484", "abs": "https://arxiv.org/abs/2511.19484", "authors": ["Randall Balestriero", "Hugues Van Assel", "Sami BuGhanem", "Lucas Maes"], "title": "stable-pretraining-v1: Foundation Model Research Made Simple", "comment": null, "summary": "Foundation models and self-supervised learning (SSL) have become central to modern AI, yet research in this area remains hindered by complex codebases, redundant re-implementations, and the heavy engineering burden of scaling experiments. We present stable-pretraining, a modular, extensible, and performance-optimized library built on top of PyTorch, Lightning, Hugging Face, and TorchMetrics. Unlike prior toolkits focused narrowly on reproducing state-of-the-art results, stable-pretraining is designed for flexibility and iteration speed: it unifies essential SSL utilities--including probes, collapse detection metrics, augmentation pipelines, and extensible evaluation routines--within a coherent and reliable framework. A central design principle is logging everything, enabling fine-grained visibility into training dynamics that makes debugging, monitoring, and reproducibility seamless. We validate the library by demonstrating its ability to generate new research insights with minimal overhead, including depthwise representation probing and the analysis of CLIP degradation under synthetic data finetuning. By lowering barriers to entry while remaining scalable to large experiments, stable-pretraining aims to accelerate discovery and expand the possibilities of foundation model research.", "AI": {"tldr": "stable-pretraining\u662f\u4e00\u4e2a\u57fa\u4e8ePyTorch\u3001Lightning\u3001Hugging Face\u548cTorchMetrics\u6784\u5efa\u7684\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u4e14\u6027\u80fd\u4f18\u5316\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u5e93\uff0c\u65e8\u5728\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u7814\u7a76\u4e2d\u4ee3\u7801\u590d\u6742\u3001\u91cd\u590d\u5b9e\u73b0\u548c\u5de5\u7a0b\u8d1f\u62c5\u91cd\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7814\u7a76\u53d7\u5230\u590d\u6742\u4ee3\u7801\u5e93\u3001\u5197\u4f59\u91cd\u65b0\u5b9e\u73b0\u4ee5\u53ca\u6269\u5c55\u5b9e\u9a8c\u7684\u6c89\u91cd\u5de5\u7a0b\u8d1f\u62c5\u7684\u963b\u788d\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u5f00\u53d1\u5de5\u5177\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u7edf\u4e00\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u5de5\u5177\u5e93\uff0c\u5305\u542b\u63a2\u9488\u3001\u5d29\u6e83\u68c0\u6d4b\u6307\u6807\u3001\u589e\u5f3a\u7ba1\u9053\u548c\u53ef\u6269\u5c55\u8bc4\u4f30\u4f8b\u7a0b\u7b49\u6838\u5fc3\u529f\u80fd\uff0c\u91c7\u7528\"\u8bb0\u5f55\u4e00\u5207\"\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u8bad\u7ec3\u52a8\u6001\u53ef\u89c1\u6027\u3002", "result": "\u9a8c\u8bc1\u4e86\u8be5\u5e93\u80fd\u591f\u4ee5\u6700\u5c0f\u5f00\u9500\u751f\u6210\u65b0\u7684\u7814\u7a76\u89c1\u89e3\uff0c\u5305\u62ec\u6df1\u5ea6\u8868\u793a\u63a2\u9488\u548cCLIP\u5728\u5408\u6210\u6570\u636e\u5fae\u8c03\u4e0b\u7684\u9000\u5316\u5206\u6790\u3002", "conclusion": "stable-pretraining\u901a\u8fc7\u964d\u4f4e\u5165\u95e8\u95e8\u69db\u540c\u65f6\u4fdd\u6301\u5927\u89c4\u6a21\u5b9e\u9a8c\u7684\u53ef\u6269\u5c55\u6027\uff0c\u65e8\u5728\u52a0\u901f\u57fa\u7840\u6a21\u578b\u7814\u7a76\u7684\u53d1\u73b0\u548c\u6269\u5c55\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.19669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19669", "abs": "https://arxiv.org/abs/2511.19669", "authors": ["Souradip Poddar", "Chia-Tung Ho", "Ziming Wei", "Weidong Cao", "Haoxing Ren", "David Z. Pan"], "title": "HeaRT: A Hierarchical Circuit Reasoning Tree-Based Agentic Framework for AMS Design Optimization", "comment": null, "summary": "Conventional AI-driven AMS design automation algorithms remain constrained by their reliance on high-quality datasets to capture underlying circuit behavior, coupled with poor transferability across architectures, and a lack of adaptive mechanisms. This work proposes HeaRT, a foundational reasoning engine for automation loops and a first step toward intelligent, adaptive, human-style design optimization. HeaRT consistently demonstrates reasoning accuracy >97% and Pass@1 performance >98% across our 40-circuit benchmark repository, even as circuit complexity increases, while operating at <0.5x real-time token budget of SOTA baselines. Our experiments show that HeaRT yields >3x faster convergence in both sizing and topology design adaptation tasks across diverse optimization approaches, while preserving prior design intent.", "AI": {"tldr": "HeaRT\u662f\u4e00\u4e2a\u57fa\u7840\u63a8\u7406\u5f15\u64ce\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5faa\u73af\uff0c\u662f\u8fc8\u5411\u667a\u80fd\u3001\u81ea\u9002\u5e94\u3001\u7c7b\u4eba\u8bbe\u8ba1\u4f18\u5316\u7684\u7b2c\u4e00\u6b65\u3002\u5b83\u572840\u7535\u8def\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa>97%\u7684\u63a8\u7406\u51c6\u786e\u7387\u548c>98%\u7684Pass@1\u6027\u80fd\uff0c\u540c\u65f6\u8fd0\u884c\u6210\u672c\u4ec5\u4e3aSOTA\u57fa\u7ebf\u7684<0.5\u500d\u3002", "motivation": "\u4f20\u7edf\u7684AI\u9a71\u52a8AMS\u8bbe\u8ba1\u81ea\u52a8\u5316\u7b97\u6cd5\u53d7\u9650\u4e8e\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3001\u8de8\u67b6\u6784\u53ef\u79fb\u690d\u6027\u5dee\u4ee5\u53ca\u7f3a\u4e4f\u81ea\u9002\u5e94\u673a\u5236\u3002", "method": "\u63d0\u51faHeaRT\u57fa\u7840\u63a8\u7406\u5f15\u64ce\uff0c\u4f5c\u4e3a\u81ea\u52a8\u5316\u5faa\u73af\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u667a\u80fd\u3001\u81ea\u9002\u5e94\u7684\u8bbe\u8ba1\u4f18\u5316\u3002", "result": "\u572840\u7535\u8def\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u63a8\u7406\u51c6\u786e\u7387>97%\uff0cPass@1\u6027\u80fd>98%\uff0c\u8fd0\u884c\u6210\u672c\u4ec5\u4e3aSOTA\u57fa\u7ebf\u7684<0.5\u500d\uff0c\u5728\u5c3a\u5bf8\u548c\u62d3\u6251\u8bbe\u8ba1\u9002\u5e94\u4efb\u52a1\u4e2d\u6536\u655b\u901f\u5ea6\u63d0\u9ad8>3\u500d\u3002", "conclusion": "HeaRT\u662f\u4e00\u4e2a\u6709\u6548\u7684\u667a\u80fd\u8bbe\u8ba1\u4f18\u5316\u5f15\u64ce\uff0c\u80fd\u591f\u4fdd\u6301\u5148\u524d\u7684\u8bbe\u8ba1\u610f\u56fe\uff0c\u540c\u65f6\u5728\u590d\u6742\u7535\u8def\u8bbe\u8ba1\u4e2d\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2511.19670", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.19670", "abs": "https://arxiv.org/abs/2511.19670", "authors": ["Luis Ferreirinha", "Iberia Medeiros"], "title": "BASICS: Binary Analysis and Stack Integrity Checker System for Buffer Overflow Mitigation", "comment": "17 pages, Submitted to IEEE Transactions on Reliability", "summary": "Cyber-Physical Systems have played an essential role in our daily lives, providing critical services such as power and water, whose operability, availability, and reliability must be ensured. The C programming language, prevalent in CPS development, is crucial for system control where reliability is critical. However, it is also commonly susceptible to vulnerabilities, particularly buffer overflows. Traditional vulnerability discovery techniques often struggle with scalability and precision when applied directly to the binary code of C programs, which can thereby keep programs vulnerable. This work introduces a novel approach designed to overcome these limitations by leveraging model checking and concolic execution techniques to automatically verify security properties of a program's stack memory in binary code, trampoline techniques to perform automated repair of the issues, and crash-inducing inputs to verify if they were successfully removed. The approach constructs a Memory State Space -- MemStaCe -- from the binary program's control flow graph and simulations, provided by concolic execution, of C function calls and loop constructs. The security properties, defined in LTL, model the correct behaviour of functions associated with vulnerabilities and allow the approach to identify vulnerabilities in MemStaCe by analysing counterexample traces that are generated when a security property is violated. These vulnerabilities are then addressed with a trampoline-based binary patching method, and the effectiveness of the patches is checked with crash-inducing inputs extracted during concolic execution. We implemented the approach in the BASICS tool for BO mitigation and evaluated using the Juliet C/C++ and SARD datasets and real applications, achieving an accuracy and precision above 87%, both in detection and correction. Also, we compared it with CWE Checker, outperforming it.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u68c0\u6d4b\u548c\u7b26\u53f7\u6267\u884c\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u548c\u4fee\u590dC\u7a0b\u5e8f\u4e8c\u8fdb\u5236\u4ee3\u7801\u4e2d\u7684\u7f13\u51b2\u533a\u6ea2\u51fa\u6f0f\u6d1e\uff0c\u901a\u8fc7\u6784\u5efa\u5185\u5b58\u72b6\u6001\u7a7a\u95f4\u548c\u81ea\u52a8\u5316\u8865\u4e01\u4fee\u590d\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u523087%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u548c\u7cbe\u786e\u5ea6\u3002", "motivation": "C\u8bed\u8a00\u5728\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684CPS\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u7f13\u51b2\u533a\u6ea2\u51fa\u7b49\u6f0f\u6d1e\u5f71\u54cd\u3002\u4f20\u7edf\u6f0f\u6d1e\u53d1\u73b0\u6280\u672f\u5728\u4e8c\u8fdb\u5236\u4ee3\u7801\u5c42\u9762\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u7cbe\u786e\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u7a0b\u5e8f\u6301\u7eed\u5b58\u5728\u6f0f\u6d1e\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u6a21\u578b\u68c0\u6d4b\u548c\u7b26\u53f7\u6267\u884c\u6280\u672f\u6784\u5efa\u5185\u5b58\u72b6\u6001\u7a7a\u95f4(MemStaCe)\uff0c\u901a\u8fc7LTL\u5b9a\u4e49\u7684\u5b89\u5168\u5c5e\u6027\u5206\u6790\u7a0b\u5e8f\u884c\u4e3a\uff0c\u8bc6\u522b\u6f0f\u6d1e\u540e\u91c7\u7528trampoline\u6280\u672f\u8fdb\u884c\u81ea\u52a8\u5316\u4e8c\u8fdb\u5236\u8865\u4e01\u4fee\u590d\uff0c\u5e76\u4f7f\u7528\u5d29\u6e83\u8bf1\u5bfc\u8f93\u5165\u9a8c\u8bc1\u4fee\u590d\u6548\u679c\u3002", "result": "\u5728Juliet C/C++\u548cSARD\u6570\u636e\u96c6\u53ca\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u68c0\u6d4b\u548c\u4fee\u590d\u7684\u51c6\u786e\u7387\u548c\u7cbe\u786e\u5ea6\u5747\u8d85\u8fc787%\uff0c\u4f18\u4e8eCWE Checker\u5de5\u5177\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e8c\u8fdb\u5236\u4ee3\u7801\u4e2d\u7f13\u51b2\u533a\u6ea2\u51fa\u6f0f\u6d1e\u7684\u81ea\u52a8\u68c0\u6d4b\u548c\u4fee\u590d\u95ee\u9898\uff0c\u4e3aCPS\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u53ef\u9760\u4fdd\u969c\u3002"}}
{"id": "2511.19489", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19489", "abs": "https://arxiv.org/abs/2511.19489", "authors": ["Zhe Zhao", "Yuheng Yang", "Haibin Wen", "Xiaojie Qiu", "Zaixi Zhang", "Qingfu Zhang"], "title": "Evolution without an Oracle: Driving Effective Evolution with LLM Judges", "comment": "14 pages, 5 figures", "summary": "The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through \"Problem Specification.\" By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing \"computable metrics\" to \"describable qualities,\" thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.", "AI": {"tldr": "MADE\u6846\u67b6\u901a\u8fc7\u95ee\u9898\u5206\u89e3\u5c06\u4e3b\u89c2LLM\u8bc4\u4f30\u8f6c\u5316\u4e3a\u7a33\u5b9a\u9009\u62e9\u538b\u529b\uff0c\u5728\u65e0\u5ba2\u89c2\u9002\u5e94\u5ea6\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8fdb\u5316\u4f18\u5316\uff0c\u5728\u8f6f\u4ef6\u9700\u6c42\u6ee1\u8db3\u548c\u590d\u6742\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u6253\u7834\u4f20\u7edf\u8fdb\u5316\u8ba1\u7b97\u4f9d\u8d56\u5ba2\u89c2\u9002\u5e94\u5ea6\u51fd\u6570\u7684\u9650\u5236\uff0c\u63a2\u7d22\u5728\u7eaf\u4e3b\u89c2LLM\u8bc4\u5224\u4e0b\u7684\u8fdb\u5316\u4f18\u5316\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51faMADE\u6846\u67b6\uff0c\u901a\u8fc7\u95ee\u9898\u89c4\u8303\u5c06\u6a21\u7cca\u6307\u4ee4\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5b50\u8981\u6c42\uff0c\u964d\u4f4e\u4e3b\u89c2\u8bc4\u4f30\u7684\u566a\u58f0\u3002", "result": "\u5728DevAI\u548cInfoBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8f6f\u4ef6\u9700\u6c42\u6ee1\u8db3\u7387\u4ece39.9%\u63d0\u5347\u81f361.9%\uff0c\u590d\u6742\u6307\u4ee4\u5b8c\u7f8e\u901a\u8fc7\u7387\u8fbe\u523095%\u3002", "conclusion": "\u5b9e\u73b0\u4e86\u4ece\u4f18\u5316\u53ef\u8ba1\u7b97\u6307\u6807\u5230\u4f18\u5316\u53ef\u63cf\u8ff0\u8d28\u91cf\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u65e0\u771f\u5b9e\u6807\u7b7e\u7684\u5f00\u653e\u9886\u57df\u5f00\u542f\u4e86\u8fdb\u5316\u4f18\u5316\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.19671", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19671", "abs": "https://arxiv.org/abs/2511.19671", "authors": ["Rishab Sharma", "Iman Saberi", "Elham Alipour", "Jie JW Wu", "Fatemeh Fard"], "title": "FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking", "comment": "3 tables, 11 pages, 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Generative AI in Finance", "summary": "Financial applications of large language models (LLMs) require factual reliability and computational efficiency, yet current systems often hallucinate details and depend on prohibitively large models. We propose FISCAL (Financial Synthetic Claim-Document Augmented Learning), a modular framework for generating synthetic data tailored to financial fact-checking. Using FISCAL, we generate a dataset called FISCAL-data and use it to train MiniCheck-FISCAL, a lightweight verifier for numerical financial claims. MiniCheck-FISCAL outperforms its baseline, surpasses GPT-3.5 Turbo and other open-source peers of similar size, and approaches the accuracy of much larger systems (20x), such as Mixtral-8x22B and Command R+. On external datasets FinDVer and Fin-Fact, it rivals GPT-4o and Claude-3.5 while outperforming Gemini-1.5 Flash. These results show that domain-specific synthetic data, combined with efficient fine-tuning, enables compact models to achieve state-of-the-art accuracy, robustness, and scalability for practical financial AI. The dataset and scripts are available in the project repository (link provided in the paper).", "AI": {"tldr": "FISCAL\u6846\u67b6\u901a\u8fc7\u751f\u6210\u91d1\u878d\u9886\u57df\u5408\u6210\u6570\u636e\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668MiniCheck-FISCAL\uff0c\u5728\u91d1\u878d\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u4e2d\u8d85\u8d8aGPT-3.5 Turbo\u7b49\u6a21\u578b\uff0c\u63a5\u8fd1\u66f4\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u5e94\u7528\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u6311\u6218\uff0c\u9700\u8981\u4e8b\u5b9e\u53ef\u9760\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFISCAL\u6846\u67b6\u751f\u6210\u91d1\u878d\u5408\u6210\u6570\u636e\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668MiniCheck-FISCAL\u7528\u4e8e\u91d1\u878d\u6570\u5b57\u58f0\u660e\u9a8c\u8bc1\u3002", "result": "MiniCheck-FISCAL\u5728\u591a\u4e2a\u91d1\u878d\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aGPT-3.5 Turbo\u7b49\u540c\u7c7b\u6a21\u578b\uff0c\u63a5\u8fd1Mixtral-8x22B\u7b49\u592720\u500d\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u5ab2\u7f8eGPT-4o\u548cClaude-3.5\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u7684\u5408\u6210\u6570\u636e\u7ed3\u5408\u9ad8\u6548\u5fae\u8c03\u53ef\u4f7f\u7d27\u51d1\u6a21\u578b\u5728\u91d1\u878dAI\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.19711", "categories": ["cs.CR", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.19711", "abs": "https://arxiv.org/abs/2511.19711", "authors": ["Jinyu Liu", "Gang Tan", "Kiwan Maeng"], "title": "CrypTorch: PyTorch-based Auto-tuning Compiler for Machine Learning with Multi-party Computation", "comment": "28 pages, 17 figures. Submitted to PLDI 2026", "summary": "Machine learning (ML) involves private data and proprietary model parameters. MPC-based ML allows multiple parties to collaboratively run an ML workload without sharing their private data or model parameters using multi-party computing (MPC). Because MPC cannot natively run ML operations such as Softmax or GELU, existing frameworks use different approximations. Our study shows that, on a well-optimized framework, these approximations often become the dominating bottleneck. Popular approximations are often insufficiently accurate or unnecessarily slow, and these issues are hard to identify and fix in existing frameworks. To tackle this issue, we propose a compiler for MPC-based ML, CrypTorch. CrypTorch disentangles these approximations with the rest of the MPC runtime, allows easily adding new approximations through its programming interface, and automatically selects approximations to maximize both performance and accuracy. Built as an extension to PyTorch 2's compiler, we show that CrypTorch's auto-tuning alone provides 1.20--1.7$\\times$ immediate speedup without sacrificing accuracy, and 1.31--1.8$\\times$ speedup when some accuracy degradation is allowed, compared to our well-optimized baseline. Combined with better engineering and adoption of state-of-the-art practices, the entire framework brings 3.22--8.6$\\times$ end-to-end speedup compared to the popular framework, CrypTen.", "AI": {"tldr": "CrypTorch\u662f\u4e00\u4e2a\u57fa\u4e8eMPC\u7684\u673a\u5668\u5b66\u4e60\u7f16\u8bd1\u5668\uff0c\u901a\u8fc7\u89e3\u8026\u8fd1\u4f3c\u8ba1\u7b97\u4e0eMPC\u8fd0\u884c\u65f6\uff0c\u63d0\u4f9b\u81ea\u52a8\u8c03\u4f18\u529f\u80fd\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709MPC\u6846\u67b6\u4e2d\u7684\u8fd1\u4f3c\u8ba1\u7b97\uff08\u5982Softmax\u3001GELU\uff09\u5f80\u5f80\u6210\u4e3a\u6027\u80fd\u74f6\u9888\uff0c\u4e14\u96be\u4ee5\u8bc6\u522b\u548c\u4fee\u590d\uff0c\u9700\u8981\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faCrypTorch\u7f16\u8bd1\u5668\uff0c\u4f5c\u4e3aPyTorch 2\u7684\u6269\u5c55\uff0c\u901a\u8fc7\u7f16\u7a0b\u63a5\u53e3\u8f7b\u677e\u6dfb\u52a0\u65b0\u8fd1\u4f3c\uff0c\u5e76\u81ea\u52a8\u9009\u62e9\u8fd1\u4f3c\u4ee5\u6700\u5927\u5316\u6027\u80fd\u548c\u51c6\u786e\u6027\u3002", "result": "\u4ec5\u81ea\u52a8\u8c03\u4f18\u5373\u53ef\u63d0\u4f9b1.20-1.7\u500d\u901f\u5ea6\u63d0\u5347\u4e14\u4e0d\u727a\u7272\u51c6\u786e\u6027\uff0c\u5141\u8bb8\u4e00\u5b9a\u7cbe\u5ea6\u635f\u5931\u65f6\u53ef\u8fbe\u52301.31-1.8\u500d\u52a0\u901f\uff1b\u6574\u4e2a\u6846\u67b6\u76f8\u6bd4CrypTen\u5e26\u67653.22-8.6\u500d\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "CrypTorch\u6709\u6548\u89e3\u51b3\u4e86MPC-based ML\u4e2d\u7684\u8fd1\u4f3c\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6613\u7528\u6027\u3002"}}
{"id": "2511.19510", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19510", "abs": "https://arxiv.org/abs/2511.19510", "authors": ["Asif Zaman", "Kallol Naha", "Khalid Belhajjame", "Hasan M. Jamil"], "title": "CodeR3: A GenAI-Powered Workflow Repair and Revival Ecosystem", "comment": "9 pages, 4 figures", "summary": "Scientific workflows encode valuable domain expertise and computational methodologies. Yet studies consistently show that a significant proportion of published workflows suffer from decay over time. This problem is particularly acute for legacy workflow systems like Taverna, where discontinued services, obsolete dependencies, and system retirement render previously functional workflows unusable. We present a novel legacy workflow migration system, called CodeR$^3$ (stands for Code Repair, Revival and Reuse), that leverages generative AI to analyze the characteristics of decayed workflows, reproduce them into modern workflow technologies like Snakemake and VisFlow. Our system additionally integrates stepwise workflow analysis visualization, automated service substitution, and human-in-the-loop validation. Through several case studies of Taverna workflow revival, we demonstrate the feasibility of this approach while identifying key challenges that require human oversight. Our findings reveal that automation significantly reduces manual effort in workflow parsing and service identification. However, critical tasks such as service substitution and data validation still require domain expertise. Our result will be a crowdsourcing platform that enables the community to collaboratively revive decayed workflows and validate the functionality and correctness of revived workflows. This work contributes a framework for workflow revival that balances automation efficiency with necessary human judgment.", "AI": {"tldr": "CodeR\u00b3\u7cfb\u7edf\u5229\u7528\u751f\u6210\u5f0fAI\u5c06\u8fc7\u65f6\u7684Taverna\u5de5\u4f5c\u6d41\u8fc1\u79fb\u5230Snakemake\u548cVisFlow\u7b49\u73b0\u4ee3\u5de5\u4f5c\u6d41\u6280\u672f\u4e2d\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5206\u6790\u3001\u670d\u52a1\u66ff\u6362\u548c\u4eba\u5de5\u9a8c\u8bc1\u6765\u4fee\u590d\u548c\u91cd\u7528\u8870\u9000\u7684\u5de5\u4f5c\u6d41\u3002", "motivation": "\u79d1\u5b66\u5de5\u4f5c\u6d41\u5305\u542b\u5b9d\u8d35\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f46\u5927\u91cf\u5df2\u53d1\u5e03\u7684\u5de5\u4f5c\u6d41\u4f1a\u968f\u65f6\u95f4\u8870\u9000\uff0c\u7279\u522b\u662f\u50cfTaverna\u8fd9\u6837\u7684\u9057\u7559\u7cfb\u7edf\uff0c\u7531\u4e8e\u670d\u52a1\u7ec8\u6b62\u3001\u4f9d\u8d56\u8fc7\u65f6\u548c\u7cfb\u7edf\u9000\u5f79\uff0c\u5bfc\u81f4\u539f\u672c\u53ef\u7528\u7684\u5de5\u4f5c\u6d41\u65e0\u6cd5\u4f7f\u7528\u3002", "method": "\u5f00\u53d1CodeR\u00b3\u7cfb\u7edf\uff0c\u96c6\u6210\u751f\u6210\u5f0fAI\u5206\u6790\u8870\u9000\u5de5\u4f5c\u6d41\u7279\u5f81\uff0c\u9010\u6b65\u5de5\u4f5c\u6d41\u5206\u6790\u53ef\u89c6\u5316\uff0c\u81ea\u52a8\u5316\u670d\u52a1\u66ff\u6362\uff0c\u4ee5\u53ca\u4eba\u5de5\u5728\u73af\u9a8c\u8bc1\uff0c\u5c06\u5de5\u4f5c\u6d41\u91cd\u73b0\u4e3a\u73b0\u4ee3\u6280\u672f\u3002", "result": "\u901a\u8fc7\u591a\u4e2aTaverna\u5de5\u4f5c\u6d41\u590d\u5174\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u81ea\u52a8\u5316\u663e\u8457\u51cf\u5c11\u4e86\u5de5\u4f5c\u6d41\u89e3\u6790\u548c\u670d\u52a1\u8bc6\u522b\u7684\u624b\u52a8\u5de5\u4f5c\uff0c\u4f46\u670d\u52a1\u66ff\u6362\u548c\u6570\u636e\u9a8c\u8bc1\u4ecd\u9700\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e73\u8861\u81ea\u52a8\u5316\u6548\u7387\u4e0e\u5fc5\u8981\u4eba\u5de5\u5224\u65ad\u7684\u5de5\u4f5c\u6d41\u590d\u5174\u6846\u67b6\uff0c\u5c06\u5f00\u53d1\u4f17\u5305\u5e73\u53f0\u8ba9\u793e\u533a\u534f\u4f5c\u590d\u5174\u8870\u9000\u5de5\u4f5c\u6d41\u5e76\u9a8c\u8bc1\u529f\u80fd\u6b63\u786e\u6027\u3002"}}
{"id": "2511.19749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19749", "abs": "https://arxiv.org/abs/2511.19749", "authors": ["Farzan Karimi-Malekabadi", "Pooya Razavi", "Sonya Powers"], "title": "Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions", "comment": null, "summary": "As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6559\u80b2\u8bc4\u4f30\u9879\u76ee\u4e0e\u5185\u5bb9\u6807\u51c6\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LLMs\u5728\u8bc6\u522b\u9519\u4f4d\u9879\u76ee\u3001\u9009\u62e9\u6b63\u786e\u6280\u80fd\u548c\u9884\u7b5b\u9009\u5019\u9009\u6280\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u5de5\u5bf9\u9f50\u5ba1\u67e5\u867d\u7136\u51c6\u786e\u4f46\u8017\u65f6\u8017\u529b\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u9879\u76ee\u5e93\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u662f\u5426\u80fd\u52a0\u901f\u8fd9\u4e00\u8fc7\u7a0b\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u8d85\u8fc712,000\u4e2a\u9879\u76ee-\u6280\u80fd\u5bf9\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cdLLMs\uff08GPT-3.5 Turbo\u3001GPT-4o-mini\u548cGPT-4o\uff09\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1a\u8bc6\u522b\u9519\u4f4d\u9879\u76ee\u3001\u4ece\u5b8c\u6574\u6807\u51c6\u96c6\u4e2d\u9009\u62e9\u6b63\u786e\u6280\u80fd\u3001\u4ee5\u53ca\u5728\u5206\u7c7b\u524d\u7f29\u5c0f\u5019\u9009\u5217\u8868\u3002", "result": "GPT-4o-mini\u5728\u8bc6\u522b\u5bf9\u9f50\u72b6\u6001\u65b9\u9762\u8fbe\u523083-94%\u7684\u51c6\u786e\u7387\uff1b\u6570\u5b66\u9886\u57df\u8868\u73b0\u5f3a\u52b2\uff0c\u9605\u8bfb\u9886\u57df\u56e0\u8bed\u4e49\u91cd\u53e0\u800c\u8868\u73b0\u8f83\u4f4e\uff1b\u9884\u7b5b\u9009\u5019\u9009\u6280\u80fd\u540e\uff0c\u6b63\u786e\u6280\u80fd\u51fa\u73b0\u5728\u524d\u4e94\u5efa\u8bae\u4e2d\u7684\u6982\u7387\u8d85\u8fc795%\u3002", "conclusion": "LLMs\uff0c\u7279\u522b\u662f\u7ed3\u5408\u5019\u9009\u7b5b\u9009\u7b56\u7565\u65f6\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u9879\u76ee\u5ba1\u67e5\u7684\u4eba\u5de5\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u9f50\u51c6\u786e\u6027\u3002\u5efa\u8bae\u5f00\u53d1\u6df7\u5408\u6d41\u7a0b\uff0c\u5c06\u57fa\u4e8eLLM\u7684\u7b5b\u9009\u4e0e\u6a21\u7cca\u60c5\u51b5\u4e0b\u7684\u4eba\u5de5\u5ba1\u67e5\u76f8\u7ed3\u5408\u3002"}}
{"id": "2511.19727", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19727", "abs": "https://arxiv.org/abs/2511.19727", "authors": ["Steven Peh"], "title": "Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts", "comment": "44 pages, 1 figure", "summary": "Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish between trusted instructions and untrusted content. While current LLMs lack native fence awareness, we demonstrate that simulated awareness through prompt instructions achieved complete prevention of injection attacks in our experiments, reducing success rates from 86.7% (260/300 successful attacks) to 0% (0/300 successful attacks) across 300 test cases with two leading LLM providers. We implement a proof-of-concept fence generation and verification pipeline with a total overhead of 0.224 seconds (0.130s for fence generation, 0.094s for validation) across 100 samples. Our approach is platform-agnostic and can be incrementally deployed as a security layer above existing LLM infrastructure, with the expectation that future models will be trained with native fence awareness for optimal security.", "AI": {"tldr": "\u63d0\u51faPrompt Fencing\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bc6\u7801\u5b66\u8ba4\u8bc1\u548c\u6570\u636e\u67b6\u6784\u539f\u5219\u5728LLM\u63d0\u793a\u4e2d\u5efa\u7acb\u5b89\u5168\u8fb9\u754c\uff0c\u4f7f\u7528\u52a0\u5bc6\u7b7e\u540d\u5143\u6570\u636e\u6807\u8bb0\u63d0\u793a\u6bb5\uff0c\u533a\u5206\u53ef\u4fe1\u6307\u4ee4\u548c\u4e0d\u53ef\u4fe1\u5185\u5bb9\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5c06\u6ce8\u5165\u653b\u51fb\u6210\u529f\u7387\u4ece86.7%\u964d\u81f30%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u4ea7\u90e8\u7f72\u4e2d\u5bf9\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4ecd\u7136\u8106\u5f31\uff0c\u8fd9\u662f\u6700\u91cd\u8981\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8981\u6709\u6548\u7684\u9632\u62a4\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5bc6\u7801\u5b66\u8ba4\u8bc1\u548c\u6570\u636e\u67b6\u6784\u539f\u5219\uff0c\u4e3a\u63d0\u793a\u6bb5\u6dfb\u52a0\u5305\u542b\u4fe1\u4efb\u8bc4\u7ea7\u548c\u5185\u5bb9\u7c7b\u578b\u7684\u52a0\u5bc6\u7b7e\u540d\u5143\u6570\u636e\uff0c\u5efa\u7acb\u660e\u786e\u7684\u5b89\u5168\u8fb9\u754c\u3002\u901a\u8fc7\u63d0\u793a\u6307\u4ee4\u6a21\u62df\u8fb9\u754c\u610f\u8bc6\u3002", "result": "\u5728300\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u5c06\u6ce8\u5165\u653b\u51fb\u6210\u529f\u7387\u4ece86.7%(260/300)\u964d\u81f30%(0/300)\uff0c\u6982\u5ff5\u9a8c\u8bc1\u7684\u8fb9\u754c\u751f\u6210\u548c\u9a8c\u8bc1\u7ba1\u9053\u603b\u5f00\u9500\u4e3a0.224\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5e73\u53f0\u65e0\u5173\uff0c\u53ef\u4f5c\u4e3a\u5b89\u5168\u5c42\u5728\u73b0\u6709LLM\u57fa\u7840\u8bbe\u65bd\u4e0a\u589e\u91cf\u90e8\u7f72\uff0c\u672a\u6765\u6a21\u578b\u6709\u671b\u8bad\u7ec3\u539f\u751f\u8fb9\u754c\u610f\u8bc6\u4ee5\u83b7\u5f97\u6700\u4f73\u5b89\u5168\u6027\u3002"}}
{"id": "2511.19875", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19875", "abs": "https://arxiv.org/abs/2511.19875", "authors": ["Qingyu Zhang", "Puzhuo Liu", "Peng Di", "Chenxiong Qian"], "title": "CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection", "comment": null, "summary": "Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level \"purpose\" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.", "AI": {"tldr": "CODEFUSE-COMMITEVAL\u662f\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u68c0\u6d4b\u63d0\u4ea4\u6d88\u606f\u4e0e\u4ee3\u7801\u4e0d\u4e00\u81f4\u6027(MCI)\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u89c4\u5219\u5f15\u5bfc\u7684\u7a81\u53d8\u751f\u62107\u79cd\u4e0d\u4e00\u81f4\u6d88\u606f\u7c7b\u578b\uff0c\u8bc4\u4f30\u4e866\u4e2a\u5f00\u6e90LLM\u5728\u68c0\u6d4bMCI\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u63d0\u4ea4\u6d88\u606f\u8d28\u91cf\u4f4e\u4e14\u7ecf\u5e38\u4e0e\u4ee3\u7801\u53d8\u66f4\u4e0d\u4e00\u81f4\uff0c\u8fd9\u4f1a\u8bef\u5bfc\u4ee3\u7801\u5ba1\u67e5\u3001\u963b\u788d\u7ef4\u62a4\u3001\u6c61\u67d3\u7814\u7a76\u6570\u636e\u96c6\uff0c\u751a\u81f3\u53ef\u80fd\u63a9\u76d6\u5b89\u5168\u8865\u4e01\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u8bc4\u4f30MCI\u68c0\u6d4b\u6a21\u578b\u7684\u57fa\u51c6\u3002", "method": "\u57fa\u4e8eApacheCM\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u89c4\u5219\u5f15\u5bfc\u7684\u7a81\u53d8\u751f\u62107\u79cd\u4e0d\u4e00\u81f4\u6d88\u606f\u7c7b\u578b\uff0c\u5e76\u8fdb\u884c\u53cc\u91cd\u9a8c\u8bc1\u3002\u8bc4\u4f30\u4e866\u4e2a\u5f00\u6e90LLM\u5728\u666e\u901a\u8bbe\u7f6e\u548c\u4e09\u79cd\u589e\u5f3a\u7b56\u7565\uff08\u5c11\u6837\u672c\u63d0\u793a\u3001\u601d\u7ef4\u94fe\u3001\u6269\u5c55\u4e0a\u4e0b\u6587\uff09\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u68c0\u6d4b\u4e0d\u4e00\u81f4\u63d0\u4ea4\u6bd4\u4e00\u81f4\u63d0\u4ea4\u66f4\u53ef\u9760\uff08\u5e73\u5747\u53ec\u56de\u738785.95%\uff0c\u7cbe\u786e\u738780.28%\uff0c\u7279\u5f02\u602763.8%\uff09\uff1bgpt-oss-20B\u8868\u73b0\u6700\u4f73\u4f46token\u4f7f\u7528\u91cf\u662f\u5176\u4ed6\u6a21\u578b\u7684\u4e24\u500d\u591a\u3002\u589e\u5f3a\u7b56\u7565\u6548\u679c\u5404\u5f02\uff1a\u6269\u5c55\u4e0a\u4e0b\u6587\u5bf9\u5927\u6a21\u578b\u6709\u5e2e\u52a9\u4f46\u5bf9\u5c0f\u6a21\u578b\u589e\u52a0\u566a\u58f0\uff1b\u5c11\u6837\u672c\u63d0\u9ad8\u51c6\u786e\u6027\u4f46\u589e\u52a0\u9519\u8bef\u9884\u6d4b\uff1b\u601d\u7ef4\u94fe\u63d0\u5347\u7cbe\u786e\u7387\u548c\u7279\u5f02\u6027\u4f46\u964d\u4f4e\u53ec\u56de\u7387\u3002", "conclusion": "CODEFUSE-COMMITEVAL\u4e3aMCI\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u8bc4\u4f30\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u9700\u8981\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u548c\u5e73\u8861\u7684\u6570\u636e\u6765\u6355\u6349\u9ad8\u5c42\u6b21\u8bed\u4e49\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u610f\u56fe\u5c42\u9762\u7684'\u76ee\u7684'\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002"}}
{"id": "2511.19886", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19886", "abs": "https://arxiv.org/abs/2511.19886", "authors": ["Chi Liu", "Tianqing Zhu", "Wanlei Zhou", "Wei Zhao"], "title": "Frequency Bias Matters: Diving into Robust and Generalized Deep Image Forgery Detection", "comment": "Accepted for publication in IEEE Transactions on Dependable and Secure Computing", "summary": "As deep image forgery powered by AI generative models, such as GANs, continues to challenge today's digital world, detecting AI-generated forgeries has become a vital security topic. Generalizability and robustness are two critical concerns of a forgery detector, determining its reliability when facing unknown GANs and noisy samples in an open world. Although many studies focus on improving these two properties, the root causes of these problems have not been fully explored, and it is unclear if there is a connection between them. Moreover, despite recent achievements in addressing these issues from image forensic or anti-forensic aspects, a universal method that can contribute to both sides simultaneously remains practically significant yet unavailable. In this paper, we provide a fundamental explanation of these problems from a frequency perspective. Our analysis reveals that the frequency bias of a DNN forgery detector is a possible cause of generalization and robustness issues. Based on this finding, we propose a two-step frequency alignment method to remove the frequency discrepancy between real and fake images, offering double-sided benefits: it can serve as a strong black-box attack against forgery detectors in the anti-forensic context or, conversely, as a universal defense to improve detector reliability in the forensic context. We also develop corresponding attack and defense implementations and demonstrate their effectiveness, as well as the effect of the frequency alignment method, in various experimental settings involving twelve detectors, eight forgery models, and five metrics.", "AI": {"tldr": "\u672c\u6587\u4ece\u9891\u7387\u89d2\u5ea6\u5206\u6790AI\u751f\u6210\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u9891\u7387\u5bf9\u9f50\u65b9\u6cd5\uff0c\u65e2\u80fd\u4f5c\u4e3a\u53cd\u53d6\u8bc1\u653b\u51fb\u624b\u6bb5\uff0c\u4e5f\u80fd\u4f5c\u4e3a\u53d6\u8bc1\u9632\u5fa1\u624b\u6bb5\u3002", "motivation": "AI\u751f\u6210\u7684\u56fe\u50cf\u4f2a\u9020\u5bf9\u6570\u5b57\u4e16\u754c\u6784\u6210\u6311\u6218\uff0c\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u672a\u6df1\u5165\u63a2\u8ba8\u8fd9\u4e9b\u95ee\u9898\u6839\u6e90\uff0c\u4e14\u7f3a\u4e4f\u540c\u65f6\u6539\u5584\u8fd9\u4e24\u65b9\u9762\u6027\u80fd\u7684\u901a\u7528\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u9891\u7387\u5bf9\u9f50\u65b9\u6cd5\uff0c\u6d88\u9664\u771f\u5b9e\u4e0e\u4f2a\u9020\u56fe\u50cf\u95f4\u7684\u9891\u7387\u5dee\u5f02\u3002\u8be5\u65b9\u6cd5\u53ef\u53cc\u5411\u5e94\u7528\uff1a\u4f5c\u4e3a\u53cd\u53d6\u8bc1\u7684\u9ed1\u76d2\u653b\u51fb\u624b\u6bb5\uff0c\u6216\u4f5c\u4e3a\u53d6\u8bc1\u7684\u901a\u7528\u9632\u5fa1\u624b\u6bb5\u3002", "result": "\u5728\u6d89\u53ca12\u4e2a\u68c0\u6d4b\u5668\u30018\u4e2a\u4f2a\u9020\u6a21\u578b\u548c5\u4e2a\u6307\u6807\u7684\u591a\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u9891\u7387\u5bf9\u9f50\u65b9\u6cd5\u7684\u4f5c\u7528\u3002", "conclusion": "\u9891\u7387\u504f\u5dee\u662fDNN\u4f2a\u9020\u68c0\u6d4b\u5668\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u95ee\u9898\u7684\u53ef\u80fd\u539f\u56e0\uff0c\u9891\u7387\u5bf9\u9f50\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53cc\u5411\u89e3\u51b3\u65b9\u6848\uff0c\u65e2\u53ef\u7528\u4e8e\u653b\u51fb\u4e5f\u53ef\u7528\u4e8e\u9632\u5fa1\u3002"}}
{"id": "2511.20403", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20403", "abs": "https://arxiv.org/abs/2511.20403", "authors": ["Andrea Lops", "Fedelucio Narducci", "Azzurra Ragone", "Michelantonio Trizio", "Claudio Barto"], "title": "LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework", "comment": "Accepted at 40th IEEE/ACM International Conference on Automated Software Engineering", "summary": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.", "AI": {"tldr": "AgoneTest\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u7684Java\u5355\u5143\u6d4b\u8bd5\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5305\u542bClasses2Test\u6570\u636e\u96c6\u548c\u7efc\u5408\u8bc4\u4f30\u6307\u6807\uff0c\u5b9e\u9a8c\u663e\u793aLLM\u751f\u6210\u7684\u6d4b\u8bd5\u5728\u7f16\u8bd1\u901a\u8fc7\u7684\u60c5\u51b5\u4e0b\u53ef\u4ee5\u8fbe\u5230\u6216\u8d85\u8fc7\u4eba\u5de5\u6d4b\u8bd5\u7684\u8d28\u91cf\u3002", "motivation": "\u5355\u5143\u6d4b\u8bd5\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u91cd\u8981\u4f46\u8d44\u6e90\u5bc6\u96c6\u7684\u6b65\u9aa4\uff0c\u9700\u8981\u652f\u6301\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u8005\u6bd4\u8f83\u4e0d\u540cLLM\u548c\u63d0\u793a\u7b56\u7565\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165Classes2Test\u6570\u636e\u96c6\u6620\u5c04Java\u7c7b\u5230\u5bf9\u5e94\u6d4b\u8bd5\u7c7b\uff0c\u6784\u5efa\u5305\u542b\u53d8\u5f02\u5206\u6570\u548c\u6d4b\u8bd5\u5f02\u5473\u7b49\u9ad8\u7ea7\u6307\u6807\u7684\u6807\u51c6\u5316\u7aef\u5230\u7aef\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5bf9\u4e8e\u80fd\u591f\u7f16\u8bd1\u901a\u8fc7\u7684\u6d4b\u8bd5\u5b50\u96c6\uff0cLLM\u751f\u6210\u7684\u6d4b\u8bd5\u5728\u8986\u76d6\u7387\u548c\u7f3a\u9677\u68c0\u6d4b\u65b9\u9762\u80fd\u591f\u5339\u914d\u6216\u8d85\u8fc7\u4eba\u5de5\u7f16\u5199\u7684\u6d4b\u8bd5\uff0c\u589e\u5f3a\u7684\u63d0\u793a\u7b56\u7565\u6709\u52a9\u4e8e\u63d0\u9ad8\u6d4b\u8bd5\u8d28\u91cf\u3002", "conclusion": "AgoneTest\u9610\u660e\u4e86LLM\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u6a21\u578b\u8bbe\u8ba1\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u6d4b\u8bd5\u5b9e\u8df5\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2511.19798", "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.19798", "abs": "https://arxiv.org/abs/2511.19798", "authors": ["Weizhi Liu", "Xi Chen", "Zekun Jiang", "Liang Zhao", "Kunyuan Jiang", "Ruisi Tang", "Li Wang", "Mingke You", "Hanyu Zhou", "Hongyu Chen", "Qiankun Xiong", "Yong Nie", "Kang Li", "Jian Li"], "title": "KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)", "comment": null, "summary": "Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.", "AI": {"tldr": "\u5f00\u53d1\u4e86KOM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u819d\u9aa8\u5173\u8282\u708e\u7684\u8bc4\u4f30\u3001\u98ce\u9669\u9884\u6d4b\u548c\u6cbb\u7597\u5904\u65b9\uff0c\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u53ef\u663e\u8457\u63d0\u9ad8\u62a4\u7406\u6548\u7387\u3002", "motivation": "\u819d\u9aa8\u5173\u8282\u708e\u5f71\u54cd\u5168\u74036\u4ebf\u591a\u4eba\uff0c\u4e2a\u6027\u5316\u591a\u5b66\u79d1\u5e72\u9884\u9700\u8981\u5927\u91cf\u533b\u7597\u8d44\u6e90\uff0c\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\u96be\u4ee5\u5b9e\u65bd\u3002", "method": "\u5f00\u53d1KOM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u81ea\u52a8\u5316KOA\u8bc4\u4f30\u3001\u98ce\u9669\u9884\u6d4b\u548c\u6cbb\u7597\u5904\u65b9\uff0c\u57fa\u4e8e\u60a3\u8005\u4e2a\u4eba\u8d44\u6599\u3001\u75be\u75c5\u72b6\u6001\u3001\u98ce\u9669\u56e0\u7d20\u548c\u7981\u5fcc\u75c7\u751f\u6210\u5b9a\u5236\u7ba1\u7406\u8ba1\u5212\u3002", "result": "\u57fa\u51c6\u5b9e\u9a8c\u663e\u793aKOM\u5728\u5f71\u50cf\u5206\u6790\u548c\u5904\u65b9\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff1b\u968f\u673a\u4e09\u81c2\u6a21\u62df\u7814\u7a76\u8868\u660eKOM\u4e0e\u4e34\u5e8a\u533b\u751f\u5408\u4f5c\u53ef\u5c06\u8bca\u65ad\u548c\u89c4\u5212\u65f6\u95f4\u51cf\u5c1138.5%\uff0c\u5e76\u63d0\u9ad8\u6cbb\u7597\u8d28\u91cf\u3002", "conclusion": "KOM\u6709\u52a9\u4e8e\u4fc3\u8fdb\u81ea\u52a8\u5316KOA\u7ba1\u7406\uff0c\u5176\u6a21\u5757\u5316\u67b6\u6784\u4e3a\u5f00\u53d1\u5176\u4ed6\u6162\u6027\u75c5\u7684AI\u8f85\u52a9\u7ba1\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2511.20617", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.20617", "abs": "https://arxiv.org/abs/2511.20617", "authors": ["Saman Dehghan", "Tianran Sun", "Tianxiang Wu", "Zihan Li", "Reyhaneh Jabbarvand"], "title": "Translating Large-Scale C Repositories to Idiomatic Rust", "comment": "21 pages, 14 figures", "summary": "Existing C to Rust translation techniques fail to balance quality and scalability: transpilation-based approaches scale to large projects but produce code with poor safety, idiomaticity, and readability. In contrast, LLM-based techniques are prohibitively expensive due to their reliance on frontier models (without which they cannot reliably generate compilable translations), thus limiting scalability. This paper proposes Rustine, a fully automated pipeline for effective and efficient repository-level C to idiomatic safe Rust translation. Evaluating on a diverse set of 23 C programs, ranging from 27 to 13,200 lines of code, Rustine can generate fully compilable Rust code for all and achieve 87% functional equivalence (passing 1,063,099 assertions out of 1,221,192 in test suites with average function and line coverage of 74.7% and 72.2%). Compared to six prior repository-level C to Rust translation techniques, the translations by Rustine are overall safer (fewer raw pointers, pointer arithmetic, and unsafe constructs), more idiomatic (fewer Rust linter violations), and more readable. When the translations cannot pass all tests to fulfill functional equivalence, human developers were able to complete the task in 4.5 hours, on average, using Rustine as debugging support.", "AI": {"tldr": "Rustine\u662f\u4e00\u4e2a\u81ea\u52a8\u5316C\u5230Rust\u7ffb\u8bd1\u7ba1\u9053\uff0c\u572823\u4e2aC\u7a0b\u5e8f\u4e0a\u5b9e\u73b087%\u529f\u80fd\u7b49\u4ef7\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5b89\u5168\u3001\u66f4\u5730\u9053\u3001\u66f4\u53ef\u8bfb\uff0c\u4e14\u652f\u6301\u9ad8\u6548\u8c03\u8bd5\u3002", "motivation": "\u73b0\u6709C\u5230Rust\u7ffb\u8bd1\u6280\u672f\u65e0\u6cd5\u5e73\u8861\u8d28\u91cf\u4e0e\u53ef\u6269\u5c55\u6027\uff1a\u8f6c\u8bd1\u65b9\u6cd5\u53ef\u6269\u5c55\u4f46\u4ee3\u7801\u8d28\u91cf\u5dee\uff0cLLM\u65b9\u6cd5\u8d28\u91cf\u597d\u4f46\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51faRustine\u5168\u81ea\u52a8\u7ba1\u9053\uff0c\u7528\u4e8e\u4ed3\u5e93\u7ea7\u522b\u7684C\u5230\u5730\u9053\u5b89\u5168Rust\u7ffb\u8bd1\u3002", "result": "\u572823\u4e2aC\u7a0b\u5e8f\uff0827-13,200\u884c\u4ee3\u7801\uff09\u4e0a\u6d4b\u8bd5\uff0c\u6240\u6709\u7ffb\u8bd1\u90fd\u80fd\u7f16\u8bd1\uff0c\u8fbe\u523087%\u529f\u80fd\u7b49\u4ef7\uff08\u901a\u8fc71,063,099/1,221,192\u4e2a\u65ad\u8a00\uff09\uff0c\u51fd\u6570\u548c\u884c\u8986\u76d6\u7387\u5206\u522b\u4e3a74.7%\u548c72.2%\u3002", "conclusion": "Rustine\u751f\u6210\u7684\u7ffb\u8bd1\u6bd4\u516d\u79cd\u73b0\u6709\u65b9\u6cd5\u66f4\u5b89\u5168\u3001\u66f4\u5730\u9053\u3001\u66f4\u53ef\u8bfb\uff0c\u5f53\u7ffb\u8bd1\u672a\u901a\u8fc7\u6240\u6709\u6d4b\u8bd5\u65f6\uff0c\u5f00\u53d1\u8005\u5e73\u5747\u53ea\u97004.5\u5c0f\u65f6\u5b8c\u6210\u8c03\u8bd5\u3002"}}
{"id": "2511.19829", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19829", "abs": "https://arxiv.org/abs/2511.19829", "authors": ["Ke Chen", "Yifeng Wang", "Hassan Almosapeeh", "Haohan Wang"], "title": "A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization", "comment": null, "summary": "Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc4\u4f30\u6307\u5bfc\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u7acb\u7cfb\u7edf\u5316\u7684\u63d0\u793a\u8bc4\u4f30\u6846\u67b6\u548c\u514d\u6267\u884c\u8bc4\u4f30\u5668\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u3001\u67e5\u8be2\u76f8\u5173\u7684\u63d0\u793a\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u9aa8\u5e72\u6a21\u578b\u4e0a\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u5355\u4e00\u9759\u6001\u6a21\u677f\uff0c\u5728\u590d\u6742\u52a8\u6001\u7528\u6237\u573a\u666f\u4e2d\u6548\u679c\u6709\u9650\uff1b\u73b0\u6709\u67e5\u8be2\u76f8\u5173\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u7a33\u5b9a\u7684\u6587\u672c\u53cd\u9988\u6216\u9ed1\u76d2\u5956\u52b1\u6a21\u578b\uff0c\u63d0\u4f9b\u5f31\u4e14\u4e0d\u53ef\u89e3\u91ca\u7684\u4f18\u5316\u4fe1\u53f7\uff1b\u63d0\u793a\u8d28\u91cf\u672c\u8eab\u7f3a\u4e4f\u7edf\u4e00\u7cfb\u7edf\u5b9a\u4e49\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4fe1\u53f7\u788e\u7247\u5316\u4e0d\u53ef\u9760\u3002", "method": "\u9996\u5148\u5efa\u7acb\u4ee5\u6027\u80fd\u4e3a\u5bfc\u5411\u7684\u7cfb\u7edf\u5316\u63d0\u793a\u8bc4\u4f30\u6846\u67b6\uff1b\u5f00\u53d1\u5e76\u5fae\u8c03\u514d\u6267\u884c\u8bc4\u4f30\u5668\uff0c\u76f4\u63a5\u4ece\u6587\u672c\u9884\u6d4b\u591a\u7ef4\u5ea6\u8d28\u91cf\u5206\u6570\uff1b\u8bc4\u4f30\u5668\u6307\u5bfc\u6307\u6807\u611f\u77e5\u4f18\u5316\u5668\uff0c\u4ee5\u53ef\u89e3\u91ca\u65b9\u5f0f\u8bca\u65ad\u5931\u8d25\u6a21\u5f0f\u5e76\u91cd\u5199\u63d0\u793a\u3002", "result": "\u8bc4\u4f30\u5668\u5728\u9884\u6d4b\u63d0\u793a\u6027\u80fd\u65b9\u9762\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u7387\uff1b\u8bc4\u4f30\u6307\u5bfc\u7684\u4f18\u5316\u57288\u4e2a\u6570\u636e\u96c6\u548c3\u4e2a\u9aa8\u5e72\u6a21\u578b\u4e0a\u6301\u7eed\u8d85\u8d8a\u9759\u6001\u6a21\u677f\u548c\u67e5\u8be2\u76f8\u5173\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u6307\u6807\u7684\u63d0\u793a\u8d28\u91cf\u89c6\u89d2\uff0c\u8bc1\u660e\u8bc4\u4f30\u6307\u5bfc\u7684\u4f18\u5316\u6d41\u7a0b\u80fd\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u63d0\u4f9b\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u4e14\u6a21\u578b\u65e0\u5173\u7684\u6539\u8fdb\u3002"}}
{"id": "2511.19849", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19849", "abs": "https://arxiv.org/abs/2511.19849", "authors": ["Dominik Wagner", "Leon Witzman", "Luke Ong"], "title": "Reinforcement Learning with $\u03c9$-Regular Objectives and Constraints", "comment": null, "summary": "Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $\u03c9$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.\n  We address both limitations simultaneously by combining $\u03c9$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $\u03c9$-regular objective while also adhering to $\u03c9$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u03c9-\u6b63\u5219\u76ee\u6807\u4e0e\u663e\u5f0f\u7ea6\u675f\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u89c4\u5212\u7b97\u6cd5\u6700\u5927\u5316\u6ee1\u8db3\u03c9-\u6b63\u5219\u76ee\u6807\u7684\u6982\u7387\uff0c\u540c\u65f6\u786e\u4fdd\u6ee1\u8db3\u03c9-\u6b63\u5219\u7ea6\u675f\u5728\u6307\u5b9a\u9608\u503c\u5185\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u6807\u91cf\u5956\u52b1\uff0c\u8868\u8fbe\u80fd\u529b\u6709\u9650\u4e14\u5bb9\u6613\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff0c\u65e0\u6cd5\u6709\u6548\u8868\u8fbe\u65f6\u95f4\u6027\u3001\u6761\u4ef6\u6027\u6216\u5b89\u5168\u5173\u952e\u76ee\u6807\u3002\u03c9-\u6b63\u5219\u76ee\u6807\u80fd\u7cbe\u786e\u6307\u5b9a\u4e30\u5bcc\u7684\u884c\u4e3a\u5c5e\u6027\uff0c\u4f46\u5355\u4e00\u6807\u91cf\u6027\u80fd\u5ea6\u91cf\u63a9\u76d6\u4e86\u5b89\u5168\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5c06\u03c9-\u6b63\u5219\u76ee\u6807\u4e0e\u7ea6\u675f\u5206\u79bb\u5904\u7406\uff0c\u5efa\u7acb\u5230\u7ea6\u675f\u6781\u9650\u5e73\u5747\u95ee\u9898\u7684\u8f6c\u6362\u5e76\u4fdd\u6301\u6700\u4f18\u6027\u4fdd\u8bc1\u3002", "result": "\u5728\u6781\u9650\u60c5\u51b5\u4e0b\uff0c\u8be5\u7b97\u6cd5\u80fd\u4ea7\u751f\u6700\u5927\u5316\u6ee1\u8db3\u03c9-\u6b63\u5219\u76ee\u6807\u6982\u7387\u7684\u7b56\u7565\uff0c\u540c\u65f6\u786e\u4fdd\u03c9-\u6b63\u5219\u7ea6\u675f\u5728\u6307\u5b9a\u9608\u503c\u5185\u5f97\u5230\u6ee1\u8db3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u540c\u65f6\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u8868\u8fbe\u80fd\u529b\u548c\u5b89\u5168-\u6027\u80fd\u6743\u8861\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u5904\u7406\u590d\u6742\u884c\u4e3a\u89c4\u8303\u548c\u5b89\u5168\u8981\u6c42\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2511.20252", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.20252", "abs": "https://arxiv.org/abs/2511.20252", "authors": ["Gabriel K. Gegenhuber", "Philipp \u00c9. Frenzel", "Maximilian G\u00fcnther", "Johanna Ullrich", "Aljosha Judmayer"], "title": "Hey there! You are using WhatsApp: Enumerating Three Billion Accounts for Security and Privacy", "comment": "Accepted to NDSS2026, Artifacts available at https://github.com/sbaresearch/whatsapp-census", "summary": "WhatsApp, with 3.5 billion active accounts as of early 2025, is the world's largest instant messaging platform. Given its massive user base, WhatsApp plays a critical role in global communication.\n  To initiate conversations, users must first discover whether their contacts are registered on the platform. This is achieved by querying WhatsApp's servers with mobile phone numbers extracted from the user's address book (if they allowed access). This architecture inherently enables phone number enumeration, as the service must allow legitimate users to query contact availability. While rate limiting is a standard defense against abuse, we revisit the problem and show that WhatsApp remains highly vulnerable to enumeration at scale. In our study, we were able to probe over a hundred million phone numbers per hour without encountering blocking or effective rate limiting.\n  Our findings demonstrate not only the persistence but the severity of this vulnerability. We further show that nearly half of the phone numbers disclosed in the 2021 Facebook data leak are still active on WhatsApp, underlining the enduring risks associated with such exposures. Moreover, we were able to perform a census of WhatsApp users, providing a glimpse on the macroscopic insights a large messaging service is able to generate even though the messages themselves are end-to-end encrypted. Using the gathered data, we also discovered the re-use of certain X25519 keys across different devices and phone numbers, indicating either insecure (custom) implementations, or fraudulent activity.\n  In this updated version of the paper, we also provide insights into the collaborative remediation process through which we confirmed that the underlying rate-limiting issue had been resolved.", "AI": {"tldr": "WhatsApp\u5b58\u5728\u4e25\u91cd\u7684\u7535\u8bdd\u53f7\u7801\u679a\u4e3e\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u6bcf\u5c0f\u65f6\u53ef\u63a2\u6d4b\u8d85\u8fc71\u4ebf\u4e2a\u53f7\u7801\u800c\u4e0d\u88ab\u963b\u6b62\uff0c\u8fd1\u534a\u65702021\u5e74Facebook\u6570\u636e\u6cc4\u9732\u4e2d\u7684\u7535\u8bdd\u53f7\u7801\u4ecd\u5728WhatsApp\u6d3b\u8dc3\uff0c\u4e14\u53d1\u73b0X25519\u5bc6\u94a5\u5728\u4e0d\u540c\u8bbe\u5907\u548c\u53f7\u7801\u95f4\u91cd\u590d\u4f7f\u7528\u7684\u95ee\u9898\u3002", "motivation": "WhatsApp\u4f5c\u4e3a\u5168\u7403\u6700\u5927\u7684\u5373\u65f6\u901a\u8baf\u5e73\u53f0\uff0c\u5176\u7535\u8bdd\u53f7\u7801\u67e5\u8be2\u673a\u5236\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8be5\u5e73\u53f0\u5728\u7535\u8bdd\u53f7\u7801\u679a\u4e3e\u65b9\u9762\u7684\u6301\u7eed\u8106\u5f31\u6027\u53ca\u5176\u5bf9\u7528\u6237\u9690\u79c1\u7684\u5a01\u80c1\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u63a2\u6d4bWhatsApp\u670d\u52a1\u5668\uff0c\u6d4b\u8bd5\u5176\u7535\u8bdd\u53f7\u7801\u67e5\u8be2\u529f\u80fd\u7684\u901f\u7387\u9650\u5236\u673a\u5236\uff0c\u5206\u6790\u6570\u636e\u6cc4\u9732\u4e2d\u7535\u8bdd\u53f7\u7801\u7684\u6d3b\u8dc3\u72b6\u6001\uff0c\u5e76\u68c0\u67e5\u52a0\u5bc6\u5bc6\u94a5\u7684\u4f7f\u7528\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0WhatsApp\u7f3a\u4e4f\u6709\u6548\u7684\u901f\u7387\u9650\u5236\uff0c\u6bcf\u5c0f\u65f6\u53ef\u63a2\u6d4b\u8d851\u4ebf\u53f7\u7801\uff1b47%\u7684Facebook\u6cc4\u9732\u53f7\u7801\u4ecd\u5728WhatsApp\u6d3b\u8dc3\uff1b\u53d1\u73b0X25519\u5bc6\u94a5\u91cd\u590d\u4f7f\u7528\u95ee\u9898\uff1b\u6700\u7ec8\u901a\u8fc7\u4e0e\u5e73\u53f0\u5408\u4f5c\u786e\u8ba4\u6f0f\u6d1e\u5df2\u4fee\u590d\u3002", "conclusion": "WhatsApp\u7684\u7535\u8bdd\u53f7\u7801\u679a\u4e3e\u6f0f\u6d1e\u4e25\u91cd\u5a01\u80c1\u7528\u6237\u9690\u79c1\uff0c\u5373\u4f7f\u6d88\u606f\u7aef\u5230\u7aef\u52a0\u5bc6\uff0c\u5e73\u53f0\u4ecd\u80fd\u751f\u6210\u5b8f\u89c2\u7528\u6237\u6d1e\u5bdf\uff0c\u9700\u8981\u52a0\u5f3a\u901f\u7387\u9650\u5236\u548c\u5bc6\u94a5\u7ba1\u7406\u673a\u5236\u3002"}}
{"id": "2511.19864", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19864", "abs": "https://arxiv.org/abs/2511.19864", "authors": ["Valerie Lockhart", "Dan McCreary", "Troy A. Peterson"], "title": "MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support", "comment": "42 pages, 4 figures", "summary": "Educational simulations have long been recognized as powerful tools for enhancing learning outcomes, yet their creation has traditionally required substantial resources and technical expertise. This paper introduces MicroSims a novel framework for creating lightweight, interactive educational simulations that can be rapidly generated using artificial intelligence, universally embedded across digital learning platforms, and easily customized without programming knowledge. MicroSims occupy a unique position at the intersection of three key innovations: (1) standardized design patterns that enable AI-assisted generation, (2) iframe-based architecture that provides universal embedding and sandboxed security, and (3) transparent, modifiable code that supports customization and pedagogical transparency. We present a comprehensive framework encompassing design principles, technical architecture, metadata standards, and development workflows. Drawing on empirical research from physics education studies and meta-analyses across STEM disciplines, we demonstrate that interactive simulations can improve conceptual understanding by up to 30-40\\% compared to traditional instruction. MicroSims extend these benefits while addressing persistent barriers of cost, technical complexity, and platform dependence. This work has significant implications for educational equity, and low-cost intelligent interactive textbooks that enabling educators worldwide to create customized, curriculum-aligned simulations on demand. We discuss implementation considerations, present evidence of effectiveness, and outline future directions for AI-powered adaptive learning systems built on the MicroSim foundation.", "AI": {"tldr": "MicroSims\u662f\u4e00\u4e2a\u7528\u4e8e\u521b\u5efa\u8f7b\u91cf\u7ea7\u4ea4\u4e92\u5f0f\u6559\u80b2\u6a21\u62df\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7AI\u5feb\u901f\u751f\u6210\u3001\u8de8\u5e73\u53f0\u5d4c\u5165\u548c\u65e0\u7f16\u7a0b\u5b9a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u62df\u521b\u5efa\u8d44\u6e90\u5bc6\u96c6\u548c\u6280\u672f\u8981\u6c42\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6559\u80b2\u6a21\u62df\u521b\u5efa\u9700\u8981\u5927\u91cf\u8d44\u6e90\u548c\u6280\u672f\u4e13\u957f\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002MicroSims\u65e8\u5728\u901a\u8fc7AI\u8f85\u52a9\u751f\u6210\u548c\u6807\u51c6\u5316\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u964d\u4f4e\u521b\u5efa\u95e8\u69db\uff0c\u63d0\u9ad8\u6559\u80b2\u516c\u5e73\u6027\u3002", "method": "\u91c7\u7528\u6807\u51c6\u5316\u8bbe\u8ba1\u6a21\u5f0f\u652f\u6301AI\u8f85\u52a9\u751f\u6210\uff0c\u57fa\u4e8eiframe\u7684\u67b6\u6784\u5b9e\u73b0\u901a\u7528\u5d4c\u5165\u548c\u5b89\u5168\u6c99\u7bb1\uff0c\u63d0\u4f9b\u900f\u660e\u53ef\u4fee\u6539\u4ee3\u7801\u652f\u6301\u5b9a\u5236\u548c\u6559\u5b66\u900f\u660e\u5ea6\u3002", "result": "\u7814\u7a76\u8868\u660e\u4ea4\u4e92\u5f0f\u6a21\u62df\u76f8\u6bd4\u4f20\u7edf\u6559\u5b66\u53ef\u5c06\u6982\u5ff5\u7406\u89e3\u63d0\u9ad830-40%\uff0cMicroSims\u5728\u4fdd\u6301\u8fd9\u4e9b\u4f18\u52bf\u7684\u540c\u65f6\u89e3\u51b3\u4e86\u6210\u672c\u3001\u6280\u672f\u590d\u6742\u6027\u548c\u5e73\u53f0\u4f9d\u8d56\u7b49\u957f\u671f\u969c\u788d\u3002", "conclusion": "MicroSims\u4e3a\u6559\u80b2\u516c\u5e73\u548c\u4f4e\u6210\u672c\u667a\u80fd\u4ea4\u4e92\u5f0f\u6559\u79d1\u4e66\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u4f7f\u5168\u7403\u6559\u80b2\u5de5\u4f5c\u8005\u80fd\u591f\u6309\u9700\u521b\u5efa\u5b9a\u5236\u5316\u3001\u4e0e\u8bfe\u7a0b\u5bf9\u9f50\u7684\u6a21\u62df\uff0c\u5e76\u652f\u6301\u672a\u6765AI\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2511.20284", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20284", "abs": "https://arxiv.org/abs/2511.20284", "authors": ["Friederike Groschupp", "Daniele Lain", "Aritra Dhar", "Lara Magdalena Lazier", "Srdjan \u010capkun"], "title": "Can LLMs Make (Personalized) Access Control Decisions?", "comment": null, "summary": "Precise access control decisions are crucial to the security of both traditional applications and emerging agent-based systems. Typically, these decisions are made by users during app installation or at runtime. Due to the increasing complexity and automation of systems, making these access control decisions can add a significant cognitive load on users, often overloading them and leading to suboptimal or even arbitrary access control decisions. To address this problem, we propose to leverage the processing and reasoning capabilities of large language models (LLMs) to make dynamic, context-aware decisions aligned with the user's security preferences. For this purpose, we conducted a user study, which resulted in a dataset of 307 natural-language privacy statements and 14,682 access control decisions made by users. We then compare these decisions against those made by two versions of LLMs: a general and a personalized one, for which we also gathered user feedback on 1,446 of its decisions.\n  Our results show that in general, LLMs can reflect users' preferences well, achieving up to 86\\% accuracy when compared to the decision made by the majority of users. Our study also reveals a crucial trade-off in personalizing such a system: while providing user-specific privacy preferences to the LLM generally improves agreement with individual user decisions, adhering to those preferences can also violate some security best practices. Based on our findings, we discuss design and risk considerations for implementing a practical natural-language-based access control system that balances personalization, security, and utility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bbf\u95ee\u63a7\u5236\u51b3\u7b56\uff0c\u4ee5\u51cf\u8f7b\u7528\u6237\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002\u901a\u8fc7\u7528\u6237\u7814\u7a76\u6536\u96c6\u4e86307\u4e2a\u81ea\u7136\u8bed\u8a00\u9690\u79c1\u58f0\u660e\u548c14,682\u4e2a\u7528\u6237\u51b3\u7b56\uff0c\u6bd4\u8f83\u4e86\u901a\u7528\u548c\u4e2a\u6027\u5316LLM\u7684\u51b3\u7b56\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u7cfb\u7edf\u590d\u6742\u6027\u548c\u81ea\u52a8\u5316\u7a0b\u5ea6\u7684\u63d0\u9ad8\uff0c\u7528\u6237\u5728\u8fdb\u884c\u8bbf\u95ee\u63a7\u5236\u51b3\u7b56\u65f6\u9762\u4e34\u5de8\u5927\u7684\u8ba4\u77e5\u8d1f\u62c5\uff0c\u5bfc\u81f4\u51b3\u7b56\u8d28\u91cf\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7406\u89e3\u7528\u6237\u5b89\u5168\u504f\u597d\u5e76\u505a\u51fa\u52a8\u6001\u51b3\u7b56\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u6536\u96c6\u81ea\u7136\u8bed\u8a00\u9690\u79c1\u58f0\u660e\u548c\u7528\u6237\u51b3\u7b56\u6570\u636e\uff0c\u7136\u540e\u6bd4\u8f83\u901a\u7528LLM\u548c\u4e2a\u6027\u5316LLM\uff08\u57fa\u4e8e\u7528\u6237\u7279\u5b9a\u9690\u79c1\u504f\u597d\uff09\u7684\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u5e76\u6536\u96c6\u7528\u6237\u5bf91,446\u4e2aLLM\u51b3\u7b56\u7684\u53cd\u9988\u3002", "result": "LLMs\u80fd\u591f\u8f83\u597d\u5730\u53cd\u6620\u7528\u6237\u504f\u597d\uff0c\u4e0e\u5927\u591a\u6570\u7528\u6237\u51b3\u7b56\u76f8\u6bd4\u8fbe\u523086%\u7684\u51c6\u786e\u7387\u3002\u4e2a\u6027\u5316LLM\u867d\u7136\u80fd\u63d0\u9ad8\u4e0e\u4e2a\u4f53\u7528\u6237\u51b3\u7b56\u7684\u4e00\u81f4\u6027\uff0c\u4f46\u53ef\u80fd\u8fdd\u53cd\u67d0\u4e9b\u5b89\u5168\u6700\u4f73\u5b9e\u8df5\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5b9e\u73b0\u5b9e\u7528\u81ea\u7136\u8bed\u8a00\u8bbf\u95ee\u63a7\u5236\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u98ce\u9669\u8003\u8651\uff0c\u9700\u8981\u5728\u4e2a\u6027\u5316\u3001\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2511.20290", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20290", "abs": "https://arxiv.org/abs/2511.20290", "authors": ["Xuebo Qiu", "Mingqi Lv", "Yimei Zhang", "Tieming Chen", "Tiantian Zhu", "Qijie Song", "Shouling Ji"], "title": "APT-CGLP: Advanced Persistent Threat Hunting via Contrastive Graph-Language Pre-Training", "comment": "Accepted by SIGKDD 2026 Research Track", "summary": "Provenance-based threat hunting identifies Advanced Persistent Threats (APTs) on endpoints by correlating attack patterns described in Cyber Threat Intelligence (CTI) with provenance graphs derived from system audit logs. A fundamental challenge in this paradigm lies in the modality gap -- the structural and semantic disconnect between provenance graphs and CTI reports. Prior work addresses this by framing threat hunting as a graph matching task: 1) extracting attack graphs from CTI reports, and 2) aligning them with provenance graphs. However, this pipeline incurs severe \\textit{information loss} during graph extraction and demands intensive manual curation, undermining scalability and effectiveness.\n  In this paper, we present APT-CGLP, a novel cross-modal APT hunting system via Contrastive Graph-Language Pre-training, facilitating end-to-end semantic matching between provenance graphs and CTI reports without human intervention. First, empowered by the Large Language Model (LLM), APT-CGLP mitigates data scarcity by synthesizing high-fidelity provenance graph-CTI report pairs, while simultaneously distilling actionable insights from noisy web-sourced CTIs to improve their operational utility. Second, APT-CGLP incorporates a tailored multi-objective training algorithm that synergizes contrastive learning with inter-modal masked modeling, promoting cross-modal attack semantic alignment at both coarse- and fine-grained levels. Extensive experiments on four real-world APT datasets demonstrate that APT-CGLP consistently outperforms state-of-the-art threat hunting baselines in terms of accuracy and efficiency.", "AI": {"tldr": "APT-CGLP\u662f\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u56fe-\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u65b0\u578b\u8de8\u6a21\u6001APT\u72e9\u730e\u7cfb\u7edf\uff0c\u80fd\u591f\u76f4\u63a5\u5728\u6eaf\u6e90\u56fe\u548cCTI\u62a5\u544a\u4e4b\u95f4\u8fdb\u884c\u7aef\u5230\u7aef\u8bed\u4e49\u5339\u914d\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u56fe\u5339\u914d\u7684\u5a01\u80c1\u72e9\u730e\u65b9\u6cd5\u5b58\u5728\u7684\u4fe1\u606f\u4e22\u5931\u548c\u4eba\u5de5\u5e72\u9884\u95ee\u9898\uff0c\u5f25\u6eaf\u6e90\u56fe\u4e0eCTI\u62a5\u544a\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u3002", "method": "1\uff09\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5408\u6210\u9ad8\u4fdd\u771f\u6eaf\u6e90\u56fe-CTI\u62a5\u544a\u5bf9\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\uff1b2\uff09\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u63a9\u7801\u5efa\u6a21\u7684\u591a\u76ee\u6807\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5b9e\u73b0\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u7684\u8de8\u6a21\u6001\u653b\u51fb\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9eAPT\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAPT-CGLP\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5a01\u80c1\u72e9\u730e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "APT-CGLP\u901a\u8fc7\u7aef\u5230\u7aef\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u5339\u914d\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5a01\u80c1\u72e9\u730e\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86APT\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.19872", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19872", "abs": "https://arxiv.org/abs/2511.19872", "authors": ["Daniel I Jackson", "Emma L Jensen", "Syed-Amad Hussain", "Emre Sezgin"], "title": "Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy", "comment": "25 pages,5 tables, 3 figures", "summary": "Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u901a\u7528\u81ea\u6211\u6548\u80fd\u611f\u91cf\u8868(GSES)\u5e94\u7528\u4e8e10\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u56db\u79cd\u4efb\u52a1\u6761\u4ef6\u4e0b\u8bc4\u4f30\u5176\u6a21\u62df\u81ea\u6211\u8bc4\u4f30\u884c\u4e3a\u3002\u7ed3\u679c\u663e\u793a\u6a21\u578b\u81ea\u6211\u8bc4\u4f30\u7a33\u5b9a\u4f46\u666e\u904d\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff0c\u4e14\u81ea\u6211\u8bc4\u4f30\u4e0e\u5b9e\u9645\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u5b58\u5728\u8f7b\u5ea6\u9ad8\u4f30\u503e\u5411\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u81ea\u6211\u8bc4\u4f30\u8fd9\u4e00\u53ef\u9760\u667a\u80fd\u7684\u5173\u952e\u65b9\u9762\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u7684\u6a21\u62df\u81ea\u6211\u8bc4\u4f30\u884c\u4e3a\u53ca\u5176\u4e0e\u5b9e\u9645\u8868\u73b0\u7684\u5173\u7cfb\u3002", "method": "\u5c0610\u9879\u901a\u7528\u81ea\u6211\u6548\u80fd\u611f\u91cf\u8868(GSES)\u9002\u914d\u7528\u4e8e10\u4e2aLLMs\uff0c\u5728\u56db\u79cd\u6761\u4ef6\u4e0b(\u65e0\u4efb\u52a1\u3001\u8ba1\u7b97\u63a8\u7406\u3001\u793e\u4f1a\u63a8\u7406\u3001\u6458\u8981)\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5206\u6790\u81ea\u6211\u8bc4\u4f30\u7684\u7a33\u5b9a\u6027\u3001\u51c6\u786e\u6027\u4ee5\u53ca\u4e0e\u8868\u73b0\u7684\u5173\u7cfb\u3002", "result": "\u6a21\u578b\u81ea\u6211\u8bc4\u4f30\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u663e\u8457\u5dee\u5f02\uff0c\u603b\u4f53\u4f4e\u4e8e\u4eba\u7c7b\u6807\u51c6\uff1b\u6240\u6709\u6a21\u578b\u5728\u8ba1\u7b97\u548c\u793e\u4f1a\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u5b8c\u7f8e\uff0c\u4f46\u6458\u8981\u4efb\u52a1\u8868\u73b0\u5dee\u5f02\u5927\uff1b\u81ea\u6211\u8bc4\u4f30\u4e0e\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u5b58\u5728\u8f7b\u5ea6\u9ad8\u4f30\uff1b\u9ad8\u81ea\u6211\u6548\u80fd\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u81ea\u4fe1\u3001\u62df\u4eba\u5316\u7684\u63a8\u7406\u98ce\u683c\u3002", "conclusion": "\u5fc3\u7406\u6d4b\u91cf\u63d0\u793a\u4e3a\u7406\u89e3LLM\u6c9f\u901a\u884c\u4e3a\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6d1e\u5bdf\uff0c\u4f46\u4e0d\u80fd\u63d0\u4f9b\u6821\u51c6\u7684\u6027\u80fd\u4f30\u8ba1\uff0c\u81ea\u6211\u8bc4\u4f30\u4e0e\u5b9e\u9645\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u8131\u8282\u3002"}}
{"id": "2511.20313", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20313", "abs": "https://arxiv.org/abs/2511.20313", "authors": ["Li Zhou", "Marc Dacier", "Charalambos Konstantinou"], "title": "A Reality Check on SBOM-based Vulnerability Management: An Empirical Study and A Path Forward", "comment": null, "summary": "The Software Bill of Materials (SBOM) is a critical tool for securing the software supply chain (SSC), but its practical utility is undermined by inaccuracies in both its generation and its application in vulnerability scanning. This paper presents a large-scale empirical study on 2,414 open-source repositories to address these issues from a practical standpoint. First, we demonstrate that using lock files with strong package managers enables the generation of accurate and consistent SBOMs, establishing a reliable foundation for security analysis. Using this high-fidelity foundation, however, we expose a more fundamental flaw in practice: downstream vulnerability scanners produce a staggering 97.5\\% false positive rate. We pinpoint the primary cause as the flagging of vulnerabilities within unreachable code. We then demonstrate that function call analysis can effectively prune 63.3\\% of these false alarms. Our work validates a practical, two-stage approach for SSC security: first, generate an accurate SBOM using lock files and strong package managers, and second, enrich it with function call analysis to produce actionable, low-noise vulnerability reports that alleviate developers' alert fatigue.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u8f6f\u4ef6\u7269\u6599\u6e05\u5355(SBOM)\u5728\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5b89\u5168\u4e2d\u7684\u5b9e\u7528\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u9501\u5b9a\u6587\u4ef6\u548c\u51fd\u6570\u8c03\u7528\u5206\u6790\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u6765\u63d0\u9ad8\u6f0f\u6d1e\u626b\u63cf\u51c6\u786e\u6027\u3002", "motivation": "SBOM\u662f\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5b89\u5168\u7684\u5173\u952e\u5de5\u5177\uff0c\u4f46\u5176\u5b9e\u9645\u6548\u7528\u53d7\u5230\u751f\u6210\u4e0d\u51c6\u786e\u548c\u6f0f\u6d1e\u626b\u63cf\u8bef\u62a5\u7387\u9ad8\u7684\u5f71\u54cd\uff0c\u9700\u8981\u4ece\u5b9e\u8df5\u89d2\u5ea6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5bf92,414\u4e2a\u5f00\u6e90\u4ed3\u5e93\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff1a\u9996\u5148\u4f7f\u7528\u9501\u5b9a\u6587\u4ef6\u548c\u5f3a\u5305\u7ba1\u7406\u5668\u751f\u6210\u51c6\u786e\u7684SBOM\uff0c\u7136\u540e\u901a\u8fc7\u51fd\u6570\u8c03\u7528\u5206\u6790\u6765\u51cf\u5c11\u6f0f\u6d1e\u626b\u63cf\u7684\u8bef\u62a5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u4f7f\u7528\u9501\u5b9a\u6587\u4ef6\u53ef\u4ee5\u751f\u6210\u51c6\u786e\u4e00\u81f4\u7684SBOM\uff1b\u4e0b\u6e38\u6f0f\u6d1e\u626b\u63cf\u5668\u4ea7\u751f\u9ad8\u8fbe97.5%\u7684\u8bef\u62a5\u7387\uff1b\u51fd\u6570\u8c03\u7528\u5206\u6790\u53ef\u4ee5\u6709\u6548\u51cf\u5c1163.3%\u7684\u8bef\u62a5\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528\u9501\u5b9a\u6587\u4ef6\u548c\u5f3a\u5305\u7ba1\u7406\u5668\u751f\u6210\u51c6\u786eSBOM\uff0c\u7136\u540e\u901a\u8fc7\u51fd\u6570\u8c03\u7528\u5206\u6790\u4ea7\u751f\u53ef\u64cd\u4f5c\u3001\u4f4e\u566a\u58f0\u7684\u6f0f\u6d1e\u62a5\u544a\uff0c\u51cf\u8f7b\u5f00\u53d1\u8005\u7684\u8b66\u62a5\u75b2\u52b3\u3002"}}
{"id": "2511.19895", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19895", "abs": "https://arxiv.org/abs/2511.19895", "authors": ["Yuanyuan Lin", "Xiangyu Ouyang", "Teng Zhang", "Kaixin Sui"], "title": "RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation", "comment": "Accepted at AAAI 2026", "summary": "Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.", "AI": {"tldr": "RPM-MCTS\u662f\u4e00\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u68c0\u7d22\u4f5c\u4e3a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u6765\u8bc4\u4f30\u4e2d\u95f4\u7b97\u6cd5\u6b65\u9aa4\uff0c\u65e0\u9700\u590d\u6742\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u76f8\u4f3c\u6027\u8fc7\u6ee4\u53bb\u9664\u5197\u4f59\u8282\u70b9\uff0c\u5e76\u5229\u7528\u6c99\u7bb1\u6267\u884c\u53cd\u9988\u5b9a\u4f4d\u548c\u7ea0\u6b63\u9519\u8bef\u6b65\u9aa4\uff0c\u5728\u51cf\u5c1115%\u4ee4\u724c\u6d88\u8017\u7684\u540c\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6811\u641c\u7d22\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\u4e2d\u95f4\u7b97\u6cd5\u6b65\u9aa4\uff0c\u65e0\u6cd5\u53ca\u65f6\u5b9a\u4f4d\u548c\u7ea0\u6b63\u9519\u8bef\u6b65\u9aa4\uff0c\u5bfc\u81f4\u751f\u6210\u9519\u8bef\u4ee3\u7801\u548c\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u3002", "method": "\u63d0\u51faRPM-MCTS\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u77e5\u8bc6\u68c0\u7d22\u4f5c\u4e3a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u4e2d\u95f4\u6b65\u9aa4\uff1b2\uff09\u5728\u6269\u5c55\u9636\u6bb5\u91c7\u7528\u76f8\u4f3c\u6027\u8fc7\u6ee4\u53bb\u9664\u5197\u4f59\u8282\u70b9\uff1b3\uff09\u5229\u7528\u6c99\u7bb1\u6267\u884c\u53cd\u9988\u5b9a\u4f4d\u548c\u7ea0\u6b63\u9519\u8bef\u7b97\u6cd5\u6b65\u9aa4\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRPM-MCTS\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u5b9e\u73b0\u7ea615%\u7684\u4ee4\u724c\u6d88\u8017\u51cf\u5c11\u3002\u4f7f\u7528RPM-MCTS\u6784\u5efa\u7684\u6570\u636e\u5bf9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5168\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u5176\u4ee3\u7801\u80fd\u529b\u3002", "conclusion": "RPM-MCTS\u901a\u8fc7\u77e5\u8bc6\u68c0\u7d22\u548c\u6c99\u7bb1\u53cd\u9988\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4e2d\u95f4\u6b65\u9aa4\u8bc4\u4f30\u548c\u9519\u8bef\u7ea0\u6b63\u95ee\u9898\uff0c\u5728\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2511.19925", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19925", "abs": "https://arxiv.org/abs/2511.19925", "authors": ["Qiyao Wei", "Edward Morrell", "Lea Goetz", "Mihaela van der Schaar"], "title": "Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity", "comment": null, "summary": "Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at https://github.com/QiyaoWei/semantic-kg and the dataset is available at https://huggingface.co/datasets/QiyaoWei/Semantic-KG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u57fa\u51c6\u6570\u636e\u96c6\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u8f93\u51fa\u8bed\u4e49\u76f8\u4f3c\u5ea6\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u3001\u6210\u672c\u9ad8\u3001\u9886\u57df\u9002\u7528\u6027\u6709\u9650\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30LLM\u6587\u672c\u8f93\u51fa\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u65b9\u6cd5\u53ef\u80fd\u66f4\u5173\u6ce8\u53e5\u6cd5\u6216\u8bcd\u6c47\u5f62\u5f0f\u800c\u975e\u8bed\u4e49\u5185\u5bb9\uff0c\u4e14\u73b0\u6709\u57fa\u51c6\u5b58\u5728\u751f\u6210\u6210\u672c\u9ad8\u3001\u9886\u57df\u9002\u7528\u6027\u6709\u9650\u3001\u8bed\u4e49\u7b49\u4ef7\u5b9a\u4e49\u4e0d\u660e\u786e\u7b49\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u8bed\u4e49\u76f8\u4f3c\u6216\u4e0d\u76f8\u4f3c\u7684\u81ea\u7136\u8bed\u8a00\u9648\u8ff0\u5bf9\uff0c\u5176\u4e2d\u4e0d\u76f8\u4f3c\u5bf9\u5206\u4e3a\u56db\u79cd\u5b50\u7c7b\u578b\uff0c\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\u751f\u6210\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u6bd4\u8f83\u4f20\u7edfNLP\u8bc4\u5206\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bed\u4e49\u53d8\u5316\u7684\u5b50\u7c7b\u578b\u4ee5\u53ca\u57fa\u51c6\u9886\u57df\u90fd\u4f1a\u5f71\u54cd\u8bed\u4e49\u76f8\u4f3c\u5ea6\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u6ca1\u6709\u4e00\u79cd\u65b9\u6cd5\u59cb\u7ec8\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u68c0\u6d4b\u6587\u672c\u8bed\u4e49\u5185\u5bb9\u5177\u6709\u91cd\u8981\u542f\u793a\uff0c\u8868\u660e\u9700\u8981\u66f4\u7ec6\u81f4\u5730\u8003\u8651\u8bed\u4e49\u53d8\u5316\u7c7b\u578b\u548c\u9886\u57df\u7279\u6027\u3002"}}
{"id": "2511.20533", "categories": ["cs.CR", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.20533", "abs": "https://arxiv.org/abs/2511.20533", "authors": ["Ilias Cherkaoui", "Indrakshi Dey"], "title": "Engel p-adic Isogeny-based Cryptography over Laurent Series: Foundations, Security, and an ESP32 Implementation", "comment": null, "summary": "Securing the Internet of Things (IoT) against quantum attacks requires public-key cryptography that (i) remains compact and (ii) runs efficiently on microcontrollers, capabilities many post-quantum (PQ) schemes lack due to large keys and heavy arithmetic. We address both constraints simultaneously with, to our knowledge, the first-ever isogeny framework that encodes super-singular elliptic-curve isogeny data via novel Engel expansions over the p-adic Laurent series. Engel coefficients compress torsion information, thereby addressing the compactness constraint, yielding public keys of ~1.1 - 16.9 kbits preserving the hallmark small sizes of isogeny systems. Engel arithmetic is local and admits fixed-precision p-adic operations, enabling micro-controller efficiency with low-memory, branch-regular kernels suitable for embedded targets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4f7f\u7528p\u8fdb\u6d1b\u6717\u7ea7\u6570\u4e0a\u7684Engel\u5c55\u5f00\u7f16\u7801\u8d85\u5947\u5f02\u692d\u5706\u66f2\u7ebf\u540c\u6e90\u6570\u636e\u7684\u540c\u6e90\u6846\u67b6\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u540e\u91cf\u5b50\u5bc6\u7801\u5728\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u7684\u7d27\u51d1\u6027\u548c\u6548\u7387\u7ea6\u675f\u3002", "motivation": "\u4fdd\u62a4\u7269\u8054\u7f51\u514d\u53d7\u91cf\u5b50\u653b\u51fb\u9700\u8981\u65e2\u7d27\u51d1\u53c8\u80fd\u5728\u5fae\u63a7\u5236\u5668\u4e0a\u9ad8\u6548\u8fd0\u884c\u7684\u540e\u91cf\u5b50\u516c\u94a5\u5bc6\u7801\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6848\u56e0\u5bc6\u94a5\u5927\u548c\u7b97\u672f\u8fd0\u7b97\u91cd\u800c\u96be\u4ee5\u6ee1\u8db3\u8981\u6c42\u3002", "method": "\u4f7f\u7528\u65b0\u9896\u7684Engel\u5c55\u5f00\u5728p\u8fdb\u6d1b\u6717\u7ea7\u6570\u4e0a\u7f16\u7801\u8d85\u5947\u5f02\u692d\u5706\u66f2\u7ebf\u540c\u6e90\u6570\u636e\uff0cEngel\u7cfb\u6570\u538b\u7f29\u6320\u4fe1\u606f\u4ee5\u5b9e\u73b0\u7d27\u51d1\u6027\uff0cEngel\u7b97\u672f\u652f\u6301\u56fa\u5b9a\u7cbe\u5ea6p\u8fdb\u8fd0\u7b97\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u76ee\u6807\u3002", "result": "\u5b9e\u73b0\u4e86~1.1-16.9 kbits\u7684\u7d27\u51d1\u516c\u94a5\u5927\u5c0f\uff0c\u4fdd\u6301\u4e86\u540c\u6e90\u7cfb\u7edf\u7684\u5c0f\u5c3a\u5bf8\u7279\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u9002\u5408\u5fae\u63a7\u5236\u5668\u7684\u4f4e\u5185\u5b58\u3001\u5206\u652f\u89c4\u5219\u7684\u5185\u6838\u3002", "conclusion": "\u8be5\u540c\u6e90\u6846\u67b6\u540c\u65f6\u89e3\u51b3\u4e86\u540e\u91cf\u5b50\u5bc6\u7801\u5728\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u7684\u7d27\u51d1\u6027\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u91cf\u5b50\u5b89\u5168\u7684\u7269\u8054\u7f51\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.19933", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19933", "abs": "https://arxiv.org/abs/2511.19933", "authors": ["Vaishali Vinay"], "title": "A System-Level Taxonomy of Failure Modes in Large Language Model Applications", "comment": null, "summary": "Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u73b0\u5b9e\u4e16\u754cLLM\u5e94\u7528\u768415\u79cd\u9690\u85cf\u6545\u969c\u6a21\u5f0f\u7684\u7cfb\u7edf\u7ea7\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u4e86\u8bc4\u4f30\u4e0e\u76d1\u63a7\u5b9e\u8df5\u7684\u5dee\u8ddd\uff0c\u5e76\u63a2\u8ba8\u4e86\u90e8\u7f72LLM\u7684\u751f\u4ea7\u6311\u6218\u548c\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "LLM\u88ab\u5feb\u901f\u96c6\u6210\u5230\u51b3\u7b56\u652f\u6301\u5de5\u5177\u548c\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4f46\u5176\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u4ecd\u77e5\u4e4b\u751a\u5c11\uff0c\u4e14\u6545\u969c\u6a21\u5f0f\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6709\u6839\u672c\u4e0d\u540c\u3002", "method": "\u63d0\u51fa\u7cfb\u7edf\u7ea7\u6545\u969c\u6a21\u5f0f\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u8bc4\u4f30\u76d1\u63a7\u5dee\u8ddd\uff0c\u7814\u7a76\u90e8\u7f72\u6311\u6218\uff0c\u5e76\u5236\u5b9a\u8bbe\u8ba1\u539f\u5219\u3002", "result": "\u8bc6\u522b\u4e8615\u79cd\u9690\u85cf\u6545\u969c\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u8bc4\u4f30\u5b9e\u8df5\u4e0e\u751f\u4ea7\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u6784\u5efa\u53ef\u9760LLM\u7cfb\u7edf\u7684\u8bbe\u8ba1\u539f\u5219\u3002", "conclusion": "\u901a\u8fc7\u5c06LLM\u53ef\u9760\u6027\u89c6\u4e3a\u7cfb\u7edf\u5de5\u7a0b\u95ee\u9898\u800c\u975e\u7eaf\u6a21\u578b\u4e2d\u5fc3\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u8bc4\u4f30\u65b9\u6cd5\u3001AI\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u9760LLM\u90e8\u7f72\u7814\u7a76\u63d0\u4f9b\u4e86\u5206\u6790\u57fa\u7840\u3002"}}
{"id": "2511.20555", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20555", "abs": "https://arxiv.org/abs/2511.20555", "authors": ["Momoko Shiraishi", "Yinzhi Cao", "Takahiro Shinagawa"], "title": "Effective Command-line Interface Fuzzing with Path-Aware Large Language Model Orchestration", "comment": null, "summary": "Command-line interface (CLI) fuzzing tests programs by mutating both command-line options and input file contents, thus enabling discovery of vulnerabilities that only manifest under specific option-input combinations. Prior works of CLI fuzzing face the challenges of generating semantics-rich option strings and input files, which cannot reach deeply embedded target functions. This often leads to a misdetection of such a deep vulnerability using existing CLI fuzzing techniques. In this paper, we design a novel Path-guided, Iterative LLM-Orchestrated Testing framework, called PILOT, to fuzz CLI applications. The key insight is to provide potential call paths to target functions as context to LLM so that it can better generate CLI option strings and input files. Then, PILOT iteratively repeats the process, and provides reached functions as additional context so that target functions are reached. Our evaluation on real-world CLI applications demonstrates that PILOT achieves higher coverage than state-of-the-art fuzzing approaches and discovers 51 zero-day vulnerabilities. We responsibly disclosed all the vulnerabilities to their developers and so far 41 have been confirmed by their developers with 33 being fixed and three assigned CVE identifiers.", "AI": {"tldr": "PILOT\u662f\u4e00\u4e2a\u8def\u5f84\u5f15\u5bfc\u7684\u8fed\u4ee3LLM\u7f16\u6392\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u7cca\u6d4b\u8bd5\u547d\u4ee4\u884c\u754c\u9762\u5e94\u7528\u7a0b\u5e8f\uff0c\u901a\u8fc7\u63d0\u4f9b\u76ee\u6807\u51fd\u6570\u7684\u6f5c\u5728\u8c03\u7528\u8def\u5f84\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u8ba9LLM\u751f\u6210\u66f4\u6709\u6548\u7684CLI\u9009\u9879\u5b57\u7b26\u4e32\u548c\u8f93\u5165\u6587\u4ef6\uff0c\u4ece\u800c\u53d1\u73b0\u6df1\u5c42\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u7684CLI\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\u9762\u4e34\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u9009\u9879\u5b57\u7b26\u4e32\u548c\u8f93\u5165\u6587\u4ef6\u7684\u6311\u6218\uff0c\u65e0\u6cd5\u5230\u8fbe\u6df1\u5ea6\u5d4c\u5165\u7684\u76ee\u6807\u51fd\u6570\uff0c\u5bfc\u81f4\u6df1\u5c42\u6f0f\u6d1e\u7684\u8bef\u68c0\u6d4b\u3002", "method": "\u8bbe\u8ba1PILOT\u6846\u67b6\uff0c\u901a\u8fc7\u5411LLM\u63d0\u4f9b\u76ee\u6807\u51fd\u6570\u7684\u6f5c\u5728\u8c03\u7528\u8def\u5f84\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u8fed\u4ee3\u751f\u6210CLI\u9009\u9879\u5b57\u7b26\u4e32\u548c\u8f93\u5165\u6587\u4ef6\uff0c\u5e76\u5c06\u5df2\u5230\u8fbe\u7684\u51fd\u6570\u4f5c\u4e3a\u989d\u5916\u4e0a\u4e0b\u6587\uff0c\u9010\u6b65\u63a5\u8fd1\u76ee\u6807\u51fd\u6570\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754cCLI\u5e94\u7528\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cPILOT\u6bd4\u6700\u5148\u8fdb\u7684\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8986\u76d6\u7387\uff0c\u53d1\u73b0\u4e8651\u4e2a\u96f6\u65e5\u6f0f\u6d1e\uff0c\u5176\u4e2d41\u4e2a\u88ab\u5f00\u53d1\u8005\u786e\u8ba4\uff0c33\u4e2a\u5df2\u4fee\u590d\uff0c3\u4e2a\u83b7\u5f97\u4e86CVE\u6807\u8bc6\u7b26\u3002", "conclusion": "PILOT\u901a\u8fc7\u8def\u5f84\u5f15\u5bfc\u548cLLM\u8fed\u4ee3\u7f16\u6392\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86CLI\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u6df1\u5c42\u6f0f\u6d1e\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u6f0f\u6d1e\u53d1\u73b0\u80fd\u529b\u3002"}}
{"id": "2511.19969", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19969", "abs": "https://arxiv.org/abs/2511.19969", "authors": ["Weizi Shao", "Taolin Zhang", "Zijie Zhou", "Chen Chen", "Chengyu Wang", "Xiaofeng He"], "title": "M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation", "comment": null, "summary": "Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.", "AI": {"tldr": "\u63d0\u51fa\u4e86M\u00b3Prune\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u5c42\u6b21\u5316\u901a\u4fe1\u56fe\u526a\u679d\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u5f00\u9500\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u7684token\u5f00\u9500\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u90e8\u7f72\u3002\u9700\u8981\u627e\u5230\u4efb\u52a1\u6027\u80fd\u548ctoken\u5f00\u9500\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u5c42\u6b21\u5316\u901a\u4fe1\u56fe\u526a\u679d\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u56fe\u7a00\u758f\u5316\uff1b2\uff09\u6784\u5efa\u52a8\u6001\u901a\u4fe1\u62d3\u6251\uff1b3\uff09\u9010\u6b65\u526a\u679d\u5197\u4f59\u8fb9\u3002", "result": "\u5728\u901a\u7528\u548c\u9886\u57df\u7279\u5b9a\u7684mRAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u9c81\u68d2\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u6d88\u8017\u3002", "conclusion": "M\u00b3Prune\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684token\u5f00\u9500\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\uff0c\u5177\u6709\u5927\u89c4\u6a21\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.20630", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20630", "abs": "https://arxiv.org/abs/2511.20630", "authors": ["Vaibhav Kumar", "Kaiwalya Joshi", "Bhavya Dixit", "Gaurav S. Kasbekar"], "title": "Quantum-Resistant Authentication Scheme for RFID Systems Using Lattice-Based Cryptography", "comment": null, "summary": "We propose a novel quantum-resistant mutual authentication scheme for radio-frequency identification (RFID) systems. Our scheme uses lattice-based cryptography and, in particular, achieves quantum-resistance by leveraging the hardness of the inhomogeneous short integer solution (ISIS) problem. In contrast to prior work, which assumes that the reader-server communication channel is secure, our scheme is secure even when both the reader-server and tag-reader communication channels are insecure. Our proposed protocol provides robust security against man-in-the-middle (MITM), replay, impersonation, and reflection attacks, while also ensuring unforgeability and preserving anonymity. We present a detailed security analysis, including semi-formal analysis and formal verification using the Automated Validation of Internet Security Protocols and Applications (AVISPA) tool. In addition, we analyze the storage, computation, and communication costs of the proposed protocol and compare its security properties with those of existing protocols, demonstrating that our scheme offers strong security guarantees. To the best of our knowledge, this paper is the first quantum-resistant authentication protocol for RFID systems that comprehensively addresses the insecurity of both the reader-server and tag-reader communication channels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u683c\u5bc6\u7801\u5b66\u7684\u91cf\u5b50\u6297\u6027RFID\u76f8\u4e92\u8ba4\u8bc1\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5728\u8bfb\u5199\u5668-\u670d\u52a1\u5668\u548c\u6807\u7b7e-\u8bfb\u5199\u5668\u901a\u4fe1\u4fe1\u9053\u90fd\u4e0d\u5b89\u5168\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u8bc1\u5b89\u5168\u6027\uff0c\u89e3\u51b3\u4e86ISIS\u95ee\u9898\u7684\u56f0\u96be\u6027\u6765\u83b7\u5f97\u91cf\u5b50\u6297\u6027\u3002", "motivation": "\u73b0\u6709RFID\u8ba4\u8bc1\u65b9\u6848\u901a\u5e38\u5047\u8bbe\u8bfb\u5199\u5668-\u670d\u52a1\u5668\u901a\u4fe1\u4fe1\u9053\u662f\u5b89\u5168\u7684\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8be5\u4fe1\u9053\u4e5f\u53ef\u80fd\u4e0d\u5b89\u5168\u3002\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u5728\u4e24\u79cd\u901a\u4fe1\u4fe1\u9053\u90fd\u4e0d\u5b89\u5168\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u63d0\u4f9b\u5f3a\u5b89\u5168\u4fdd\u8bc1\u7684\u91cf\u5b50\u6297\u6027\u8ba4\u8bc1\u534f\u8bae\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u683c\u5bc6\u7801\u5b66\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5229\u7528\u975e\u9f50\u6b21\u77ed\u6574\u6570\u89e3(ISIS)\u95ee\u9898\u7684\u56f0\u96be\u6027\u6765\u6784\u5efa\u91cf\u5b50\u6297\u6027\u3002\u534f\u8bae\u8bbe\u8ba1\u786e\u4fdd\u5728\u4e24\u79cd\u901a\u4fe1\u4fe1\u9053\u90fd\u4e0d\u5b89\u5168\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u63d0\u4f9b\u76f8\u4e92\u8ba4\u8bc1\u3002", "result": "\u534f\u8bae\u80fd\u591f\u62b5\u6297\u4e2d\u95f4\u4eba\u653b\u51fb\u3001\u91cd\u653e\u653b\u51fb\u3001\u5192\u5145\u653b\u51fb\u548c\u53cd\u5c04\u653b\u51fb\uff0c\u540c\u65f6\u786e\u4fdd\u4e0d\u53ef\u4f2a\u9020\u6027\u548c\u533f\u540d\u6027\u3002\u901a\u8fc7AVISPA\u5de5\u5177\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u786e\u8ba4\u4e86\u534f\u8bae\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5168\u9762\u89e3\u51b3\u8bfb\u5199\u5668-\u670d\u52a1\u5668\u548c\u6807\u7b7e-\u8bfb\u5199\u5668\u901a\u4fe1\u4fe1\u9053\u4e0d\u5b89\u5168\u6027\u7684\u91cf\u5b50\u6297\u6027RFID\u8ba4\u8bc1\u534f\u8bae\uff0c\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u5e76\u5728\u5b58\u50a8\u3001\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u65b9\u9762\u8fdb\u884c\u4e86\u4f18\u5316\u3002"}}
{"id": "2511.20085", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.20085", "abs": "https://arxiv.org/abs/2511.20085", "authors": ["Chujie Wang", "Zhiyuan Luo", "Ruiqi Liu", "Can Ran", "Shenghua Fan", "Xi Chen", "Chu He"], "title": "VICoT-Agent: A Vision-Interleaved Chain-of-Thought Framework for Interpretable Multimodal Reasoning and Scalable Remote Sensing Analysis", "comment": null, "summary": "The current remote sensing image analysis task is increasingly evolving from traditional object recognition to complex intelligence reasoning, which places higher requirements on the model's reasoning ability and the flexibility of tool invocation. To this end, we propose a new multimodal agent framework, Vision-Interleaved Chain-of-Thought Framework (VICoT), which implements explicit multi-round reasoning by dynamically incorporating visual tools into the chain of thought. Through a stack-based reasoning structure and a modular MCP-compatible tool suite, VICoT enables LLMs to efficiently perform multi-round, interleaved vision-language reasoning tasks with strong generalization and flexibility.We also propose the Reasoning Stack distillation method to migrate complex Agent behaviors to small, lightweight models, which ensures the reasoning capability while significantly reducing complexity. Experiments on multiple remote sensing benchmarks demonstrate that VICoT significantly outperforms existing SOTA frameworks in reasoning transparency, execution efficiency, and generation quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u6846\u67b6VICoT\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u5de5\u5177\u52a8\u6001\u96c6\u6210\u5230\u601d\u7ef4\u94fe\u4e2d\u5b9e\u73b0\u663e\u5f0f\u591a\u8f6e\u63a8\u7406\uff0c\u5728\u9065\u611f\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u6846\u67b6\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u6b63\u4ece\u4f20\u7edf\u76ee\u6807\u8bc6\u522b\u5411\u590d\u6742\u667a\u80fd\u63a8\u7406\u6f14\u8fdb\uff0c\u5bf9\u6a21\u578b\u63a8\u7406\u80fd\u529b\u548c\u5de5\u5177\u8c03\u7528\u7075\u6d3b\u6027\u63d0\u51fa\u66f4\u9ad8\u8981\u6c42\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5806\u6808\u7684\u63a8\u7406\u7ed3\u6784\u548c\u6a21\u5757\u5316MCP\u517c\u5bb9\u5de5\u5177\u5957\u4ef6\uff0c\u4f7fLLM\u80fd\u591f\u9ad8\u6548\u6267\u884c\u591a\u8f6e\u4ea4\u9519\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\uff1b\u63d0\u51fa\u63a8\u7406\u5806\u6808\u84b8\u998f\u65b9\u6cd5\u5c06\u590d\u6742\u667a\u80fd\u4f53\u884c\u4e3a\u8fc1\u79fb\u5230\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVICoT\u5728\u63a8\u7406\u900f\u660e\u5ea6\u3001\u6267\u884c\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u6846\u67b6\u3002", "conclusion": "VICoT\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u96c6\u6210\u89c6\u89c9\u5de5\u5177\u5230\u601d\u7ef4\u94fe\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u8f6e\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\uff0c\u540c\u65f6\u901a\u8fc7\u84b8\u998f\u65b9\u6cd5\u786e\u4fdd\u4e86\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u3002"}}
{"id": "2511.20200", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20200", "abs": "https://arxiv.org/abs/2511.20200", "authors": ["Yitian Huang", "Yuxuan Lei", "Jianxun Lian", "Hao Liao"], "title": "Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025", "comment": null, "summary": "This report presents the solution and results of our team MSRA\\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u5728CPDC 2025\u6311\u6218\u8d5b\u4e2d\u7edf\u4e00\u6539\u8fdb\u4e86GPU\u548cAPI\u4e24\u4e2a\u8d5b\u9053\u3002\u6838\u5fc3\u5305\u62ec\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff08\u52a8\u6001\u5de5\u5177\u526a\u679d\u3001\u89d2\u8272\u88c1\u526a\u3001\u53c2\u6570\u5f52\u4e00\u5316\u7b49\uff09\u548cGRPO\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u83b7\u5f97\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u89e3\u51b3\u5e38\u8bc6\u89d2\u8272\u5bf9\u8bdd\u4e2d\u7684\u5de5\u5177\u8c03\u7528\u7a33\u5b9a\u6027\u3001\u6267\u884c\u53ef\u9760\u6027\u548c\u89d2\u8272\u626e\u6f14\u6307\u5bfc\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7f13\u89e3\u5c0f\u6837\u672c\u8fc7\u62df\u5408\uff0c\u63d0\u5347\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u6027\u80fd\u3002", "method": "1. \u4e0a\u4e0b\u6587\u5de5\u7a0b\uff1a\u52a8\u6001\u5de5\u5177\u526a\u679d\u3001\u89d2\u8272\u88c1\u526a\u8fdb\u884c\u8f93\u5165\u538b\u7f29\uff0c\u7ed3\u5408\u53c2\u6570\u5f52\u4e00\u5316\u3001\u51fd\u6570\u5408\u5e76\u7b49\u540e\u5904\u7406\u6280\u672f\uff1b2. GPU\u8d5b\u9053\u91c7\u7528GRPO\u8bad\u7ec3\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u66ff\u4ee3\u76d1\u7763\u5fae\u8c03\uff0c\u76f4\u63a5\u4f18\u5316\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u56e2\u961f\u5728\u6700\u7ec8\u8bc4\u4f30\u4e2d\uff1aTask 2 API\u6392\u540d\u7b2c1\uff0cTask 1 API\u6392\u540d\u7b2c2\uff0cTask 3 API\u548cGPU\u8d5b\u9053\u5747\u6392\u540d\u7b2c3\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548cGRPO\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u5728\u5e38\u8bc6\u89d2\u8272\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2511.20216", "categories": ["cs.AI", "cs.CE", "cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20216", "abs": "https://arxiv.org/abs/2511.20216", "authors": ["Haebin Seong", "Sungmin Kim", "Minchan Kim", "Yongjun Cho", "Myunchul Joe", "Suhwan Choi", "Jaeyoon Jung", "Jiyong Youn", "Yoonshik Kim", "Samwoo Seong", "Yubeen Park", "Youngjae Yu", "Yunsung Lee"], "title": "CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents", "comment": null, "summary": "Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \\emph{CostNav}, a \\textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \\textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\\% SLA compliance but is \\emph{not} commercially viable: yielding a loss of \\$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.", "AI": {"tldr": "CostNav\u662f\u9996\u4e2a\u5fae\u5bfc\u822a\u7ecf\u6d4e\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u6210\u672c-\u6536\u76ca\u5206\u6790\u8bc4\u4f30\u81ea\u4e3b\u9001\u8d27\u673a\u5668\u4eba\u7684\u5546\u4e1a\u53ef\u884c\u6027\uff0c\u63ed\u793a\u4e86\u5bfc\u822a\u7814\u7a76\u6307\u6807\u4e0e\u5546\u4e1a\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4f46\u5ffd\u89c6\u4e86\u7ecf\u6d4e\u53ef\u884c\u6027\u8fd9\u4e00\u5bf9\u81ea\u4e3b\u9001\u8d27\u673a\u5668\u4eba\u5546\u4e1a\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u7684\u56e0\u7d20\u3002", "method": "\u5f15\u5165CostNav\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5efa\u6a21\u5b8c\u6574\u7ecf\u6d4e\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u786c\u4ef6\u3001\u8bad\u7ec3\u3001\u80fd\u6e90\u3001\u7ef4\u62a4\u6210\u672c\u548c\u9001\u8d27\u6536\u5165\uff0c\u4f7f\u7528\u884c\u4e1a\u53c2\u6570\u8fdb\u884c\u6210\u672c-\u6536\u76ca\u5206\u6790\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a43.0%\u7684\u670d\u52a1\u6c34\u5e73\u534f\u8bae\u5408\u89c4\u7387\uff0c\u4f46\u5546\u4e1a\u4e0d\u53ef\u884c\uff1a\u6bcf\u6b21\u8fd0\u884c\u4e8f\u635f30.009\u7f8e\u5143\uff0c\u7ef4\u62a4\u6210\u672c\u5360\u8fd0\u884c\u6210\u672c\u768499.7%\uff0c\u78b0\u649e\u907f\u514d\u662f\u5173\u952e\u4f18\u5316\u76ee\u6807\u3002", "conclusion": "CostNav\u586b\u8865\u4e86\u5bfc\u822a\u7814\u7a76\u4e0e\u5546\u4e1a\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u8bc4\u4f30\u57fa\u4e8e\u89c4\u5219\u7684\u5bfc\u822a\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u6210\u672c\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u652f\u6301\u8de8\u5bfc\u822a\u8303\u5f0f\u7684\u7ecf\u6d4e\u6743\u8861\u51b3\u7b56\u3002"}}
{"id": "2511.20297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20297", "abs": "https://arxiv.org/abs/2511.20297", "authors": ["Shashank Kirtania", "Param Biyani", "Priyanshu Gupta", "Yasharth Bajpai", "Roshni Iyer", "Sumit Gulwani", "Gustavo Soares"], "title": "Improving Language Agents through BREW", "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $\u03c4^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\\%$ improvement in task precision, $10-15\\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.", "AI": {"tldr": "BREW\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u548c\u4f18\u5316\u7ecf\u9a8c\u77e5\u8bc6\u5e93\u6765\u6539\u8fdbLLM\u667a\u80fd\u4f53\uff0c\u76f8\u6bd4\u4f20\u7edf\u6743\u91cd\u4f18\u5316\u65b9\u6cd5\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4efb\u52a1\u7cbe\u5ea6\u5e76\u51cf\u5c11API\u8c03\u7528\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dLLM\u667a\u80fd\u4f53\u8bad\u7ec3\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u7b56\u7565\u96be\u4ee5\u89e3\u91ca\u548c\u589e\u91cf\u6539\u8fdb\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u901a\u8fc7\u7ecf\u9a8c\u77e5\u8bc6\u5e93\u6784\u5efa\u4f5c\u4e3a\u667a\u80fd\u4f53\u4f18\u5316\u7684\u66ff\u4ee3\u9014\u5f84\u3002", "method": "\u63d0\u51faBREW\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u8bc4\u5206\u548c\u884c\u4e3a\u51c6\u5219\u5b66\u4e60\u6d1e\u5bdf\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u641c\u7d22\u786e\u4fdd\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165\u6709\u6548\u7684\u8bb0\u5fc6\u5206\u533a\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u68c0\u7d22\u548c\u4f18\u5316\u6548\u7387\u3002", "result": "\u5728OSWorld\u3001\u03c4\u00b2Bench\u548cSpreadsheetBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBREW\u5b9e\u73b0\u4e8610-20%\u7684\u4efb\u52a1\u7cbe\u5ea6\u63d0\u5347\uff0c10-15%\u7684API/\u5de5\u5177\u8c03\u7528\u51cf\u5c11\uff0c\u6267\u884c\u65f6\u95f4\u66f4\u5feb\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7840\u6a21\u578b\u76f8\u5f53\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "BREW\u5c06\u77e5\u8bc6\u5e93\u786e\u7acb\u4e3a\u667a\u80fd\u4f53\u4f18\u5316\u7684\u6a21\u5757\u5316\u548c\u53ef\u63a7\u57fa\u7840\uff0c\u63d0\u4f9b\u900f\u660e\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u884c\u4e3a\u5851\u9020\u673a\u5236\uff0c\u76f8\u6bd4\u9759\u6001\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.20321", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20321", "abs": "https://arxiv.org/abs/2511.20321", "authors": ["Patrick Kenny"], "title": "Active Inference in Discrete State Spaces from First Principles", "comment": "56 pages", "summary": "We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u6f84\u6e05\u4e3b\u52a8\u63a8\u7406\u6982\u5ff5\uff0c\u5c06\u5176\u4e0e\u81ea\u7531\u80fd\u539f\u7406\u5206\u79bb\u3002\u4f5c\u8005\u5c55\u793a\u4e86\u5728\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e3b\u52a8\u63a8\u7406\u7684\u4f18\u5316\u95ee\u9898\u53ef\u4ee5\u8868\u8ff0\u4e3a\u7ea6\u675f\u6563\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u53ef\u901a\u8fc7\u6807\u51c6\u5e73\u5747\u573a\u65b9\u6cd5\u6c42\u89e3\uff0c\u65e0\u9700\u8bc9\u8bf8\u671f\u671b\u81ea\u7531\u80fd\u6982\u5ff5\u3002", "motivation": "\u6f84\u6e05\u4e3b\u52a8\u63a8\u7406\u4e0e\u81ea\u7531\u80fd\u539f\u7406\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u4f9b\u4e00\u79cd\u4e0d\u4f9d\u8d56\u671f\u671b\u81ea\u7531\u80fd\u7684\u66ff\u4ee3\u5b9e\u73b0\u65b9\u6cd5\u3002", "method": "\u5c06\u4e3b\u52a8\u63a8\u7406\u4f18\u5316\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u7ea6\u675f\u6563\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u6807\u51c6\u5e73\u5747\u573a\u65b9\u6cd5\u6c42\u89e3\u3002\u5728\u611f\u77e5\u5efa\u6a21\u65f6\u91c7\u7528\u611f\u77e5/\u884c\u52a8\u6563\u5ea6\u51c6\u5219\uff0c\u5728\u884c\u52a8\u5efa\u6a21\u65f6\u5f15\u5165\u71b5\u6b63\u5219\u5316\u9879\u3002", "result": "\u63d0\u51fa\u7684\u611f\u77e5/\u884c\u52a8\u6563\u5ea6\u51c6\u5219\u5728\u611f\u77e5\u5efa\u6a21\u65f6\u4e0e\u53d8\u5206\u81ea\u7531\u80fd\u4e00\u81f4\uff0c\u5728\u884c\u52a8\u5efa\u6a21\u65f6\u4e0e\u671f\u671b\u81ea\u7531\u80fd\u6cdb\u51fd\u76f8\u5dee\u4e00\u4e2a\u71b5\u6b63\u5219\u5316\u9879\u3002", "conclusion": "\u4e3b\u52a8\u63a8\u7406\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u671f\u671b\u81ea\u7531\u80fd\u6982\u5ff5\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\uff0c\u901a\u8fc7\u7ea6\u675f\u6563\u5ea6\u6700\u5c0f\u5316\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u7684\u5b9e\u73b0\u9014\u5f84\u3002"}}
{"id": "2511.20422", "categories": ["cs.AI", "cs.CV", "cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.20422", "abs": "https://arxiv.org/abs/2511.20422", "authors": ["Bo Pang", "Chenxi Xu", "Jierui Ren", "Guoping Wang", "Sheng Li"], "title": "VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning", "comment": null, "summary": "Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.", "AI": {"tldr": "VibraVerse\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u51e0\u4f55-\u58f0\u5b66\u5bf9\u9f50\u6570\u636e\u96c6\uff0c\u901a\u8fc7CLASP\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u5efa\u7acb\u7269\u4f53\u7269\u7406\u7ed3\u6784\u4e0e\u58f0\u5b66\u54cd\u5e94\u7684\u56e0\u679c\u5bf9\u5e94\u5173\u7cfb\uff0c\u4e3a\u7269\u7406\u4e00\u81f4\u7684\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\u7f3a\u4e4f\u7269\u7406\u4e00\u81f4\u6027\uff0c\u5ffd\u89c6\u4e86\u7269\u4f53\u51e0\u4f55\u3001\u6750\u6599\u3001\u632f\u52a8\u6a21\u5f0f\u548c\u4ea7\u751f\u58f0\u97f3\u4e4b\u95f4\u7684\u5185\u5728\u56e0\u679c\u5173\u7cfb\u3002\u9700\u8981\u5efa\u7acb\u57fa\u4e8e\u7269\u7406\u5b9a\u5f8b\u800c\u975e\u7edf\u8ba1\u76f8\u5173\u6027\u7684\u611f\u77e5\u6a21\u578b\u3002", "method": "\u6784\u5efaVibraVerse\u6570\u636e\u96c6\uff0c\u5305\u542b3D\u6a21\u578b\u7684\u7269\u7406\u5c5e\u6027\uff08\u5bc6\u5ea6\u3001\u6768\u6c0f\u6a21\u91cf\u3001\u6cca\u677e\u6bd4\uff09\u548c\u4f53\u79ef\u51e0\u4f55\uff0c\u8ba1\u7b97\u6a21\u6001\u7279\u5f81\u53c2\u6570\u7528\u4e8e\u51b2\u51fb\u58f0\u5408\u6210\u3002\u4f7f\u7528CLASP\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u5728\u51e0\u4f55\u5230\u58f0\u97f3\u9884\u6d4b\u3001\u58f0\u97f3\u5f15\u5bfc\u5f62\u72b6\u91cd\u5efa\u548c\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u7b49\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0c\u57fa\u4e8eVibraVerse\u8bad\u7ec3\u7684\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VibraVerse\u4e3a\u7269\u7406\u4e00\u81f4\u548c\u56e0\u679c\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u5b66\u4e60\u5efa\u7acb\u4e86\u57fa\u51c6\uff0c\u4e3a\u58f0\u97f3\u5f15\u5bfc\u7684\u5177\u8eab\u611f\u77e5\u548c\u5bf9\u7269\u7406\u4e16\u754c\u7684\u6df1\u5165\u7406\u89e3\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.20468", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20468", "abs": "https://arxiv.org/abs/2511.20468", "authors": ["Yuanhao Li", "Mingshan Liu", "Hongbo Wang", "Yiding Zhang", "Yifei Ma", "Wei Tan"], "title": "DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs", "comment": null, "summary": "Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed", "AI": {"tldr": "DRAFT-RL\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u94fe\u5f0f\u8349\u7a3f\u63a8\u7406\uff0c\u8ba9\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u751f\u6210\u591a\u4e2a\u8349\u7a3f\uff0c\u7136\u540e\u7531\u540c\u4f34\u667a\u80fd\u4f53\u548c\u5b66\u4e60\u5230\u7684\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\uff0c\u9009\u62e9\u6700\u6709\u5e0c\u671b\u7684\u8f68\u8ff9\u6765\u4f18\u5316\u63a8\u7406\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u53cd\u601d\u6846\u67b6\u901a\u5e38\u4f9d\u8d56\u5355\u6b21\u54cd\u5e94\uff0c\u7f3a\u4e4f\u63a8\u7406\u63a2\u7d22\u7684\u7ed3\u6784\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86LLM\u667a\u80fd\u4f53\u7684\u63a8\u7406\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faDRAFT-RL\u6846\u67b6\uff0c\u5c06\u94fe\u5f0f\u8349\u7a3f\u63a8\u7406\u96c6\u6210\u5230\u591a\u667a\u80fd\u4f53RL\u8bad\u7ec3\u4e2d\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u751f\u6210\u591a\u4e2a\u8349\u7a3f\uff0c\u901a\u8fc7\u540c\u4f34\u8bc4\u4f30\u548c\u5956\u52b1\u6a21\u578b\u9009\u62e9\u6700\u4f18\u8f68\u8ff9\uff0c\u4f7f\u7528actor-critic\u5b66\u4e60\u4f18\u5316\u63a8\u7406\u7b56\u7565\u3002", "result": "\u5728\u4ee3\u7801\u5408\u6210\u3001\u7b26\u53f7\u6570\u5b66\u548c\u77e5\u8bc6\u5bc6\u96c6\u578bQA\u7b49\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cDRAFT-RL\u5728\u51c6\u786e\u6027\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u53cd\u601d\u548c\u57fa\u4e8eRL\u7684\u667a\u80fd\u4f53\u3002", "conclusion": "DRAFT-RL\u901a\u8fc7\u663e\u5f0f\u591a\u8def\u5f84\u63a2\u7d22\u3001\u540c\u4f34\u5f15\u5bfc\u53cd\u601d\u548c\u5956\u52b1\u5bf9\u9f50\u9009\u62e9\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684LLM\u667a\u80fd\u4f53\u884c\u4e3a\u3002"}}
{"id": "2511.20471", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20471", "abs": "https://arxiv.org/abs/2511.20471", "authors": ["Yuto Suzuki", "Farnoush Banaei-Kashani"], "title": "Universe of Thoughts: Enabling Creative Reasoning with Large Language Models", "comment": null, "summary": "Reasoning based on Large Language Models (LLMs) has garnered increasing attention due to outstanding performance of these models in mathematical and complex logical tasks. Beginning with the Chain-of-Thought (CoT) prompting technique, numerous reasoning methods have emerged that decompose problems into smaller, sequential steps (or thoughts). However, existing reasoning models focus on conventional problem-solving and do not necessarily generate creative solutions by ``creative reasoning''. In domains where the solution space is expansive and conventional solutions are suboptimal, such as drug discovery or business strategization, creative reasoning to discover innovative solutions is crucial. To address this gap, first we introduce a computational framework for creative reasoning inspired by established cognitive science principles. With this framework, we propose three core creative reasoning paradigms, namely, \\textit{combinational}, \\textit{exploratory}, and \\textit{transformative} reasoning, where each offers specific directions for systematic exploration of the universe of thoughts to generate creative solutions. Next, to materialize this framework using LLMs, we introduce the \\textit{Universe of Thoughts} (or \\textit{UoT}, for short), a novel set of methods to implement the aforementioned three creative processes. Finally, we introduce three novel tasks that necessitate creative problem-solving, along with an evaluation benchmark to assess creativity from three orthogonal perspectives: feasibility as constraint, and utility and novelty as metrics. With a comparative analysis against the state-of-the-art (SOTA) reasoning techniques as well as representative commercial models with reasoning capability, we show that UoT demonstrates superior performance in creative reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u521b\u9020\u6027\u63a8\u7406\u6846\u67b6\uff0c\u5305\u542b\u7ec4\u5408\u5f0f\u3001\u63a2\u7d22\u5f0f\u548c\u8f6c\u5316\u5f0f\u4e09\u79cd\u6838\u5fc3\u63a8\u7406\u8303\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86Universe of Thoughts\u65b9\u6cd5\u6765\u5b9e\u73b0\u8fd9\u4e9b\u521b\u9020\u6027\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u5e38\u89c4\u95ee\u9898\u89e3\u51b3\uff0c\u4f46\u5728\u836f\u7269\u53d1\u73b0\u3001\u5546\u4e1a\u6218\u7565\u7b49\u9700\u8981\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u7684\u9886\u57df\uff0c\u521b\u9020\u6027\u63a8\u7406\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u539f\u7406\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u521b\u9020\u6027\u63a8\u7406\u8303\u5f0f\uff1a\u7ec4\u5408\u5f0f\u3001\u63a2\u7d22\u5f0f\u548c\u8f6c\u5316\u5f0f\u63a8\u7406\uff0c\u5e76\u5f00\u53d1\u4e86Universe of Thoughts\u65b9\u6cd5\u6765\u5b9e\u73b0\u8fd9\u4e9b\u8fc7\u7a0b\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6280\u672f\u548c\u4ee3\u8868\u6027\u5546\u4e1a\u6a21\u578b\u76f8\u6bd4\uff0cUoT\u5728\u521b\u9020\u6027\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "UoT\u6846\u67b6\u80fd\u591f\u6709\u6548\u652f\u6301\u521b\u9020\u6027\u63a8\u7406\uff0c\u5728\u9700\u8981\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u7684\u590d\u6742\u95ee\u9898\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.20510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20510", "abs": "https://arxiv.org/abs/2511.20510", "authors": ["Yuto Suzuki", "Paul Awolade", "Daniel V. LaBarbera", "Farnoush Banaei-Kashani"], "title": "FRAGMENTA: End-to-end Fragmentation-based Generative Model with Agentic Tuning for Drug Lead Optimization", "comment": null, "summary": "Molecule generation using generative AI is vital for drug discovery, yet class-specific datasets often contain fewer than 100 training examples. While fragment-based models handle limited data better than atom-based approaches, existing heuristic fragmentation limits diversity and misses key fragments. Additionally, model tuning typically requires slow, indirect collaboration between medicinal chemists and AI engineers. We introduce FRAGMENTA, an end-to-end framework for drug lead optimization comprising: 1) a novel generative model that reframes fragmentation as a \"vocabulary selection\" problem, using dynamic Q-learning to jointly optimize fragmentation and generation; and 2) an agentic AI system that refines objectives via conversational feedback from domain experts. This system removes the AI engineer from the loop and progressively learns domain knowledge to eventually automate tuning. In real-world cancer drug discovery experiments, FRAGMENTA's Human-Agent configuration identified nearly twice as many high-scoring molecules as baselines. Furthermore, the fully autonomous Agent-Agent system outperformed traditional Human-Human tuning, demonstrating the efficacy of agentic tuning in capturing expert intent.", "AI": {"tldr": "FRAGMENTA\u662f\u4e00\u4e2a\u7528\u4e8e\u836f\u7269\u5148\u5bfc\u5316\u5408\u7269\u4f18\u5316\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u542b\u751f\u6210\u6a21\u578b\u548c\u667a\u80fdAI\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001Q\u5b66\u4e60\u4f18\u5316\u5206\u5b50\u788e\u7247\u5316\u548c\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u5bf9\u8bdd\u53cd\u9988\u4ece\u9886\u57df\u4e13\u5bb6\u5b66\u4e60\u77e5\u8bc6\uff0c\u5728\u764c\u75c7\u836f\u7269\u53d1\u73b0\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5206\u5b50\u751f\u6210\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u7c7b\u522b\u7279\u5b9a\u7684\u6570\u636e\u96c6\u901a\u5e38\u5c11\u4e8e100\u4e2a\u8bad\u7ec3\u6837\u672c\u3002\u73b0\u6709\u542f\u53d1\u5f0f\u788e\u7247\u5316\u65b9\u6cd5\u9650\u5236\u4e86\u591a\u6837\u6027\u5e76\u9057\u6f0f\u5173\u952e\u7247\u6bb5\uff0c\u4e14\u6a21\u578b\u8c03\u4f18\u9700\u8981\u836f\u7269\u5316\u5b66\u5bb6\u548cAI\u5de5\u7a0b\u5e08\u4e4b\u95f4\u7f13\u6162\u7684\u95f4\u63a5\u534f\u4f5c\u3002", "method": "1) \u5c06\u788e\u7247\u5316\u91cd\u6784\u4e3a\"\u8bcd\u6c47\u9009\u62e9\"\u95ee\u9898\u7684\u65b0\u751f\u6210\u6a21\u578b\uff0c\u4f7f\u7528\u52a8\u6001Q\u5b66\u4e60\u8054\u5408\u4f18\u5316\u788e\u7247\u5316\u548c\u751f\u6210\uff1b2) \u901a\u8fc7\u9886\u57df\u4e13\u5bb6\u7684\u5bf9\u8bdd\u53cd\u9988\u7cbe\u70bc\u76ee\u6807\u7684\u667a\u80fdAI\u7cfb\u7edf\uff0c\u4ece\u5faa\u73af\u4e2d\u79fb\u9664AI\u5de5\u7a0b\u5e08\u5e76\u9010\u6b65\u5b66\u4e60\u9886\u57df\u77e5\u8bc6\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u8c03\u4f18\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u764c\u75c7\u836f\u7269\u53d1\u73b0\u5b9e\u9a8c\u4e2d\uff0cFRAGMENTA\u7684\u4eba-\u667a\u80fd\u4f53\u914d\u7f6e\u8bc6\u522b\u7684\u9ad8\u5206\u5206\u5b50\u6570\u91cf\u51e0\u4e4e\u662f\u57fa\u51c6\u7ebf\u7684\u4e24\u500d\u3002\u5b8c\u5168\u81ea\u4e3b\u7684\u667a\u80fd\u4f53-\u667a\u80fd\u4f53\u7cfb\u7edf\u4f18\u4e8e\u4f20\u7edf\u7684\u4eba-\u4eba\u8c03\u4f18\u65b9\u6cd5\u3002", "conclusion": "FRAGMENTA\u5c55\u793a\u4e86\u667a\u80fd\u8c03\u4f18\u5728\u6355\u6349\u4e13\u5bb6\u610f\u56fe\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u836f\u7269\u5148\u5bfc\u5316\u5408\u7269\u4f18\u5316\u3002"}}
{"id": "2511.20526", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20526", "abs": "https://arxiv.org/abs/2511.20526", "authors": ["Xinran Wang", "Boran Zhu", "Shujuan Zhou", "Ziwen Long", "Dehua Zhou", "Shu Zhang"], "title": "Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam", "comment": "15 pages, 4 figures", "summary": "Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain underexplored.In China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86ChatGPT-4o\u548cDeepSeek-R1\u5728\u4e2d\u56fd\u836f\u5e08\u6267\u4e1a\u8d44\u683c\u8003\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0DeepSeek-R1\u5728\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8eChatGPT-4o\uff0890.0% vs 76.1%\uff09\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b57\u5065\u5eb7\u6559\u80b2\u548c\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5728\u4e13\u4e1a\u8ba4\u8bc1\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u4e2d\u56fd\u836f\u5e08\u6267\u4e1a\u8d44\u683c\u8003\u8bd5\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u3002", "method": "\u6536\u96c62017-2021\u5e74\u5b98\u65b9\u8003\u8bd5\u4e2d\u76842,306\u9053\u7eaf\u6587\u672c\u9009\u62e9\u9898\uff0c\u6392\u9664\u542b\u8868\u683c\u6216\u56fe\u7247\u7684\u9898\u76ee\uff0c\u4ee5\u539f\u59cb\u4e2d\u6587\u683c\u5f0f\u8f93\u5165\u6a21\u578b\uff0c\u8bc4\u4f30\u6a21\u578b\u54cd\u5e94\u7684\u51c6\u786e\u7387\uff0c\u4f7f\u7528\u5361\u65b9\u68c0\u9a8c\u548cFisher\u7cbe\u786e\u68c0\u9a8c\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002", "result": "DeepSeek-R1\u603b\u4f53\u51c6\u786e\u7387\u663e\u8457\u9ad8\u4e8eChatGPT-4o\uff0890.0% vs 76.1%\uff0cp < 0.001\uff09\uff0c\u5728\u57fa\u7840\u548c\u4e34\u5e8a\u7efc\u5408\u6a21\u5757\u4e2d\u8868\u73b0\u4e00\u81f4\u66f4\u4f18\uff0c\u4f46\u5e74\u5ea6\u95f4\u5dee\u5f02\u672a\u8fbe\u5230\u7edf\u8ba1\u663e\u8457\u6027\u3002", "conclusion": "DeepSeek-R1\u5728\u836f\u5e08\u6267\u4e1a\u8d44\u683c\u8003\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8868\u660e\u9886\u57df\u7279\u5b9a\u6a21\u578b\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u6cd5\u5f8b\u548c\u4f26\u7406\u654f\u611f\u60c5\u5883\u4e0b\u4ecd\u9700\u4eba\u7c7b\u76d1\u7763\u3002"}}
{"id": "2511.20531", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20531", "abs": "https://arxiv.org/abs/2511.20531", "authors": ["Shamima Hossain"], "title": "Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models", "comment": "Accepted as poster at NewInML Workshop ICML, 2025", "summary": "Visual Language Models (VLMs) are powerful generative tools but often produce factually inaccurate outputs due to a lack of robust reasoning capabilities. While extensive research has been conducted on integrating external knowledge for reasoning in large language models (LLMs), such efforts remain underexplored in VLMs, where the challenge is compounded by the need to bridge multiple modalities seamlessly. This work introduces a framework for knowledge-guided reasoning in VLMs, leveraging structured knowledge graphs for multi-hop verification using image-captioning task to illustrate our framework. Our approach enables systematic reasoning across multiple steps, including visual entity recognition, knowledge graph traversal, and fact-based caption refinement. We evaluate the framework using hierarchical, triple-based and bullet-point based knowledge representations, analyzing their effectiveness in factual accuracy and logical inference. Empirical results show that our approach improves factual accuracy by approximately 31% on preliminary experiments on a curated dataset of mixtures from Google Landmarks v2, Conceptual captions and Coco captions revealing key insights into reasoning patterns and failure modes. This work demonstrates the potential of integrating external knowledge for advancing reasoning in VLMs, paving the way for more reliable and knowledgable multimodal systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8df3\u9a8c\u8bc1\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\u4e8b\u5b9e\u51c6\u786e\u7387\u63d0\u5347\u7ea631%\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u4e8b\u5b9e\u4e0d\u51c6\u786e\u7684\u8f93\u51fa\uff0c\u7f3a\u4e4f\u7a33\u5065\u7684\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5916\u90e8\u77e5\u8bc6\u96c6\u6210\uff0c\u800c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u591a\u6a21\u6001\u878d\u5408\u6311\u6218\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u77e5\u8bc6\u5f15\u5bfc\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u591a\u8df3\u9a8c\u8bc1\uff0c\u901a\u8fc7\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u5c55\u793a\u6846\u67b6\u3002\u5305\u62ec\u89c6\u89c9\u5b9e\u4f53\u8bc6\u522b\u3001\u77e5\u8bc6\u56fe\u8c31\u904d\u5386\u548c\u57fa\u4e8e\u4e8b\u5b9e\u7684\u63cf\u8ff0\u4f18\u5316\u7b49\u7cfb\u7edf\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728Google Landmarks v2\u3001Conceptual Captions\u548cCOCO Captions\u7684\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u521d\u6b65\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u4e8b\u5b9e\u51c6\u786e\u7387\u63d0\u5347\u7ea631%\uff0c\u5e76\u63ed\u793a\u4e86\u63a8\u7406\u6a21\u5f0f\u548c\u5931\u8d25\u6848\u4f8b\u7684\u5173\u952e\u6d1e\u5bdf\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u96c6\u6210\u5916\u90e8\u77e5\u8bc6\u5728\u63a8\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u77e5\u8bc6\u66f4\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.20610", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20610", "abs": "https://arxiv.org/abs/2511.20610", "authors": ["Gaspard Merten", "Mahmoud Sakr", "Gilles Dejaegere"], "title": "Building a Foundation Model for Trajectory from Scratch", "comment": null, "summary": "Foundation models are transformative in artificial intelligence, but building them from scratch, especially for mobility trajectories, is not yet clear or documented. This tutorial bridges this gap by demonstrating the steps and code of a minimal implementation of a trajectory-focused foundation model starting from GPT-2. Through a concise, step-by-step, code-driven process, we demonstrate adapting GPT-2 for spatiotemporal data. We then review and compare representative trajectory foundation models, such as TrajFM and TrajGPT, highlighting their architectural innovations and differences. Additionally, we introduce complementary techniques from related domains, like TimesFM's patching approach. Targeted at researchers and practitioners, this tutorial aims to explain the concepts and terminology of foundation models, at the implementation level. We find it timely and indispensable to create this educational material in order to support the SIGSPATIAL community in building and evaluating mobility foundation models, enhancing both research clarity and peer-review effectiveness in mobility AI.", "AI": {"tldr": "\u672c\u6559\u7a0b\u901a\u8fc7\u4ee3\u7801\u5b9e\u73b0\u5c55\u793a\u4e86\u5982\u4f55\u4eceGPT-2\u6784\u5efa\u9762\u5411\u8f68\u8ff9\u6570\u636e\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u6bd4\u8f83\u4e86TrajFM\u3001TrajGPT\u7b49\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u65e8\u5728\u5e2e\u52a9SIGSPATIAL\u793e\u533a\u7406\u89e3\u548c\u6784\u5efa\u79fb\u52a8\u6027\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u79fb\u52a8\u6027\u8f68\u8ff9\u57fa\u7840\u6a21\u578b\u7684\u660e\u786e\u6307\u5bfc\u548c\u6587\u6863\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u652f\u6301\u7814\u7a76\u793e\u533a\u5728\u79fb\u52a8\u6027AI\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u5206\u6b65\u4ee3\u7801\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u6f14\u793a\u5982\u4f55\u5c06GPT-2\u9002\u914d\u5230\u65f6\u7a7a\u6570\u636e\uff0c\u5e76\u56de\u987e\u6bd4\u8f83\u4ee3\u8868\u6027\u8f68\u8ff9\u57fa\u7840\u6a21\u578b\u7684\u67b6\u6784\u521b\u65b0\u548c\u5dee\u5f02\u3002", "result": "\u63d0\u4f9b\u4e86\u6784\u5efa\u8f68\u8ff9\u57fa\u7840\u6a21\u578b\u7684\u5177\u4f53\u5b9e\u73b0\u6b65\u9aa4\u548c\u4ee3\u7801\u793a\u4f8b\uff0c\u540c\u65f6\u4ecb\u7ecd\u4e86\u76f8\u5173\u9886\u57df\u7684\u8865\u5145\u6280\u672f\u5982TimesFM\u7684\u5206\u5757\u65b9\u6cd5\u3002", "conclusion": "\u521b\u5efa\u8fd9\u6837\u7684\u6559\u80b2\u6750\u6599\u5bf9\u4e8e\u652f\u6301SIGSPATIAL\u793e\u533a\u6784\u5efa\u548c\u8bc4\u4f30\u79fb\u52a8\u6027\u57fa\u7840\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u63d0\u9ad8\u79fb\u52a8\u6027AI\u9886\u57df\u7684\u7814\u7a76\u6e05\u6670\u5ea6\u548c\u540c\u884c\u8bc4\u5ba1\u6548\u679c\u3002"}}
{"id": "2511.20623", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20623", "abs": "https://arxiv.org/abs/2511.20623", "authors": ["David Szczecina", "Senan Gaffori", "Edmond Li"], "title": "Copyright Detection in Large Language Models: An Ethical Approach to Generative AI Development", "comment": "4 pages, 3 figures", "summary": "The widespread use of Large Language Models (LLMs) raises critical concerns regarding the unauthorized inclusion of copyrighted content in training data. Existing detection frameworks, such as DE-COP, are computationally intensive, and largely inaccessible to independent creators. As legal scrutiny increases, there is a pressing need for a scalable, transparent, and user-friendly solution. This paper introduce an open-source copyright detection platform that enables content creators to verify whether their work was used in LLM training datasets. Our approach enhances existing methodologies by facilitating ease of use, improving similarity detection, optimizing dataset validation, and reducing computational overhead by 10-30% with efficient API calls. With an intuitive user interface and scalable backend, this framework contributes to increasing transparency in AI development and ethical compliance, facilitating the foundation for further research in responsible AI development and copyright enforcement.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u7248\u6743\u68c0\u6d4b\u5e73\u53f0\uff0c\u5e2e\u52a9\u5185\u5bb9\u521b\u4f5c\u8005\u9a8c\u8bc1\u5176\u4f5c\u54c1\u662f\u5426\u88ab\u7528\u4e8eLLM\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c11\u4e8610-30%\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u4f7f\u7528\u5f15\u53d1\u7248\u6743\u62c5\u5fe7\uff0c\u73b0\u6709\u68c0\u6d4b\u6846\u67b6\u8ba1\u7b97\u5bc6\u96c6\u4e14\u72ec\u7acb\u521b\u4f5c\u8005\u96be\u4ee5\u4f7f\u7528\uff0c\u9700\u8981\u53ef\u6269\u5c55\u3001\u900f\u660e\u4e14\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u4f18\u5316API\u8c03\u7528\u63d0\u9ad8\u6548\u7387\uff0c\u6539\u8fdb\u76f8\u4f3c\u6027\u68c0\u6d4b\uff0c\u4f18\u5316\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u5e76\u63d0\u4f9b\u76f4\u89c2\u7528\u6237\u754c\u9762\u548c\u53ef\u6269\u5c55\u540e\u7aef\u3002", "result": "\u5e73\u53f0\u5b9e\u73b0\u4e8610-30%\u7684\u8ba1\u7b97\u5f00\u9500\u964d\u4f4e\uff0c\u63d0\u9ad8\u4e86\u6613\u7528\u6027\u548c\u76f8\u4f3c\u6027\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u63d0\u9ad8AI\u5f00\u53d1\u7684\u900f\u660e\u5ea6\u548c\u9053\u5fb7\u5408\u89c4\u6027\uff0c\u4e3a\u8d1f\u8d23\u4efbAI\u5f00\u53d1\u548c\u7248\u6743\u6267\u6cd5\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
