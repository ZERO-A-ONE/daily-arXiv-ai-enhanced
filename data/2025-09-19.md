<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.CR](#cs.CR) [Total: 21]
- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models](https://arxiv.org/abs/2509.14265)
*Siyuan Chen,Zhichao Lu,Qingfu Zhang*

Main category: cs.SE

TL;DR: EoK是一个基于LLM的进化程序搜索框架，通过挖掘现有内核库的开发历史来获取可重用的优化思想，为缺乏参考资料的RISC-V等新兴硬件平台自动化内核设计，在80个内核设计任务中实现了1.27倍的中位数加速，超越了人类专家和现有LLM方法。


<details>
  <summary>Details</summary>
Motivation: 解决新兴硬件平台（如RISC-V）因缺乏技术文档和成熟代码库而难以进行自动化内核优化的问题，特别是在参考材料稀缺的领域中LLM的有效性尚未得到验证。

Method: 提出EoK框架：1）从成熟内核库的开发历史中挖掘和形式化可重用的优化思想（通用设计原则+可操作思路）；2）使用RAG技术增强RISC-V特定上下文；3）基于历史有效技术指导并行LLM探索。

Result: 在80个内核设计任务评估中，EoK实现了1.27倍的中位数加速，在所有任务上都超越了人类专家，比之前基于LLM的自动化内核设计方法提升了20%。

Conclusion: EoK证明了将人类经验融入新兴领域的可行性，凸显了基于LLM的自动化内核优化的巨大潜力，特别是在参考材料稀缺的硬件平台上。

Abstract: Automated kernel design is critical for overcoming software ecosystem
barriers in emerging hardware platforms like RISC-V. While large language
models (LLMs) have shown promise for automated kernel optimization,
demonstrating success in CUDA domains with comprehensive technical documents
and mature codebases, their effectiveness remains unproven for reference-scarce
domains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based
evolutionary program search framework that automates kernel design for domains
with limited reference material. EoK mitigates reference scarcity by mining and
formalizing reusable optimization ideas (general design principles + actionable
thoughts) from established kernel libraries' development histories; it then
guides parallel LLM explorations using these ideas, enriched via
Retrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing
historically effective techniques. Empirically, EoK achieves a median 1.27x
speedup, surpassing human experts on all 80 evaluated kernel design tasks and
improving upon prior LLM-based automated kernel design methods by 20%. These
results underscore the viability of incorporating human experience into
emerging domains and highlight the immense potential of LLM-based automated
kernel optimization.

</details>


### [2] [Automated and Context-Aware Code Documentation Leveraging Advanced LLMs](https://arxiv.org/abs/2509.14273)
*Swapnil Sharma Sarker,Tanzina Taher Ifty*

Main category: cs.SE

TL;DR: 本研究开发了一个上下文感知的Javadoc生成数据集，评估了5个开源LLM在Javadoc自动生成任务上的性能，发现LLaMA 3.1表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有自动化文档生成方法主要关注代码摘要，缺乏针对模板化文档（如Javadoc）的研究，且缺乏包含现代语言特性和必要上下文信息的专用数据集。

Method: 构建了包含现代Java代码库结构和语义信息的新型上下文感知数据集，评估了5个开源LLM（LLaMA-3.1、Gemma-2、Phi-3、Mistral、Qwen-2.5）在零样本、少样本和微调设置下的性能。

Result: LLaMA 3.1在所有设置下表现一致良好，是自动化Javadoc生成的可靠选择，可作为专有系统的可行替代方案。

Conclusion: 该研究填补了模板化文档生成的空白，证明了开源LLM在Javadoc自动生成任务上的实用性，为实际应用提供了有效解决方案。

Abstract: Code documentation is essential to improve software maintainability and
comprehension. The tedious nature of manual code documentation has led to much
research on automated documentation generation. Existing automated approaches
primarily focused on code summarization, leaving a gap in template-based
documentation generation (e.g., Javadoc), particularly with publicly available
Large Language Models (LLMs). Furthermore, progress in this area has been
hindered by the lack of a Javadoc-specific dataset that incorporates modern
language features, provides broad framework/library coverage, and includes
necessary contextual information. This study aims to address these gaps by
developing a tailored dataset and assessing the capabilities of publicly
available LLMs for context-aware, template-based Javadoc generation. In this
work, we present a novel, context-aware dataset for Javadoc generation that
includes critical structural and semantic information from modern Java
codebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2,
Phi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and
provide a comparative analysis of their performance. Our results demonstrate
that LLaMA 3.1 performs consistently well and is a reliable candidate for
practical, automated Javadoc generation, offering a viable alternative to
proprietary systems.

</details>


### [3] [Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization](https://arxiv.org/abs/2509.14279)
*Robert Tjarko Lange,Qi Sun,Aaditya Prasad,Maxence Faldor,Yujin Tang,David Ha*

Main category: cs.SE

TL;DR: 提出了一个新的robust-kbench标准测试集和自动化框架，用于评估和优化CUDA内核的性能与正确性，解决了现有方法在低级代码优化和测试多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在软件工程任务中尽管表现良好，但对低级CUDA内核实现的优化关注不够，而且现有的内核生成测试集存在可利用的漏洞和测试条件不足的问题，影响了真正的统一性评估。

Method: 提出了一个综合的自动化框架，包含序列化工作流：首先将PyTorch代码翻译为相应的CUDA内核，然后使用一种新的适配CUDA生态系统的进化元生成过程来优化运行时间，这个过程由基于LLM的验证器指导正确性和高效过滤。

Result: 在robust-kbench上评估显示，该方法生成的CUDA内核在实际应用中超过了torch实现，包括前向和向后传播。该方法能够融合操作并部署各种运行时优化策略。验证器工作流准确分类错误内核，提高了硬件验证的效率。

Conclusion: 该研究提供了一个坚固的评估标准测试集和自动化框架，有效地解决了现有方法在CUDA内核优化和测试方面的不足，为前沿大语言模型在软件工程任务中的应用提供了更好的支持。

Abstract: Recent advances in large language models (LLMs) demonstrate their
effectiveness in scaling test-time compute for software engineering tasks.
However, these approaches often focus on high-level solutions, with limited
attention to optimizing low-level CUDA kernel implementations. Additionally,
existing kernel generation benchmarks suffer from exploitable loopholes and
insufficient diversity in testing conditions, hindering true generalization
assessment. To address these limitations, we introduce robust-kbench, a new
benchmark for rigorous evaluation of kernel performance and correctness across
varied scenarios. Furthermore, we present a comprehensive agentic framework
that automates CUDA kernel discovery, verification, and optimization. This
pipeline enables frontier LLMs to translate torch code to CUDA kernels and
iteratively improve their runtime within our robust evaluation setting. Our
sequential workflow first translates PyTorch code into equivalent CUDA kernels.
It then optimizes their runtime using a novel evolutionary meta-generation
procedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for
correctness and efficient filtering. Evaluated on robust-kbench, our approach
produces CUDA kernels outperforming torch implementations for practical
applications, including forward and backward passes. It can fuse operations and
deploy various runtime optimization strategies. The verifier workflow
accurately classifies incorrect kernels, enhancing hardware verification
efficiency.

</details>


### [4] [SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems](https://arxiv.org/abs/2509.14281)
*Xifeng Yao,Dongyu Lang,Wu Zhang,Xintong Guo,Huarui Xie,Yinhao Ni,Ping Liu,Guang Shen,Yi Bai,Dandan Tu,Changzheng Zhang*

Main category: cs.SE

TL;DR: 通过从真实编程数据集提取领域知识、领域技能和编程技能，构建场景图进行采样，生成贴近真实的代码问题


<details>
  <summary>Details</summary>
Motivation: 解决代码大语言模型进一步发展受限于真实编程问题数据稀缺的问题

Method: 从Stack Overflow和Kaggle等真实数据集提取领域知识、技能和编程技能，构建场景图进行结构化表示，设计图采样策略控制问题复杂度和多样性

Result: 在多样化的真实测试集上超越了各种规格和功能的最先进开源大语言模型

Conclusion: 该框架能够有效生成贴近真实场景的代码问题，为代码LLM的进一步发展提供了质量高的训练数据

Abstract: Significant advancements have been made in the capabilities of code large
language models, leading to their rapid adoption and application across a wide
range of domains. However, their further advancements are often constrained by
the scarcity of real-world coding problems. To bridge this gap, we propose a
novel framework for synthesizing code problems that emulate authentic
real-world scenarios. This framework systematically integrates domain
knowledge, domain skills, and coding skills, all of which are meticulously
extracted from real-world programming-related datasets, including Stack
Overflow and Kaggle. The extracted elements serve as the foundational building
blocks for constructing code problems. To align the generated problems with
practical applications, application scenarios are also mined from the
aforementioned datasets. These scenarios are then utilized to construct a
scenario-centric graph that interconnects domain knowledge, domain skills, and
coding skills. Based on this structured representation, a sampling strategy on
the graph is designed, which effectively controls the generation of a code
problem with complexity and diversity, reflects real-world challenges.
Experimental results demonstrate that the proposed method consistently achieves
superior performance over state-of-the-art open-source large language models of
varying sizes and functionalities, including both coders and general-purpose
models, across a diverse set of real-world benchmarks.

</details>


### [5] [Monitoring Machine Learning Systems: A Multivocal Literature Review](https://arxiv.org/abs/2509.14294)
*Hira Naveed,Scott Barnett,Chetan Arora,John Grundy,Hourieh Khalajzadeh,Omar Haggag*

Main category: cs.SE

TL;DR: 这是一份关于机器学习监控的多源文献综述，分析了136篇论文，提供了ML监控的完整概览、当前实践、差距和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 动态生产环境中机器学习系统的运行问题导致模型性能下降，需要监控来及早发现和缓解这些问题，以维护用户信任和避免不良后果。

Method: 采用多源文献综述(MLR)方法，按照Garousi的指南分析了136篇论文，从四个关键领域进行研究：动机目标和上下文、监控方面和技术、贡献和收益、当前限制。

Result: 分析了选定研究的四个关键领域，发现了形式文献与灰色文献之间的相似性和断层，提供了监控实践的综合概览。

Conclusion: MLR识别并总结了ML监控实践和空白，强调了形式与灰色文献间的相似性和差异，该研究对学术界和实践者都具有价值，能够帮助选择适当解决方案、指出当前方法的限制并提供未来研究和工具开发方向。

Abstract: Context: Dynamic production environments make it challenging to maintain
reliable machine learning (ML) systems. Runtime issues, such as changes in data
patterns or operating contexts, that degrade model performance are a common
occurrence in production settings. Monitoring enables early detection and
mitigation of these runtime issues, helping maintain users' trust and prevent
unwanted consequences for organizations. Aim: This study aims to provide a
comprehensive overview of the ML monitoring literature. Method: We conducted a
multivocal literature review (MLR) following the well established guidelines by
Garousi to investigate various aspects of ML monitoring approaches in 136
papers. Results: We analyzed selected studies based on four key areas: (1) the
motivations, goals, and context; (2) the monitored aspects, specific
techniques, metrics, and tools; (3) the contributions and benefits; and (4) the
current limitations. We also discuss several insights found in the studies,
their implications, and recommendations for future research and practice.
Conclusion: Our MLR identifies and summarizes ML monitoring practices and gaps,
emphasizing similarities and disconnects between formal and gray literature.
Our study is valuable for both academics and practitioners, as it helps select
appropriate solutions, highlights limitations in current approaches, and
provides future directions for research and tool development.

</details>


### [6] [On the Illusion of Success: An Empirical Study of Build Reruns and Silent Failures in Industrial CI](https://arxiv.org/abs/2509.14347)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 这篇论文首次实证研究了CI中的沉默失败问题，通过分析81个工业项目的14万个构建任务，发现11%成功任务被重运行，其中35%在24小时后重运行，并识别了11类沉默失败原因。


<details>
  <summary>Details</summary>
Motivation: 开发者在CI中遇到非确定性问题时通常重复运行任务希望获得真正成功，但沉默失败（构建任务标记为成功但未完成所有任务）往往被忽略，导致bug漏入生产环境。之前的研究主要关注间毫失败，而没有研究沉默失败。

Method: 通过分析81个工业项目的142,387个构建任务进行实证研究，使用混合效应模型分析32个独立变量（AUC为85%）识别关键因素，并分析92个公开问题进一步分析沉默失败的类别和原因。

Result: 研究发现11%的成功任务被重运行，其中35%在24小时后重运行。识别出了与成功任务重运行相关的关键因素，包括测试和静态分析任务、Shell脚本语言以及开发者之前的重运行习惯。分析出11类沉默失败，最常见的是构建产物操作错误、缓存错误和忽略退出码。

Conclusion: 这项研究首次揭示了CI中沉默失败的范围和影响，为开发团队提供了有价值的见解，并提出了改善CI可靠性的解决方案。研究结果有助于提高对沉默失败的认知，减少bug漏入生产环境的风险。

Abstract: Reliability of build outcomes is a cornerstone of effective Continuous
Integration (CI). Yet in practice, developers often struggle with
non-deterministic issues in the code or CI infrastructure, which undermine
trust in build results. When faced with such unexpected outcomes, developers
often repeatedly rerun jobs hoping for true success, but this practice is known
to increase CI costs and reduce productivity. While recent studies have focused
on intermittent job failures, no prior work has investigated silent failures,
where build jobs are marked as successful but fail to complete all or part of
their tasks. Such silent failures often go unnoticed, creating an illusion of
success with detrimental consequences such as bugs escaping into production.
This paper presents the first empirical study of silent failures through the
practice of rerunning successful jobs. An analysis of 142,387 jobs across 81
industrial projects shows that 11% of successful jobs are rerun, with 35% of
these reruns occurring after more than 24 hours. Using mixed-effects models on
32 independent variables (AUC of 85%), we identified key factors associated
with reruns of successful jobs, notably testing and static analysis tasks,
scripting languages like Shell, and developers prior rerun tendencies. A
further analysis of 92 public issues revealed 11 categories of silent failures
aligning with these factors, the most frequent being artifact operation errors,
caching errors, and ignored exit codes. Overall, our findings provide valuable
insights into the circumstances and causes of silent failures to raise
awareness among teams, and present solutions to improve CI reliability.

</details>


### [7] [CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning](https://arxiv.org/abs/2509.14373)
*Huy Le,Phong Nguyen,Hao Do,Tuan Nguyen,Thien Pham,Anh Nguyen-Duc,Tho Quan*

Main category: cs.SE

TL;DR: CodeLSI是一个结合低秩优化和领域特定指令调优的框架，用于生成高质量、领域特定的代码，无需依赖第三方API，在资源效率和代码质量方面优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决基于基础模型的自动化代码生成在领域特异性、成本效益和安全性方面的挑战，特别是在依赖第三方API时的问题。

Method: 应用低秩适应技术降低模型预训练和微调的计算成本，采用领域特定指令调优使代码生成与组织需求对齐，在真实JavaScript编码任务上进行测试。

Result: CodeLSI生成高质量、上下文感知的代码，在相关性、准确性和领域适应性方面优于基线模型，低秩优化显著降低了资源需求。

Conclusion: 低秩优化与领域特定调优相结合可以提升基础模型在自动化代码生成中的实用性和性能，为商业API解决方案提供了安全、经济高效的替代方案。

Abstract: Context: Automated code generation using Foundation Models (FMs) offers
promising solutions for enhancing software development efficiency. However,
challenges remain in ensuring domain specificity, cost-effectiveness, and
security - especially when relying on third-party APIs. This paper introduces
CodeLSI, a framework that combines low-rank optimization and domain-specific
instruction tuning to address these challenges.
  Objectives: The aim of this study is to develop and evaluate CodeLSI, a novel
approach for generating high-quality code tailored to specific domains, using
FMs fine-tuned on company infrastructure without dependence on external APIs.
  Methods: CodeLSI applies low-rank adaptation techniques to reduce the
computational cost of model pre-training and fine-tuning. Domain-specific
instruction tuning is employed to align code generation with organizational
needs. We implemented and tested the framework on real-world JavaScript coding
tasks using datasets drawn from internal software projects.
  Results: Experimental evaluations show that CodeLSI produces high-quality,
context aware code. It outperforms baseline models in terms of relevance,
accuracy, and domain fit. The use of low-rank optimization significantly
reduced resource requirements, enabling scalable training on company-owned
infrastructure.
  Conclusion: CodeLSI demonstrates that combining low-rank optimization with
domain specific tuning can enhance the practicality and performance of FMs for
automated code generation. This approach provides a secure, cost-efficient
alternative to commercial API based solutions and supports faster, more
targeted innovation in software development.

</details>


### [8] [A Taxonomy of Prompt Defects in LLM Systems](https://arxiv.org/abs/2509.14404)
*Haoye Tian,Chong Wang,BoYang Yang,Lyuye Zhang,Yang Liu*

Main category: cs.SE

TL;DR: 这篇论文系统性调查了提示缺陷，将其分为6个维度和更细粒度的子类型，提供具体例子、根因分析和缩减策略，并构建了缺陷-影响-治理方案的主要分类系统。


<details>
  <summary>Details</summary>
Motivation: 虽然提示作为大语言模型的编程接口，但提示设计仍然主要靠经验，小错误可能导致不可靠、不安全或效率低下的行为，需要系统性的缺陷分析和缩减方法。

Method: 采用软件工程原则，将提示缺陷组织成6个维度：规范与意图、输入与内容、结构与格式化、上下文与记忆、性能与效率、可维护性与工程化，每个维度进一步精炼为细粒度子类型，并提供具体例子、根因分析和缩减策略。

Result: 构建了一个涉及提示缺陷、影响和治理方案的主要分类系统，涵盖了新兴的提示工程模式、自动化护栏、测试套件和评估框架等缩凍策略。

Conclusion: 提出了开放性研究挑战，并呼吁采用严格的工程化方法论来确保由大语言模型驱动的系统能够通过设计实现可靠性。

Abstract: Large Language Models (LLMs) have become key components of modern software,
with prompts acting as their de-facto programming interface. However, prompt
design remains largely empirical and small mistakes can cascade into
unreliable, insecure, or inefficient behavior. This paper presents the first
systematic survey and taxonomy of prompt defects, recurring ways that prompts
fail to elicit their intended behavior from LLMs. We organize defects along six
dimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure
and Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)
Maintainability and Engineering. Each dimension is refined into fine-grained
subtypes, illustrated with concrete examples and root cause analysis. Grounded
in software engineering principles, we show how these defects surface in real
development workflows and examine their downstream effects. For every subtype,
we distill mitigation strategies that span emerging prompt engineering
patterns, automated guardrails, testing harnesses, and evaluation frameworks.
We then summarize these strategies in a master taxonomy that links defect,
impact, and remedy. We conclude with open research challenges and a call for
rigorous engineering-oriented methodologies to ensure that LLM-driven systems
are dependable by design.

</details>


### [9] [An LLM-based multi-agent framework for agile effort estimation](https://arxiv.org/abs/2509.14483)
*Thanh-Long Bui,Hoa Khanh Dam,Rashina Hoda*

Main category: cs.SE

TL;DR: 一种基于大语言模型的多代理框架，能够与人类开发者协作进行效果估算，解决了传统主观估算和机器学习方法的问题


<details>
  <summary>Details</summary>
Motivation: 潜动式软件开发中的效果估算主要依靠主观判断，导致不准确和不一致性。而现有的机器学习方法虽然准确却无法解释估算结果，也不能与人类团队成员交互

Method: 提出了一种基于大语言模型的多代理框架，这个框架不仅能够生成估算结果，还能够与人类开发者协调沟通、讨论，并达成共识

Result: 在真实数据集上的评估结果显示，该方法在大多数情况下都超过了最先进的技术。与软件开发实践者的人类研究也显示，与该框架代理协作的体验极为正面

Conclusion: 该研究成功开发了一种能够解决潜动效果估算中主观性和可解释性问题的多代理框架，为人机协作提供了有效解决方案

Abstract: Effort estimation is a crucial activity in agile software development, where
teams collaboratively review, discuss, and estimate the effort required to
complete user stories in a product backlog. Current practices in agile effort
estimation heavily rely on subjective assessments, leading to inaccuracies and
inconsistencies in the estimates. While recent machine learning-based methods
show promising accuracy, they cannot explain or justify their estimates and
lack the capability to interact with human team members. Our paper fills this
significant gap by leveraging the powerful capabilities of Large Language
Models (LLMs). We propose a novel LLM-based multi-agent framework for agile
estimation that not only can produce estimates, but also can coordinate,
communicate and discuss with human developers and other agents to reach a
consensus. Evaluation results on a real-life dataset show that our approach
outperforms state-of-the-art techniques across all evaluation metrics in the
majority of the cases. Our human study with software development practitioners
also demonstrates an overwhelmingly positive experience in collaborating with
our agents in agile effort estimation.

</details>


### [10] [Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language](https://arxiv.org/abs/2509.14623)
*Hanlong Wan,Xing Lu,Yan Chen,Karthik Devaprasad,Laura Hinkle*

Main category: cs.SE

TL;DR: 本文研究了大语言模型在自动生成Modelica控制模块中的应用，通过结构化工作流提高了开发效率，节省40-60%时间，但仍需要人工修复和验证。


<details>
  <summary>Details</summary>
Motivation: 解决Modelica控制模块开发劳动密集、需要专业知识的问题，通过LLM自动化生成代码来提高效率。

Method: 开发结构化工作流，包括标准化提示架构、库知识基础、OpenModelica自动编译和人工评估，在四种基础逻辑任务和五个控制模块上进行实验。

Result: GPT 4o零样本方式失败，Claude Sonnet 4通过精心设计提示在基础逻辑块上达到完全成功，控制模块成功率83%，平均开发时间从10-20小时降低到4-6小时。

Conclusion: LLM辅助工作流显示出强大潜力，但当前存在限制，需要进一步研究预模拟验证、更强的基础支撑和闭环评估。

Abstract: Dynamic energy systems and controls require advanced modeling frameworks to
design and test supervisory and fault tolerant strategies. Modelica is a widely
used equation based language, but developing control modules is labor intensive
and requires specialized expertise. This paper examines the use of large
language models (LLMs) to automate the generation of Control Description
Language modules in the Building Modelica Library as a case study. We developed
a structured workflow that combines standardized prompt scaffolds, library
aware grounding, automated compilation with OpenModelica, and human in the loop
evaluation. Experiments were carried out on four basic logic tasks (And, Or,
Not, and Switch) and five control modules (chiller enable/disable, bypass valve
control, cooling tower fan speed, plant requests, and relief damper control).
The results showed that GPT 4o failed to produce executable Modelica code in
zero shot mode, while Claude Sonnet 4 achieved up to full success for basic
logic blocks with carefully engineered prompts. For control modules, success
rates reached 83 percent, and failed outputs required medium level human repair
(estimated one to eight hours). Retrieval augmented generation often produced
mismatches in module selection (for example, And retrieved as Or), while a
deterministic hard rule search strategy avoided these errors. Human evaluation
also outperformed AI evaluation, since current LLMs cannot assess simulation
results or validate behavioral correctness. Despite these limitations, the LLM
assisted workflow reduced the average development time from 10 to 20 hours down
to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.
These results highlight both the potential and current limitations of LLM
assisted Modelica generation, and point to future research in pre simulation
validation, stronger grounding, and closed loop evaluation.

</details>


### [11] [Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs](https://arxiv.org/abs/2509.14626)
*Feiran Qin,M. M. Abid Naziri,Hengyu Ai,Saikat Dutta,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: FlashFuzz是首个将覆盖率引导模糊测试(CGF)应用于深度学习库的技术，利用LLM自动合成API级测试工具，显著提高了代码覆盖率、bug检测效率和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习库模糊测试方法缺乏覆盖率引导，限制了测试效果和效率。研究探索CGF能否有效应用于DL库，并相比现有方法提供改进。

Method: 提出FlashFuzz技术，利用大型语言模型(LLM)自动合成API级测试工具，通过模板、辅助函数和API文档结合，采用反馈驱动的迭代策略来合成和修复工具。

Result: 为1151个PyTorch和662个TensorFlow API合成工具，相比现有方法覆盖率提高101.13-212.88%，有效性提高1.0-5.4倍，输入生成速度提升1-1182倍，发现42个未知bug。

Conclusion: 研究证实CGF可有效应用于DL库，为未来测试方法提供了强基线，FlashFuzz在覆盖率、bug检测和可扩展性方面均优于现有方法。

Abstract: Deep Learning (DL) libraries such as PyTorch provide the core components to
build major AI-enabled applications. Finding bugs in these libraries is
important and challenging. Prior approaches have tackled this by performing
either API-level fuzzing or model-level fuzzing, but they do not use coverage
guidance, which limits their effectiveness and efficiency. This raises an
intriguing question: can coverage guided fuzzing (CGF), in particular
frameworks like LibFuzzer, be effectively applied to DL libraries, and does it
offer meaningful improvements in code coverage, bug detection, and scalability
compared to prior methods?
  We present the first in-depth study to answer this question. A key challenge
in applying CGF to DL libraries is the need to create a test harness for each
API that can transform byte-level fuzzer inputs into valid API inputs. To
address this, we propose FlashFuzz, a technique that leverages Large Language
Models (LLMs) to automatically synthesize API-level harnesses by combining
templates, helper functions, and API documentation. FlashFuzz uses a feedback
driven strategy to iteratively synthesize and repair harnesses. With this
approach, FlashFuzz synthesizes harnesses for 1,151 PyTorch and 662 TensorFlow
APIs. Compared to state-of-the-art fuzzing methods (ACETest, PathFinder, and
TitanFuzz), FlashFuzz achieves up to 101.13 to 212.88 percent higher coverage
and 1.0x to 5.4x higher validity rate, while also delivering 1x to 1182x
speedups in input generation. FlashFuzz has discovered 42 previously unknown
bugs in PyTorch and TensorFlow, 8 of which are already fixed. Our study
confirms that CGF can be effectively applied to DL libraries and provides a
strong baseline for future testing approaches.

</details>


### [12] [SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation](https://arxiv.org/abs/2509.14646)
*Yongpan Wang,Xin Xu,Xiaojie Zhu,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: SALTM是一种新颖的二进制反编译方法，通过抽象二进制和源代码之间的稳定逻辑特征，构建源级抽象逻辑树来指导LLM进行语义恢复，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的反编译方法将汇编代码视为线性指令序列，忽略了二进制文件固有的任意跳转模式和孤立数据段，限制了从汇编代码正确推断源代码语义的能力。

Method: SALTM从汇编代码构建源级抽象逻辑树来近似高级语言的逻辑结构，然后微调LLM生成反编译代码，最后通过错误修正和符号恢复来提高可读性和正确性。

Result: 在三个知名数据集上的实验表明，SALTM在恢复源代码逻辑方面非常有效，显著优于最先进方法（如Decompile-Eval上70.4%的TCP率，提升10.6%），并对四种常用混淆技术具有鲁棒性。

Conclusion: SALTM通过抽象逻辑特征和构建逻辑树的方法，有效解决了现有LLM反编译方法的局限性，为二进制函数理解提供了优越的辅助工具。

Abstract: Decompilation is widely used in reverse engineering to recover high-level
language code from binary executables. While recent approaches leveraging Large
Language Models (LLMs) have shown promising progress, they typically treat
assembly code as a linear sequence of instructions, overlooking arbitrary jump
patterns and isolated data segments inherent to binary files. This limitation
significantly hinders their ability to correctly infer source code semantics
from assembly code. To address this limitation, we propose \saltm, a novel
binary decompilation method that abstracts stable logical features shared
between binary and source code. The core idea of \saltm is to abstract selected
binary-level operations, such as specific jumps, into a high-level logic
framework that better guides LLMs in semantic recovery. Given a binary
function, \saltm constructs a Source-level Abstract Logic Tree (\salt) from
assembly code to approximate the logic structure of high-level language. It
then fine-tunes an LLM using the reconstructed \salt to generate decompiled
code. Finally, the output is refined through error correction and symbol
recovery to improve readability and correctness. We compare \saltm to three
categories of baselines (general-purpose LLMs, commercial decompilers, and
decompilation methods) using three well-known datasets (Decompile-Eval, MBPP,
Exebench). Our experimental results demonstrate that \saltm is highly effective
in recovering the logic of the source code, significantly outperforming
state-of-the-art methods (e.g., 70.4\% TCP rate on Decompile-Eval with a 10.6\%
improvement). The results further validate its robustness against four commonly
used obfuscation techniques. Additionally, analyses of real-world software and
a user study confirm that our decompiled output offers superior assistance to
human analysts in comprehending binary functions.

</details>


### [13] [Wireless Communication Performance Testing: From Laboratory Environment to Research Vessel](https://arxiv.org/abs/2509.14740)
*Andrei-Raoul Morariu,Andreas Strandberg,Bogdan Iancu,Jerker Bjorkqvist*

Main category: cs.SE

TL;DR: 实验室和户外环境中的共享频谱信号传输研究，重点分析视线遮挡物对信号衰减的影响以及距离和位置对传输效率的作用


<details>
  <summary>Details</summary>
Motivation: 研究动态和遮挡环境中环境因素对无线通信的影响，特别是在共享频谱场景下信号传输的可靠性

Method: 在实验室和户外环境中进行测量，分析实验室物体遮挡视线对Tx-Rx信号衰减的影响，并研究距离和电动研究船上不同位置对信号传输效率的影响

Result: 证实了视线遮挡物会导致信号衰减，距离和位置布置对信号传输效率有显著影响

Conclusion: 环境因素确实影响无线通信在动态和遮挡环境中的性能，这些发现有助于理解共享频谱场景下的信号传输特性

Abstract: This study investigates signal transmission within a shared spectrum,
focusing on measurements conducted both in laboratory and outdoor environments.
The objective was to demonstrate how laboratory objects obstructing the line of
sight can attenuate the signal between a transmitter (Tx) and a receiver (Rx).
Additionally, we examined the impact of distance and placement in various
locations aboard an electric research boat on signal transmission efficiency.
These findings contribute to understanding whether the environmental factors
influence wireless communication in dynamic and obstructed environments.

</details>


### [14] [On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code](https://arxiv.org/abs/2509.14744)
*Worawalan Chatlatanagulchai,Kundjanasith Thonglek,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: 研究分析了253个Claude.md配置文件，揭示代理编程工具中代理清单的结构模式和内容特征


<details>
  <summary>Details</summary>
Motivation: 代理编程工具需要通过代理清单获得项目上下文和操作规则，但缺乏综合性文档支持创建这些清单

Method: 分析242个仓库中的253个Claude.md文件，识别结构模式和常见内容

Result: 清单通常具有浅层次结构（一个主标题和几个子部分），内容以操作命令、技术实现说明和高层次架构为主

Conclusion: 研究为创建代理清单提供了结构和内容方面的实证指南

Abstract: Agentic coding tools receive goals written in natural language as input,
break them down into specific tasks, and write/execute the actual code with
minimal human intervention. Key to this process are agent manifests,
configuration files (such as Claude.md) that provide agents with essential
project context, identity, and operational rules. However, the lack of
comprehensive and accessible documentation for creating these manifests
presents a significant challenge for developers. We analyzed 253 Claude.md
files from 242 repositories to identify structural patterns and common content.
Our findings show that manifests typically have shallow hierarchies with one
main heading and several subsections, with content dominated by operational
commands, technical implementation notes, and high-level architecture.

</details>


### [15] [On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub](https://arxiv.org/abs/2509.14745)
*Miku Watanabe,Hao Li,Yutaro Kashiwa,Brittany Reid,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 对567个由Claude Code生成的GitHub PR进行实证研究，发现83.8%的AI辅助PR被接受合并，其中54.9%无需修改直接集成，但仍有45.1%需要人工修订


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地集成到软件开发中，AI代理生成代码和提交PR的能力可能成为标准实践，但缺乏对这些PR实际有用性和接受程度的实证研究

Method: 对157个开源项目中的567个由Claude Code生成的GitHub PR进行实证分析

Result: 开发者倾向于使用AI代理进行重构、文档和测试任务；83.8%的AI辅助PR被接受合并；54.9%的合并PR无需修改直接集成；45.1%需要人工修订，特别是在bug修复、文档和项目标准遵循方面

Conclusion: AI辅助的PR在很大程度上是可接受的，但仍然需要人工监督和精炼，特别是在bug修复、文档和项目特定标准方面

Abstract: Large language models (LLMs) are increasingly being integrated into software
development processes. The ability to generate code and submit pull requests
with minimal human intervention, through the use of autonomous AI agents, is
poised to become a standard practice. However, little is known about the
practical usefulness of these pull requests and the extent to which their
contributions are accepted in real-world projects. In this paper, we
empirically study 567 GitHub pull requests (PRs) generated using Claude Code,
an agentic coding tool, across 157 diverse open-source projects. Our analysis
reveals that developers tend to rely on agents for tasks such as refactoring,
documentation, and testing. The results indicate that 83.8% of these
agent-assisted PRs are eventually accepted and merged by project maintainers,
with 54.9% of the merged PRs are integrated without further modification. The
remaining 45.1% require additional changes benefit from human revisions,
especially for bug fixes, documentation, and adherence to project-specific
standards. These findings suggest that while agent-assisted PRs are largely
acceptable, they still benefit from human oversight and refinement.

</details>


### [16] [RulER: Automated Rule-Based Semantic Error Localization and Repair for Code Translation](https://arxiv.org/abs/2509.14829)
*Shuo Jin,Songqiang Chen,Xiaoyuan Xie,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: RulER是一种基于规则的代码翻译调试方法，通过从LLM生成的正确翻译中自动推导代码翻译规则，有效提升错误定位和修复效果


<details>
  <summary>Details</summary>
Motivation: 现有代码翻译调试方法缺乏可靠的代码对齐和修复模板参考，导致定位准确性和修复效果受限

Method: 从LLM生成的正确翻译中自动推导代码翻译规则，动态组合规则以适应更多语句对齐，利用规则进行可靠的代码对齐和修复模板设计

Result: 在Java到C++和Python到C++翻译任务中，RulER的错误定位率和修复成功率分别比最佳基线方法高出20%和272%

Conclusion: RulER展示了从LLMs中提取和利用编码知识的有前景方法，在代码翻译调试方面优于现有方法和直接使用LLM生成补丁

Abstract: Automated code translation aims to convert programs between different
programming languages while maintaining their functionality. Due to the
imperfections of code translation models, the generated translations may
contain errors that compromise their reliability. Existing automated debugging
methods for code translation rely on code alignments and repair patch templates
to locate and fix erroneous translations. However, existing methods lack
reliable references to construct code alignments and design repair patch
templates, which significantly impacts their localization accuracy and repair
effectiveness. To address these limitations, we reintroduce code translation
rules and propose a rule-based debugging method for code translation, called
RulER. RulER automatically derives code translation rules from correct
translations generated by LLMs, enabling the efficient collection of diverse
translation rules. In addition, RulER dynamically combines the existing rules
on expandable nodes like expressions and tokens to further adaptively align
more statements. These rules capture clear and detailed structural
correspondences between source and target programming languages. Therefore,
they can serve as reliable and reusable references for code alignment and
repair template design, enabling RulER to locate and fix translation errors
effectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++
translations produced by four code translation models demonstrates that RulER
outperforms state-of-the-art methods, BatFix and TransMap. Our experimental
results show that RulER outperformed the best baseline by 20% and 272% in terms
of error localization rates and repair success rates, respectively. RulER
exhibits superior repair performance compared to directly prompting LLMs for
patch generation, demonstrating a promising methodology for extracting and
leveraging coding knowledge from LLMs.

</details>


### [17] [CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects](https://arxiv.org/abs/2509.14856)
*Hanyang Guo,Xunjin Zheng,Zihan Liao,Hang Yu,Peng DI,Ziyin Zhang,Hong-Ning Dai*

Main category: cs.SE

TL;DR: 提出了CodeFuse-CR-Bench基准测试，这是首个针对代码审查的综合性仓库级评估基准，包含601个高质量实例，覆盖9个PR问题领域，并提供丰富的上下文信息。


<details>
  <summary>Details</summary>
Motivation: 现有的代码审查基准测试存在"现实差距"，使用简化的、缺乏上下文的数据评估孤立子任务，无法反映真实世界代码审查的整体性和上下文丰富性。

Method: 构建包含601个实例的基准测试，来自70个Python项目，提供问题、PR详情和仓库状态等丰富上下文。提出结合基于规则的定位和语法检查与基于模型的审查质量评估的新评估框架。

Result: 进行了大规模评估，发现：(1)没有单一LLM在所有CR方面都占优；(2)Gemini 2.5 Pro综合表现最佳；(3)不同LLM对冗余上下文的鲁棒性不同。

Conclusion: 研究强调了进行整体、多维度评估的必要性，为推进真正智能且实用的代码审查助手提供了可操作的见解。

Abstract: Automated code review (CR) is a key application for Large Language Models
(LLMs), but progress is hampered by a "reality gap": existing benchmarks
evaluate models on isolated sub-tasks using simplified, context-poor data. This
fails to reflect the holistic context-rich nature of real-world CR. To bridge
this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware
benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601
high-quality instances from 70 Python projects covering nine Pull-Request (PR)
problem domains, where each instance provides rich, multi-faceted context
including the associated issue, PR details, and repository state, enabling
end-to-end evaluation. Beyond superficial metrics, we also propose a novel
evaluation framework that combines rule-based checks for location and syntax
with model-based judgments of review quality. We present the first large-scale
assessment of state-of-the-art LLMs on this comprehensive CR task. Our results
establish crucial baselines and reveal that (1) no single LLM dominates all
aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive
performance; and (3) different LLMs exhibit varying robustness to redundant
context. These findings highlight the necessity of holistic, multi-dimensional
evaluation and provide actionable insights for advancing truly intelligent yet
practical CR assistants.

</details>


### [18] [CARGO: A Framework for Confidence-Aware Routing of Large Language Models](https://arxiv.org/abs/2509.14899)
*Amine Barrak,Yosr Fourati,Michael Olchawa,Emna Ksontini,Khalil Zoghlami*

Main category: cs.SE

TL;DR: CARGO是一个轻量级的LLM路由框架，通过嵌入回归器和二元分类器实现动态模型选择，无需人工标注，在多个任务领域达到专家级性能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模、专业化和延迟特性的多样化，如何将用户提示路由到最合适的模型以平衡性能和成本变得至关重要

Method: 使用基于嵌入的回归器预测模型性能，配合可选的二元分类器处理不确定情况，支持跨五个任务组的类别特定回归器

Result: 在四个竞争性LLM上实现76.4%的top-1路由准确率，对抗单个专家的胜率在72%到89%之间

Conclusion: 置信度引导的轻量级路由能够以最小开销实现专家级性能，为现实世界多模型LLM部署提供实用解决方案

Abstract: As large language models (LLMs) proliferate in scale, specialization, and
latency profiles, the challenge of routing user prompts to the most appropriate
model has become increasingly critical for balancing performance and cost. We
introduce CARGO (Category-Aware Routing with Gap-based Optimization), a
lightweight, confidence-aware framework for dynamic LLM selection. CARGO
employs a single embedding-based regressor trained on LLM-judged pairwise
comparisons to predict model performance, with an optional binary classifier
invoked when predictions are uncertain. This two-stage design enables precise,
cost-aware routing without the need for human-annotated supervision. To capture
domain-specific behavior, CARGO also supports category-specific regressors
trained across five task groups: mathematics, coding, reasoning, summarization,
and creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5
Sonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing
accuracy of 76.4% and win rates ranging from 72% to 89% against individual
experts. These results demonstrate that confidence-guided, lightweight routing
can achieve expert-level performance with minimal overhead, offering a
practical solution for real-world, multi-model LLM deployments.

</details>


### [19] ["Let it be Chaos in the Plumbing!" Usage and Efficacy of Chaos Engineering in DevOps Pipelines](https://arxiv.org/abs/2509.14931)
*Stefano Fossati,Damian Andrew Tamburri,Massimiliano Di Penta,Marco Tonnarelli*

Main category: cs.SE

TL;DR: 本文通过系统性的灰色文献综述，分析了2019-2024年间50个来源，将混沌工程原则扩展为10个概念分类框架，发现行业实践在保持核心原则的同时更强调受控实验、自动化和风险缓解策略。


<details>
  <summary>Details</summary>
Motivation: 混沌工程作为提高分布式系统弹性的主动方法，在DevOps环境中日益重要。需要系统研究行业实践者如何采纳和调整混沌工程原则，以应对敏捷和持续演进的开发运维管道的需求。

Method: 采用系统性灰色文献综述方法，分析2019年至2024年初发布的50个行业实践来源，开发了扩展的混沌工程概念分类框架。

Result: 研究揭示了混沌工程核心原则仍然具有影响力，但实践者越来越强调受控实验、自动化和风险缓解策略，形成了包含10个概念的扩展分类框架。

Conclusion: 研究结果增强了对混沌工程实践意图和实施方式的理解，为未来研究和工业应用提供了指导，旨在提高动态生产环境中的系统鲁棒性。

Abstract: Chaos Engineering (CE) has emerged as a proactive method to improve the
resilience of modern distributed systems, particularly within DevOps
environments. Originally pioneered by Netflix, CE simulates real-world failures
to expose weaknesses before they impact production. In this paper, we present a
systematic gray literature review that investigates how industry practitioners
have adopted and adapted CE principles over recent years. Analyzing 50 sources
published between 2019 and early 2024, we developed a comprehensive
classification framework that extends the foundational CE principles into ten
distinct concepts. Our study reveals that while the core tenets of CE remain
influential, practitioners increasingly emphasize controlled experimentation,
automation, and risk mitigation strategies to align with the demands of agile
and continuously evolving DevOps pipelines. Our results enhance the
understanding of how CE is intended and implemented in practice, and offer
guidance for future research and industrial applications aimed at improving
system robustness in dynamic production environments.

</details>


### [20] [Code Less to Code More: Streamlining Language Server Protocol and Type System Development for Language Families](https://arxiv.org/abs/2509.15150)
*Federico Bruzzone,Walter Cazzola,Luca Favalli*

Main category: cs.SE

TL;DR: 这篇论文提出了Typelang和模块化语言服务器生成方案，通过类型系统重用和自动化插件生成，大幅减少语言编辑支持的开发工作量。


<details>
  <summary>Details</summary>
Motivation: 解决多语言多编辑器支持的复杂性问题，LSP协议虽减少了组合数量，但语言组件重用和类型系统利用仍是挑战。

Method: 提出Typelang基于域特定语言家族，实现模块化、可组合、可重用的类型系统；模块化语言服务器生成过程；变体导向编程范式和跨产物协调层；LSP插件生成器。

Result: 在Neverlang中实现Typelang，为三种编辑器生成了语言服务器和LSP插件。实验评估显示：类型系统实现字符数减少93.48%，LSP插件生成完全自动化。

Conclusion: 该方案显著降低了语言家族编辑支持的开发苦难，特别是在语言产物重用时效果更佳，为语言工程领域提供了重要提升。

Abstract: Developing editing support for $L$ languages in $E$ editors is complex and
time-consuming. Some languages do not provide dedicated editors, while others
offer a single native editor. The $\textit{language server protocol}$ (LSP)
reduces the language-editor combinations $L \times E$ to $L + E$, where a
single language server communicates with editors via LSP plugins. However,
overlapping implementations of linguistic components remain an issue. Existing
language workbenches struggle with modularity, reusability, and leveraging type
systems for language server generation. In this work, we propose: (i) Typelang,
a family of domain-specific languages for modular, composable, and reusable
type system implementation, (ii) a modular language server generation process,
producing servers for languages built in a modular workbench, (iii) the
variant-oriented programming paradigm and a cross-artifact coordination layer
to manage interdependent software variants, and (iv) an LSP plugin generator,
reducing $E$ to $1$ by automating plugin creation for multiple editors. To
simplify editing support for language families, each language artifact
integrates its own Typelang variant, used to generate language servers. This
reduces combinations to $T \times 1$, where $T = L$ represents the number of
type systems. Further reuse of language artifacts across languages lowers this
to $N \times 1$, where $N << T$, representing unique type systems. We implement
Typelang in Neverlang, generating language servers for each artifact and LSP
plugins for three editors. Empirical evaluation shows a 93.48% reduction in
characters needed for type system implementation and 100% automation of LSP
plugin generation, significantly lowering effort for editing support in
language families, especially when artifacts are reused.

</details>


### [21] [Orion: Fuzzing Workflow Automation](https://arxiv.org/abs/2509.15195)
*Max Bazalii,Marius Fleischer*

Main category: cs.SE

TL;DR: Orion是一个自动化模糊测试框架，通过整合LLM推理与传统工具，大幅减少人工干预，在clib库中发现两个新漏洞


<details>
  <summary>Details</summary>
Motivation: 现代模糊测试虽然能自动生成输入和监控执行，但从代码库分析到结果分类的整个工作流程仍需要大量人工努力，现有研究只关注单个阶段

Method: Orion框架将LLM用于代码推理和语义指导，同时依赖确定性工具进行验证、迭代优化和需要精确性的任务

Result: 在基准测试套件中，Orion将人工工作量减少了46-204倍（取决于工作流程阶段），并在广泛使用的开源clib库中发现两个先前未知的漏洞

Conclusion: Orion通过整合LLM推理与传统工具，成功自动化了模糊测试的手动瓶颈，证明了在人工努力不切实际的场景下扩展模糊测试活动的可行性

Abstract: Fuzz testing is one of the most effective techniques for finding software
vulnerabilities. While modern fuzzers can generate inputs and monitor
executions automatically, the overall workflow, from analyzing a codebase, to
configuring harnesses, to triaging results, still requires substantial manual
effort. Prior attempts focused on single stages such as harness synthesis or
input minimization, leaving researchers to manually connect the pieces into a
complete fuzzing campaign.
  We introduce Orion, a framework that automates the the manual bottlenecks of
fuzzing by integrating LLM reasoning with traditional tools, allowing campaigns
to scale to settings where human effort alone was impractical. Orion uses LLMs
for code reasoning and semantic guidance, while relying on deterministic tools
for verification, iterative refinement, and tasks that require precision.
Across our benchmark suite, Orion reduces human effort by 46-204x depending on
the workflow stage, and we demonstrate its effectiveness through the discovery
of two previously unknown vulnerabilities in the widely used open-source clib
library.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models](https://arxiv.org/abs/2509.14271)
*Gustavo Sandoval,Denys Fenchenko,Junyao Chen*

Main category: cs.CR

TL;DR: 2022年早期研究，针对LLM的提示注入和目标劫持攻击提出对抗性微调防御方法，成功将攻击成功率从31%降至接近0%，为大模型安全防御奠定基础


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，提示注入攻击成为严重安全威胁，需要研究有效的防御方法来保护模型免受恶意提示操控

Method: 研究两种对抗攻击（提示注入和目标劫持），提出并评估新型防御技术"对抗性微调"，在多种GPT模型上进行测试比较

Result: 无防御时攻击成功率31%，使用对抗性微调后小型GPT-3模型攻击成功率降至接近0%；发现模型越灵活越易受攻击，大模型比小模型更脆弱

Conclusion: 虽然测试的具体模型已过时，但核心方法和实证发现为现代提示注入防御研究（包括指令层级系统和宪法AI方法）奠定了基础，后续研究揭示了基于微调防御的局限性

Abstract: This paper documents early research conducted in 2022 on defending against
prompt injection attacks in large language models, providing historical context
for the evolution of this critical security domain. This research focuses on
two adversarial attacks against Large Language Models (LLMs): prompt injection
and goal hijacking. We examine how to construct these attacks, test them on
various LLMs, and compare their effectiveness. We propose and evaluate a novel
defense technique called Adversarial Fine-Tuning. Our results show that,
without this defense, the attacks succeeded 31\% of the time on GPT-3 series
models. When using our Adversarial Fine-Tuning approach, attack success rates
were reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie),
though we note that subsequent research has revealed limitations of
fine-tuning-based defenses. We also find that more flexible models exhibit
greater vulnerability to these attacks. Consequently, large models such as
GPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the
specific models tested are now superseded, the core methodology and empirical
findings contributed to the foundation of modern prompt injection defense
research, including instruction hierarchy systems and constitutional AI
approaches.

</details>


### [23] [FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health](https://arxiv.org/abs/2509.14275)
*Nobin Sarwar,Shubhashis Roy Dipta*

Main category: cs.CR

TL;DR: 聚合学习框架FedMentor，结合LoRA和领域感知差分隐私，在保持模型性能的同时满足敏感领域的隐私预算要求


<details>
  <summary>Details</summary>
Motivation: 解决在敏感领域（如心理健康）中调整大语言模型时的隐私保护、模型效用和安全性之间的平衡问题

Method: 采用聚合学习框架，结合Low-Rank Adaptation (LoRA)和领域感知差分隐私(DP)。每个客户端根据数据敏感性应用自定义DP噪声规模，服务器在效用下降时自适应性降低噪声

Result: 在三个心理健康数据集上，FedMentor提高了安全性（安全输出率提高有3%，比无隐私聚合学习更好），低毒性，同时保持效用在非隐私基准0.5%以内，接近中央化上限

Conclusion: FedMentor提供了一种实用的方法，可以在医疗健康等敏感领域私有地微调大语言模型，实现更安全的部署

Abstract: Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive
domains (e.g., mental health) requires balancing strict confidentiality with
model utility and safety. We propose FedMentor, a federated fine-tuning
framework that integrates Low-Rank Adaptation (LoRA) and domain-aware
Differential Privacy (DP) to meet per-domain privacy budgets while maintaining
performance. Each client (domain) applies a custom DP noise scale proportional
to its data sensitivity, and the server adaptively reduces noise when utility
falls below a threshold. In experiments on three mental health datasets, we
show that FedMentor improves safety over standard Federated Learning without
privacy, raising safe output rates by up to three points and lowering toxicity,
while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the
non-private baseline and close to the centralized upper bound. The framework
scales to backbones with up to 1.7B parameters on single-GPU clients, requiring
< 173 MB of communication per round. FedMentor demonstrates a practical
approach to privately fine-tune LLMs for safer deployments in healthcare and
other sensitive fields.

</details>


### [24] [Beyond Data Privacy: New Privacy Risks for Large Language Models](https://arxiv.org/abs/2509.14278)
*Yuntao Du,Zitao Li,Ninghui Li,Bolin Ding*

Main category: cs.CR

TL;DR: 大语言模型在部署阶段带来了新的隐私风险，包括数据泄漏和恶意攻击，需要系统性的突破和防御策略


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在训练阶段的隐私风险已得到关注，但部署阶段的新风险被忽视，需要系统分析这些新兴风险并提出对策

Method: 系统性检查LLM部署阶段的新兴隐私风险，分析自主决策能力被武器化带来的漏洞，讨论潜在的缓解策略

Result: 识别了LLM部署阶段的多种隐私威胁，包括无意数据泄漏、恶意数据提取、精巧大规模隐私攻击，这些风险威胁到个人隐私、金融安全和社会信任

Conclusion: 研究社群需要扩大关注范围，不仅要关注训练阶段的数据隐私风险，还需开发新的防御机制来应对日益强大的LLM和LLM驱动系统带来的演进性威胁

Abstract: Large Language Models (LLMs) have achieved remarkable progress in natural
language understanding, reasoning, and autonomous decision-making. However,
these advancements have also come with significant privacy concerns. While
significant research has focused on mitigating the data privacy risks of LLMs
during various stages of model training, less attention has been paid to new
threats emerging from their deployment. The integration of LLMs into widely
used applications and the weaponization of their autonomous abilities have
created new privacy vulnerabilities. These vulnerabilities provide
opportunities for both inadvertent data leakage and malicious exfiltration from
LLM-powered systems. Additionally, adversaries can exploit these systems to
launch sophisticated, large-scale privacy attacks, threatening not only
individual privacy but also financial security and societal trust. In this
paper, we systematically examine these emerging privacy risks of LLMs. We also
discuss potential mitigation strategies and call for the research community to
broaden its focus beyond data privacy risks, developing new defenses to address
the evolving threats posed by increasingly powerful LLMs and LLM-powered
systems.

</details>


### [25] [Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning](https://arxiv.org/abs/2509.14282)
*Ali Al-kuwari,Noureldin Mohamed,Saif Al-kuwari,Ahmed Farouk,Bikash K. Behera*

Main category: cs.CR

TL;DR: 采用混合量子长短期记忆模型(QLSTM)检测量子加密分配(QKD)攻击，精度达93.7%，超越传统深度学习模型


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁现代加密安全，QKD实际实施存在硬件缺陷和高级攻击漏洞，需要更好的攻击检测方法

Method: 提出混合量子长短期记忆模型(QLSTM)，结合量子增强学习和经典深度学习，捕捉QKD数据中的复杂时间模式

Result: 在模拟7种QKD攻击场景的数据集上，混合QLSTM模型达到93.7%准确率，超过传统LSTM和CNN模型

Conclusion: 量子机器学习方法在QKD攻击检测中展现了潇望潜力，混合技术有助于提升未来量子通信网络的安全性

Abstract: The emergence of quantum computing poses significant risks to the security of
modern communication networks as it breaks today's public-key cryptographic
algorithms. Quantum Key Distribution (QKD) offers a promising solution by
harnessing the principles of quantum mechanics to establish secure keys.
However, practical QKD implementations remain vulnerable to hardware
imperfections and advanced attacks such as Photon Number Splitting and
Trojan-Horse attacks. In this work, we investigate the potential of using
quantum machine learning (QML) to detect popular QKD attacks. In particular, we
propose a Hybrid Quantum Long Short-Term Memory (QLSTM) model to improve the
detection of common QKD attacks. By combining quantum-enhanced learning with
classical deep learning, the model captures complex temporal patterns in QKD
data, improving detection accuracy. To evaluate the proposed model, we
introduce a realistic QKD dataset simulating normal QKD operations along with
seven attack scenarios, Intercept-and-Resend, Photon-Number Splitting (PNS),
Trojan-Horse attacks Random Number Generator (RNG), Detector Blinding,
Wavelength-dependent Trojan Horse, and Combined attacks. The dataset includes
quantum security metrics such as Quantum Bit Error Rate (QBER), measurement
entropy, signal and decoy loss rates, and time-based metrics, ensuring an
accurate representation of real-world conditions. Our results demonstrate
promising performance of the quantum machine learning approach compared to
traditional classical machine learning models, highlighting the potential of
hybrid techniques to enhance the security of future quantum communication
networks. The proposed Hybrid QLSTM model achieved an accuracy of 93.7.0\%
after 50 training epochs, outperforming classical deep learning models such as
LSTM, and CNN.

</details>


### [26] [The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration](https://arxiv.org/abs/2509.14284)
*Vaidehi Patil,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CR

TL;DR: 该论文首次系统研究了多智能体LLM系统中的组合隐私泄露问题，提出了两种防御策略（ToM和CoDef），并在隐私-效用权衡方面进行了量化评估。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体系统中的广泛应用，出现了超越记忆化、直接推理或单轮评估的新型隐私风险，即看似无害的响应在跨交互组合时可能累积泄露敏感信息。

Method: 开发了一个建模辅助知识和智能体交互如何联合放大隐私风险的框架，提出了两种防御策略：心理理论防御（ToM）和协作共识防御（CoDef），并通过实验评估了这些策略的效果。

Result: 思维链单独提供有限保护（约39%敏感信息阻止率），ToM防御显著改善敏感查询阻止（高达97%）但会降低良性任务成功率，CoDef实现最佳平衡（79.8%平衡结果）。

Conclusion: 研究揭示了协作LLM部署中的新型风险类别，并为设计针对组合性、上下文驱动的隐私泄露的防护措施提供了可行见解，强调结合显式推理与防御者协作的益处。

Abstract: As large language models (LLMs) become integral to multi-agent systems, new
privacy risks emerge that extend beyond memorization, direct inference, or
single-turn evaluations. In particular, seemingly innocuous responses, when
composed across interactions, can cumulatively enable adversaries to recover
sensitive information, a phenomenon we term compositional privacy leakage. We
present the first systematic study of such compositional privacy leaks and
possible mitigation methods in multi-agent LLM systems. First, we develop a
framework that models how auxiliary knowledge and agent interactions jointly
amplify privacy risks, even when each response is benign in isolation. Next, to
mitigate this, we propose and evaluate two defense strategies: (1)
Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent
by anticipating how their outputs may be exploited by adversaries, and (2)
Collaborative Consensus Defense (CoDef), where responder agents collaborate
with peers who vote based on a shared aggregated state to restrict sensitive
information spread. Crucially, we balance our evaluation across compositions
that expose sensitive information and compositions that yield benign
inferences. Our experiments quantify how these defense strategies differ in
balancing the privacy-utility trade-off. We find that while chain-of-thought
alone offers limited protection to leakage (~39% sensitive blocking rate), our
ToM defense substantially improves sensitive query blocking (up to 97%) but can
reduce benign task success. CoDef achieves the best balance, yielding the
highest Balanced Outcome (79.8%), highlighting the benefit of combining
explicit reasoning with defender collaboration. Together, our results expose a
new class of risks in collaborative LLM deployments and provide actionable
insights for designing safeguards against compositional, context-driven privacy
leakage.

</details>


### [27] [A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks](https://arxiv.org/abs/2509.14285)
*S M Asif Hossain,Ruksat Khan Shayoni,Mohd Ruhul Ameen,Akif Islam,M. F. Mridha,Jungpil Shin*

Main category: cs.CR

TL;DR: 这篇论文提出了一种新的多模型治理框架，通过协调多个LLM模型以实时检测和阻挡提示注入攻击，实现了100%的防御效果。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击是LLM部署中的主要漏洞，恶意指令可以覆盖系统提示导致意外行为，需要有效的防御方案。

Method: 采用专门的LLM模型代理构建协调管道，包括两种架构：顺序链式流水线和层次协调器系统。

Result: 在55种独特提示注入攻击、400个攻击实例的测试中，基准攻击成功率为20-30%，而多模型管理框架实现了100%的防御效果，将ASR降低到0%。

Conclusion: 该多模型防御框架在保持系统功能性的同时，显示出对各种提示注入攻击类型的稳健防御能力，为LLM安全部署提供了有效解决方案。

Abstract: Prompt injection attacks represent a major vulnerability in Large Language
Model (LLM) deployments, where malicious instructions embedded in user inputs
can override system prompts and induce unintended behaviors. This paper
presents a novel multi-agent defense framework that employs specialized LLM
agents in coordinated pipelines to detect and neutralize prompt injection
attacks in real-time. We evaluate our approach using two distinct
architectures: a sequential chain-of-agents pipeline and a hierarchical
coordinator-based system. Our comprehensive evaluation on 55 unique prompt
injection attacks, grouped into 8 categories and totaling 400 attack instances
across two LLM platforms (ChatGLM and Llama2), demonstrates significant
security improvements. Without defense mechanisms, baseline Attack Success
Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent
pipeline achieved 100% mitigation, reducing ASR to 0% across all tested
scenarios. The framework demonstrates robustness across multiple attack
categories including direct overrides, code execution attempts, data
exfiltration, and obfuscation techniques, while maintaining system
functionality for legitimate queries.

</details>


### [28] [A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness](https://arxiv.org/abs/2509.14297)
*Xuan Luo,Yue Wang,Zefeng He,Geng Tu,Jing Li,Ruifeng Xu*

Main category: cs.CR

TL;DR: HILL是一种新型越狱方法，通过将有害指令转换为学习式问题来绕过LLM安全防护，在多个模型上表现出高攻击成功率和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 揭示LLM安全机制在应对学习式诱导时的脆弱性，评估现有防御方法的有效性，探索帮助性与安全性平衡的挑战

Method: HILL方法将强制性有害请求系统性地转换为带有假设性指示的学习式问题，并引入两个新指标来全面评估越狱方法的效用

Result: 在AdvBench数据集上的实验显示，HILL在大多数模型和恶意类别上达到最高攻击成功率，保持高效简洁的提示，且对多种防御方法表现出鲁棒性

Conclusion: 该工作暴露了安全措施在学习式诱导下的显著脆弱性，突显了平衡帮助性和安全对齐的关键挑战，现有防御方法效果有限甚至可能增加攻击成功率

Abstract: Safety alignment aims to prevent Large Language Models (LLMs) from responding
to harmful queries. To strengthen safety protections, jailbreak methods are
developed to simulate malicious attacks and uncover vulnerabilities. In this
paper, we introduce HILL (Hiding Intention by Learning from LLMs), a novel
jailbreak approach that systematically transforms imperative harmful requests
into learning-style questions with only straightforward hypotheticality
indicators. Further, we introduce two new metrics to thoroughly evaluate the
utility of jailbreak methods. Experiments on the AdvBench dataset across a wide
range of models demonstrate HILL's strong effectiveness, generalizability, and
harmfulness. It achieves top attack success rates on the majority of models and
across malicious categories while maintaining high efficiency with concise
prompts. Results of various defense methods show the robustness of HILL, with
most defenses having mediocre effects or even increasing the attack success
rates. Moreover, the assessment on our constructed safe prompts reveals
inherent limitations of LLMs' safety mechanisms and flaws in defense methods.
This work exposes significant vulnerabilities of safety measures against
learning-style elicitation, highlighting a critical challenge of balancing
helpfulness and safety alignments.

</details>


### [29] [Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing](https://arxiv.org/abs/2509.14335)
*Xinran Zheng,Xingzhi Qian,Yiling He,Shuo Yang,Lorenzo Cavallaro*

Main category: cs.CR

TL;DR: MalEval是一个用于评估LLM在Android恶意软件审计中能力的框架，通过专家验证报告和静态可达性分析来解决数据稀缺和噪声问题，定义了四个审计任务并系统评估了7个主流LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 恶意软件行为审计需要因果性和可验证的解释，但手动审计成本高且效率低。LLM有潜力帮助解决这个问题，但由于缺乏细粒度标注、良性代码噪声和幻觉输出等问题，其审计潜力尚未被充分探索。

Method: 提出MalEval框架，提供专家验证报告和更新的敏感API列表，通过静态可达性分析减少噪声。使用函数级结构表示作为可验证评估的中间归因单元，定义了四个分析师对齐的任务和相关指标。

Result: 系统评估了7个广泛使用的LLM在精选数据集上的表现，揭示了LLM在审计各阶段的潜力和关键局限性。

Conclusion: MalEval为LLM增强的恶意软件行为审计提供了可复现的基准和基础，展示了LLM在此领域的应用前景和当前限制。

Abstract: Automated malware classification has achieved strong detection performance.
Yet, malware behavior auditing seeks causal and verifiable explanations of
malicious activities -- essential not only to reveal what malware does but also
to substantiate such claims with evidence. This task is challenging, as
adversarial intent is often hidden within complex, framework-heavy
applications, making manual auditing slow and costly. Large Language Models
(LLMs) could help address this gap, but their auditing potential remains
largely unexplored due to three limitations: (1) scarce fine-grained
annotations for fair assessment; (2) abundant benign code obscuring malicious
signals; and (3) unverifiable, hallucination-prone outputs undermining
attribution credibility. To close this gap, we introduce MalEval, a
comprehensive framework for fine-grained Android malware auditing, designed to
evaluate how effectively LLMs support auditing under real-world constraints.
MalEval provides expert-verified reports and an updated sensitive API list to
mitigate ground truth scarcity and reduce noise via static reachability
analysis. Function-level structural representations serve as intermediate
attribution units for verifiable evaluation. Building on this, we define four
analyst-aligned tasks -- function prioritization, evidence attribution,
behavior synthesis, and sample discrimination -- together with domain-specific
metrics and a unified workload-oriented score. We evaluate seven widely used
LLMs on a curated dataset of recent malware and misclassified benign apps,
offering the first systematic assessment of their auditing capabilities.
MalEval reveals both promising potential and critical limitations across audit
stages, providing a reproducible benchmark and foundation for future research
on LLM-enhanced malware behavior auditing. MalEval is publicly available at
https://github.com/ZhengXR930/MalEval.git

</details>


### [30] [LLM Jailbreak Detection for (Almost) Free!](https://arxiv.org/abs/2509.14558)
*Guorui Chen,Yifan Xia,Xiaojun Jia,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.CR

TL;DR: 通过在输入前添加肯定指令和温度缩放方法，利用第一个token的信心度区分盗窃提示咈喂提示，实现高效的盗窃检济方法


<details>
  <summary>Details</summary>
Motivation: 现有的盗窃检济方法需要多次模型推理或其他模型协助，计算成本高，需要一种更高效的方法

Method: 在输入前添加肯定指令，通过温度缩放调整输出分布，利用第一个token的信心度来区分盗窃提示咈喂提示，结合虚拟指令学习提升检济性能

Result: 实验结果显示FJD方法能够有效检济盗窃提示，且在LLM推理过程中几乎不需额外计算成本

Conclusion: 该方法提供了一种高效、低成本的盗窃检济方案，通过分析输出分布差异来识别恶意提示，具有实际应用价值

Abstract: Large language models (LLMs) enhance security through alignment when widely
used, but remain susceptible to jailbreak attacks capable of producing
inappropriate content. Jailbreak detection methods show promise in mitigating
jailbreak attacks through the assistance of other models or multiple model
inferences. However, existing methods entail significant computational costs.
In this paper, we first present a finding that the difference in output
distributions between jailbreak and benign prompts can be employed for
detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak
Detection (FJD) which prepends an affirmative instruction to the input and
scales the logits by temperature to further distinguish between jailbreak and
benign prompts through the confidence of the first token. Furthermore, we
enhance the detection performance of FJD through the integration of virtual
instruction learning. Extensive experiments on aligned LLMs show that our FJD
can effectively detect jailbreak prompts with almost no additional
computational costs during LLM inference.

</details>


### [31] [What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System](https://arxiv.org/abs/2509.14583)
*Johnny So,Michael Ferdman,Nick Nikiforakis*

Main category: cs.CR

TL;DR: LiMS是一个透明系统，通过可定制的完整性策略来验证和执行Web资源完整性，能够有效防御供应链攻击，初始页面加载开销仅数百毫秒，重载开销可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏健壮的Web资源完整性验证方法，供应链攻击成为Web应用的主要攻击面，需要一种通用且高性能的完整性保障方案。

Method: 设计LiMS系统，使用可定制的完整性策略声明资源预期属性，验证并执行这些策略，为网站访问者提供链接完整性保证。

Result: 在450个代表性域名的测试中，系统在初始页面加载时仅增加数百毫秒开销，重载时开销可忽略不计，多个策略构建块适合依赖使用模式且管理开销最小。

Conclusion: LiMS能够以最小开销引导显著的安全改进，提供足够的保障来防御最近的供应链攻击，适合实际部署。

Abstract: The web continues to grow, but dependency-monitoring tools and standards for
resource integrity lag behind. Currently, there exists no robust method to
verify the integrity of web resources, much less in a generalizable yet
performant manner, and supply chains remain one of the most targeted parts of
the attack surface of web applications.
  In this paper, we present the design of LiMS, a transparent system to
bootstrap link integrity guarantees in web browsing sessions with minimal
overhead. At its core, LiMS uses a set of customizable integrity policies to
declare the (un)expected properties of resources, verifies these policies, and
enforces them for website visitors. We discuss how basic integrity policies can
serve as building blocks for a comprehensive set of integrity policies, while
providing guarantees that would be sufficient to defend against recent supply
chain attacks detailed by security industry reports. Finally, we evaluate our
open-sourced prototype by simulating deployments on a representative sample of
450 domains that are diverse in ranking and category. We find that our proposal
offers the ability to bootstrap marked security improvements with an overall
overhead of hundreds of milliseconds on initial page loads, and negligible
overhead on reloads, regardless of network speeds. In addition, from examining
archived data for the sample sites, we find that several of the proposed policy
building blocks suit their dependency usage patterns, and would incur minimal
administrative overhead.

</details>


### [32] [ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System](https://arxiv.org/abs/2509.14589)
*Taesoo Kim,HyungSeok Han,Soyeon Park,Dae R. Jeong,Dohyeok Kim,Dongkwan Kim,Eunsoo Kim,Jiho Kim,Joshua Wang,Kangsu Kim,Sangwoo Ji,Woosun Song,Hanqing Zhao,Andrew Chin,Gyejin Lee,Kevin Stevens,Mansour Alharthi,Yizhuo Zhai,Cen Zhang,Joonun Jang,Yeongjin Jang,Ammar Askar,Dongju Kim,Fabian Fleischer,Jeongin Cho,Junsik Kim,Kyungjoon Ko,Insu Yun,Sangdon Park,Dowoo Baik,Haein Lee,Hyeon Heo,Minjae Gwon,Minjae Lee,Minwoo Baek,Seunggi Min,Wonyoung Kim,Yonghwi Jin,Younggi Park,Yunjae Choi,Jinho Jung,Gwanhyun Lee,Junyoung Jang,Kyuheon Kim,Yeonghyeon Cha,Youngjoon Kim*

Main category: cs.CR

TL;DR: ATLANTIS是DARPA AI Cyber Challenge冠军系统，结合大语言模型与程序分析技术，实现自动化漏洞发现和修复


<details>
  <summary>Details</summary>
Motivation: 解决现代软件开发中漏洞发现和修复的速度与规模挑战，克服传统自动化安全工具的局限性

Method: 集成大语言模型(LLMs)与符号执行、定向模糊测试、静态分析等程序分析技术

Result: 在DARPA AIxCC决赛中获得第一名，能够跨C到Java等多种代码库实现高精度漏洞发现和语义正确的补丁生成

Conclusion: 展示了程序分析与现代AI结合在自动化安全领域的突破性进展，为未来研究提供了可复现的基础

Abstract: We present ATLANTIS, the cyber reasoning system developed by Team Atlanta
that won 1st place in the Final Competition of DARPA's AI Cyber Challenge
(AIxCC) at DEF CON 33 (August 2025). AIxCC (2023-2025) challenged teams to
build autonomous cyber reasoning systems capable of discovering and patching
vulnerabilities at the speed and scale of modern software. ATLANTIS integrates
large language models (LLMs) with program analysis -- combining symbolic
execution, directed fuzzing, and static analysis -- to address limitations in
automated vulnerability discovery and program repair. Developed by researchers
at Georgia Institute of Technology, Samsung Research, KAIST, and POSTECH, the
system addresses core challenges: scaling across diverse codebases from C to
Java, achieving high precision while maintaining broad coverage, and producing
semantically correct patches that preserve intended behavior. We detail the
design philosophy, architectural decisions, and implementation strategies
behind ATLANTIS, share lessons learned from pushing the boundaries of automated
security when program analysis meets modern AI, and release artifacts to
support reproducibility and future research.

</details>


### [33] [Threats and Security Strategies for IoMT Infusion Pumps](https://arxiv.org/abs/2509.14604)
*Ramazan Yener,Muhammad Hassan,Masooda Bashir*

Main category: cs.CR

TL;DR: 这篇论文通过对过去五年132篇文献的综述分析，确定了医疗物联网(IoMT)注射泥的7个主要安全漏洞领域，为制定主动防御策略提供了结构化指引。


<details>
  <summary>Details</summary>
Motivation: 医疗物联网(IoMT)在提升病人养护质量的同时也扩大了网络攻击面，注射泥作为关键医疗设备面临着严重的网络安全风险。

Method: 采用目标导向的文献综述方法，从过去五年132篇相关论文中选取分析了7项现有研究，识别并分类安全漏洞。

Result: 识别出注射泥存在5大类漏洞：设备级缺陷、认证和访问控制问题、网络通信弱点、数据安全隐私风险、运营组织挑战，这些漏洞可能导致医疗网络内部的横向移动攻击。

Conclusion: 研究提供了对注射泥安全漏洞的结构化理解，为医疗IT专业人员和设备制造商开发主动防御策略提供了价值，最终目的是保障注射泥安全和保护病人健康。

Abstract: The integration of the Internet of Medical Things (IoMT) into healthcare
systems has transformed patient care by enabling real-time monitoring, enhanced
diagnostics, and enhanced operational efficiency. However, this increased
connectivity has also expanded the attack surface for cybercriminals, raising
significant cybersecurity and privacy concerns. This study focuses on the
cybersecurity vulnerabilities of IoMT infusion pumps, which are critical
devices in modern healthcare. Through a targeted literature review of the past
five years, we analyzed seven current studies from a pool of 132 papers to
identify security vulnerabilities. Our findings indicate that infusion pumps
face vulnerabilities such as device-level flaws, authentication and access
control issues, network and communication weaknesses, data security and privacy
risks, and operational or organizational challenges that can expose them to
lateral attacks within healthcare networks. Our analysis synthesizes findings
from seven recent studies to clarify how and why infusion pumps remain
vulnerable in each of these areas. By categorizing the security gaps, we
highlight critical risk patterns and their implications. This work underscores
the scope of the issue and provides a structured understanding that is valuable
for healthcare IT professionals and device manufacturers. Ultimately, the
findings can inform the development of targeted, proactive security strategies
to better safeguard infusion pumps and protect patient well-being.

</details>


### [34] [Enterprise AI Must Enforce Participant-Aware Access Control](https://arxiv.org/abs/2509.14608)
*Shashank Shreedhar Bhatt,Tanmay Rajore,Khushboo Aggarwal,Ganesh Ananthanarayanan,Ranveer Chandra,Nishanth Chandran,Suyash Choudhury,Divya Gupta,Emre Kiciman,Sumit Kumar Pandey,Srinath Setty,Rahul Sharma,Teijia Zhao*

Main category: cs.CR

TL;DR: 这篇论文揭示了大语言模型企业部署中的数据泄漏风险，提出通过细粒度访问控制来防范精细化调整和RAG流程中的敏感信息泄漏。


<details>
  <summary>Details</summary>
Motivation: 企业环境中的LLM部署存在严重安全风险，精细化调整和RAG流程可能导致敏感数据泄漏给未授权用户，而现有防御措施无法提供可靠保护。

Method: 提出了一种基于细粒度访问控制的框架，要求所有用于训练、检索或生成的内容都必须明确授权给所有参与交互的用户。

Result: 证明了现有防御方案的概率性质和弱点，并实现了一种确定性的访问控制方案来防止数据泄漏。

Conclusion: 只有通过细粒度访问控制的严格执行才能可靠防止多用户LLM系统中的敏感数据泄漏，该方案已在Microsoft Copilot Tuning中部署。

Abstract: Large language models (LLMs) are increasingly deployed in enterprise settings
where they interact with multiple users and are trained or fine-tuned on
sensitive internal data. While fine-tuning enhances performance by
internalizing domain knowledge, it also introduces a critical security risk:
leakage of confidential training data to unauthorized users. These risks are
exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG)
pipelines that dynamically fetch contextual documents at inference time.
  We demonstrate data exfiltration attacks on AI assistants where adversaries
can exploit current fine-tuning and RAG architectures to leak sensitive
information by leveraging the lack of access control enforcement. We show that
existing defenses, including prompt sanitization, output filtering, system
isolation, and training-level privacy mechanisms, are fundamentally
probabilistic and fail to offer robust protection against such attacks.
  We take the position that only a deterministic and rigorous enforcement of
fine-grained access control during both fine-tuning and RAG-based inference can
reliably prevent the leakage of sensitive data to unauthorized recipients.
  We introduce a framework centered on the principle that any content used in
training, retrieval, or generation by an LLM is explicitly authorized for
\emph{all users involved in the interaction}. Our approach offers a simple yet
powerful paradigm shift for building secure multi-user LLM systems that are
grounded in classical access control but adapted to the unique challenges of
modern AI workflows. Our solution has been deployed in Microsoft Copilot
Tuning, a product offering that enables organizations to fine-tune models using
their own enterprise-specific data.

</details>


### [35] [Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection](https://arxiv.org/abs/2509.14622)
*Yihao Guo,Haocheng Bian,Liutong Zhou,Ze Wang,Zhaoyi Zhang,Francois Kawala,Milan Dean,Ian Fischer,Yuantao Peng,Noyan Tokgozoglu,Ivan Barrientos,Riyaaz Shaik,Rachel Li,Chandru Venkataraman,Reza Shifteh Far,Moses Pawar,Venkat Sundaranatha,Michael Xu,Frank Chu*

Main category: cs.CR

TL;DR: ADRAG是一个两阶段框架，通过对抗蒸馏和检索增强技术实现高效实时的恶意意图检测，在保持高性能的同时显著降低延迟


<details>
  <summary>Details</summary>
Motivation: 现有方法无法实时处理多样复杂的用户查询，需要开发既能保持高性能又能实现低延迟的在线恶意意图检测系统

Method: 两阶段框架：训练阶段使用对抗扰动和检索增强的高容量教师模型学习鲁棒决策边界；推理阶段通过蒸馏调度器将知识转移到紧凑学生模型，并利用在线更新的知识库进行实时检测

Result: 在10个安全基准测试中，149M参数的ADRAG达到WildGuard-7B 98.5%的性能，OOD检测超越GPT-4 3.3%和Llama-Guard-3-8B 9.5%，延迟降低5.6倍，支持300 QPS

Conclusion: ADRAG框架成功实现了高性能与低延迟的平衡，为在线实时恶意意图检测提供了有效的解决方案

Abstract: With the deployment of Large Language Models (LLMs) in interactive
applications, online malicious intent detection has become increasingly
critical. However, existing approaches fall short of handling diverse and
complex user queries in real time. To address these challenges, we introduce
ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework
for robust and efficient online malicious intent detection. In the training
stage, a high-capacity teacher model is trained on adversarially perturbed,
retrieval-augmented inputs to learn robust decision boundaries over diverse and
complex user queries. In the inference stage, a distillation scheduler
transfers the teacher's knowledge into a compact student model, with a
continually updated knowledge base collected online. At deployment, the compact
student model leverages top-K similar safety exemplars retrieved from the
online-updated knowledge base to enable both online and real-time malicious
query detection. Evaluations across ten safety benchmarks demonstrate that
ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's
performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on
out-of-distribution detection, while simultaneously delivering up to 5.6x lower
latency at 300 queries per second (QPS) in real-time applications.

</details>


### [36] [Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework](https://arxiv.org/abs/2509.14657)
*Sergio Benlloch-Lopez,Miquel Viel-Vazquez,Javier Naranjo-Alcazar,Jordi Grau-Haro,Pedro Zuccarello*

Main category: cs.CR

TL;DR: 一种保护IoT音频数据的深度防御体系结构，通过TPM远程证言、TLS 1.3加密和后量子加密技术确保设备启动安全、数据传输安全和数据存储安全。


<details>
  <summary>Details</summary>
Motivation: 解决IoT节点在资源约束下处理敏感音频数据时的安全风险，防止恶意设备或被篡改设备活动。

Method: 采用三个信任域架构，结合TPM远程证言、双方认证TLS 1.3、Kyber和Dilithium后量子加密、LUKS磁盘加密、空尾编码和涨回保护AI模型等技术。

Result: 提出了一个完整的安全协议框架，能够确保只有经过验证的设备才能启动，数据在传输和存储过程中都得到保护，具备后量子安全能力。

Conclusion: 该方案通过深度防御设计有效地保护了IoT音频分析系统的安全性，为资源受限环境下的敏感数据处理提供了完整的安全保障方案。

Abstract: The rapid proliferation of IoT nodes equipped with microphones and capable of
performing on-device audio classification exposes highly sensitive data while
operating under tight resource constraints. To protect against this, we present
a defence-in-depth architecture comprising a security protocol that treats the
edge device, cellular network and cloud backend as three separate trust
domains, linked by TPM-based remote attestation and mutually authenticated TLS
1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At
startup, each boot stage is measured into TPM PCRs. The node can only decrypt
its LUKS-sealed partitions after the cloud has verified a TPM quote and
released a one-time unlock key. This ensures that rogue or tampered devices
remain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber
and Dilithium to provide post-quantum resilience. Meanwhile, end-to-end
encryption and integrity hashes safeguard extracted audio features. Signed,
rollback-protected AI models and tamper-responsive sensors harden firmware and
hardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive
sealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum
cipher and an encrypted cloud replica. Finally, we set out a plan for
evaluating the physical and logical security of the proposed protocol.

</details>


### [37] [Security Analysis of Web Applications Based on Gruyere](https://arxiv.org/abs/2509.14706)
*Yonghao Ni,Zhongwen Li,Xiaoqi Li*

Main category: cs.CR

TL;DR: 该论文基于Gruyere平台分析Web安全漏洞，重现经典漏洞案例并提出修复策略，证明传统漏洞原理对现代安全研究仍有重要价值


<details>
  <summary>Details</summary>
Motivation: 随着互联网技术快速发展，Web系统安全漏洞日益增多，对数据保护、隐私安全和业务连续性构成严重威胁，需要进行系统性的Web安全研究来增强网络系统的可靠性和鲁棒性

Method: 首先回顾OWASP Top 10总结常见Web漏洞类型和利用机制，然后以Gruyere平台为实验对象分析已知漏洞，详细重现特定漏洞步骤并提出全面修复策略，最后将Gruyere漏洞与当代实际案例进行比较

Result: 研究发现虽然Gruyere平台的漏洞相对过时，但其底层原理对于解释现代安全缺陷仍然高度相关，基于Gruyere的Web系统安全分析能够深化对漏洞机制的理解

Conclusion: 基于Gruyere的Web安全分析不仅有助于理解漏洞机制，还为技术创新和安全防御提供实际支持，传统漏洞研究对现代网络安全仍具有重要指导意义

Abstract: With the rapid development of Internet technologies, web systems have become
essential infrastructures for modern information exchange and business
operations. However, alongside their expansion, numerous security
vulnerabilities have emerged, making web security a critical research focus
within the broader field of cybersecurity. These issues are closely related to
data protection, privacy preservation, and business continuity, and systematic
research on web security is crucial for mitigating malicious attacks and
enhancing the reliability and robustness of network systems. This paper first
reviews the OWASP Top 10, summarizing the types, causes, and impacts of common
web vulnerabilities, and illustrates their exploitation mechanisms through
representative cases. Building upon this, the Gruyere platform is adopted as an
experimental subject for analyzing known vulnerabilities. The study presents
detailed reproduction steps for specific vulnerabilities, proposes
comprehensive remediation strategies, and further compares Gruyere's
vulnerabilities with contemporary real-world cases. The findings suggest that,
although Gruyere's vulnerabilities are relatively outdated, their underlying
principles remain highly relevant for explaining a wide range of modern
security flaws. Overall, this research demonstrates that web system security
analysis based on Gruyere not only deepens the understanding of vulnerability
mechanisms but also provides practical support for technological innovation and
security defense.

</details>


### [38] [Variables Ordering Optimization in Boolean Characteristic Set Method Using Simulated Annealing and Machine Learning-based Time Prediction](https://arxiv.org/abs/2509.14754)
*Minzhong Luo,Yudong Sun,Yin Long*

Main category: cs.CR

TL;DR: 本文提出了一种结合机器学习时间预测和模拟退火算法的新框架，用于优化布尔特征集方法的变量排序，显著提高了布尔方程系统的求解效率。


<details>
  <summary>Details</summary>
Motivation: 布尔特征集方法在求解布尔方程系统时性能高度依赖于变量排序，不同排序下的求解时间差异巨大，这限制了该方法的实际应用效率。

Method: 构建包含变量频率谱和对应求解时间的数据集，训练机器学习预测器估计任意变量排序的求解时间，然后将其作为模拟退火算法的成本函数来寻找最优排序。

Result: 实验表明该方法在较大规模系统（如n=32）上显著优于标准BCS算法、Gröbner基方法和SAT求解器。

Conclusion: 该工作不仅为代数密码分析提供了实用的加速工具，还为符号计算中机器学习增强的组合优化建立了理论基础，并推导了算法的概率时间复杂性界限。

Abstract: Solving systems of Boolean equations is a fundamental task in symbolic
computation and algebraic cryptanalysis, with wide-ranging applications in
cryptography, coding theory, and formal verification. Among existing
approaches, the Boolean Characteristic Set (BCS) method[1] has emerged as one
of the most efficient algorithms for tackling such problems. However, its
performance is highly sensitive to the ordering of variables, with solving
times varying drastically under different orderings for fixed variable counts n
and equations size m. To address this challenge, this paper introduces a novel
optimization framework that synergistically integrates machine learning
(ML)-based time prediction with simulated annealing (SA) to efficiently
identify high-performance variables orderings. Weconstruct a dataset comprising
variable frequency spectrum X and corresponding BCS solving time t for
benchmark systems(e.g., n = m = 28). Utilizing this data, we train an accurate
ML predictor ft(X) to estimate solving time for any given variables ordering.
For each target system, ft serves as the cost function within an SA algorithm,
enabling rapid discovery of low-latency orderings that significantly expedite
subsequent BCS execution. Extensive experiments demonstrate that our method
substantially outperforms the standard BCS algorithm[1], Gr\"obner basis method
[2] and SAT solver[3], particularly for larger-scale systems(e.g., n = 32).
Furthermore, we derive probabilistic time complexity bounds for the overall
algorithm using stochastic process theory, establishing a quantitative
relationship between predictor accuracy and expected solving complexity. This
work provides both a practical acceleration tool for algebraic cryptanalysis
and a theoretical foundation for ML-enhanced combinatorial optimization in
symbolic computation.

</details>


### [39] [Blockchain-Enabled Explainable AI for Trusted Healthcare Systems](https://arxiv.org/abs/2509.14987)
*Md Talha Mohsin*

Main category: cs.CR

TL;DR: 一种基于区块链和可解释AI的医疗健康框架，解决医疗数据交换安全和临床决策透明性问题


<details>
  <summary>Details</summary>
Motivation: 解决医疗健康系统中的两大关键挑战：数据交换安全性和AI驱动的临床决策的可理解性

Method: 结合区块链技术（确保病人记录不可篡改、可审计）和可解释AI方法，构建混合边缘-云算框架，支持联邦学习计算

Result: 实现了数据层面的信任（通过验证和加密记录共享）和决策层面的信任（通过可审计且临床相关的解释）

Conclusion: 该框架通过确保透明性、可审计性和遵规性，提升了AI在医疗健康领域的可靠性、采纳率和效果，为更安全可靠的临床决策奠定基础

Abstract: This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF)
for healthcare systems to tackle two essential challenges confronting health
information networks: safe data exchange and comprehensible AI-driven clinical
decision-making. Our architecture incorporates blockchain, ensuring patient
records are immutable, auditable, and tamper-proof, alongside Explainable AI
(XAI) methodologies that yield transparent and clinically relevant model
predictions. By incorporating security assurances and interpretability
requirements into a unified optimization pipeline, BXHF ensures both data-level
trust (by verified and encrypted record sharing) and decision-level trust (with
auditable and clinically aligned explanations). Its hybrid edge-cloud
architecture allows for federated computation across different institutions,
enabling collaborative analytics while protecting patient privacy. We
demonstrate the framework's applicability through use cases such as
cross-border clinical research networks, uncommon illness detection and
high-risk intervention decision support. By ensuring transparency,
auditability, and regulatory compliance, BXHF improves the credibility, uptake,
and effectiveness of AI in healthcare, laying the groundwork for safer and more
reliable clinical decision-making.

</details>


### [40] [Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting](https://arxiv.org/abs/2509.15170)
*Aarushi Mahajan,Wayne Burleson*

Main category: cs.CR

TL;DR: 基于ResNet-34和变分自动编码器的强化无线设备指纹识别系统，结合水印技术和异常检测，提高了身份验证的净确性和安全性


<details>
  <summary>Details</summary>
Motivation: 解决基于深度学习的无线设备指纹识别模型容易被复制、篡改和避免攻击的安全漏洞

Method: 使用ResNet-34处理对数Mel谱图进行识别，嵌入三种水印：简单触发器、对抗性训练的噪声和筛选耐受触发器、隐藏的梯度/权重签名；使用卷积变分自动编码器进行异常检测

Result: 在LoRa数据集上实现94.6%的净确度、98%的水印成功率和0.94的AUROC值

Conclusion: 该系统提供了可验证、抗篡改的身份验证方案，有效解决了传统RFFI模型的安全问题

Abstract: Radio frequency fingerprint identification (RFFI) distinguishes wireless
devices by the small variations in their analog circuits, avoiding heavy
cryptographic authentication. While deep learning on spectrograms improves
accuracy, models remain vulnerable to copying, tampering, and evasion. We
present a stronger RFFI system combining watermarking for ownership proof and
anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel
spectrograms, we embed three watermarks: a simple trigger, an adversarially
trained trigger robust to noise and filtering, and a hidden gradient/weight
signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler
(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,
our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,
offering verifiable, tamper-resistant authentication.

</details>


### [41] [Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via Probabilistically Ablating Refusal Direction](https://arxiv.org/abs/2509.15202)
*Yuanbo Xie,Yingjie Zhang,Tianyun Liu,Duohe Ma,Tingwen Liu*

Main category: cs.CR

TL;DR: DeepRefusal是一个针对大语言模型越狱攻击的鲁棒安全对齐框架，通过概率性消融拒绝方向来动态重建拒绝机制，显著降低攻击成功率约95%


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法存在安全对齐深度不足和内部防御机制不鲁棒的问题，容易受到预填充和拒绝方向操纵等对抗攻击

Method: 在微调过程中概率性地在不同层和token深度消融拒绝方向，强制模型从越狱状态动态重建其拒绝机制

Result: 在四个开源LLM家族和六种代表性攻击上的广泛评估显示，攻击成功率降低约95%，同时模型能力保持良好，性能退化最小

Conclusion: DeepRefusal框架有效解决了现有安全对齐方法的局限性，对预填充、拒绝方向攻击及其他未见越狱策略都表现出强韧性

Abstract: Jailbreak attacks pose persistent threats to large language models (LLMs).
Current safety alignment methods have attempted to address these issues, but
they experience two significant limitations: insufficient safety alignment
depth and unrobust internal defense mechanisms. These limitations make them
vulnerable to adversarial attacks such as prefilling and refusal direction
manipulation. We introduce DeepRefusal, a robust safety alignment framework
that overcomes these issues. DeepRefusal forces the model to dynamically
rebuild its refusal mechanisms from jailbreak states. This is achieved by
probabilistically ablating the refusal direction across layers and token depths
during fine-tuning. Our method not only defends against prefilling and refusal
direction attacks but also demonstrates strong resilience against other unseen
jailbreak strategies. Extensive evaluations on four open-source LLM families
and six representative attacks show that DeepRefusal reduces attack success
rates by approximately 95%, while maintaining model capabilities with minimal
performance degradation.

</details>


### [42] [Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems](https://arxiv.org/abs/2509.15213)
*Yicheng Zhang,Zijian Huang,Sophie Chen,Erfan Shayegani,Jiasi Chen,Nael Abu-Ghazaleh*

Main category: cs.CR

TL;DR: 本文分析了XR设备集成大语言模型的安全漏洞，展示了针对多平台的概念验证攻击，并提出了防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着XR应用越来越多地集成LLM来增强用户体验和内容生成，这种集成使得XR应用面临新的安全威胁，需要系统性地分析和防御这些漏洞。

Method: 通过文献和实践分析LLM集成的XR系统，建立分类框架，识别共同威胁模型，并在多个XR平台（Meta Quest 3、Ray-Ban、Android、HoloLens 2）上演示概念验证攻击。

Result: 发现尽管各平台实现方式不同，但都存在公共上下文被篡改的漏洞，导致用户接收到错误的视听反馈，危及安全和隐私。

Conclusion: 提出了针对开发者的缓解策略和最佳实践，包括初始防御原型，并呼吁社区开发新的保护机制来降低这些风险。

Abstract: Extended reality (XR) applications increasingly integrate Large Language
Models (LLMs) to enhance user experience, scene understanding, and even
generate executable XR content, and are often called "AI glasses". Despite
these potential benefits, the integrated XR-LLM pipeline makes XR applications
vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR
systems in the literature and in practice and categorize them along different
dimensions from a systems perspective. Building on this categorization, we
identify a common threat model and demonstrate a series of proof-of-concept
attacks on multiple XR platforms that employ various LLM models (Meta Quest 3,
Meta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).
Although these platforms each implement LLM integration differently, they share
vulnerabilities where an attacker can modify the public context surrounding a
legitimate LLM query, resulting in erroneous visual or auditory feedback to
users, thus compromising their safety or privacy, sowing confusion, or other
harmful effects. To defend against these threats, we discuss mitigation
strategies and best practices for developers, including an initial defense
prototype, and call on the community to develop new protection mechanisms to
mitigate these risks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity](https://arxiv.org/abs/2509.14251)
*Qihang Chen*

Main category: cs.AI

TL;DR: 基于层次时空网络模型的统一优化框架，解决多线路地铁班制计划和应急重新规划问题，实现跨线路协同运营和故障快速响应


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在单个地铁线路，缺乏对跨线路协调和故障情况下快速重新规划的关注，而这对大规模无缝连接运营至关重要

Method: 提出层次时空网络模型表示统一员工动作空间，建立计算高效的约束条件和数学形式，并发展基于列生成和最短路径调整的求解算法

Result: 上海和北京地铁实际数据验证显示，方法在成本降低和任务完成方面都超过对照算法，通过跨线路运作特别是在故障情况下实现了显著效率提升

Conclusion: 这项工作强调了全局优化和跨线路协调在多线路地铁系统运营中的重要作用，为智慧城市公共交通的高效可靠运行提供了见解

Abstract: Metro crew planning is a key component of smart city development as it
directly impacts the operational efficiency and service reliability of public
transportation. With the rapid expansion of metro networks, effective
multi-line scheduling and emergency management have become essential for
large-scale seamless operations. However, current research focuses primarily on
individual metro lines,with insufficient attention on cross-line coordination
and rapid replanning during disruptions. Here, a unified optimization framework
is presented for multi-line metro crew planning and replanning with
heterogeneous workforce. Specifically, a hierarchical time-space network model
is proposed to represent the unified crew action space, and computationally
efficient constraints and formulations are derived for the crew's heterogeneous
qualifications and preferences. Solution algorithms based on column generation
and shortest path adjustment are further developed, utilizing the proposed
network model. Experiments with real data from Shanghai and Beijing Metro
demonstrate that the proposed methods outperform benchmark heuristics in both
cost reduction and task completion,and achieve notable efficiency gains by
incorporating cross-line operations, particularly for urgent tasks during
disruptions. This work highlights the role of global optimization and
cross-line coordination in multi-line metro system operations, providing
insights into the efficient and reliable functioning of public transportation
in smart cities.

</details>


### [44] [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
*Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling*

Main category: cs.AI

TL;DR: 这篇论文通过实验评估了多种LLM基于漏洞扫描测试代理的性能，并通过五种核心功能增强提升了模块化代理在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在漏洞扫描测试中的应用日益增多，需要全面评估其在不同攻击阶段的效果和可靠性。

Method: 设计了从单代理到模块化设计的多种LLM基于代理，在实际漏洞扫描测试场景中进行评估，并通过目标增强定向研究五种核心功能的影响。

Result: 目标增强措施显著提升了模块化代理的性能，尤其在复杂、多步骤和实时漏洞扫描测试任务中表现更优。

Conclusion: 通过系统化的功能增强可以有效提升LLM基于漏洞扫描测试代理的性能和可靠性，为实际应用提供重要支撑。

Abstract: Large language models (LLMs) are increasingly used to automate or augment
penetration testing, but their effectiveness and reliability across attack
phases remain unclear. We present a comprehensive evaluation of multiple
LLM-based agents, from single-agent to modular designs, across realistic
penetration testing scenarios, measuring empirical performance and recurring
failure patterns. We also isolate the impact of five core functional
capabilities via targeted augmentations: Global Context Memory (GCM),
Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive
Planning (AP), and Real-Time Monitoring (RTM). These interventions support,
respectively: (i) context coherence and retention, (ii) inter-component
coordination and state management, (iii) tool use accuracy and selective
execution, (iv) multi-step strategic planning, error detection, and recovery,
and (v) real-time dynamic responsiveness. Our results show that while some
architectures natively exhibit subsets of these properties, targeted
augmentations substantially improve modular agent performance, especially in
complex, multi-step, and real-time penetration testing tasks.

</details>


### [45] [Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents](https://arxiv.org/abs/2509.14382)
*Daniel Röder,Akhil Juneja,Roland Roller,Sven Schmeier*

Main category: cs.AI

TL;DR: 这篇论文提出了一种模块化评估框架，通过将web agent相关任务分解为可解释的阶段来进行详细错误分析，以充分发现标准指标没有涉及的问题模式。


<details>
  <summary>Details</summary>
Motivation: 当前对基于大语言模型的web agent评估主要关注整体成功率，而忽视了中间过程中的错误，这限制了对失败模式的深入理解和系统性改进。

Method: 提出了一种模块化评估框架，将agent管道分解为可解释的阶段进行详细错误分析，并使用SeeAct框架和Mind2Web数据集进行案例研究。

Result: 该方法能够更好地揭示标准指标没有涉及的可操作性弱点，为构建更稳健和可推广的web agent排除障碍。

Conclusion: 通过模块化的详细错误分析框架，可以更深入地理解web agent的失败模式，为系统性改进提供了有效途径，最终推动web agent技术的发展。

Abstract: Web agents powered by large language models (LLMs) can autonomously perform
complex, multistep tasks in dynamic web environments. However, current
evaluations mostly focus on the overall success while overlooking intermediate
errors. This limits insight into failure modes and hinders systematic
improvement. This work analyzes existing benchmarks and highlights the lack of
fine-grained diagnostic tools. To address this gap, we propose a modular
evaluation framework that decomposes agent pipelines into interpretable stages
for detailed error analysis. Using the SeeAct framework and the Mind2Web
dataset as a case study, we show how this approach reveals actionable
weaknesses missed by standard metrics - paving the way for more robust and
generalizable web agents.

</details>


### [46] [VCBench: Benchmarking LLMs in Venture Capital](https://arxiv.org/abs/2509.14448)
*Rick Chen,Joseph Ternasky,Afriyie Samuel Kwesi,Ben Griffin,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: VCBench是首个用于预测风险投资中创始人成功的基准测试，包含9000个匿名创始人档案，评估显示LLM模型表现优于人类基准和顶级投资机构。


<details>
  <summary>Details</summary>
Motivation: 现有的SWE-bench和ARC-AGI等基准测试加速了AGI发展，但在风险投资领域缺乏标准化的评估基准，该领域信号稀疏、结果不确定，即使顶级投资者表现也一般。

Method: 创建包含9000个匿名创始人档案的VCBench数据集，进行标准化处理以保留预测特征同时防止身份泄露，通过对抗性测试降低90%以上的重新识别风险，评估9个最先进的LLM模型。

Result: 市场基准精度1.9%，Y Combinator比基准高1.7倍，顶级机构高2.9倍。DeepSeek-V3精度比基准高6倍多，GPT-4o获得最高F0.5分数，大多数模型超越人类基准。

Conclusion: VCBench作为公开且不断发展的资源，为早期风险预测中的AGI评估建立了社区驱动的可重复和隐私保护标准，推动了风险投资领域的人工智能发展。

Abstract: Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets
accelerate progress toward artificial general intelligence (AGI). We introduce
VCBench, the first benchmark for predicting founder success in venture capital
(VC), a domain where signals are sparse, outcomes are uncertain, and even top
investors perform modestly. At inception, the market index achieves a precision
of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1
firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,
standardized to preserve predictive features while resisting identity leakage,
with adversarial tests showing more than 90% reduction in re-identification
risk. We evaluate nine state-of-the-art large language models (LLMs).
DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the
highest F0.5, and most models surpass human benchmarks. Designed as a public
and evolving resource available at vcbench.com, VCBench establishes a
community-driven standard for reproducible and privacy-preserving evaluation of
AGI in early-stage venture forecasting.

</details>


### [47] [From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence](https://arxiv.org/abs/2509.14474)
*Meltem Subasioglu,Nevzat Subasioglu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于认知机制的真正智能(TI)定义，包含六个核心组件，并建立了五级AGI分类系统，为研究提供了清晰的发展路径。


<details>
  <summary>Details</summary>
Motivation: 当前基于表现的AGI定义存在缺陷：缺乏机制化研究路线图，无法定义真正智能的质性特征。需要从人脑获得启发，将重点从外部模仿转向基础认知架构的开发。

Method: 提出真正智能(TI)的6个核心组件：体验感官融合、核心指令、动态模式创建、高度联网的多专家架构、协调层、以及不可测量的联结性。建立五级AGI分类法，基于系统实现的可测量组件数量。

Result: 提供了一个清晰的研究发展路径，包含具体的发展里程碑。认为实现五个可测量组件的等级-5 AGI系统，在功能和实践上等同于真正智能(TI)。

Conclusion: 该框架综合了分析心理学、模式理论、元认知、现代脑结构和最新AI研究的见解，提供了第一个基于机制的全面AGI定义，为研究社区提供了清晰可行的发展路径。

Abstract: The debate around Artificial General Intelligence (AGI) remains open due to
two fundamentally different goals: replicating human-like performance versus
replicating human-like cognitive processes. We argue that current
performance-based definitions are inadequate because they provide no clear,
mechanism-focused roadmap for research, and they fail to properly define the
qualitative nature of genuine intelligence. Drawing inspiration from the human
brain, we propose a new paradigm that shifts the focus from external mimicry to
the development of foundational cognitive architectures. We define True
Intelligence (TI) as a system characterized by six core components: embodied
sensory fusion, core directives, dynamic schemata creation, a
highly-interconnected multi-expert architecture, an orchestration layer, and
lastly, the unmeasurable quality of Interconnectedness, which we hypothesize
results in consciousness and a subjective experience. We propose a practical,
five-level taxonomy of AGI based on the number of the first five measurable
components a system exhibits. This framework provides a clear path forward with
developmental milestones that directly address the challenge of building
genuinely intelligent systems. We contend that once a system achieves Level-5
AGI by implementing all five measurable components, the difference between it
and TI remains as a purely philosophical debate. For practical purposes - and
given theories indicate consciousness is an emergent byproduct of integrated,
higher-order cognition - we conclude that a fifth-level AGI is functionally and
practically equivalent to TI. This work synthesizes diverse insights from
analytical psychology, schema theory, metacognition, modern brain architectures
and latest works in AI to provide the first holistic, mechanism-based
definition of AGI that offers a clear and actionable path for the research
community.

</details>


### [48] [Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485)
*Marko Tesic,Yue Zhao,Joel Z. Leibo,Rakshit S. Trivedi,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 本文应用贝叶斯测量布局方法分析AI代理在Melting Pot竞赛中的社交能力，发现高社会性能力并不总是带来更好表现，顶级团队可能通过针对特定环境优化而非真正合作来获得高分。


<details>
  <summary>Details</summary>
Motivation: 评估AI代理的社交合作能力需要复杂环境，但传统方法难以控制抽象行为如约定遵循。Melting Pot竞赛旨在评估AI系统的合作能力，但现有评估框架可能存在局限性。

Method: 采用贝叶斯测量布局方法推断多智能体系统在Melting Pot竞赛中的能力特征，分析其预测性能和揭示潜在亲社会能力。

Result: 研究发现高亲社会能力有时与更好表现相关，但并非普遍趋势；一些低分代理表现出更强的合作能力；顶级参赛作品更可能在不需要亲社会能力的场景中获得高分。

Conclusion: 测量布局方法提供强预测准确性和可操作见解，有助于在复杂社交环境中更透明、可泛化地评估AI系统。需要改进合作需求标注并考虑不同测试环境引入的偏差。

Abstract: The development and evaluation of social capabilities in AI agents require
complex environments where competitive and cooperative behaviours naturally
emerge. While game-theoretic properties can explain why certain teams or agent
populations outperform others, more abstract behaviours, such as convention
following, are harder to control in training and evaluation settings. The
Melting Pot contest is a social AI evaluation suite designed to assess the
cooperation capabilities of AI systems. In this paper, we apply a Bayesian
approach known as Measurement Layouts to infer the capability profiles of
multi-agent systems in the Melting Pot contest. We show that these capability
profiles not only predict future performance within the Melting Pot suite but
also reveal the underlying prosocial abilities of agents. Our analysis
indicates that while higher prosocial capabilities sometimes correlate with
better performance, this is not a universal trend-some lower-scoring agents
exhibit stronger cooperation abilities. Furthermore, we find that
top-performing contest submissions are more likely to achieve high scores in
scenarios where prosocial capabilities are not required. These findings,
together with reports that the contest winner used a hard-coded solution
tailored to specific environments, suggest that at least one top-performing
team may have optimised for conditions where cooperation was not necessary,
potentially exploiting limitations in the evaluation framework. We provide
recommendations for improving the annotation of cooperation demands and propose
future research directions to account for biases introduced by different
testing environments. Our results demonstrate that Measurement Layouts offer
both strong predictive accuracy and actionable insights, contributing to a more
transparent and generalisable approach to evaluating AI systems in complex
social settings.

</details>


### [49] [DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction](https://arxiv.org/abs/2509.14507)
*Jian Chen,Zhenyan Chen,Xuming Hu,Peilin Zhou,Yining Hua,Han Fang,Cissy Hing Yee Choy,Xinmei Ke,Jingfeng Luo,Zixuan Yuan*

Main category: cs.AI

TL;DR: 提出了DeKeyNLU数据集和DeKeySQL管道，通过改进任务分解和关键词提取来提升NL2SQL性能，在BIRD和Spider数据集上显著提高了SQL生成准确率


<details>
  <summary>Details</summary>
Motivation: 解决现有NL2SQL系统中任务分解不准确和关键词提取错误的问题，这些问题是SQL生成错误的主要瓶颈

Method: 创建包含1500个标注QA对的DeKeyNLU数据集，并基于此开发DeKeySQL管道，包含三个模块：用户问题理解、实体检索和生成

Result: 在BIRD数据集上准确率从62.31%提升到69.10%，在Spider数据集上从84.2%提升到88.7%

Conclusion: DeKeyNLU数据集和DeKeySQL管道有效解决了NL2SQL中的任务分解和关键词提取问题，显著提升了SQL生成准确性

Abstract: Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that
simplifies database access for non-technical users by converting natural
language queries into SQL commands. Recent advancements, particularly those
integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)
reasoning, have made significant strides in enhancing NL2SQL performance.
However, challenges such as inaccurate task decomposition and keyword
extraction by LLMs remain major bottlenecks, often leading to errors in SQL
generation. While existing datasets aim to mitigate these issues by fine-tuning
models, they struggle with over-fragmentation of tasks and lack of
domain-specific keyword annotations, limiting their effectiveness. To address
these limitations, we present DeKeyNLU, a novel dataset which contains 1,500
meticulously annotated QA pairs aimed at refining task decomposition and
enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with
DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three
distinct modules for user question understanding, entity retrieval, and
generation to improve SQL generation accuracy. We benchmarked multiple model
configurations within DeKeySQL RAG pipeline. Experimental results demonstrate
that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy
on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.

</details>


### [50] [Rationality Check! Benchmarking the Rationality of Large Language Models](https://arxiv.org/abs/2509.14546)
*Zhilun Zhou,Jing Yi Wang,Nicholas Sukiennik,Chen Gao,Fengli Xu,Yong Li,James Evans*

Main category: cs.AI

TL;DR: 首个评估大语言模型全方位理性的基准测试，涵盖理论理性和实践理性，包含工具包、实验结果和分析


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在模拟人类行为方面的应用日益广泛，需要评估它们在何种程度上像真实人类一样思考和行动，理性是评估人类行为最重要的概念之一

Method: 提出了一个全面的基准测试，包括易于使用的工具包，覆盖多个领域和不同LLMs，通过实验评估模型的理性表现

Result: 提供了广泛的实验结果和分析，揭示了LLMs与理想人类理性之间的收敛和分歧点

Conclusion: 该基准可以作为LLMs开发者和用户的基础工具，帮助评估和改进模型的理性表现

Abstract: Large language models (LLMs), a recent advance in deep learning and machine
intelligence, have manifested astonishing capacities, now considered among the
most promising for artificial general intelligence. With human-like
capabilities, LLMs have been used to simulate humans and serve as AI assistants
across many applications. As a result, great concern has arisen about whether
and under what circumstances LLMs think and behave like real human agents.
Rationality is among the most important concepts in assessing human behavior,
both in thinking (i.e., theoretical rationality) and in taking action (i.e.,
practical rationality). In this work, we propose the first benchmark for
evaluating the omnibus rationality of LLMs, covering a wide range of domains
and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental
results, and analysis that illuminates where LLMs converge and diverge from
idealized human rationality. We believe the benchmark can serve as a
foundational tool for both developers and users of LLMs.

</details>


### [51] [(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration](https://arxiv.org/abs/2509.14547)
*Yi Lin,Lujin Zhao,Yijie Shi*

Main category: cs.AI

TL;DR: 基于Q表学习和先验决策的动态自动工作流构建框架，结合历史经验和任务特性，在四个数据集上实现了4.05%的性能提升，并将成本降低到30.68%-48.31%。


<details>
  <summary>Details</summary>
Motivation: 现有自动工作流构建方法仅依赖历史经验，导致效率和适配性受限。需要一种能够灵活响应每个任务特性的动态构建方法。

Method: 提出先验动态框架：1）利用Q表学习优化决策空间和利用历史经验；2）代理评估当前任务进度并做出先验决策；3）结合冷启动初始化、提前停止和剪枝机制提高效率。

Result: 在四个标准数据集上，方法比最优基线平均提升4.05%，工作流构建和推理成本降低到30.68%-48.31%。

Conclusion: 该框架通过结合历史经验和任务特性的动态决策，有效提升了自动工作流构建的效率、适配性和性能。

Abstract: Recent studies have shown that carefully designed workflows coordinating
large language models(LLMs) significantly enhance task-solving capabilities
compared to using a single model. While an increasing number of works focus on
autonomous workflow construction, most existing approaches rely solely on
historical experience, leading to limitations in efficiency and adaptability.
We argue that while historical experience is valuable, workflow construction
should also flexibly respond to the unique characteristics of each task. To
this end, we propose an a priori dynamic framework for automated workflow
construction. Our framework first leverages Q-table learning to optimize the
decision space, guiding agent decisions and enabling effective use of
historical experience. At the same time, agents evaluate the current task
progress and make a priori decisions regarding the next executing agent,
allowing the system to proactively select the more suitable workflow structure
for each given task. Additionally, we incorporate mechanisms such as cold-start
initialization, early stopping, and pruning to further improve system
efficiency. Experimental evaluations on four benchmark datasets demonstrate the
feasibility and effectiveness of our approach. Compared to state-of-the-art
baselines, our method achieves an average improvement of 4.05%, while reducing
workflow construction and inference costs to only 30.68%-48.31% of those
required by existing methods.

</details>


### [52] [SynBench: A Benchmark for Differentially Private Text Generation](https://arxiv.org/abs/2509.14594)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Yulong Wu,Hao Li,Jie Zhang,Warren Del-Pinto,Goran Nenadic,Siew Kei Lam,Anil Anthony Bharath*

Main category: cs.AI

TL;DR: 本文研究了在高风险领域中使用差分隐私生成对话的挑战，提出了评估框架、进行大规模基准测试并发现领域复杂性会导致性能下降，还提出了成员推断攻击方法来证明公开数据集可能无效化隐私保证。


<details>
  <summary>Details</summary>
Motivation: 高风险领域如医疗和金融中的数据共享遇到法规、机构和隐私问题的阻碍，而现有匿名化方法对非结构化文本效果不佳，需要差分隐私来提供正式隐私保证的合成数据生成方案。

Method: 首先引入了包含标准化效用性和保真度指标的综合评估框架，涵盖9个经过精选的数据集；然后进行大规模实证研究，基准了最先进的DP文本生成方法和不同规模、细调策略的大语言模型；最后开发了专门针对合成文本的成员推断攻击方法。

Result: 研究发现在DP约束下生成高质量领域特定合成数据仍然是一个未解决的挑战，性能随着领域复杂性的增加而下降；同时提供了首次实证证据，证明使用可能存在于预训练语料库中的公开数据集可能会使声称的隐私保证失效。

Conclusion: 研究结果强调了对严格隐私审计的紧迫需求，并指出了开放领域与专业领域评估之间持续存在的差距，为在隐私敏感的高风险环境中负责任地部署生成式AI提供了重要信息。

Abstract: Data-driven decision support in high-stakes domains like healthcare and
finance faces significant barriers to data sharing due to regulatory,
institutional, and privacy concerns. While recent generative AI models, such as
large language models, have shown impressive performance in open-domain tasks,
their adoption in sensitive environments remains limited by unpredictable
behaviors and insufficient privacy-preserving datasets for benchmarking.
Existing anonymization methods are often inadequate, especially for
unstructured text, as redaction and masking can still allow re-identification.
Differential Privacy (DP) offers a principled alternative, enabling the
generation of synthetic data with formal privacy assurances. In this work, we
address these challenges through three key contributions. First, we introduce a
comprehensive evaluation framework with standardized utility and fidelity
metrics, encompassing nine curated datasets that capture domain-specific
complexities such as technical jargon, long-context dependencies, and
specialized document structures. Second, we conduct a large-scale empirical
study benchmarking state-of-the-art DP text generation methods and LLMs of
varying sizes and different fine-tuning strategies, revealing that high-quality
domain-specific synthetic data generation under DP constraints remains an
unsolved challenge, with performance degrading as domain complexity increases.
Third, we develop a membership inference attack (MIA) methodology tailored for
synthetic text, providing first empirical evidence that the use of public
datasets - potentially present in pre-training corpora - can invalidate claimed
privacy guarantees. Our findings underscore the urgent need for rigorous
privacy auditing and highlight persistent gaps between open-domain and
specialist evaluations, informing responsible deployment of generative AI in
privacy-sensitive, high-stakes settings.

</details>


### [53] [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
*NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek*

Main category: cs.AI

TL;DR: AgentCompass是首个专门为多智能体工作流设计的后部署监控和调试评估框架，通过结构化分析流程和双记忆系统实现持续学习，在TRAIL基准测试中达到最先进效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体工作流中的广泛应用，现有评估方法无法有效捕捉错误、涌现行为和系统性故障带来的风险，需要专门的监控调试工具。

Method: 采用结构化多阶段分析流程：错误识别分类、主题聚类、量化评分和策略总结，并配备情景记忆和语义记忆的双记忆系统实现持续学习。

Result: 在真实部署中验证实用性，在TRAIL基准测试中取得最先进结果，发现了人工标注遗漏的关键问题。

Conclusion: AgentCompass作为面向开发者的强大工具，能够可靠地监控和改进生产环境中的智能体系统。

Abstract: With the growing adoption of Large Language Models (LLMs) in automating
complex, multi-agent workflows, organizations face mounting risks from errors,
emergent behaviors, and systemic failures that current evaluation methods fail
to capture. We present AgentCompass, the first evaluation framework designed
specifically for post-deployment monitoring and debugging of agentic workflows.
AgentCompass models the reasoning process of expert debuggers through a
structured, multi-stage analytical pipeline: error identification and
categorization, thematic clustering, quantitative scoring, and strategic
summarization. The framework is further enhanced with a dual memory
system-episodic and semantic-that enables continual learning across executions.
Through collaborations with design partners, we demonstrate the framework's
practical utility on real-world deployments, before establishing its efficacy
against the publicly available TRAIL benchmark. AgentCompass achieves
state-of-the-art results on key metrics, while uncovering critical issues
missed in human annotations, underscoring its role as a robust,
developer-centric tool for reliable monitoring and improvement of agentic
systems in production.

</details>


### [54] [Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory](https://arxiv.org/abs/2509.14662)
*Ming Li,Nan Zhang,Chenrui Fan,Hong Jiao,Yanbin Fu,Sydney Peters,Qingshu Xu,Robert Lissitz,Tianyi Zhou*

Main category: cs.AI

TL;DR: 应用Schoenfeld的认知框架分析大型推理模型的思维过程，构建了第一个精细化机器推理分析的公开标准数据集


<details>
  <summary>Details</summary>
Motivation: 缺乏理论基础来理解大型推理模型生成的链式思维的结构化特征

Method: 应用Schoenfeld认知感知理论的7个标签（如计划、实施、验证）对数千个模型生成的数学问题解决方案进行注释

Result: 发现了大型推理模型思维的特征模式，如认知状态之间的过渡动态，并构建了包含大规模注释语料库和详细注释指南的标准数据集

Conclusion: 该框架为解释大型推理模型的认知过程提供了理论基础，有助于开发更可控制和透明的推理系统

Abstract: While Large Reasoning Models (LRMs) generate extensive chain-of-thought
reasoning, we lack a principled framework for understanding how these thoughts
are structured. In this paper, we introduce a novel approach by applying
Schoenfeld's Episode Theory, a classic cognitive framework for human
mathematical problem-solving, to analyze the reasoning traces of LRMs. We
annotated thousands of sentences and paragraphs from model-generated solutions
to math problems using seven cognitive labels (e.g., Plan, Implement, Verify).
The result is the first publicly available benchmark for the fine-grained
analysis of machine reasoning, including a large annotated corpus and detailed
annotation guidebooks. Our preliminary analysis reveals distinct patterns in
LRM reasoning, such as the transition dynamics between cognitive states. This
framework provides a theoretically grounded methodology for interpreting LRM
cognition and enables future work on more controllable and transparent
reasoning systems.

</details>


### [55] [RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning](https://arxiv.org/abs/2509.14693)
*Song Xu,Yilun Liu,Minggui He,Mingchen Dai,Ziang Chen,Chunguang Zhao,Jingzhou Du,Shimin Tao,Weibin Meng,Shenglin Zhang,Yongqian Sun,Boxing Chen,Daimeng Wei*

Main category: cs.AI

TL;DR: RationAnomaly是一个结合思维链微调和强化学习的日志异常检测框架，通过专家修正数据集和多方奖励函数，在提高检测精度的同时增强可解释性和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型缺乏可解释性和泛化能力，而基于大语言模型的方法存在不可靠性和事实错误问题，需要一种既能保持高精度又具备透明推理能力的日志异常检测方法。

Method: 首先通过思维链引导的监督微调注入专家推理模式，使用专家修正的高质量数据集；然后采用强化学习阶段，通过多方奖励函数优化准确性和逻辑一致性，有效减少幻觉问题。

Result: 在关键基准测试中实现了优越的F1分数，超越了最先进的基线方法，同时提供透明的逐步分析输出。

Conclusion: RationAnomaly框架成功解决了现有日志异常检测方法的局限性，在保持高检测精度的同时提供了可解释的推理过程，相关代码和数据集已开源。

Abstract: Logs constitute a form of evidence signaling the operational status of
software systems. Automated log anomaly detection is crucial for ensuring the
reliability of modern software systems. However, existing approaches face
significant limitations: traditional deep learning models lack interpretability
and generalization, while methods leveraging Large Language Models are often
hindered by unreliability and factual inaccuracies. To address these issues, we
propose RationAnomaly, a novel framework that enhances log anomaly detection by
synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our
approach first instills expert-like reasoning patterns using CoT-guided
supervised fine-tuning, grounded in a high-quality dataset corrected through a
rigorous expert-driven process. Subsequently, a reinforcement learning phase
with a multi-faceted reward function optimizes for accuracy and logical
consistency, effectively mitigating hallucinations. Experimentally,
RationAnomaly outperforms state-of-the-art baselines, achieving superior
F1-scores on key benchmarks while providing transparent, step-by-step
analytical outputs. We have released the corresponding resources, including
code and datasets.

</details>


### [56] [The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs](https://arxiv.org/abs/2509.14704)
*Masaharu Mizumoto,Dat Nguyen,Zhiheng Han,Jiyuan Fang,Heyuan Guan,Xingfu Li,Naoya Shiraishi,Xuyang Tian,Yo Nakawake,Le Minh Nguyen*

Main category: cs.AI

TL;DR: Nazonazo是一个基于日本儿童谜语构建的成本效益高、可扩展的基准测试，用于评估基于洞察力的推理能力，解决了LLM评估中的基准饱和和污染问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估面临基准饱和和污染问题，需要开发成本效益高、可扩展且易于更新的评估基准来测试洞察式推理能力。

Method: 使用日本儿童谜语构建基准测试，谜语短小（大多一句话）、无需专业知识、可大规模生成，支持在怀疑泄露时快速刷新盲测集。评估了38个前沿模型和126名成年人。

Result: 除GPT-5外，没有模型能达到人类52.9%的平均准确率。推理模型显著优于非推理模型，模型大小与准确率无可靠关联。分析发现模型经常产生正确答案但未能选择为最终答案。

Conclusion: Nazonazo提供了成本效益高、可扩展且易于更新的基准格式，解决了当前评估危机，同时揭示了元认知弱点，为未来控制和校准方法提供了明确目标。

Abstract: Benchmark saturation and contamination undermine confidence in LLM
evaluation. We present Nazonazo, a cost-effective and extensible benchmark
built from Japanese children's riddles to test insight-based reasoning. Items
are short (mostly one sentence), require no specialized domain knowledge, and
can be generated at scale, enabling rapid refresh of blind sets when leakage is
suspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No
model except for GPT-5 is comparable to human performance, which achieves a
52.9% mean accuracy. Model comparison on extended 201 items shows that
reasoning models significantly outperform non-reasoning peers, while model size
shows no reliable association with accuracy. Beyond aggregate accuracy, an
informal candidate-tracking analysis of thought logs reveals many cases of
verification failure: models often produce the correct solution among
intermediate candidates yet fail to select it as the final answer, which we
illustrate with representative examples observed in multiple models. Nazonazo
thus offers a cost-effective, scalable, and easily renewable benchmark format
that addresses the current evaluation crisis while also suggesting a recurrent
meta-cognitive weakness, providing clear targets for future control and
calibration methods.

</details>


### [57] [Enhancing Retrieval Augmentation via Adversarial Collaboration](https://arxiv.org/abs/2509.14750)
*Letian Zhang,Guanghao Meng,Xudong Ren,Yiming Wang,Shu-Tao Xia*

Main category: cs.AI

TL;DR: 提出了对抗性协作RAG框架(AC-RAG)，通过两个异构代理的对抗性协作来解决检索幻觉问题，显著提升了检索准确性和领域性能


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成(RAG)中存在的"检索幻觉"问题，即微调模型无法识别和处理低质量检索文档，导致性能下降

Method: AC-RAG框架包含两个异构代理：通用检测器(识别知识缺口)和领域专家解析器(提供精确解决方案)，在调解器指导下进行对抗性协作，通过持续质疑实现迭代问题分析和精细化知识检索

Result: 大量实验表明，AC-RAG显著提高了检索准确性，在各种垂直领域都优于最先进的RAG方法

Conclusion: AC-RAG框架通过对抗性协作机制有效解决了检索幻觉问题，为领域特定LLMs提供了更可靠的检索增强生成方案

Abstract: Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.

</details>


### [58] [OpenLens AI: Fully Autonomous Research Agent for Health Infomatics](https://arxiv.org/abs/2509.14778)
*Yuxiao Cheng,Jinli Suo*

Main category: cs.AI

TL;DR: OpenLens AI是一个专为健康信息学设计的全自动化框架，通过集成文献综述、数据分析、代码生成和手稿准备等专业代理，结合视觉语言反馈和质量控制，实现端到端的研究自动化。


<details>
  <summary>Details</summary>
Motivation: 健康信息学研究具有数据模态多样、知识快速扩展的特点，需要整合生物医学科学、数据分析和临床实践。现有基于大语言模型的代理系统在医学可视化解释和领域特定质量要求方面存在不足。

Method: 开发OpenLens AI框架，集成专业代理进行文献综述、数据分析、代码生成和手稿准备，通过视觉语言反馈处理医学可视化，并实施质量控制确保可重复性。

Result: 该框架能够自动化整个研究流程，生成可直接发表的LaTeX手稿，提供透明可追溯的工作流程。

Conclusion: OpenLens AI为健康信息学研究提供了领域适应的自动化解决方案，能够有效推进该领域的研究进展。

Abstract: Health informatics research is characterized by diverse data modalities,
rapid knowledge expansion, and the need to integrate insights across biomedical
science, data analytics, and clinical practice. These characteristics make it
particularly well-suited for agent-based approaches that can automate knowledge
exploration, manage complex workflows, and generate clinically meaningful
outputs. Recent progress in large language model (LLM)-based agents has
demonstrated promising capabilities in literature synthesis, data analysis, and
even end-to-end research execution. However, existing systems remain limited
for health informatics because they lack mechanisms to interpret medical
visualizations and often overlook domain-specific quality requirements. To
address these gaps, we introduce OpenLens AI, a fully automated framework
tailored to health informatics. OpenLens AI integrates specialized agents for
literature review, data analysis, code generation, and manuscript preparation,
enhanced by vision-language feedback for medical visualization and quality
control for reproducibility. The framework automates the entire research
pipeline, producing publication-ready LaTeX manuscripts with transparent and
traceable workflows, thereby offering a domain-adapted solution for advancing
health informatics research.

</details>


### [59] [Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers](https://arxiv.org/abs/2509.14942)
*Minh-Khoi Pham,Tai Tan Mai,Martin Crane,Rob Brennan,Marie E. Ward,Una Geary,Declan Byrne,Brian O Connell,Colm Bergin,Donncha Creagh,Nick McDonald,Marija Bezbradica*

Main category: cs.AI

TL;DR: 这研究提出了一种可解释的AI框架，使用Transformer模型预测肠菌科抗菌素生成菌感染对患者结局的影响，TabTransformer模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 肠菌科抗菌素生成菌感染是医院感染控制的关键问题，但使用现代深度学习方法进行预测模型研究仍较少。

Method: 分析爱尔兰医院的电子病历数据，包括诊断代码、病房转换、患者人口统计学、感染相关变量和接触网络特征。比较Transformer架构与传统机器学习模型的表现。

Result: TabTransformer模型在多个临床预测任务中持续超过基线模型，特别是在CPE获得预测上（AUROC和敏感度）。感染相关特征、网络中心性指标等影响最大。

Conclusion: 该研究提供了一个健壮且可解释的AI框架，证明了Transformer模型的优越性，并强调了多样化临床和网络特征在预测CPE相关结局中的重要性。

Abstract: Carbapenemase-Producing Enterobacteriace poses a critical concern for
infection prevention and control in hospitals. However, predictive modeling of
previously highlighted CPE-associated risks such as readmission, mortality, and
extended length of stay (LOS) remains underexplored, particularly with modern
deep learning approaches. This study introduces an eXplainable AI modeling
framework to investigate CPE impact on patient outcomes from Electronic Medical
Records data of an Irish hospital. We analyzed an inpatient dataset from an
Irish acute hospital, incorporating diagnostic codes, ward transitions, patient
demographics, infection-related variables and contact network features. Several
Transformer-based architectures were benchmarked alongside traditional machine
learning models. Clinical outcomes were predicted, and XAI techniques were
applied to interpret model decisions. Our framework successfully demonstrated
the utility of Transformer-based models, with TabTransformer consistently
outperforming baselines across multiple clinical prediction tasks, especially
for CPE acquisition (AUROC and sensitivity). We found infection-related
features, including historical hospital exposure, admission context, and
network centrality measures, to be highly influential in predicting patient
outcomes and CPE acquisition risk. Explainability analyses revealed that
features like "Area of Residence", "Admission Ward" and prior admissions are
key risk factors. Network variables like "Ward PageRank" also ranked highly,
reflecting the potential value of structural exposure information. This study
presents a robust and explainable AI framework for analyzing complex EMR data
to identify key risk factors and predict CPE-related outcomes. Our findings
underscore the superior performance of the Transformer models and highlight the
importance of diverse clinical and network features.

</details>


### [60] [Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems](https://arxiv.org/abs/2509.14956)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的多自愿系统安全框架，通过分布式监视和统一管理组合，有效防范多种安全威胁。


<details>
  <summary>Details</summary>
Motivation: 多自愿系统面临着指令注入、幽灵生成、隐私泄漏等多种安全挑战，需要动态适应的安全保护机制。

Method: 设计了双层安全架构：Sentinel Agents进行分布式监控和异常检测，Coordinator Agent负责政策管理和威胁处置。结合语义分析、行为分析等技术。

Result: 在162次模拟攻击测试中，Sentinel Agents成功检测到了指令注入、幽灵生成和数据泄漏攻击，证明了框架的可行性。

Conclusion: 该框架能够提供动态适应的安全防护，提高系统可观测性，支持监管遵循，为多自愿系统安全提供了有效解决方案。

Abstract: This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.

</details>


### [61] [Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles](https://arxiv.org/abs/2509.14963)
*Filip Naudot,Andreas Brännström,Vicenç Torra,Timotheus Kampik*

Main category: cs.AI

TL;DR: 本文提出了量化论证集合在双极论证图中对目标论证强度贡献的函数，是现有单论证贡献函数的泛化，并进行了基于原则的分析。


<details>
  <summary>Details</summary>
Motivation: 现有的论证贡献分析主要关注单个论证对目标论证的影响，缺乏对论证集合整体贡献的量化方法，特别是在双极论证框架中需要考虑论证间的相互作用。

Method: 提出了集合贡献函数来量化论证集对目标论证强度的贡献，泛化了现有的单论证贡献函数，建立了相应的原则框架进行分析，并引入了专注于集合内论证相互作用特性的新原则。

Result: 开发了一套完整的集合贡献函数理论框架，能够更好地捕捉论证集合的整体贡献特性，特别是在考虑论证间相互作用时的表现。

Conclusion: 集合贡献函数为量化论证集合的整体影响提供了有效工具，新的集合特定原则有助于更好地理解和分析论证系统中的集体贡献行为，在推荐系统等应用场景中具有实用价值。

Abstract: We present functions that quantify the contribution of a set of arguments in
quantitative bipolar argumentation graphs to (the final strength of) an
argument of interest, a so-called topic. Our set contribution functions are
generalizations of existing functions that quantify the contribution of a
single contributing argument to a topic. Accordingly, we generalize existing
contribution function principles for set contribution functions and provide a
corresponding principle-based analysis. We introduce new principles specific to
set-based functions that focus on properties pertaining to the interaction of
arguments within a set. Finally, we sketch how the principles play out across
different set contribution functions given a recommendation system application
scenario.

</details>


### [62] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Yanyuan Qiao,Imran Razzak,Yutong Xie*

Main category: cs.AI

TL;DR: KAMAC是一个知识驱动的自适应多智能体协作框架，通过动态组建专家团队来提升医疗决策的准确性和适应性，在复杂临床场景中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体协作框架采用静态预分配角色，限制了知识整合的适应性和动态性，无法有效处理复杂医疗场景中不断变化的诊断需求。

Method: 提出KAMAC框架，从初始专家智能体开始，通过知识驱动讨论识别知识缺口，动态招募额外专家，支持灵活可扩展的协作，最终通过审查更新后的智能体评论做出决策。

Result: 在两个真实医疗基准测试中，KAMAC显著优于单智能体和先进多智能体方法，特别是在需要动态跨专业知识的复杂临床场景（如癌症预后）中表现突出。

Conclusion: KAMAC框架通过动态组建专家团队有效解决了传统多智能体协作的局限性，为复杂医疗决策提供了更灵活、适应性更强的解决方案。

Abstract: Medical decision-making often involves integrating knowledge from multiple
clinical specialties, typically achieved through multidisciplinary teams.
Inspired by this collaborative process, recent work has leveraged large
language models (LLMs) in multi-agent collaboration frameworks to emulate
expert teamwork. While these approaches improve reasoning through agent
interaction, they are limited by static, pre-assigned roles, which hinder
adaptability and dynamic knowledge integration. To address these limitations,
we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration
framework that enables LLM agents to dynamically form and expand expert teams
based on the evolving diagnostic context. KAMAC begins with one or more expert
agents and then conducts a knowledge-driven discussion to identify and fill
knowledge gaps by recruiting additional specialists as needed. This supports
flexible, scalable collaboration in complex clinical scenarios, with decisions
finalized through reviewing updated agent comments. Experiments on two
real-world medical benchmarks demonstrate that KAMAC significantly outperforms
both single-agent and advanced multi-agent methods, particularly in complex
clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty
expertise. Our code is publicly available at:
https://github.com/XiaoXiao-Woo/KAMAC.

</details>


### [63] [Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews](https://arxiv.org/abs/2509.15035)
*Gabriela C. Zapata,Bill Cope,Mary Kalantzis,Duane Searsmith*

Main category: cs.AI

TL;DR: 研究探讨使用生成式AI为研究生在线课程的同伴互评提供元评价反馈，分析显示AI反馈能近似有效人类反馈的修辞和关系特征


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI如何支持形成性评估，特别是在同伴互评中提供机器生成的元评价，以探索AI反馈在意义构建方面的能力

Method: 基于系统功能语言学和评价理论，分析了120个AI生成的元评价，从概念、人际和文本三个维度探讨意义构建

Result: AI反馈能够平衡表扬与建设性批评，符合评分标准预期，具有结构化组织并突出学生主体性，能够提供清晰的指导性同时保持支持性立场

Conclusion: AI元反馈具有搭建反馈素养支架和增强学习者参与同伴互评的潜力，能够模拟有效人类反馈的关键特征

Abstract: This study investigates the use of generative AI to support formative
assessment through machine generated reviews of peer reviews in graduate online
courses in a public university in the United States. Drawing on Systemic
Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to
explore how generative AI feedback constructs meaning across ideational,
interpersonal, and textual dimensions. The findings suggest that generative AI
can approximate key rhetorical and relational features of effective human
feedback, offering directive clarity while also maintaining a supportive
stance. The reviews analyzed demonstrated a balance of praise and constructive
critique, alignment with rubric expectations, and structured staging that
foregrounded student agency. By modeling these qualities, AI metafeedback has
the potential to scaffold feedback literacy and enhance leaner engagement with
peer review.

</details>


### [64] [From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support](https://arxiv.org/abs/2509.15084)
*Doreen Jirak,Pieter Maes,Armeen Saroukanoff,Dirk van Rooy*

Main category: cs.AI

TL;DR: 本文强调可解释AI(XAI)在海事领域的重要性，提出针对海事专业人员的调查问卷，旨在开发以用户为中心的XAI系统。


<details>
  <summary>Details</summary>
Motivation: 随着自主技术在海事运营中的广泛应用，AI系统的决策透明度变得与决策本身同等重要。在复杂动态的海事环境中，对AI的信任不仅取决于性能，还需要透明度和可解释性。

Method: 提出针对海事领域的专门调查问卷，用于收集海事专业人员对信任、可用性和可解释性的看法，支持以用户为中心的XAI集成。

Result: 通过调查问卷收集海事专业人员的反馈，为开发适合海员和海事团队需求的用户中心型XAI系统提供指导。

Conclusion: 可解释AI是海事领域人机协作有效性的基础，需要开发专门针对海事环境的用户中心型XAI系统，以促进知情监督和共享理解。

Abstract: As autonomous technologies increasingly shape maritime operations,
understanding why an AI system makes a decision becomes as crucial as what it
decides. In complex and dynamic maritime environments, trust in AI depends not
only on performance but also on transparency and interpretability. This paper
highlights the importance of Explainable AI (XAI) as a foundation for effective
human-machine teaming in the maritime domain, where informed oversight and
shared understanding are essential. To support the user-centered integration of
XAI, we propose a domain-specific survey designed to capture maritime
professionals' perceptions of trust, usability, and explainability. Our aim is
to foster awareness and guide the development of user-centric XAI systems
tailored to the needs of seafarers and maritime teams.

</details>


### [65] [Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment](https://arxiv.org/abs/2509.15172)
*Ankur Samanta,Akshayaa Magesh,Youliang Yu,Runzhe Wu,Ayush Jain,Daniel Jiang,Boris Vidolov,Paul Sajda,Yonathan Efroni,Kaveh Hassani*

Main category: cs.AI

TL;DR: MACA是一个强化学习框架，通过多智能体辩论来提升语言模型的自洽性，使模型更倾向于选择与内部共识一致推理路径，显著提高了推理一致性和性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型存在推理不一致问题，对相同提示会生成矛盾回答。现有推理时方法无法解决根本问题——模型难以可靠选择导致一致结果的推理路径。

Method: 提出多智能体共识对齐(MACA)强化学习框架，通过多智能体辩论产生多数/少数结果，让模型后训练以偏好与内部共识一致的推理轨迹，基于同行论证而非简单投票聚合。

Result: 在自洽性(+27.6% GSM8K)、单智能体推理(+23.7% MATH)、采样推理(+22.4% Pass@20 MATH)和多智能体决策(+42.7% MathQA)等方面显著提升，在未见基准上也有强泛化能力。

Conclusion: MACA实现了稳健的自对齐，更可靠地释放语言模型的潜在推理能力，通过多智能体辩论产生更丰富的共识信号，使智能体学会更果断简洁并更好利用同行见解。

Abstract: Language Models (LMs) are inconsistent reasoners, often generating
contradictory responses to identical prompts. While inference-time methods can
mitigate these inconsistencies, they fail to address the core problem: LMs
struggle to reliably select reasoning pathways leading to consistent outcomes
under exploratory sampling. To address this, we formalize self-consistency as
an intrinsic property of well-aligned reasoning models and introduce
Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that
post-trains models to favor reasoning trajectories aligned with their internal
consensus using majority/minority outcomes from multi-agent debate. These
trajectories emerge from deliberative exchanges where agents ground reasoning
in peer arguments, not just aggregation of independent attempts, creating
richer consensus signals than single-round majority voting. MACA enables agents
to teach themselves to be more decisive and concise, and better leverage peer
insights in multi-agent settings without external supervision, driving
substantial improvements across self-consistency (+27.6% on GSM8K),
single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%
Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).
These findings, coupled with strong generalization to unseen benchmarks (+16.3%
on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more
reliably unlocks latent reasoning potential of language models.

</details>


### [66] [Generalizable Geometric Image Caption Synthesis](https://arxiv.org/abs/2509.15217)
*Yue Xin,Wenyuan Wang,Rui Pan,Ruida Wang,Howard Meng,Renjie Pi,Shizhe Diao,Tong Zhang*

Main category: cs.AI

TL;DR: 该论文提出RLVR方法，通过强化学习和可验证奖励来改进几何图像标注的数据生成流程，提升多模态大语言模型在几何问题解决和通用推理任务上的性能


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在解决复杂几何问题时表现不佳，主要原因是缺乏高质量的几何图像-文本对数据集，且传统模板化数据合成方法泛化能力有限

Method: 采用强化学习与可验证奖励(RLVR)方法，基于50个基本几何关系合成几何图像，利用数学问题求解任务产生的奖励信号来优化图像标注

Result: 在MathVista和MathVerse的非几何输入任务上获得2.8%-4.8%的准确率提升，在MMMU的艺术、设计、技术和工程任务上获得2.4%-3.9%的改进

Conclusion: RLVR方法能有效捕捉几何问题求解的关键特征，提升模型的任务泛化能力，即使在分布外场景下也能增强多模态大语言模型的通用推理能力

Abstract: Multimodal large language models have various practical applications that
demand strong reasoning abilities. Despite recent advancements, these models
still struggle to solve complex geometric problems. A key challenge stems from
the lack of high-quality image-text pair datasets for understanding geometric
images. Furthermore, most template-based data synthesis pipelines typically
fail to generalize to questions beyond their predefined templates. In this
paper, we bridge this gap by introducing a complementary process of
Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation
pipeline. By adopting RLVR to refine captions for geometric images synthesized
from 50 basic geometric relations and using reward signals derived from
mathematical problem-solving tasks, our pipeline successfully captures the key
features of geometry problem-solving. This enables better task generalization
and yields non-trivial improvements. Furthermore, even in out-of-distribution
scenarios, the generated dataset enhances the general reasoning capabilities of
multimodal large language models, yielding accuracy improvements of
$2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks
with non-geometric input images of MathVista and MathVerse, along with
$2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks
in MMMU.

</details>
