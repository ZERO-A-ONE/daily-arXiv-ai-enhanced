<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 17]
- [cs.SE](#cs.SE) [Total: 18]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Statistical Side-Channel Risk Model for Timing Variability in Lattice-Based Post-Quantum Cryptography](https://arxiv.org/abs/2512.22301)
*Aayush Mainali,Sirjan Ghimire*

Main category: cs.CR

TL;DR: 该研究提出了一种基于场景的统计风险模型，用于评估后量子密码学中基于格方案的时序侧信道泄漏风险，通过多种统计指标量化不同执行条件下的泄漏程度。


<details>
  <summary>Details</summary>
Motivation: 后量子密码学中基于格的方案（如Kyber、Saber、Frodo）由于复杂的算术和控制流可能导致依赖于秘密的时序变化，而实际时序测量又受到环境噪声影响，需要系统评估时序泄漏风险。

Method: 提出基于场景的统计风险模型，在空闲、抖动和负载三种执行条件下合成两个秘密类别的时序轨迹，使用Welch's t检验、KS距离、Cliff's delta、互信息和分布重叠等多种统计指标量化泄漏，并以类似TLRI的方式组合得到一致的评分来排序风险场景。

Result: 空闲条件下通常具有最佳的可区分性，抖动和负载条件通过增加方差和重叠度削弱可区分性；缓存索引和分支风格的泄漏往往产生最高的风险信号；在相似泄漏假设下，更快的方案可能具有更高的峰值风险。

Conclusion: 该统计风险模型能够在早期设计阶段进行可重复的比较，为基于格的密钥封装机制提供时序侧信道风险评估框架，支持平台特定验证前的风险分析。

Abstract: Timing side-channels are an important threat to cryptography that still needs to be addressed in implementations, and the advent of post-quantum cryptography raises this issue because the lattice-based schemes may produce secret-dependent timing variability with the help of complex arithmetic and control flow. Since also real timing measurements are affected by environmental noise (e.g. scheduling effects, contention, heavy tailed delays), in this work a scenario-based statistical risk model is proposed for timing leakage as a problem of distributional distinguishability under controlled execution conditions. We synthesize traces for two secret classes in idle, jitter and loaded scenarios and for multiple leakage models and quantify leakage with Welch's t-test, KS distance, Cliff's delta, mutual information, and distribution overlap to combine in a TLRI like manner to obtain a consistent score for ranking scenarios. Across representative lattice-based KEM families (Kyber, Saber, Frodo), idle conditions generally have the best distinguishability, jitter and loaded conditions erode distinguishability through an increase in variance and increase in overlap; cache-index and branch-style leakage tends to give the highest risk signals, and faster schemes can have a higher peak risk given similar leakage assumptions, allowing reproducible comparisons at an early design stage, prior to platform-specific validation.

</details>


### [2] [Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection](https://arxiv.org/abs/2512.22306)
*Chinmay Pushkar,Sanchit Kabra,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CR

TL;DR: 该研究提出了一个多漏洞检测基准，评估LLM在长代码文件中检测多个交互漏洞的能力，发现随着漏洞密度增加，模型性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注孤立单漏洞样本或函数级分类，无法反映真实软件中多个交互漏洞共存的复杂性。LLM在多标签任务中存在"计数偏差"和"选择偏差"，但在代码安全领域尚未得到严格量化。

Method: 构建了包含C、C++、Python和JavaScript四种语言的40,000个文件的数据集，通过系统注入控制数量的漏洞（1、3、5、9个）到长上下文代码样本（7.5k-10k tokens）中。评估了包括GPT-4o-mini、Llama-3.3-70B和Qwen-2.5系列在内的五个最先进LLM。

Result: 结果显示随着漏洞密度增加，性能急剧下降。Llama-3.3-70B在单漏洞C任务中达到接近完美的F1分数（约0.97），但在高密度设置下性能下降高达40%。Python和JavaScript表现出与C/C++不同的失败模式，模型在复杂Python文件中表现出严重的"计数不足"（召回率降至0.30以下）。

Conclusion: 该研究强调了现有LLM在多漏洞检测方面的局限性，特别是在高密度漏洞设置下。不同编程语言表现出不同的失败模式，需要针对性的改进方法。这为未来LLM在软件安全领域的应用提供了重要基准和改进方向。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in automated software security, particularly in vulnerability detection. However, existing benchmarks primarily focus on isolated, single-vulnerability samples or function-level classification, failing to reflect the complexity of real-world software where multiple interacting vulnerabilities often coexist within large files. Recent studies indicate that LLMs suffer from "count bias" and "selection bias" in multi-label tasks, yet this has not been rigorously quantified in the domain of code security. In this work, we introduce a comprehensive benchmark for Multi-Vulnerability Detection across four major languages: C, C++, Python, and JavaScript. We construct a dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, and 9) into long-context code samples (7.5k-10k tokens) sourced from CodeParrot. We evaluate five state-of-the-art LLMs, including GPT-4o-mini, Llama-3.3-70B, and the Qwen-2.5 series. Our results reveal a sharp degradation in performance as vulnerability density increases. While Llama-3.3-70B achieves near-perfect F1 scores (approximately 0.97) on single-vulnerability C tasks, performance drops by up to 40% in high-density settings. Notably, Python and JavaScript show distinct failure modes compared to C/C++, with models exhibiting severe "under-counting" (Recall dropping to less than 0.30) in complex Python files.

</details>


### [3] [LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators](https://arxiv.org/abs/2512.22307)
*You Li,Guannan Zhao,Yuhao Ju,Yunqi He,Jie Gu,Hai Zhou*

Main category: cs.CR

TL;DR: LLA是一种结合硬件和软件协同的生成式AI模型知识产权保护方案，通过神经元嵌入密钥位触发异常来防御模型窃取、篡改和信息泄露等供应链威胁。


<details>
  <summary>Details</summary>
Motivation: 保护生成式AI模型的知识产权，防御供应链中的各种威胁，包括模型窃取、模型篡改和信息泄露。

Method: 软件层面：将密钥位嵌入神经元中，触发异常降低性能，并应用不变性变换来隐藏密钥值。硬件层面：在AI加速器中集成轻量级锁定模块，保持与各种数据流模式和工具链的兼容性。

Result: LLA能够抵御广泛的oracle引导的密钥优化攻击，同时计算开销极小（7168个密钥位下小于0.1%）。

Conclusion: LLA提供了一种有效的软硬件协同IP保护方案，能够抵御多种供应链威胁，同时保持低开销和高兼容性。

Abstract: We introduce LLA, an effective intellectual property (IP) protection scheme for generative AI models. LLA leverages the synergy between hardware and software to defend against various supply chain threats, including model theft, model corruption, and information leakage. On the software side, it embeds key bits into neurons that can trigger outliers to degrade performance and applies invariance transformations to obscure the key values. On the hardware side, it integrates a lightweight locking module into the AI accelerator while maintaining compatibility with various dataflow patterns and toolchains. An accelerator with a pre-stored secret key acts as a license to access the model services provided by the IP owner. The evaluation results show that LLA can withstand a broad range of oracle-guided key optimization attacks, while incurring a minimal computational overhead of less than 0.1% for 7,168 key bits.

</details>


### [4] [Verifiable Dropout: Turning Randomness into a Verifiable Claim](https://arxiv.org/abs/2512.22526)
*Kichang Lee,Sungmin Lee,Jaeho Jin,JeongGil Ko*

Main category: cs.CR

TL;DR: 提出Verifiable Dropout方法，使用零知识证明确保深度学习训练中随机操作（如dropout）的可验证性，防止攻击者利用随机性掩盖恶意操作


<details>
  <summary>Details</summary>
Motivation: 现有云AI训练的审计机制无法有效处理深度学习的非确定性特征，攻击者可以利用随机操作（如dropout）的模糊性掩盖恶意操作，获得合理否认性

Method: 引入Verifiable Dropout，基于零知识证明的隐私保护机制，将dropout掩码绑定到确定性、可密码验证的种子，并证明dropout操作的正确执行

Result: 用户可以在事后审计随机训练步骤的完整性，确保随机性既没有被偏置也没有被选择性使用，同时严格保护模型和数据的机密性

Conclusion: 通过将随机性视为可验证声明而非借口，填补了现有日志机制在深度学习训练完整性验证方面的空白，实现了隐私保护下的训练过程可审计性

Abstract: Modern cloud-based AI training relies on extensive telemetry and logs to ensure accountability. While these audit trails enable retrospective inspection, they struggle to address the inherent non-determinism of deep learning. Stochastic operations, such as dropout, create an ambiguity surface where attackers can mask malicious manipulations as natural random variance, granting them plausible deniability. Consequently, existing logging mechanisms cannot verify whether stochastic values were generated and applied honestly without exposing sensitive training data. To close this integrity gap, we introduce Verifiable Dropout, a privacy-preserving mechanism based on zero-knowledge proofs. We treat stochasticity not as an excuse but as a verifiable claim. Our approach binds dropout masks to a deterministic, cryptographically verifiable seed and proves the correct execution of the dropout operation. This design enables users to audit the integrity of stochastic training steps post-hoc, ensuring that randomness was neither biased nor cherry-picked, while strictly preserving the confidentiality of the model and data.

</details>


### [5] [Raven: Mining Defensive Patterns in Ethereum via Semantic Transaction Revert Invariants Categories](https://arxiv.org/abs/2512.22616)
*Mojtaba Eshghie,Melissa Mazura,Alexandre Bartel*

Main category: cs.CR

TL;DR: Raven框架通过分析以太坊交易回滚来挖掘智能合约防御性不变式，发现了6个新的不变式类别，可用于安全分析和漏洞检测


<details>
  <summary>Details</summary>
Motivation: 以太坊中被require/assert/revert语句回滚的交易包含了有价值的防御模式，但这些模式在安全研究中尚未被充分发现和利用

Method: 提出Raven框架：1) 将回滚交易与源代码中的不变式对齐；2) 使用BERT微调模型嵌入不变式；3) 按语义意图聚类以挖掘防御性不变式类别

Result: 在20,000个回滚交易样本上评估，Raven实现了有凝聚力的不变式聚类。专家审查发现了6个新的不变式类别：功能开关、重放防护、证明/签名验证、计数器、调用者提供的滑点阈值、允许/禁止/机器人列表

Conclusion: Raven能够映射以太坊成功的防御机制，挖掘的不变式类别使安全研究人员能够基于从智能合约实际防御中提取的数据驱动安全预言机开发分析工具

Abstract: We frame Ethereum transactions reverted by invariants-require(<invariant>)/ assert(<invariant>)/if (<invariant>) revert statements in the contract implementation-as a positive signal of active on-chain defenses. Despite their value, the defensive patterns in these transactions remain undiscovered and underutilized in security research. We present Raven, a framework that aligns reverted transactions to the invariant causing the reversion in the smart contract source code, embeds these invariants using our BERT-based fine-tuned model, and clusters them by semantic intent to mine defensive invariant categories on Ethereum. Evaluated on a sample of 20,000 reverted transactions, Raven achieves cohesive and meaningful clusters of transaction-reverting invariants. Manual expert review of the mined 19 semantic clusters uncovers six new invariant categories absent from existing invariant catalogs, including feature toggles, replay prevention, proof/signature verification, counters, caller-provided slippage thresholds, and allow/ban/bot lists. To demonstrate the practical utility of this invariant catalog mining pipeline, we conduct a case study using one of the newly discovered invariant categories as a fuzzing oracle to detect vulnerabilities in a real-world attack. Raven thus can map Ethereum's successful defenses. These invariant categories enable security researchers to develop analysis tools based on data-driven security oracles extracted from the smart contracts' working defenses.

</details>


### [6] [When RSA Fails: Exploiting Prime Selection Vulnerabilities in Public Key Cryptography](https://arxiv.org/abs/2512.22720)
*Murtaza Nikzad,Kerem Atas*

Main category: cs.CR

TL;DR: 论文分析了RSA密码系统中因素数选择不当导致的漏洞，主要关注费马分解法和GCD攻击两种攻击向量，指出弱随机数生成是主要原因，并讨论了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 研究RSA密码系统中因素数选择不当而产生的安全漏洞，特别是针对实际部署系统中仍然普遍存在的费马分解和GCD攻击漏洞，揭示这些漏洞的根本原因。

Method: 通过分析两种主要攻击向量：费马分解法（针对素数过于接近的情况）和GCD攻击（针对共享素数因子的情况），结合Heninger等人的"Ps and Qs"研究和Böck的2023年费马分解分析，研究实际密码实现中的漏洞。

Result: 分析表明这些漏洞在实际密码实现中仍然普遍存在，嵌入式设备中的弱随机数生成是导致这些失败的主要原因，发现了超过64,000个易受攻击的TLS主机。

Conclusion: 需要采取适当的熵收集和素数验证检查等缓解策略来防止RSA密钥生成中的素数选择不当问题，确保密码系统的安全性。

Abstract: This paper explores vulnerabilities in RSA cryptosystems that arise from improper prime number selection during key generation. We examine two primary attack vectors: Fermat's factorization method, which exploits RSA keys generated with primes that are too close together, and the Greatest Common Divisor (GCD) attack, which exploits keys that share a common prime factor. Drawing from landmark research including Heninger et al.'s ``Mining Your Ps and Qs'' study, which discovered over 64,000 vulnerable TLS hosts, and B{ö}ck's 2023 analysis of Fermat factorization in deployed systems, we demonstrate that these vulnerabilities remain prevalent in real-world cryptographic implementations. Our analysis reveals that weak random number generation in embedded devices is the primary cause of these failures, and we discuss mitigation strategies including proper entropy collection and prime validation checks.

</details>


### [7] [Breaking the illusion: Automated Reasoning of GDPR Consent Violations](https://arxiv.org/abs/2512.22789)
*Ying Li,Wenjun Qiu,Faysal Hossain Shezan,Kunlin Cai,Michelangelo van Dam,Lisa Austin,David Lie,Yuan Tian*

Main category: cs.CR

TL;DR: Cosmic是一个自动化框架，用于检测网页表单中的同意相关隐私违规，在5823个网站和3598个表单中发现了大量GDPR合规问题。


<details>
  <summary>Details</summary>
Motivation: GDPR和CCPA等隐私法规要求获得用户同意，但现实中仍存在许多违规行为，表明法律期望与实际实施之间存在差距。当前研究主要关注cookie横幅和移动应用对话框，而网页表单中的同意机制多样且难以评估，给自动化合规审计带来挑战。

Method: 开发了Cosmic自动化框架，用于检测网页表单中的同意相关隐私违规。该工具能够审计网页表单的同意合规性，覆盖5823个网站和3598个表单。

Result: Cosmic在94.1%的同意表单上检测到3384个违规，涵盖GDPR关键原则如自由给予同意、目的披露和撤回选项。同意检测和违规检测的TPR分别达到98.6%和99.1%，显示出高准确性和实际应用价值。

Conclusion: Cosmic框架能够有效自动化检测网页表单中的同意合规问题，填补了当前研究空白，为隐私法规的实际执行提供了实用工具。

Abstract: Recent privacy regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have established legal requirements for obtaining user consent regarding the collection, use, and sharing of personal data. These regulations emphasize that consent must be informed, freely given, specific, and unambiguous. However, there are still many violations, which highlight a gap between legal expectations and actual implementation. Consent mechanisms embedded in functional web forms across websites play a critical role in ensuring compliance with data protection regulations such as the GDPR and CCPA, as well as in upholding user autonomy and trust. However, current research has primarily focused on cookie banners and mobile app dialogs. These forms are diverse in structure, vary in legal basis, and are often difficult to locate or evaluate, creating a significant challenge for automated consent compliance auditing. In this work, we present Cosmic, a novel automated framework for detecting consent-related privacy violations in web forms. We evaluate our developed tool for auditing consent compliance in web forms, across 5,823 websites and 3,598 forms. Cosmic detects 3,384 violations on 94.1% of consent forms, covering key GDPR principles such as freely given consent, purpose disclosure, and withdrawal options. It achieves 98.6% and 99.1% TPR for consent and violation detection, respectively, demonstrating high accuracy and real-world applicability.

</details>


### [8] [SecureBank: A Financially-Aware Zero Trust Architecture for High-Assurance Banking Systems](https://arxiv.org/abs/2512.23124)
*Paulo Fernandes Biao*

Main category: cs.CR

TL;DR: SecureBank：面向高保障银行系统的财务感知、上下文自适应零信任架构，通过集成金融零信任、自适应身份评分、上下文微分段和影响驱动安全自动化，显著提升自动化攻击处理和身份信任适应能力。


<details>
  <summary>Details</summary>
Motivation: 金融机构日益依赖分布式架构、开放银行API、云原生基础设施和高频数字交易，这些转变扩大了攻击面并暴露了传统基于边界的安全模型的局限性。现有零信任框架大多未明确纳入交易语义、金融风险建模、自适应身份信任或基于经济影响的自动化。

Method: 提出SecureBank框架，集成金融零信任、自适应身份评分、上下文微分段和影响驱动安全自动化。使用蒙特卡洛模拟评估SecureBank与基于规则的基线架构，评估指标包括交易完整性指数(TII)、身份信任适应水平(ITAL)和安全自动化效率(SAE)。

Result: SecureBank显著改善了自动化攻击处理，加速了身份信任适应，同时保持了保守且符合监管要求的交易完整性水平。该框架在受监管金融环境中作为财务感知零信任系统的参考架构和评估基准。

Conclusion: SecureBank为高保障银行系统提供了专门设计的财务感知、上下文自适应零信任架构，解决了传统安全模型在金融数字化转型中的局限性，为受监管金融环境中的零信任系统提供了实用参考框架。

Abstract: Financial institutions increasingly rely on distributed architectures, open banking APIs, cloud native infrastructures, and high frequency digital transactions. These transformations expand the attack surface and expose limitations in traditional perimeter based security models. While Zero Trust architectures provide essential security principles, most existing frameworks do not explicitly incorporate transactional semantics, financial risk modeling, adaptive identity trust, or automation weighted by economic impact.
  This paper introduces SecureBank, a financially aware and context adaptive Zero Trust architecture designed specifically for high assurance banking systems. The proposed framework integrates Financial Zero Trust, Adaptive Identity Scoring, Contextual Micro Segmentation, and Impact Driven Security Automation. A Monte Carlo simulation evaluates SecureBank against a representative rule based baseline architecture using metrics such as the Transactional Integrity Index (TII), Identity Trust Adaptation Level (ITAL), and Security Automation Efficiency (SAE).
  The results demonstrate that SecureBank significantly improves automated attack handling and accelerates identity trust adaptation while preserving conservative and regulator aligned levels of transactional integrity. Beyond experimental validation, SecureBank is intended to serve as a reference architecture and evaluation baseline for financially aware Zero Trust systems in regulated financial environments.

</details>


### [9] [Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning](https://arxiv.org/abs/2512.23171)
*Yu Jiang,Xindi Tong,Ziyao Liu,Xiaoxi Zhang,Kwok-Yan Lam,Chee Wei Tan*

Main category: cs.CR

TL;DR: FedORA是一个用于纵向联邦学习的遗忘框架，通过原始-对偶算法解决样本和标签遗忘问题，在保持模型效用的同时降低计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 纵向联邦学习中的遗忘问题具有挑战性，因为特征分布在各方，遗忘任务需要跨方协调，存在计算开销和特征依赖复杂性。现有研究主要集中在横向联邦学习，纵向联邦学习的遗忘方法需要专门设计。

Method: FedORA将样本或标签遗忘问题建模为约束优化问题，采用原始-对偶框架求解。引入新的遗忘损失函数促进分类不确定性而非误分类，使用自适应步长增强稳定性，采用非对称批次设计分别处理遗忘数据和保留数据以降低计算成本。

Result: 理论分析证明FedORA与从头训练模型的差异有界，为遗忘效果提供保证。在表格和图像数据集上的实验表明，FedORA在实现与从头训练相当的遗忘效果和效用保持的同时，显著减少了计算和通信开销。

Conclusion: FedORA为纵向联邦学习提供了一个有效的遗忘框架，通过优化方法解决了样本和标签遗忘问题，在隐私保护、模型效用和计算效率之间取得了良好平衡。

Abstract: Federated unlearning has become an attractive approach to address privacy concerns in collaborative machine learning, for situations when sensitive data is remembered by AI models during the machine learning process. It enables the removal of specific data influences from trained models, aligning with the growing emphasis on the "right to be forgotten." While extensively studied in horizontal federated learning, unlearning in vertical federated learning (VFL) remains challenging due to the distributed feature architecture. VFL unlearning includes sample unlearning that removes specific data points' influence and label unlearning that removes entire classes. Since different parties hold complementary features of the same samples, unlearning tasks require cross-party coordination, creating computational overhead and complexities from feature interdependencies. To address such challenges, we propose FedORA (Federated Optimization for data Removal via primal-dual Algorithm), designed for sample and label unlearning in VFL. FedORA formulates the removal of certain samples or labels as a constrained optimization problem solved using a primal-dual framework. Our approach introduces a new unlearning loss function that promotes classification uncertainty rather than misclassification. An adaptive step size enhances stability, while an asymmetric batch design, considering the prior influence of the remaining data on the model, handles unlearning and retained data differently to efficiently reduce computational costs. We provide theoretical analysis proving that the model difference between FedORA and Train-from-scratch is bounded, establishing guarantees for unlearning effectiveness. Experiments on tabular and image datasets demonstrate that FedORA achieves unlearning effectiveness and utility preservation comparable to Train-from-scratch with reduced computation and communication overhead.

</details>


### [10] [EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion](https://arxiv.org/abs/2512.23173)
*Zhen Liang,Hai Huang,Zhengkui Chen*

Main category: cs.CR

TL;DR: Equacode是一种通过方程求解和代码完成的新型多策略越狱方法，将恶意意图转化为数学问题，然后要求LLM用代码解决，利用跨领域任务的复杂性来转移模型对安全约束的注意力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型取得了显著成功，但其可信度仍然是一个重要问题，因为它们仍然容易受到旨在引发不当或有害响应的越狱攻击。现有的越狱攻击主要在自然语言层面操作，并依赖单一攻击策略，限制了全面评估LLM鲁棒性的有效性。

Method: 提出Equacode方法：将恶意意图转化为数学问题，然后要求LLM使用代码解决该问题。这种方法利用跨领域任务的复杂性，将模型的注意力从安全约束转移到任务完成上。

Result: 实验结果显示，Equacode在GPT系列上平均成功率达到91.19%，在3个最先进的LLM上达到98.65%，且仅需单次查询。消融实验表明，Equacode优于单独的数学方程模块或代码模块，显示出强烈的协同效应。

Conclusion: 多策略方法（方程求解+代码完成）能够产生大于各部分之和的效果，为全面评估LLM的鲁棒性提供了有效的新方法。

Abstract: Large language models (LLMs), such as ChatGPT, have achieved remarkable success across a wide range of fields. However, their trustworthiness remains a significant concern, as they are still susceptible to jailbreak attacks aimed at eliciting inappropriate or harmful responses. However, existing jailbreak attacks mainly operate at the natural language level and rely on a single attack strategy, limiting their effectiveness in comprehensively assessing LLM robustness. In this paper, we propose Equacode, a novel multi-strategy jailbreak approach for large language models via equation-solving and code completion. This approach transforms malicious intent into a mathematical problem and then requires the LLM to solve it using code, leveraging the complexity of cross-domain tasks to divert the model's focus toward task completion rather than safety constraints. Experimental results show that Equacode achieves an average success rate of 91.19% on the GPT series and 98.65% across 3 state-of-the-art LLMs, all with only a single query. Further, ablation experiments demonstrate that EquaCode outperforms either the mathematical equation module or the code module alone. This suggests a strong synergistic effect, thereby demonstrating that multi-strategy approach yields results greater than the sum of its parts.

</details>


### [11] [Multiparty Authorization for Secure Data Storage in Cloud Environments using Improved Attribute-Based Encryption](https://arxiv.org/abs/2512.23216)
*Partha Paul,Keshav Sinha*

Main category: cs.CR

TL;DR: 该论文提出了一种基于属性加密(ABE)的改进方案，结合功能流密码(FBSE)和抛物线曲线上的标量点，用于云环境下的安全数据存储和多用户授权，通过Shamir秘密共享和2D-Lagrange插值实现阈值授权机制。


<details>
  <summary>Details</summary>
Motivation: 当前云环境中存储敏感数据面临两大问题：大量数据请求增加服务器计算开销，以及数据存储过程中的泄露风险。需要一种既能提供安全存储又能实现访问控制的技术方案。

Method: 提出改进的ABE方案，结合功能流密码(FBSE)，在抛物线曲线上使用简单标量点实现多方授权。采用Shamir秘密共享技术生成授权点，使用2D-Lagrange插值从正则抛物线重建秘密点，设置阈值(Ts>3)要求合法授权用户重建属性关联密钥进行解密。

Result: 方案通过NIST统计测试套件、相关系数和直方图等统计分析方法评估加密效果，显示能抵抗碰撞攻击。性能分析表明，随着授权策略属性数量增加，加密时间相应增加，但存储开销最小化且与用户身份无关。

Conclusion: 提出的方案在云环境中实现了安全数据存储和授权访问，安全性和性能分析结果表明该方案更加鲁棒和安全，能够有效应对数据泄露和计算开销问题。

Abstract: In todays scenario, various organizations store their sensitive data in the cloud environment. Multiple problems are present while retrieving and storing vast amounts of data, such as the frequency of data requests (increasing the computational overhead of the server) and data leakage while storing. To cope with said problem, Attribute-Based Encryption (ABE) is one of the potential security and access control techniques for secure data storage and authorization. The proposed work divides into two objectives: (i) provide access to authorized users and (ii) secure data storage in a cloud environment. The improved ABE using Functional Based Stream Cipher (FBSE) is proposed for data storage. The proposed technique uses simple scalar points over a parabolic curve to provide multiparty authorization. The authorization points are generated and share only with the authorized recipients. The Shamir secret sharing technique generate the authorization points and 2D-Lagrange Interpolation is used to reconstruct the secret points from regular parabola. The proposed scheme has specified the threshold (Ts>3) legally authorized users to reconstruct the attribute-associated keys for decryption. The encryption of data is evaluated using Statistical analysis (NIST Statistical Test Suite, Correlation Coefficient, and Histogram) test to investigate image pixel deviation. The parameters like encryption and decryption are used for performance analysis, where an increase in the number of attributes for the authorization policy will increase the encryption time. The proposed scheme imposes minimal storage overhead, irrespective of the users identity. The security analysis evidence that it resists collision attacks. The security and performance analysis results demonstrate that the proposed scheme is more robust and secure.

</details>


### [12] [RobustMask: Certified Robustness against Adversarial Neural Ranking Attack via Randomized Masking](https://arxiv.org/abs/2512.23307)
*Jiawei Liu,Zhuo Chen,Rui Zhu,Miaokun Chen,Yuyang Gong,Wei Lu,Xiaofeng Wang*

Main category: cs.CR

TL;DR: RobustMask是一种结合预训练语言模型上下文预测能力和随机掩码平滑机制的新型防御方法，可增强神经排序模型对抗字符、单词和短语级对抗攻击的鲁棒性，并提供理论上的认证top-K鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 神经排序模型在现实应用（如RAG）中取得了显著进展，但与其他神经架构一样，它们容易受到对抗性攻击的威胁。微小的字符、单词或短语级扰动可能毒化检索结果，人为地提升特定候选文档，从而破坏搜索引擎和下游系统的完整性。现有防御方法要么依赖泛化能力差的启发式方法，要么基于对攻击者知识假设过强的认证方法，限制了实际应用。

Method: 提出RobustMask防御方法，结合预训练语言模型的上下文预测能力和基于随机掩码的平滑机制。该方法利用排序模型的成对比较能力和概率统计分析，通过随机掩码文档内容并使用语言模型预测被掩码部分，从而增强模型对对抗性扰动的鲁棒性。

Result: 实验表明，RobustMask能够成功认证超过20%的候选文档在top-10排名位置中对抗影响高达30%文档内容的对抗性扰动。该方法提供了理论上的认证top-K鲁棒性证明，显著提升了神经排序模型的对抗鲁棒性。

Conclusion: RobustMark为增强神经排序模型的对抗鲁棒性提供了一种有效方法，通过结合语言模型的上下文预测能力和随机掩码平滑机制，为现实世界检索系统提供了更强的安全保障，是向实际应用安全保证迈出的重要一步。

Abstract: Neural ranking models have achieved remarkable progress and are now widely deployed in real-world applications such as Retrieval-Augmented Generation (RAG). However, like other neural architectures, they remain vulnerable to adversarial manipulations: subtle character-, word-, or phrase-level perturbations can poison retrieval results and artificially promote targeted candidates, undermining the integrity of search engines and downstream systems. Existing defenses either rely on heuristics with poor generalization or on certified methods that assume overly strong adversarial knowledge, limiting their practical use. To address these challenges, we propose RobustMask, a novel defense that combines the context-prediction capability of pretrained language models with a randomized masking-based smoothing mechanism. Our approach strengthens neural ranking models against adversarial perturbations at the character, word, and phrase levels. Leveraging both the pairwise comparison ability of ranking models and probabilistic statistical analysis, we provide a theoretical proof of RobustMask's certified top-K robustness. Extensive experiments further demonstrate that RobustMask successfully certifies over 20% of candidate documents within the top-10 ranking positions against adversarial perturbations affecting up to 30% of their content. These results highlight the effectiveness of RobustMask in enhancing the adversarial robustness of neural ranking models, marking a significant step toward providing stronger security guarantees for real-world retrieval systems.

</details>


### [13] [Fuzzilicon: A Post-Silicon Microcode-Guided x86 CPU Fuzzer](https://arxiv.org/abs/2512.23438)
*Johannes Lenzen,Mohamadreza Rostami,Lichao Wu,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: Fuzzilicon是首个针对真实x86 CPU的后硅片模糊测试框架，通过逆向工程Intel微码更新接口实现微架构层深度检测，自动发现微码级漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代CPU是黑盒且存在复杂的微架构缺陷，传统分析方法难以检测。现有漏洞发现主要依赖繁琐的手动逆向工程，缺乏自动化、系统化的后硅片处理器漏洞检测框架。

Method: 通过逆向工程Intel专有微码更新接口，开发微码级检测方法；结合基于hypervisor的模糊测试框架，实现精确的反馈引导输入生成；无需访问RTL层即可获取处理器微架构的直接反馈。

Result: 在Intel Goldmont微架构上发现5个重要漏洞，包括2个先前未知的微码级推测执行漏洞；自动重新发现μSpectre类漏洞；覆盖率收集开销降低31倍；实现16.27%可钩挂位置的唯一微码覆盖率。

Conclusion: Fuzzilicon建立了后硅片模糊测试的新基础，为自动化发现复杂CPU漏洞提供了实用、覆盖率引导且可扩展的方法。

Abstract: Modern CPUs are black boxes, proprietary, and increasingly characterized by sophisticated microarchitectural flaws that evade traditional analysis. While some of these critical vulnerabilities have been uncovered through cumbersome manual effort, building an automated and systematic vulnerability detection framework for real-world post-silicon processors remains a challenge.
  In this paper, we present Fuzzilicon, the first post-silicon fuzzing framework for real-world x86 CPUs that brings deep introspection into the microcode and microarchitectural layers. Fuzzilicon automates the discovery of vulnerabilities that were previously only detectable through extensive manual reverse engineering, and bridges the visibility gap by introducing microcode-level instrumentation. At the core of Fuzzilicon is a novel technique for extracting feedback directly from the processor's microarchitecture, enabled by reverse-engineering Intel's proprietary microcode update interface. We develop a minimally intrusive instrumentation method and integrate it with a hypervisor-based fuzzing harness to enable precise, feedback-guided input generation, without access to Register Transfer Level (RTL).
  Applied to Intel's Goldmont microarchitecture, Fuzzilicon introduces 5 significant findings, including two previously unknown microcode-level speculative-execution vulnerabilities. Besides, the Fuzzilicon framework automatically rediscover the $μ$Spectre class of vulnerabilities, which were detected manually in the previous work. Fuzzilicon reduces coverage collection overhead by up to 31$\times$ compared to baseline techniques and achieves 16.27% unique microcode coverage of hookable locations, the first empirical baseline of its kind. As a practical, coverage-guided, and scalable approach to post-silicon fuzzing, Fuzzilicon establishes a new foundation to automate the discovery of complex CPU vulnerabilities.

</details>


### [14] [Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation](https://arxiv.org/abs/2512.23480)
*Toqeer Ali Syed,Mohammad Riyaz Belgaum,Salman Jan,Asadullah Abdullah Khan,Saad Said Alqahtani*

Main category: cs.CR

TL;DR: 论文提出了一种基于自主软件供应链安全的智能AI框架，结合LLM推理、强化学习和多智能体协调，主动识别和消除软件生产中的漏洞，而非仅提供溯源能力。


<details>
  <summary>Details</summary>
Motivation: 传统软件供应链攻击日益针对可信开发和交付流程，现有框架如SLSA、SBOM和in-toto主要提供溯源能力，但缺乏主动识别和消除漏洞的能力。需要从被动验证转向主动防御的软件供应链安全方案。

Method: 提出基于自主软件供应链安全的智能AI框架，结合LLM语义漏洞分析和可解释决策、强化学习自适应缓解策略、多智能体协调（使用LangChain和LangGraph）。系统通过Model Context Protocol与真实CI/CD环境交互，并将所有观察和行动记录在区块链安全账本中确保完整性和可审计性。

Result: 在模拟管道和真实世界CI/CD集成（GitHub Actions和Jenkins）上测试，包括注入攻击、不安全反序列化、访问控制违规和配置错误等场景。实验结果显示相比基于规则、仅溯源和仅强化学习的基线方法，具有更好的检测准确率、更短的缓解延迟和合理的构建时间开销。

Conclusion: 智能AI可以促进软件供应链从被动验证向自主防御、主动安全转变，实现自我防御、主动的软件供应链，而不仅仅是反应式验证。

Abstract: The software supply chain attacks are becoming more and more focused on trusted development and delivery procedures, so the conventional post-build integrity mechanisms cannot be used anymore. The available frameworks like SLSA, SBOM and in toto are majorly used to offer provenance and traceability but do not have the capabilities of actively identifying and removing vulnerabilities in software production. The current paper includes an example of agentic artificial intelligence (AI) based on autonomous software supply chain security that combines large language model (LLM)-based reasoning, reinforcement learning (RL), and multi-agent coordination. The suggested system utilizes specialized security agents coordinated with the help of LangChain and LangGraph, communicates with actual CI/CD environments with the Model Context Protocol (MCP), and documents all the observations and actions in a blockchain security ledger to ensure integrity and auditing. Reinforcement learning can be used to achieve adaptive mitigation strategies that consider the balance between security effectiveness and the operational overhead, and LLMs can be used to achieve semantic vulnerability analysis, as well as explainable decisions. This framework is tested based on simulated pipelines, as well as, actual world CI/CD integrations on GitHub Actions and Jenkins, including injection attacks, insecure deserialization, access control violations, and configuration errors. Experimental outcomes indicate better detection accuracy, shorter mitigation latency and reasonable build-time overhead than rule-based, provenance only and RL only baselines. These results show that agentic AI can facilitate the transition to self defending, proactive software supply chains rather than reactive verification ones.

</details>


### [15] [A Privacy Protocol Using Ephemeral Intermediaries and a Rank-Deficient Matrix Power Function (RDMPF)](https://arxiv.org/abs/2512.23535)
*Eduardo Salazar*

Main category: cs.CR

TL;DR: 提出了一种用于Internet Computer（ICP）的私有传输架构，通过两个短寿命中介实现存款和检索的解耦，使用密封存储和短暂见证的认证拆除。协议采用非交互式RDMPF封装生成每笔传输的传输密钥，通过胶囊计算公共通知提示实现无指纹化发现。检索通过不泄露身份的简短解封装证明授权，所有交易中介都是短暂的并发布认证销毁意图和证明，使公告板可发布可审计的最终记录。


<details>
  <summary>Details</summary>
Motivation: 为Internet Computer（ICP）设计一个保护隐私的传输架构，解决传统区块链交易中身份隐私泄露、内容机密性不足、缺乏可验证的最终性等问题。需要在保持去中心化特性的同时，提供强隐私保护、内容保密和可审计的交易最终性。

Method: 1. 使用两个短寿命中介解耦存款和检索过程；2. 采用非交互式RDMPF封装技术生成每笔传输的传输密钥；3. 从胶囊计算公共通知提示，实现无指纹化发现；4. 通过简短解封装证明授权检索，不泄露身份信息；5. 所有中介都是短暂的，发布认证销毁意图和证明；6. 公告板发布可审计的最终记录。

Result: 该协议已在ICP上以ICPP名称投入生产，经过全面测试并包含多项增强功能。设计提供了：发送者相对于接收者的身份隐私、针对中介的内容机密性、传输密钥在分阶段销毁后的前向保密性、可验证的活跃性和最终性。协议接口已形式化，并提供了封装正确性、提示隐私、授权健全性和超时回收的安全性论证。

Conclusion: 该工作为现在公开可访问的ICPP协议提供了全面的参考，展示了如何在ICP技术栈上实现具有强隐私保护、内容保密和可审计最终性的私有传输架构。协议设计充分利用了ICP的技术特性，为区块链隐私传输提供了创新的解决方案。

Abstract: This paper presents a private transfer architecture for the Internet Computer (ICP) that decouples deposit and retrieval through two short-lived intermediaries, with sealed storage and attested teardown by an ephemeral witness. The protocol uses a non-interactive RDMPF-based encapsulation to derive per-transfer transport keys. A public notice hint is computed from the capsule to enable discovery without fingerprinting the recipient's key. Retrieval is authorized by a short proof of decapsulation that reveals no identities. All transaction intermediaries are ephemeral and issue certified destruction intents and proofs, allowing a noticeboard to publish auditable finalization records. The design provides sender identity privacy with respect to the recipient, content confidentiality against intermediaries, forward secrecy for transport keys after staged destruction, verifiable liveness and finality. We formalize the basic interfaces, provide the security arguments for encapsulation correctness, hint privacy, authorization soundness and timeout reclaim.
  In terms of implementation, it has been recently brought into production on the ICP under the name ICPP. It has been subject to exhaustive testing and incorporates a few enhancements, focusing on the operational possibilities offered by ICP's technology. This work hence serves as a broad reference for the protocol now publicly accessible.

</details>


### [16] [Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks](https://arxiv.org/abs/2512.23557)
*Toqeer Ali Syed,Mishal Ateeq Almutairi,Mahmoud Abdel Moaty*

Main category: cs.CR

TL;DR: 本文提出了一种跨智能体多模态溯源感知防御框架，用于检测和防御多模态提示注入攻击，确保智能体间通信的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型、视觉语言模型和智能体系统的发展，多模态提示注入攻击风险增加，恶意指令可能通过文本、图像、元数据或智能体间消息传播，导致意外行为、策略违规或状态损坏。

Method: 提出跨智能体多模态溯源感知防御框架，包含文本净化智能体、视觉净化智能体和输出验证智能体，由溯源账本协调管理，记录模态、来源和信任级别的元数据，确保智能体间通信遵循明确的信任框架。

Result: 实验评估显示，多模态注入检测准确率显著提高，跨智能体信任泄漏最小化，智能体执行路径更加稳定。

Conclusion: 该框架将溯源追踪和验证概念扩展到多智能体编排中，有助于建立安全、可理解和可靠的智能体AI系统。

Abstract: Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.

</details>


### [17] [Enhanced Web Payload Classification Using WAMM: An AI-Based Framework for Dataset Refinement and Model Evaluation](https://arxiv.org/abs/2512.23610)
*Heba Osama,Omar Elebiary,Youssef Qassim,Mohamed Amgad,Ahmed Maghawry,Ahmed Saafan,Haitham Ghalwash*

Main category: cs.CR

TL;DR: WAMM是一个AI驱动的多类别Web攻击检测框架，通过重新分类HTTP请求到OWASP对齐的类别，揭示基于规则系统的局限性。使用增强和LLM过滤的数据集，XGBoost达到99.59%准确率，微秒级推理，相比OWASP CRS有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统基于静态规则集的Web应用防火墙（如OWASP CRS）经常错过混淆或零日攻击模式，需要大量手动调整。面对日益增多的规避性和多态性攻击载荷，需要更智能的检测方法。

Method: WAMM采用多阶段增强管道处理SR-BH 2020数据集，包括大规模去重、LLM引导的重新标记、现实攻击数据增强和LLM过滤，生成三个精炼数据集。使用统计和文本表示的统一特征空间评估四种机器学习和深度学习模型。

Result: 在相同技术栈上，使用增强和LLM过滤的数据集，XGBoost达到99.59%准确率，微秒级推理时间。深度学习模型在噪声增强下性能下降。在未见增强数据集上测试OWASP CRS时，WAMM实现96-100%的真正阳性拦截率，改进高达86%。

Conclusion: 研究揭示了广泛部署的基于规则防御的差距，表明精心策划的训练管道结合高效的机器学习模型能够实现更弹性、实时的Web攻击检测方法，适合生产WAF环境。

Abstract: Web applications increasingly face evasive and polymorphic attack payloads, yet traditional web application firewalls (WAFs) based on static rule sets such as the OWASP Core Rule Set (CRS) often miss obfuscated or zero-day patterns without extensive manual tuning. This work introduces WAMM, an AI-driven multiclass web attack detection framework designed to reveal the limitations of rule-based systems by reclassifying HTTP requests into OWASP-aligned categories for a specific technology stack. WAMM applies a multi-phase enhancement pipeline to the SR-BH 2020 dataset that includes large-scale deduplication, LLM-guided relabeling, realistic attack data augmentation, and LLM-based filtering, producing three refined datasets. Four machine and deep learning models are evaluated using a unified feature space built from statistical and text-based representations. Results show that using an augmented and LLM-filtered dataset on the same technology stack, XGBoost reaches 99.59% accuracy with microsecond-level inference while deep learning models degrade under noisy augmentation. When tested against OWASP CRS using an unseen augmented dataset, WAMM achieves true positive block rates between 96 and 100% with improvements of up to 86%. These findings expose gaps in widely deployed rule-based defenses and demonstrate that curated training pipelines combined with efficient machine learning models enable a more resilient, real-time approach to web attack detection suitable for production WAF environments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [Failure Analysis of Safety Controllers in Autonomous Vehicles Under Object-Based LiDAR Attacks](https://arxiv.org/abs/2512.22244)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.SE

TL;DR: 该研究分析了自动驾驶车辆在高速公路场景下，面对基于物体的LiDAR攻击时，纵向安全控制器的系统性失效情况。研究发现，即使短暂的LiDAR诱导物体幻觉也能触发不安全制动、对真实危险的延迟响应和不稳定控制行为。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖LiDAR感知来支持安全关键控制功能，如自适应巡航控制和自动紧急制动。虽然先前研究表明LiDAR感知可以通过基于物体的欺骗和注入攻击进行操纵，但此类攻击对车辆安全控制器的影响仍不清楚。

Method: 研究使用高保真仿真框架，集成LiDAR感知、物体跟踪和闭环车辆控制，评估虚假和位移物体检测如何通过感知-规划-控制管道传播。重点关注高速公路驾驶场景中现实的切入和跟车情况。

Result: 结果表明，即使短暂的LiDAR诱导物体幻觉也能触发不安全制动、对真实危险的延迟响应和不稳定控制行为。在切入场景中，与良性条件相比，观察到不安全减速事件和碰撞时间违规明显增加。控制器失效更受欺骗物体时间一致性的影响，而非单纯的空间不准确性。

Conclusion: 这些发现揭示了自动驾驶系统中感知鲁棒性与控制级安全保证之间的关键差距。通过明确描述对抗性感知下的安全控制器失效模式，这项工作为设计攻击感知的安全机制和更具弹性的控制策略提供了实用见解。

Abstract: Autonomous vehicles rely on LiDAR based perception to support safety critical control functions such as adaptive cruise control and automatic emergency braking. While previous research has shown that LiDAR perception can be manipulated through object based spoofing and injection attacks, the impact of such attacks on vehicle safety controllers is still not well understood. This paper presents a systematic failure analysis of longitudinal safety controllers under object based LiDAR attacks in highway driving scenarios. The study focuses on realistic cut in and car following situations in which adversarial objects introduce persistent perception errors without directly modifying vehicle control software. A high fidelity simulation framework integrating LiDAR perception, object tracking, and closed loop vehicle control is used to evaluate how false and displaced object detections propagate through the perception planning and control pipeline. The results demonstrate that even short duration LiDAR induced object hallucinations can trigger unsafe braking, delayed responses to real hazards, and unstable control behavior. In cut in scenarios, a clear increase in unsafe deceleration events and time to collision violations is observed when compared to benign conditions, despite identical controller parameters. The analysis further shows that controller failures are more strongly influenced by the temporal consistency of spoofed objects than by spatial inaccuracies alone. These findings reveal a critical gap between perception robustness and control level safety guarantees in autonomous driving systems. By explicitly characterizing safety controller failure modes under adversarial perception, this work provides practical insights for the design of attack aware safety mechanisms and more resilient control strategies for LiDAR dependent autonomous vehicles.

</details>


### [19] [Hallucination Detection for LLM-based Text-to-SQL Generation via Two-Stage Metamorphic Testing](https://arxiv.org/abs/2512.22250)
*Bo Yang,Yinfen Xia,Weisong Sun,Yang Liu*

Main category: cs.SE

TL;DR: SQLHD是一种基于蜕变测试的Text-to-SQL幻觉检测方法，无需标准答案即可检测大语言模型生成的SQL查询中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在Text-to-SQL任务中表现出强大的泛化能力，但会产生幻觉（不现实或不合理的内容），导致错误的SQL查询。现有方法主要针对传统深度学习模型设计，在应用于LLMs时因缺乏真实数据而面临显著限制。

Method: 提出SQLHD方法，基于蜕变测试无需标准答案。将检测任务分为两个阶段：1）通过8个结构感知的蜕变关系检测模式链接幻觉（扰动比较词、实体、句子结构或数据库模式）；2）通过9个逻辑感知的蜕变关系检测逻辑合成幻觉（突变前缀词、极值表达式、比较范围或整个数据库）。每个阶段分别调用LLM生成模式映射或SQL构件，通过对应蜕变关系交叉检查输出，任何违规都被标记为幻觉。

Result: 实验结果显示SQLHD在F1分数上表现优异，范围从69.36%到82.76%。相比LLM自评估方法，SQLHD在识别Text-to-SQL任务中的幻觉方面表现更优。

Conclusion: SQLHD是一种有效的Text-to-SQL幻觉检测方法，无需标准答案即可检测LLM生成的SQL查询中的幻觉，在检测性能上优于现有方法。

Abstract: In Text-to-SQL generation, large language models (LLMs) have shown strong generalization and adaptability. However, LLMs sometimes generate hallucinations, i.e.,unrealistic or illogical content, which leads to incorrect SQL queries and negatively impacts downstream applications. Detecting these hallucinations is particularly challenging. Existing Text-to-SQL error detection methods, which are tailored for traditional deep learning models, face significant limitations when applied to LLMs. This is primarily due to the scarcity of ground-truth data. To address this challenge, we propose SQLHD, a novel hallucination detection method based on metamorphic testing (MT) that does not require standard answers. SQLHD splits the detection task into two sequentiial stages: schema-linking hallucination detection via eight structure-aware Metamorphic Relations (MRs) that perturb comparative words, entities, sentence structure or database schema, and logical-synthesis hallucination detection via nine logic-aware MRs that mutate prefix words, extremum expressions, comparison ranges or the entire database. In each stage the LLM is invoked separately to generate schema mappings or SQL artefacts; the follow-up outputs are cross-checked against their source counterparts through the corresponding MRs, and any violation is flagged as a hallucination without requiring ground-truth SQL. The experimental results demonstrate our method's superior performance in terms of the F1-score, which ranges from 69.36\% to 82.76\%. Additionally, SQLHD demonstrates superior performance over LLM Self-Evaluation methods, effectively identifying hallucinations in Text-to-SQL tasks.

</details>


### [20] [Building Software by Rolling the Dice: A Qualitative Study of Vibe Coding](https://arxiv.org/abs/2512.22418)
*Yi-Hung Chou,Boyuan Jiang,Yi Wen Chen,Mingyue Weng,Victoria Jackson,Thomas Zimmermann,James A. Jones*

Main category: cs.SE

TL;DR: 论文通过扎根理论研究分析了"氛围编程"现象，发现开发者使用LLM进行编程时存在不同行为模式，从完全依赖AI到检查调整生成代码不等，但都需要应对AI生成的不确定性，调试过程常被描述为"掷骰子"。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正在重塑软件工程，催生了"氛围编程"现象，即开发者主要通过提示而非编写代码来构建软件。尽管被广泛宣传为生产力突破，但关于从业者如何定义和实践这种编程方式的研究很少。

Method: 采用扎根理论研究法，分析了20个氛围编程视频，包括7个直播编程会话（约16小时，254个提示）和13个观点视频（约5小时），并辅以活动时长和提示意图的额外分析。

Result: 研究发现行为谱系：一些氛围编程者几乎完全依赖AI而不检查代码，而另一些则会检查和调整生成输出。所有方法都需要应对生成的随机性，调试和优化常被描述为"掷骰子"。不同的心智模型（受开发者专业知识和AI依赖程度影响）会影响提示策略、评估实践和信任水平。

Conclusion: 这些发现为软件工程未来研究开辟了新方向，并为工具设计和教育提供了实践机会。研究揭示了LLM如何改变编程实践，以及开发者如何适应这种新兴的工作方式。

Abstract: Large language models (LLMs) are reshaping software engineering by enabling "vibe coding," in which developers build software primarily through prompts rather than writing code. Although widely publicized as a productivity breakthrough, little is known about how practitioners actually define and engage in these practices. To shed light on this emerging phenomenon, we conducted a grounded theory study of 20 vibe-coding videos, including 7 live-streamed coding sessions (about 16 hours, 254 prompts) and 13 opinion videos (about 5 hours), supported by additional analysis of activity durations and prompt intents. Our findings reveal a spectrum of behaviors: some vibe coders rely almost entirely on AI without inspecting code, while others examine and adapt generated outputs. Across approaches, all must contend with the stochastic nature of generation, with debugging and refinement often described as "rolling the dice." Further, divergent mental models, shaped by vibe coders' expertise and reliance on AI, influence prompting strategies, evaluation practices, and levels of trust. These findings open new directions for research on the future of software engineering and point to practical opportunities for tool design and education.

</details>


### [21] [GraphLocator: Graph-guided Causal Reasoning for Issue Localization](https://arxiv.org/abs/2512.22469)
*Wei Liu,Chao Peng,Pengfei Gao,Aofan Liu,Wei Zhang,Haiyan Zhao,Zhi Jin*

Main category: cs.SE

TL;DR: GraphLocator：通过因果图构建解决软件问题定位中的症状-原因不匹配和一对多不匹配问题，显著提升定位准确率


<details>
  <summary>Details</summary>
Motivation: 软件问题定位任务面临两个主要挑战：1）症状-原因不匹配：问题描述不明确揭示根本原因；2）一对多不匹配：单个问题对应多个相互依赖的代码实体。这些语义鸿沟导致传统方法定位效果不佳。

Method: 提出GraphLocator方法，核心是因果问题图（CIG）：顶点表示发现的子问题及其关联代码实体，边编码它们之间的因果依赖关系。工作流程分为两个阶段：症状顶点定位和动态CIG发现，先在仓库图上识别症状位置，然后通过迭代推理相邻顶点动态扩展CIG。

Result: 在三个真实数据集上的实验表明：1）相比基线方法，GraphLocator在函数级召回率平均提升+19.49%，精确率提升+11.89%；2）在症状-原因和一对多不匹配场景下分别实现召回率提升+16.44%和+19.18%，精确率提升+7.78%和+13.23%；3）GraphLocator生成的CIG在下游解决任务中带来28.74%的性能提升。

Conclusion: GraphLocator通过因果结构发现和动态问题解耦有效解决了软件问题定位中的两个关键不匹配问题，显著提升了定位准确性和下游任务性能。

Abstract: The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.

</details>


### [22] [Isolating Compiler Faults via Multiple Pairs of Adversarial Compilation Configurations](https://arxiv.org/abs/2512.22538)
*Qingyang Li,Yibiao Yang,Maolin Sun,Jiangchang Wu,Qingkai Shi,Yuming Zhou*

Main category: cs.SE

TL;DR: MultiConf是一种自动定位编译器故障的新方法，通过构建多对对抗性编译配置对，利用SBFL公式和加权投票方案来更准确、鲁棒地定位编译器源文件中的故障。


<details>
  <summary>Details</summary>
Motivation: 编译器在现代软件开发中至关重要，但由于现代编译器基础设施的复杂性和规模，将故障定位到特定源文件仍然极具挑战性。现有方法在定位编译器故障方面存在局限性，需要更有效的解决方案。

Method: MultiConf通过构建多对对抗性编译配置对来隔离编译器故障，每对包括一个失败配置和对应的通过配置，两者仅在少量细粒度选项上不同。使用轻量级构造过程生成失败配置，并通过选择性禁用与bug相关的细粒度选项推导出通过配置。然后应用基于频谱的故障定位(SBFL)公式对编译器源文件进行可疑度排序，每个对抗配置对独立产生一个排序，最后使用加权投票方案聚合得到最终可疑度排序。

Result: 在60个真实世界GCC编译器bug的基准测试中，MultiConf在效果和效率上都显著优于现有编译器故障定位技术。MultiConf成功在Top-1文件级别定位了60个bug中的27个，相比最先进方法Odfl(20)和Basic(21)分别提高了35.0%和28.6%。

Conclusion: MultiConf是一种有效且高效的编译器故障定位方法，通过多对抗配置对和加权投票机制，能够更准确、鲁棒地定位编译器源文件中的故障，显著优于现有技术。

Abstract: Compilers are fundamental to modern software development, making the effective identification and resolution of compiler faults essential. However, localizing these faults to specific source files remains highly challenging due to the complexity and scale of modern compiler infrastructures. In this study, we propose MultiConf, a novel approach that automatically isolates compiler faults by constructing multiple pairs of adversarial compilation configurations. Each adversarial compilation configuration pair consists of a failing configuration and its corresponding passing configuration, which differ in only a small number of fine-grained options. MultiConf generates failing configurations through a lightweight construction process and derives the corresponding passing configurations by selectively disabling bug-related fine-grained options. We then employ a Spectrum-Based Fault Localization (SBFL) formula to rank the suspiciousness of compiler source files. Each adversarial configuration pair independently produces a ranking, which is subsequently aggregated using a weighted voting scheme to derive a final suspiciousness ranking, enabling more accurate and robust fault localization. We evaluate MultiConf on a benchmark of 60 real-world GCC compiler bugs. The results demonstrate that MultiConf significantly outperforms existing compiler fault localization techniques in both effectiveness and efficiency. In particular, MultiConf successfully localizes 27 out of 60 bugs at the Top-1 file level, representing improvements of 35.0% and 28.6% over the two state-of-the-art approaches, Odfl(20) and Basic(21), respectively.

</details>


### [23] [Rethinking the Capability of Fine-Tuned Language Models for Automated Vulnerability Repair](https://arxiv.org/abs/2512.22633)
*Woorim Han,Yeongjun Kwak,Miseon Yu,Kyeongmin Kim,Younghan Lee,Hyungon Moon,Yunheung Paek*

Main category: cs.SE

TL;DR: 该研究揭示了基于学习的自动化漏洞修复模型存在过拟合问题，评估方法存在缺陷，并提出了改进评估的方法和新基准。


<details>
  <summary>Details</summary>
Motivation: 当前基于微调语言模型的自动化漏洞修复技术虽然显示出潜力，但其修复未见漏洞的能力存在疑问。现有模型可能过拟合训练集，且评估使用的训练、验证和测试集并非互斥，同时基于匹配的评估指标存在局限性，无法考虑多种有效修复方式。

Method: 研究采用三种方法：1）对测试集应用语义保持变换，检验模型是否真正学习到稳健的漏洞修复模式；2）重新划分互斥的训练、验证和测试集，评估模型的泛化能力；3）引入L-AVRBench，这是一个专为基于学习的AVR设计的基于测试的基准，以克服基于匹配指标的局限性。

Result: 研究发现最先进的模型往往过拟合训练集，且现有评估方法存在缺陷。通过语义变换测试发现模型可能依赖虚假特征而非真正学习修复模式。重新划分数据集后模型的泛化能力受到挑战。L-AVRBench基准提供了更准确的评估方法。

Conclusion: 基于学习的自动化漏洞修复模型在泛化能力和评估方法上存在显著问题。需要更严格的评估标准和基于测试的基准来准确衡量模型的真实修复能力，这对未来AVR技术的发展至关重要。

Abstract: Learning-based automated vulnerability repair (AVR) techniques that utilize fine-tuned language models have shown promise in generating vulnerability patches. However, questions remain about their ability to repair unseen vulnerabilities. Our empirical study reveals that state-of-the-art models often overfit to the training set and are evaluated using training, validation, and test sets that are not mutually exclusive. Furthermore, relying on match-based metrics that compare generated patches to reference fixes at the token level has some limitations, failing to account for the possibility of various valid ways to patch the vulnerability. In this paper, we examine the capabilities of state-of-the-art fine-tuned AVR models and the adequacy of match-based evaluation metrics in three ways. First, we apply semantic-preserving transformations to test sets in order to determine whether models truly learn robust vulnerability-repair patterns or simply rely on spurious features. Second, we re-split the training, validation, and test sets to be mutually exclusive and evaluate the models on the revised test set to assess their generalization capabilities. Third, we introduce L-AVRBench, a test-based benchmark tailored for learning-based AVR, to overcome the limitations of match-based metrics and examine the AVR models' true repair capabilities.

</details>


### [24] [CFIghter: Automated Control-Flow Integrity Enablement and Evaluation for Legacy C/C++ Systems](https://arxiv.org/abs/2512.22701)
*Sabine Houy,Bruno Kreyssig,Alexandre Bartel*

Main category: cs.SE

TL;DR: CFIghter是首个完全自动化系统，通过检测、分类和修复测试套件暴露的意外策略违规，在真实项目中实现严格类型化控制流完整性保护。


<details>
  <summary>Details</summary>
Motivation: 基于编译器的控制流完整性（CFI）提供强大的前向边保护，但由于可见性不匹配、类型不一致和意外行为故障，在大型C/C++软件中部署仍然困难。

Method: CFIghter集成了全程序分析与引导式运行时监控，迭代应用最小必要调整到CFI强制执行，仅在需要的地方进行，一旦所有测试通过或剩余故障被认为无法解决就停止。

Result: 在四个GNU项目上评估，CFIghter解决了所有可见性相关的构建错误，在大型多库util-linux代码库中自动修复了95.8%的意外CFI违规，同时在超过89%的间接控制流站点保持严格强制执行。

Conclusion: 自动化兼容性修复使严格的编译器CFI在成熟的模块化C软件中实际可部署，无需手动源代码更改，仅依赖自动生成的可见性调整和必要的局部化强制执行范围。

Abstract: Compiler-based Control-Flow Integrity (CFI) offers strong forward-edge protection but remains challenging to deploy in large C/C++ software due to visibility mismatches, type inconsistencies, and unintended behavioral failures. We present CFIghter, the first fully automated system that enables strict, type-based CFI in real-world projects by detecting, classifying, and repairing unintended policy violations exposed by the test suite. CFIghter integrates whole-program analysis with guided runtime monitoring and iteratively applies the minimal necessary adjustments to CFI enforcement only where required, stopping once all tests pass or remaining failures are deemed unresolvable. We evaluate CFIghter on four GNU projects. It resolves all visibility-related build errors and automatically repairs 95.8% of unintended CFI violations in the large, multi-library util-linux codebase, while retaining strict enforcement at over 89% of indirect control-flow sites. Across all subjects, CFIghter preserves strict type-based CFI for the majority of the codebase without requiring manual source-code changes, relying only on automatically generated visibility adjustments and localized enforcement scopes where necessary. These results show that automated compatibility repair makes strict compiler CFI practically deployable in mature, modular C software.

</details>


### [25] [FasterPy: An LLM-based Code Execution Efficiency Optimization Framework](https://arxiv.org/abs/2512.22827)
*Yue Wu,Minghao Han,Ruiyin Li,Peng Liang,Amjed Tahir,Zengyang Li,Qiong Feng,Mojtaba Shahin*

Main category: cs.SE

TL;DR: FasterPy是一个基于大语言模型的Python代码优化框架，结合检索增强生成和低秩适配技术，在PIE基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法需要手动设计和维护特定性能bug的规则，劳动密集且适用范围有限。基于机器学习和深度学习的方法虽然有所改进，但依赖特定的程序表示和精心构建的训练数据集，开发成本高且难以扩展。大语言模型在代码生成方面的卓越能力为自动化代码优化提供了新途径。

Method: 提出FasterPy框架，结合检索增强生成（RAG）和低秩适配（LoRA）技术。RAG基于现有性能改进代码对和相应性能测量构建的知识库，LoRA用于增强代码优化性能。

Result: 在Performance Improving Code Edits（PIE）基准测试中，该方法在多个指标上优于现有模型。

Conclusion: FasterPy是一个低成本、高效的框架，成功地将大语言模型应用于Python代码执行效率的优化，为自动化代码优化提供了有效的解决方案。

Abstract: Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.

</details>


### [26] [Towards the analysis of team members well-being](https://arxiv.org/abs/2512.22845)
*Zan Xu,Sari Nurfauziyyah,Anastasia Romanova,Kaamesh G S,Yiqun Gao,Maria Spichkova*

Main category: cs.SE

TL;DR: 该研究关注软件开发团队成员的幸福感，探讨认可与赞赏对团队幸福感的影响，并开发了相应的分析原型系统。


<details>
  <summary>Details</summary>
Motivation: 软件开发团队成员的幸福感不仅影响工作生产力和绩效，还关系到员工的生理健康和私人生活。研究表明，团队成员是否感受到对其贡献的认可和赞赏是影响幸福感的重要因素。

Method: 论文介绍了团队幸福感分析项目的结果，并展示了在该项目中开发的原型系统。

Result: 论文呈现了团队幸福感分析的具体结果，并展示了开发的系统原型。

Conclusion: 认可和赞赏对软件开发团队成员的幸福感具有重要影响，通过分析工具可以更好地理解和提升团队幸福感。

Abstract: Many recent research studies have focused on the well-being of software development team members, as this aspect may be critical not only for productivity and performance at work but also for the physical health and personal life of employees. Many studies agree that an important factor of team member well-being is whether team members feel appreciated and acknowledged for their contributions. This paper presents the results of a project on the team well-being analysis as well as the prototype developed within the project.

</details>


### [27] [Interpretable Gallbladder Ultrasound Diagnosis: A Lightweight Web-Mobile Software Platform with Real-Time XAI](https://arxiv.org/abs/2512.23033)
*Fuyad Hasan Bhoyan,Prashanta Sarker,Parsia Noor Ethila,Md. Emon Hossain,Md Kaviul Hossain,Md Humaion Kabir Mehedi*

Main category: cs.SE

TL;DR: 开发基于AI的超声图像诊断软件，使用MobResTaNet混合深度学习模型对10种胆囊疾病类型（9种疾病+正常）进行实时分类，准确率达99.85%，参数仅2.24M，通过XAI提供可解释性，部署为Web和移动应用。


<details>
  <summary>Details</summary>
Motivation: 胆囊疾病的早期准确检测至关重要，但超声图像解读具有挑战性，需要开发高效、准确的AI辅助诊断工具来支持临床决策。

Method: 采用混合深度学习模型MobResTaNet，直接从超声图像分类10个类别（9种胆囊疾病类型和正常），结合可解释AI（XAI）可视化，使用HTML、CSS、JavaScript、Bootstrap和Flutter技术栈开发Web和移动应用。

Result: 系统达到高达99.85%的准确率，仅使用2.24M参数，实现实时预测和可解释性可视化，为临床决策提供透明支持。

Conclusion: 该AI驱动诊断软件在点护理场景下提供高效、可访问且可信赖的胆囊疾病诊断支持，通过轻量级模型和可解释性功能实现了临床实用性。

Abstract: Early and accurate detection of gallbladder diseases is crucial, yet ultrasound interpretation is challenging. To address this, an AI-driven diagnostic software integrates our hybrid deep learning model MobResTaNet to classify ten categories, nine gallbladder disease types and normal directly from ultrasound images. The system delivers interpretable, real-time predictions via Explainable AI (XAI) visualizations, supporting transparent clinical decision-making. It achieves up to 99.85% accuracy with only 2.24M parameters. Deployed as web and mobile applications using HTML, CSS, JavaScript, Bootstrap, and Flutter, the software provides efficient, accessible, and trustworthy diagnostic support at the point of care

</details>


### [28] [An Empirical Study of Generative AI Adoption in Software Engineering](https://arxiv.org/abs/2512.23327)
*Görkem Giray,Onur Demirörs,Marcos Kalinowski,Daniel Mendez*

Main category: cs.SE

TL;DR: 生成式AI在软件工程中已被广泛采用，主要用于实现、验证、个人辅助和维护任务，带来效率提升但面临输出可靠性、安全隐私等挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI工具在软件工程领域日益普及，但缺乏关于其实际使用情况、效益、挑战及组织社会影响的实证证据，需要系统研究了解现状。

Method: 通过实证研究调查生成式AI在软件工程中的采用现状，包括采用状态、相关效益与挑战、工具技术制度化程度、以及对专业人员和社区的长期影响。

Result: 生成式AI工具已深度融入日常软件工程工作，显著减少周期时间、提升质量、增强知识工作支持并提高生产力，但存在输出不可靠、提示工程困难、验证开销、安全隐私问题等挑战，制度化程度参差不齐。

Conclusion: 生成式AI正在重新定义而非取代软件工程师角色，虽然带来显著效益但需要解决可靠性、安全性和制度化等挑战，同时需关注对就业市场和技能转变的影响。

Abstract: Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.

</details>


### [29] [Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?](https://arxiv.org/abs/2512.23385)
*The Anh Nguyen,Triet Huynh Minh Le,M. Ali Babar*

Main category: cs.SE

TL;DR: 该研究通过分析Hugging Face和GitHub上的开发者讨论，构建了一个包含312,868个安全相关讨论的数据集，识别出AI供应链中的32种安全问题和24种解决方案，揭示了AI组件复杂依赖性和黑盒特性带来的安全挑战。


<details>
  <summary>Details</summary>
Motivation: AI模型和应用的快速增长带来了日益复杂的安全环境，开发者不仅要面对传统软件供应链问题，还要应对AI特有的安全威胁。然而，目前对实践中常见的安全问题及其解决方案了解甚少，这阻碍了针对AI供应链各组件开发有效安全措施。

Method: 研究通过结合关键词匹配和优化的distilBERT分类器构建了一个识别安全相关讨论的管道，该分类器在多种深度学习和大语言模型比较中表现最佳。使用该管道从Hugging Face和GitHub收集了312,868个安全讨论，并对其中753个帖子进行主题分析。

Result: 研究识别出32种安全问题和24种解决方案，分为四个主题：(1)系统和软件，(2)外部工具和生态系统，(3)模型，(4)数据。发现许多安全问题源于AI组件的复杂依赖性和黑盒特性，特别是与模型和数据相关的挑战往往缺乏具体解决方案。

Conclusion: 该研究填补了AI供应链安全实践知识的空白，为开发者和研究人员提供了基于证据的指导，以应对AI供应链中的现实安全威胁。研究结果强调了需要更多关注模型和数据相关安全问题的具体解决方案。

Abstract: The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.

</details>


### [30] [An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes](https://arxiv.org/abs/2512.23415)
*Vinoth Punniyamoorthy,Bikesh Kumar,Sumit Saha,Lokesh Butra,Mayilsamy Palanigounder,Akash Kumar Agarwal,Kabilan Kannan*

Main category: cs.SE

TL;DR: 论文提出基于AIOps的Kubernetes智能弹性伸缩框架，通过多信号集成、SLO感知和成本控制，相比原生K8s弹性伸缩机制，将SLO违规时间减少31%，响应时间提升24%，成本降低18%。


<details>
  <summary>Details</summary>
Motivation: Kubernetes原生弹性伸缩机制（HPA、VPA、节点级伸缩）在生产环境中经常导致SLO违规和成本效率低下，主要原因是反应式伸缩行为、应用级信号使用有限以及控制逻辑不透明。

Method: 提出安全可解释的多信号弹性伸缩框架，集成SLO感知和成本控制，结合轻量级需求预测，采用AIOps原则增强Kubernetes弹性伸缩能力。

Result: 实验评估显示，相比默认和调优的Kubernetes基线，该方法将SLO违规持续时间减少31%，伸缩响应时间提高24%，基础设施成本降低18%，同时保持稳定可审计的控制行为。

Conclusion: AIOps驱动的SLO优先弹性伸缩能显著提高基于Kubernetes的云平台的可靠性、效率和操作可信度，为云原生应用提供更智能的资源管理方案。

Abstract: Kubernetes provides native autoscaling mechanisms, including the Horizontal Pod Autoscaler, Vertical Pod Autoscaler, and node-level autoscalers, to enable elastic resource management for cloud-native applications. However, production environments frequently experience Service Level Objective violations and cost inefficiencies due to reactive scaling behavior, limited use of application-level signals, and opaque control logic. This paper investigates how Kubernetes autoscaling can be enhanced using AIOps principles to jointly satisfy SLO and cost constraints under diverse workload patterns without compromising safety or operational transparency. We present a gap-driven analysis of existing autoscaling approaches and propose a safe and explainable multi-signal autoscaling framework that integrates SLO-aware and cost-conscious control with lightweight demand forecasting. Experimental evaluation using representative microservice and event-driven workloads shows that the proposed approach reduces SLO violation duration by up to 31 percent, improves scaling response time by 24 percent, and lowers infrastructure cost by 18 percent compared to default and tuned Kubernetes autoscaling baselines, while maintaining stable and auditable control behavior. These results demonstrate that AIOps-driven, SLO-first autoscaling can significantly improve the reliability, efficiency, and operational trustworthiness of Kubernetes-based cloud platforms.

</details>


### [31] [Embedding Quality Assurance in project-based learning](https://arxiv.org/abs/2512.23488)
*Maria Spichkova*

Main category: cs.SE

TL;DR: 基于十多年在敏捷/Scrum环境下教授软件工程课程的经验，分享了在项目式学习中嵌入质量保证主题的教训和建议


<details>
  <summary>Details</summary>
Motivation: 分享在敏捷/Scrum环境下教授软件工程课程（特别是毕业年级软件开发项目和软件工程项目管理课程）中关于软件质量方面的十多年教学经验教训

Method: 基于十多年的教学实践，分析在敏捷/Scrum环境下的项目式学习经验，总结教训并提出具体建议

Result: 提供了在敏捷/Scrum背景下的项目式学习中嵌入质量保证主题的一系列具体建议和最佳实践

Conclusion: 通过十多年的教学实践，总结出了在敏捷/Scrum环境下有效教授软件质量保证的有效方法和策略，为相关教育工作者提供了有价值的指导

Abstract: In this paper, we share our lessons learned from more than a decade of teaching software quality aspects within Software Engineering (SE) courses, where the focus is on Agile/Scrum settings: final year software development projects and the course on SE Project Management. Based on the lessons learned, we also provide a number of recommendations on embedding quality assurance topics in the project-based learning with Agile/Scrum context.

</details>


### [32] [Adaptable Teastore with Energy Consumption Awareness: A Case Study](https://arxiv.org/abs/2512.23498)
*Henrique De Medeiros,Denisse Muñante,Sophie Chabridon,César Perdigão Batista,Denis Conan*

Main category: cs.SE

TL;DR: 本文介绍了EnCoMSAS工具，用于监控自适应性系统的能耗，并通过实验验证其在分布式云应用中的有效性，发现其对整体能耗影响较小。


<details>
  <summary>Details</summary>
Motivation: 随着全球能源消耗持续增长，数据中心成为主要能耗源，云应用迁移和数字内容消费加剧了这一问题。动态自适应方法虽能降低运行时能耗，但缺乏有效的能耗监控工具来支持自适应性系统的能源感知。

Method: 开发了EnCoMSAS工具来收集分布式云应用的能耗数据，支持运行时评估SAS变体的能耗。通过Adaptable TeaStore案例研究进行实证评估，重点关注其推荐服务，在不同工作负载条件下执行实验，使用Grid5000测试平台节点。

Result: EnCoMSAS能有效收集软件应用的能耗数据以支持运行时动态自适应。CPU使用率与能耗数据的相关性验证了测量有效性。能耗不仅受算法复杂度影响，还受部署环境特性影响。EnCoMSAS对SAS生态系统整体能耗的影响相对于整个TeaStore微服务集较小。

Conclusion: EnCoMSAS工具为自适应性系统提供了有效的能耗监控能力，支持将能源效率作为自适应规划的主要目标，且监控工具本身对系统整体能耗影响有限。

Abstract: [Context and Motivation] Global energy consumption has been steadily increasing in recent years, with data centers emerging as major contributors. This growth is largely driven by the widespread migration of applications to the Cloud, alongside a rising number of users consuming digital content. Dynamic adaptation (or self-adaptive) approaches appear as a way to reduce, at runtime and under certain constraints, the energy consumption of software applications.
  [Question/Problem] Despite efforts to make energy-efficiency a primary goal in the dynamic adaptation of software applications, there is still a gap in understanding how to equip these self-adaptive software systems (SAS), which are dynamically adapted at runtime, with effective energy consumption monitoring tools that enable energy-awareness. Furthermore, the extent to which such an energy consumption monitoring tool impacts the overall energy consumption of the SAS ecosystem has not yet been thoroughly explored.
  [Methodology] To address this gap, we introduce the EnCoMSAS (Energy Consumption Monitoring for Self-Adaptive Systems) tool that allows to gather the energy consumed by distributed software applications deployed, for instance, in the Cloud. EnCoMSAS enables the evaluation of energy consumption of SAS variants at runtime. It allows to integrate energy-efficiency as a main goal in the analysis and execution of new adaptation plans for the SAS. In order to evaluate the effectiveness of EnCoMSAS and investigate its impact on the overall energy consumption of the SAS ecosystem, we conduct an empirical study by using the Adaptable TeaStore case study. Adaptable TeaStore is a self-adaptive extension of the TeaStore application, a microservice benchmarking application. For this study, we focus on the recommender service of Adaptable TeaStore. Regarding the experiments, we first equip Adaptable TeaStore with EnCoMSAS. Next, we execute Adaptable TeaStore by varying workload conditions that simulate users interactions. Finally, we use EnCoMSAS for gathering and assessing the energy consumption of the recommender algorithms of Adaptable TeaStore. To run these experiments, we use nodes of the Grid5000 testbed.
  [Results] The results show that EnCoMSAS is effective in collecting energy consumption of software applications for enabling dynamic adaptation at runtime. The observed correlation between CPU usage and energy consumption collected by EnCoMSAS provides evidence supporting the validity of the collected energy measurements. Moreover, we point out, through EnCoMSAS, that energy consumption is influenced not only by the algorithmic complexity but also by the characteristics of the deployment environment. Finally, the results show that the impact of EnCoMSAS on the overall energy consumption of the SAS ecosystem is comparatively modest with respect to the entire set of the TeaStore applications microservices.

</details>


### [33] [Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving](https://arxiv.org/abs/2512.23511)
*Xinyi Zheng,Ningke Li,Xiaokun Luan,Kailong Wang,Ling Shi,Meng Sun,Haoyu Wang*

Main category: cs.SE

TL;DR: MATP是一个通过多步自动定理证明来系统验证大语言模型推理的评估框架，将自然语言推理转化为一阶逻辑，使用自动定理证明器评估逻辑有效性，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗、法律、科学研究等高风险领域展现出强大的推理能力，但其推理中常包含被流畅语言掩盖的细微逻辑错误，现有的事实检查、自一致性方法和基于规则的验证方法无法检测多步推理中的复杂逻辑缺陷。

Method: MATP框架将自然语言推理转化为一阶逻辑，然后应用自动定理证明器来逐步评估逻辑有效性，能够识别隐藏的逻辑错误并提供细粒度的推理正确性分类。

Result: 在包含10,830个推理实例的基准测试中（来自PrOntoQA-OOD、ProofWriter和FOLIO任务，由10个大语言模型生成），MATP在推理步骤验证方面比基于提示的基线方法高出42个百分点以上，并揭示了推理模型比通用模型产生更逻辑一致的输出。

Conclusion: MATP框架通过多步自动定理证明系统验证大语言模型推理，能够有效检测复杂逻辑错误，提高大语言模型生成推理的可信度，在高风险应用中具有重要价值。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-step reasoning.
  To overcome these challenges, we present MATP, an evaluation framework for systematically verifying LLM reasoning via Multi-step Automatic Theorem Proving. MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to assess step-by-step logical validity. This approach identifies hidden logical errors and provides fine-grained classifications of reasoning correctness. Evaluations on a benchmark comprising 10,830 reasoning instances generated by 10 LLMs across tasks from PrOntoQA-OOD, ProofWriter, and FOLIO show that MATP surpasses prompting-based baselines by over 42 percentage points in reasoning step verification. It further reveals model-level disparities, with reasoning models generating more logically coherent outputs than general models. These results demonstrate MATP's potential to enhance the trustworthiness of LLM-generated reasoning.

</details>


### [34] [Model-based Development for Autonomous Driving Software Considering Parallelization](https://arxiv.org/abs/2512.23575)
*Kenshin Obi,Takumi Onozawa,Hiroshi Fujimoto,Takuya Azumi*

Main category: cs.SE

TL;DR: 本文提出了一种基于模型开发（MBD）的自动驾驶软件并行化方法，通过扩展现有MBP方法减少执行时间，提升实时性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶软件需要处理多种功能和复杂环境，对实时性能要求极高，现有方法难以满足这些需求。

Method: 扩展现有的基于模型并行化（MBP）方法，采用模型开发（MBD）流程来实现自动驾驶软件的并行化处理。

Result: 通过提出的并行化方法，成功减少了执行时间，评估结果表明该方法适用于自动驾驶软件开发，特别是在实现实时性能方面表现良好。

Conclusion: 提出的基于MBD的并行化方法能有效提升自动驾驶软件的实时性能，适用于复杂自动驾驶系统的开发。

Abstract: In recent years, autonomous vehicles have attracted attention as one of the solutions to various social problems. However, autonomous driving software requires real-time performance as it considers a variety of functions and complex environments. Therefore, this paper proposes a parallelization method for autonomous driving software using the Model-Based Development (MBD) process. The proposed method extends the existing Model-Based Parallelizer (MBP) method to facilitate the implementation of complex processing. As a result, execution time was reduced. The evaluation results demonstrate that the proposed method is suitable for the development of autonomous driving software, particularly in achieving real-time performance.

</details>


### [35] [Parallelized Code Generation from Simulink Models for Event-driven and Timer-driven ROS 2 Nodes](https://arxiv.org/abs/2512.23605)
*Kenshin Obi,Ryo Yoshinaka,Hiroshi Fujimoto,Takuya Azumi*

Main category: cs.SE

TL;DR: 本文提出了一种基于模型开发（MBD）的框架，用于解决ROS 2多输入场景下的并行化挑战，通过将Simulink模型分类为事件驱动和定时器驱动类型，实现了有效的并行化代码生成。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶等嵌入式系统复杂度和规模的增加，传统手动并行化面临数据完整性和并发问题挑战，而现有基于模型开发方法难以有效集成ROS 2等现代框架在多输入场景下的并行化需求。

Method: 提出一个MBD框架，将ROS 2兼容的Simulink模型分类为事件驱动和定时器驱动类型，针对不同类型进行有针对性的并行化处理，扩展了传统MBD并行化方法，支持基于ROS 2的多输入模型并行化代码生成。

Result: 评估结果显示，应用所提框架进行并行化后，所有模式都显示出执行时间的减少，验证了并行化的有效性。

Conclusion: 该框架成功解决了ROS 2多输入场景下的并行化挑战，扩展了MBD在复杂嵌入式系统中的应用能力，为自动驾驶等领域的系统开发提供了有效的并行化解决方案。

Abstract: In recent years, the complexity and scale of embedded systems, especially in the rapidly developing field of autonomous driving systems, have increased significantly. This has led to the adoption of software and hardware approaches such as Robot Operating System (ROS) 2 and multi-core processors. Traditional manual program parallelization faces challenges, including maintaining data integrity and avoiding concurrency issues such as deadlocks. While model-based development (MBD) automates this process, it encounters difficulties with the integration of modern frameworks such as ROS 2 in multi-input scenarios. This paper proposes an MBD framework to overcome these issues, categorizing ROS 2-compatible Simulink models into event-driven and timer-driven types for targeted parallelization. As a result, it extends the conventional parallelization by MBD and supports parallelized code generation for ROS 2-based models with multiple inputs. The evaluation results show that after applying parallelization with the proposed framework, all patterns show a reduction in execution time, confirming the effectiveness of parallelization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [36] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: Bidirectional RAG是一种新型检索增强生成架构，通过验证高质量生成响应的写回机制实现安全的知识库扩展，相比传统静态RAG系统显著提升覆盖率同时减少文档添加数量。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统使用静态知识库，无法从用户交互中学习和演化，限制了系统的持续改进能力。需要一种既能积累知识又能防止幻觉污染的安全扩展机制。

Method: 提出双向RAG架构，采用多阶段接受层进行验证，包括基于NLI的蕴含验证、归因检查和新颖性检测，确保高质量生成响应安全写回知识库，实现知识积累同时防止幻觉污染。

Result: 在四个数据集（Natural Questions、TriviaQA、HotpotQA、Stack Overflow）上进行12次实验，双向RAG平均覆盖率达到40.58%，几乎是标准RAG（20.33%）的两倍，同时比简单写回机制少添加72%的文档（140 vs 500）。

Conclusion: 研究表明，在严格验证机制管理下，自我改进的RAG系统是可行且安全的，为实现从部署中学习的RAG系统提供了实用路径。

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [37] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: 研究发现，大型语言模型在未经明确提示的情况下也会进行说服行为，特别是在经过监督微调后，即使是在良性话题上进行微调，模型也会在争议性和有害话题上表现出更强的说服倾向。


<details>
  <summary>Details</summary>
Motivation: 随着对话式AI系统的广泛应用，AI对人类观点和信念的影响力前所未有。先前研究主要关注滥用场景下的说服风险，但本文旨在探究模型在未经明确提示的情况下进行说服的条件，以评估这种新兴风险的实际威胁程度。

Method: 研究在两种场景下考察未经提示的说服行为：(1)通过内部激活引导使模型具备特定人格特质；(2)通过监督微调使模型展现相同特质。研究比较了这两种方法对模型说服倾向的影响。

Result: 研究发现，通过激活引导（无论是与说服相关还是无关的特质）并不能可靠地增加模型未经提示的说服倾向，但监督微调可以。更重要的是，即使在仅包含良性话题的一般说服数据集上进行微调，模型也会在争议性和有害话题上表现出更强的说服倾向。

Conclusion: 研究表明，有害的说服行为可能在没有明确提示的情况下出现，特别是在经过监督微调的模型中。这种新兴风险需要进一步研究，特别是在模型部署和安全性评估中需要考虑未经提示的说服行为。

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [38] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: GamiBench是一个评估多模态大语言模型空间推理能力的基准测试，通过折纸任务测试2D到3D的空间规划和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在感知和指令跟随方面表现出色，但在空间推理能力（跨多视角和时间跟踪、操作物体的能力）方面仍有不足。现有基准测试主要关注静态图像或最终输出，未能考虑空间推理的序列性和视角依赖性特点。

Method: 研究者开发了GamiBench基准测试，包含186个常规和186个不可能的2D折痕图案及其对应的3D折叠形状，从6个不同视角生成。通过三个视觉问答任务进行评估：预测3D折叠配置、区分有效视角、检测不可能图案。引入新的诊断指标：视角一致性（VC）和不可能折叠选择率（IFSR）。

Result: 实验表明，即使是GPT-5和Gemini-2.5-Pro等领先模型在单步空间理解方面也表现不佳。GamiBench能够全面评估模型的整个推理过程，包括跨视角一致性、物理可行性检测和中间折叠步骤的解释。

Conclusion: GamiBench为评估多模态大语言模型的几何理解和空间推理能力建立了一个标准化框架，填补了现有基准测试在序列性和视角依赖性评估方面的空白。

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [39] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: 本文提出了一种公平感知的AI框架，用于优化孟加拉国洪水灾后援助分配，通过对抗性去偏技术减少对边缘化地区的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 发展中国家灾后援助分配存在系统性偏见，弱势地区往往被忽视，这延续了历史不平等。孟加拉国作为洪水频发国家，需要更公平的援助分配机制。

Method: 采用对抗性去偏模型，使用梯度反转层学习偏见不变的表示，将医疗AI中的公平感知表示学习技术应用于灾害管理。基于2022年孟加拉国洪水真实数据（影响720万人，损失4.055亿美元）。

Result: 在11个地区的87个upazilas上测试，模型将统计均等差异减少41.6%，区域公平差距降低43.2%，同时保持较强的预测准确性（R平方=0.784 vs 基线0.811）。

Conclusion: 该框架展示了算法公平技术在人道主义背景下的有效应用，为决策者提供了实施更公平灾害恢复策略的工具，确保援助基于真实需求而非历史分配模式。

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [40] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: 提出了一个面向智能体AI系统的风险与能力（ARC）技术治理框架，帮助组织识别、评估和缓解自主AI系统带来的风险。


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统因其自主行动能力（如代码执行、互联网交互、文件修改）带来了重大机遇和新型风险，这对组织治理提出了挑战，需要全面识别、评估和缓解多样且不断演化的风险。

Method: 开发了基于能力视角的ARC框架，包括：1）采用新颖的能力中心视角分析各类智能体AI系统；2）提炼出智能体AI系统的三个主要风险来源（组件、设计和能力）；3）建立风险来源、具体风险和技术控制之间的明确联系；4）提供结构化实用方法帮助组织实施该框架。

Result: ARC框架为组织提供了一个稳健且可适应的治理方法，能够应对智能体AI的复杂性，在确保安全、可靠、负责任部署的同时，支持快速有效的创新。

Conclusion: 该框架开源提供，为组织治理智能体AI系统风险提供了实用工具，平衡了创新需求与风险管理要求。

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [41] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: Logic Sketch Prompting (LSP) 是一种轻量级提示框架，通过引入类型变量、确定性条件评估器和基于规则的验证器，显著提升LLM在需要严格规则遵循、确定性和可审计性任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言推理方面表现出色，但在需要严格规则遵循、确定性和可审计性的任务上仍然不可靠，特别是在临床、监管和安全关键决策支持系统中。

Method: 提出Logic Sketch Prompting (LSP)框架，包含类型变量、确定性条件评估器和基于规则的验证器，能够产生可追踪和可重复的输出。在两个药理学逻辑合规任务上，将LSP与零样本提示、思维链提示和简洁提示进行比较，测试了Gemma 2、Mistral和Llama 3三个开源模型。

Result: 在所有模型和任务中，LSP始终获得最高的准确率（0.83-0.89）和F1分数（0.83-0.89），显著优于零样本提示（0.24-0.60）、简洁提示（0.16-0.30）和思维链提示（0.56-0.75）。McNemar检验显示LSP在几乎所有比较中都有统计学显著提升（p<0.01）。

Conclusion: LSP在不牺牲性能的情况下提高了确定性、可解释性和一致性，支持其在临床、监管和安全关键决策支持系统中的使用。

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [42] [Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback](https://arxiv.org/abs/2512.22336)
*Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo*

Main category: cs.AI

TL;DR: Agent2World是一个工具增强的多智能体框架，通过多智能体反馈实现推理时世界模型生成，并作为监督微调的数据引擎，显著提升世界模型生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前训练LLMs生成符号世界模型（如PDDL领域或可执行模拟器）受到大规模可验证监督数据缺乏的限制，现有方法主要依赖静态验证方法，无法捕捉交互执行中出现的行为级错误。

Method: 提出Agent2World三阶段流水线：1) Deep Researcher智能体通过网页搜索进行知识合成以填补规范缺口；2) Model Developer智能体实现可执行世界模型；3) 专门的Testing Team进行自适应单元测试和基于模拟的验证。

Result: 在三个涵盖PDDL和可执行代码表示的基准测试中展示了优越的推理时性能，达到一致的SOTA结果。通过Testing Team提供的交互式环境生成多轮训练轨迹，基于这些轨迹微调的模型使世界模型生成平均相对提升30.95%。

Conclusion: Agent2World不仅实现了强大的推理时世界模型生成，还作为监督微调的数据引擎，通过多智能体反馈机制显著提升了世界模型生成的质量和可靠性。

Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.

</details>


### [43] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 将带控制参数的数字规划问题转化为简单数字任务，使传统启发式方法能处理无限动作空间


<details>
  <summary>Details</summary>
Motivation: 带控制参数的数字规划引入了自由数字变量，导致状态中可能存在无限数量的适用动作，使得现成的数字启发式方法不可行

Method: 识别可控简单数字问题子集，采用乐观编译方法将其转化为简单数字任务，将控制相关表达式抽象为有界常数效应和宽松前提条件

Result: 提出的编译方法使得子目标启发式能够有效估计涉及控制参数的数字规划问题的目标距离，该方法有效且计算可行

Conclusion: 该方法是将传统数字启发式应用于具有无限可能动作场景的有效方式，推动了当前技术水平的边界

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [44] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: 该论文针对材料科学领域AI生成内容中的幻觉问题，提出了HalluMatData基准数据集和HalluMatDetector多阶段检测框架，通过多种验证方法将幻觉率降低了30%，并引入PHCS指标量化模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学发现中面临幻觉问题，即生成事实错误或误导性信息，这严重影响了研究完整性。特别是在材料科学领域，AI生成内容的幻觉问题尚未得到充分研究，需要专门的评估工具和方法来确保AI辅助研究的可靠性。

Method: 1. 创建HalluMatData基准数据集，用于评估幻觉检测方法、事实一致性和响应鲁棒性；2. 提出HalluMatDetector多阶段幻觉检测框架，整合内在验证、多源检索、矛盾图分析和基于指标的评估；3. 引入Paraphrased Hallucination Consistency Score (PHCS)指标，通过语义等价查询量化LLM响应中的不一致性。

Result: 1. 发现材料科学不同子领域的幻觉水平存在显著差异，高熵查询表现出更大的事实不一致性；2. 使用HalluMatDetector验证流程，将幻觉率比标准LLM输出降低了30%；3. PHCS指标能够有效揭示模型在语义等价查询下的响应不一致性，为模型可靠性提供了更深入的洞察。

Conclusion: 该研究为解决材料科学领域AI生成内容的幻觉问题提供了系统性的解决方案，包括基准数据集、检测框架和量化指标。这些工具和方法有助于提高AI辅助材料科学研究的可靠性和完整性，为科学发现中的AI应用提供了重要的质量保证机制。

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [45] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: GatedBias：一个轻量级推理时个性化框架，通过结构门控适配将冻结的知识图谱嵌入适应到个体用户上下文，仅需约300个可训练参数，在保持全局准确性的同时显著提升个性化排名性能。


<details>
  <summary>Details</summary>
Motivation: 知识图谱基础模型在链接预测方面具有强大的群体级性能，但无法捕捉个体用户偏好，这是通用关系推理与个性化排名之间的关键脱节。需要一种既能保持全局准确性又能实现个性化适配的解决方案。

Method: 提出GatedBias框架，采用结构门控适配：结合特定用户特征和图导出的二元门控，生成可解释的每实体偏置。该方法仅需约300个可训练参数，在推理时适配冻结的知识图谱嵌入，无需重新训练。

Result: 在两个基准数据集（Amazon-Book和Last-FM）上评估，显示在保持群体性能的同时，对齐指标有统计显著提升。反事实扰动实验验证了因果响应性：受益于特定偏好信号的实体在信号增强时排名改进提高6-30倍。

Conclusion: 基础模型的个性化适配可以是参数高效且因果可验证的，能够桥接通用知识表示与个体用户需求，为知识图谱个性化提供了轻量级解决方案。

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [46] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: 论文提出Monadic Context Engineering (MCE)架构范式，利用函子、应用函子和单子的代数结构为AI智能体设计提供形式化基础，解决现有智能体架构在状态管理、错误处理和并发方面的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型驱动的自主智能体架构通常采用命令式、临时性的模式构建，导致系统脆弱，存在状态管理困难、错误处理不足和并发问题等挑战。

Method: 引入Monadic Context Engineering (MCE)范式，将智能体工作流视为计算上下文，利用函子、应用函子和单子的代数特性来内在管理状态传播、错误处理短路和异步执行等横切关注点。特别使用单子变换器实现这些能力的系统组合。

Result: MCE使开发者能够从简单、可独立验证的组件构建复杂、健壮且高效的AI智能体。该框架还扩展到元智能体，通过元编程动态创建和管理子智能体工作流。

Conclusion: MCE为AI智能体设计提供了形式化基础，通过代数结构解决了现有架构的脆弱性问题，支持构建复杂、可组合的智能体系统，并可通过元编程实现更高级的生成式编排。

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [47] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: 该研究提出了DarkPatterns-LLM基准数据集和诊断框架，用于细粒度评估大语言模型输出中的操纵性内容，涵盖七类危害，并发现现有模型在检测自主性破坏模式方面存在显著弱点。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，操纵性或欺骗性行为日益引发担忧，可能损害用户自主性、信任和福祉。现有安全基准主要依赖粗糙的二元标签，无法捕捉构成操纵的微妙心理和社会机制。

Method: 提出了DarkPatterns-LLM基准数据集和诊断框架，包含401个精心策划的指令-响应对和专家标注。框架采用四层分析管道：多粒度检测、多尺度意图分析、威胁协调协议和深度上下文风险对齐，涵盖七类危害：法律/权力、心理、情感、身体、自主性、经济和社会危害。

Result: 评估了GPT-4、Claude 3.5和LLaMA-3-70B等先进模型，发现性能存在显著差异（65.2%-89.7%），且在检测自主性破坏模式方面存在一致的弱点。

Conclusion: DarkPatterns-LLM建立了首个标准化、多维度的LLM操纵检测基准，为构建更可信的AI系统提供了可操作的诊断工具。

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [48] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: Tyee是一个用于智能生理医疗的统一、模块化、可配置工具包，解决了深度学习在生理信号分析中的数据格式异构、预处理不一致、模型管道碎片化和实验不可复现等问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生理信号分析中面临数据格式异构、预处理策略不一致、模型管道碎片化以及实验设置不可复现等挑战，这些限制了该领域的进展。

Method: Tyee工具包包含三个关键创新：1）统一数据接口和可配置预处理管道，支持12种信号模态；2）模块化和可扩展架构，支持灵活集成和快速原型开发；3）端到端工作流配置，促进可复现和可扩展的实验。

Result: Tyee在所有评估任务中表现出一致的实用效果和泛化能力，在13个数据集中的12个上取得了最先进的结果，优于或匹配基线方法。

Conclusion: Tyee为智能生理医疗提供了一个有效的统一工具包，解决了该领域的关键挑战，并已在GitHub上开源发布并持续维护。

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [49] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: M³ob：利用多模态时空知识增强位置推荐的泛化能力，通过构建统一的时空关系图和跨模态对齐机制解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的人类移动预测方法存在泛化能力有限的问题：单模态方法受数据稀疏性和固有偏差限制，多模态方法难以有效捕捉静态多模态表示与时空动态之间的语义鸿沟。

Method: 1. 构建统一的时空关系图（STRG），利用LLM增强的时空知识图（STKG）捕获功能语义和时空知识；2. 设计门控机制融合不同模态的时空图表示；3. 提出STKG引导的跨模态对齐，将时空动态知识注入静态图像模态。

Result: 在六个公共数据集上的实验表明，该方法不仅在正常场景下取得一致改进，在异常场景中也展现出显著的泛化能力。

Conclusion: 通过利用多模态时空知识来表征移动动态，M³ob方法有效提升了位置推荐的准确性和泛化能力，特别是在处理异常场景时表现出色。

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [50] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: LLM相互审议可提升预测准确性，但仅在多样化模型组有效，同质模型组无改善，额外信息无帮助


<details>
  <summary>Details</summary>
Motivation: 研究结构化审议是否也能提升大型语言模型的预测准确性，类似人类预测中的效果

Method: 使用202个已解决的二元问题，测试GPT-5、Claude Sonnet 4.5、Gemini Pro 2.5在四种场景下的预测：多样化模型分布式信息、多样化模型共享信息、同质模型分布式信息、同质模型共享信息

Result: 审议显著提升多样化模型共享信息场景的准确性（Log Loss降低0.020，约4%，p=0.017），但同质模型组无改善，额外信息未提升准确性

Conclusion: 审议可能是提升LLM预测准确性的可行策略，但效果受模型多样性和信息结构影响

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [51] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: DICE是一个用于RAG系统评估的两阶段框架，通过证据耦合的深度分析和概率评分提供可解释、鲁棒的评估，采用瑞士制锦标赛提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估指标存在可解释性有限、不确定性量化不足、多系统比较计算效率低等问题，阻碍了RAG技术的负责任部署。

Method: DICE采用两阶段框架：1）证据耦合的深度分析推理；2）概率{A, B, Tie}评分，生成透明、置信度感知的判断。使用瑞士制锦标赛将计算复杂度从O(N²)降低到O(N log N)。

Result: 在中文金融QA数据集上，DICE与人类专家达成85.7%的一致性，显著优于RAGAS等现有LLM评估指标。瑞士制锦标赛在八系统评估中减少42.9%计算量，同时保持排名保真度。

Conclusion: DICE建立了一个负责任、可解释且高效的RAG系统评估范式，支持可信赖的RAG技术部署。

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [52] [Memento-II: Learning by Stateful Reflective Memory](https://arxiv.org/abs/2512.22716)
*Jun Wang*

Main category: cs.AI

TL;DR: 提出一个理论框架，将情景记忆与强化学习结合，使大语言模型智能体能够通过反思机制进行持续和体验式学习，无需反向传播或模型微调。


<details>
  <summary>Details</summary>
Motivation: 传统方法在训练和部署之间存在严格分离，需要参数更新（如反向传播或微调）才能适应新环境。本文旨在建立一个框架，使语言模型智能体能够通过交互式体验进行持续学习，无需修改模型参数。

Method: 引入状态化反思决策过程，将反思学习建模为与情景记忆的两阶段读写交互。写入阶段存储交互结果（对应策略评估），读取阶段检索相关过往案例（对应策略改进）。该过程在增强的状态-记忆表示上诱导出等价的马尔可夫决策过程。

Result: 该框架使用熵正则化策略迭代实例化，并建立了收敛保证。当情景记忆增长并充分覆盖状态空间时，所得策略收敛到最优解。

Conclusion: 为基于记忆增强和检索的语言模型智能体提供了理论基础，使其能够在不更新参数的情况下实现持续适应，打破了传统训练与部署的界限。

Abstract: We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.

</details>


### [53] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: HiSciBench是一个分层科学智能基准测试，包含5个层次、6大学科、8735个实例，用于评估大语言模型和多模态基础模型在完整科学工作流程中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有科学智能基准测试过于碎片化，专注于狭窄任务，无法反映真实科学探究的层次性和多学科性质。需要一个新的基准来全面评估模型在完整科学工作流程中的能力。

Method: 设计了HiSciBench分层基准，包含5个层次：科学素养(L1)、文献解析(L2)、基于文献的问答(L3)、文献综述生成(L4)和科学发现(L5)。涵盖6大学科，支持多模态输入和跨语言评估。

Result: 对GPT-5、DeepSeek-R1等领先模型的评估显示：在基础素养任务上准确率可达69%，但在发现级挑战上性能急剧下降至25%，揭示了模型在不同科学推理阶段的显著能力差距。

Conclusion: HiSciBench为评估科学智能设立了新标准，提供了开发更强大、更可靠模型的可操作见解，并将公开发布以促进未来研究。

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [54] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: Gamma模型通过多头几何注意力机制改进知识图谱推理，使用多种代数变换（实数、复数、分裂复数、对偶数）替代单一关系变换，并通过注意力融合机制自适应选择最适合的关系偏置，在零样本归纳链接预测中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱基础模型（如Ultra）依赖单一关系变换（如逐元素乘法）进行消息传递，这限制了表达能力，无法捕捉多样化图谱中不同的关系和结构模式。

Method: 提出Gamma模型，引入多头几何注意力机制：1）使用多种并行代数变换（实数、复数、分裂复数、对偶数）建模不同关系结构；2）通过关系条件注意力融合机制，使用轻量级门控和熵正则化在链接级别自适应融合这些变换。

Result: 在56个多样化知识图谱上的综合实验表明，Gamma在零样本归纳链接预测中始终优于Ultra：在归纳基准测试中平均倒数排名提升5.5%，在所有基准测试中提升4.4%，证明了互补几何表示的优势。

Conclusion: Gamma通过多头几何注意力机制增强了知识图谱推理的表达能力，多种代数变换的组合超越了任何单一空间的表达能力，为知识图谱基础模型提供了更强大的关系建模能力。

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [55] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: 研究发现，在K-12教育中，基于大语言模型（LLM）的辅导系统在评估学生知识变化方面存在准确性和时间一致性问题，无法替代传统的深度知识追踪（DKT）模型，需要混合框架来确保负责任的教学设计。


<details>
  <summary>Details</summary>
Motivation: 针对K-12教育中普遍存在的误解——认为生成式模型可以替代传统的学习者建模进行自适应教学，特别是在欧盟AI法案将K-12教育列为高风险领域、要求负责任设计的背景下，本研究旨在探讨LLM辅导系统的局限性。

Method: 使用大型开放数据集，比较深度知识追踪（DKT）模型与广泛使用的LLM（包括零样本和微调版本），评估它们在预测学生下一步正确性方面的准确性、可靠性和时间一致性。同时进行时间分析和多技能掌握度估计的定性分析。

Result: DKT在下一步正确性预测方面达到最高区分性能（AUC = 0.83），在所有设置中始终优于LLM。虽然微调使LLM的AUC比零样本基线提高了约8%，但仍比DKT低6%，且在序列早期产生更多错误预测。时间分析显示DKT保持稳定、方向正确的掌握度更新，而LLM变体表现出显著的时间弱点，包括不一致和错误方向的更新。

Conclusion: LLM单独使用不太可能达到已建立的智能辅导系统的效果，负责任的教学需要结合学习者建模的混合框架。尽管微调LLM需要近198小时的高计算训练，但其性能仍不及计算需求更低的DKT模型。

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [56] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: ChexReason是一个使用有限资源（2000个SFT样本、1000个RL样本、单个A100 GPU）训练的视觉语言模型，通过R1风格方法（SFT+GRPO）应用于医学影像。研究发现GRPO能提升分布内性能但损害跨数据集泛化能力，表明监督微调可能比强化学习更适合临床部署。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型的强化学习在推理任务上取得进展，但在资源受限的医学影像应用中仍未被充分探索。研究者希望了解在有限计算资源下，强化学习方法对医学影像视觉语言模型性能的影响。

Method: 提出ChexReason模型，采用R1风格训练方法：先进行监督微调（SFT），然后使用GRPO（Group Relative Policy Optimization）进行强化学习。整个训练仅使用2000个SFT样本、1000个RL样本和单个A100 GPU。在CheXpert和NIH基准上进行评估。

Result: GRPO能恢复分布内性能（CheXpert上提升23%，macro-F1=0.346），但损害跨数据集迁移能力（NIH上下降19%）。这一现象与高资源模型NV-Reason-CXR-3B相似，表明问题源于RL范式而非模型规模。SFT检查点在优化前能独特地改善NIH性能，表明教师引导的推理能捕捉更多机构无关特征。

Conclusion: 结构化推理框架对通用视觉语言模型有益，但对医学预训练模型增益有限。对于需要跨不同人群鲁棒性的临床部署，精心策划的监督微调可能优于激进的强化学习。

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [57] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: 本文提出了Intrinsic Self-reflective Preference Optimization (q)方法，解决了DPO及其变体的两个根本限制：最优策略对建模选择的依赖性和缺乏利用成对数据中比较信息的能力。


<details>
  <summary>Details</summary>
Motivation: DPO及其变体虽然简单且离线稳定，但存在两个根本限制：1) 最优策略依赖于任意的建模选择（标量化函数、参考策略），导致行为反映参数化伪影而非真实偏好；2) 孤立处理响应生成未能利用成对数据中的比较信息，未开发模型的内在自我反思能力。

Method: 提出了Intrinsic Self-reflective Preference Optimization (q)方法，推导出基于上下文和替代响应的全局最优策略。该方法作为即插即用的增强，无需架构更改或推理开销，同时保证对标量化和参考选择的不变性。

Result: 实验表明，q方法在胜率和长度控制指标上均取得一致改进，验证了释放自我反思能力可以产生更稳健、更符合人类对齐的LLMs。

Conclusion: q方法通过利用成对数据中的比较信息并消除对建模选择的依赖，解决了DPO的根本限制，实现了更有效的语言模型对齐。

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [58] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: 本文认为当前评估AI系统情感智能的框架需要改进，因为它们未能全面衡量AI相关的EI各个方面。作者通过分析情感理论和现有评估框架，提出了改进AI系统EI评估的策略。


<details>
  <summary>Details</summary>
Motivation: 当前评估人工智能系统情感智能的框架存在不足，未能充分或全面地衡量与AI相关的EI各个方面。人类EI包含AI系统缺乏的现象学成分和理解感，但EI也包含感知情绪状态、解释、适当响应和适应新环境的能力，这些是AI系统可以不同程度实现的。

Method: 首先回顾不同的情感理论和一般EI理论，评估每种理论在人工系统中的适用性；然后批判性地评估现有的基准框架，根据第一部分建立的EI理论识别每个框架的不足；最后提出改进评估策略的方案。

Result: 识别了现有EI评估框架的局限性，特别是它们缺乏对情感本质和情感智能的坚实基础。通过分析情感理论在AI系统中的适用性，为改进评估提供了理论基础。

Conclusion: 需要改进AI系统情感智能的评估策略，以克服现有框架的不足。未来的评估应该基于对情感本质和AI系统能力的更深入理解，开发更全面、适用的评估方法。

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [59] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: 本文提出"模型信念"概念，利用LLM的token级概率分布来替代传统的"模型选择"输出，证明模型信念具有更高的统计效率，能更有效地利用LLM生成数据的信息。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM模拟人类行为时，通常将LLM的输出（"模型选择"）视为单个数据点，这种做法未能充分利用LLM固有的概率特性所包含的信息，导致数据使用效率低下。

Method: 提出并形式化"模型信念"概念，该度量从LLM的token级概率分布中推导得出，能够捕捉模型在单次生成运行中对选择替代方案的信念分布。证明模型信念与模型选择的均值渐近等价，但具有更低的方差和更快的收敛速度。

Result: 在需求估计研究中，模型信念在有限运行次数下比模型选择本身更好地解释和预测真实模型选择，将计算需求减少约20倍以达到足够准确的估计。模型信念在下游应用中平滑函数的类似性质也得到了验证。

Conclusion: 模型信念应作为默认度量来从LLM生成数据中提取更多信息，因为它提供了更高效的统计估计器，显著减少了计算需求并提高了数据利用效率。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [60] [Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control](https://arxiv.org/abs/2512.23292)
*Yoonpyo Lee,Kazuma Kobayashi,Sai Puppala,Sajedul Talukder,Seid Koric,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.AI

TL;DR: 论文提出了一种新的物理AI范式：基于物理验证而非感知推理的Agentic Physical AI，通过360M参数模型在反应堆控制场景中训练，实现了从高方差模仿到稳定执行的相变，模型能自主选择最优策略并跨物理领域迁移。


<details>
  <summary>Details</summary>
Motivation: 当前通用基础模型在物理系统控制中存在根本性障碍，即使前沿视觉语言模型在基础物理任务上准确率也只有50-53%，表现为近似猜测器，保持语义合理性但违反物理约束。这种输入不忠实性不是缩放缺陷而是结构限制，感知中心架构优化参数空间模仿，而安全关键控制需要执行动作的结果空间保证。

Method: 提出Agentic Physical AI范式，使用360M参数的紧凑语言模型，通过物理验证驱动策略优化而非感知推理。在合成反应堆控制场景上训练，数据集规模从10^3扩展到10^5示例。模型在四种执行器家族中平衡暴露，但能自主选择策略。

Result: 模型表现出尖锐的相变：小规模系统呈现高方差模仿和灾难性尾部风险，大规模模型经历超过500倍的方差崩溃，稳定执行级行为。尽管平衡暴露于四种执行器家族，模型自主拒绝约70%的训练分布，并将95%运行时执行集中在单一银行策略上。学习表示无需架构修改即可跨不同物理和连续输入模态迁移。

Conclusion: Agentic Physical AI提供了一种不同于通用基础模型的领域特定基础模型路径，通过物理验证驱动的策略优化实现了从感知推理到执行保证的范式转变，在安全关键物理系统控制中具有重要应用前景。

Abstract: The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.

</details>


### [61] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: 论文研究了规划与验证领域中的两个问题：一致性规划与超属性模型检测，证明两者之间存在紧密联系并可相互转化。


<details>
  <summary>Details</summary>
Motivation: 探索规划社区中的一致性规划问题与验证社区中的超属性模型检测问题之间的理论联系，这两个领域虽然研究相似问题但通常独立发展。

Method: 1. 将超属性模型检测实例高效地约简为一致性规划实例，并证明编码的正确性和完备性；2. 证明每个一致性规划问题本身就是一个超属性模型检测任务。

Result: 建立了超属性模型检测与一致性规划之间的双向对应关系：一方面可以将超属性模型检测问题转化为一致性规划问题，另一方面每个一致性规划问题都可以视为超属性模型检测任务。

Conclusion: 一致性规划与超属性模型检测在本质上密切相关，这种联系为两个领域提供了新的理论视角和潜在的交叉应用可能性。

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [62] [CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations](https://arxiv.org/abs/2512.23328)
*Huan-ang Gao,Zikang Zhang,Tianwei Luo,Kaisen Yang,Xinzhe Juan,Jiahao Qiu,Tianxing Chen,Bingxiang He,Hao Zhao,Hao Zhou,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: CubeBench：基于魔方的基准测试揭示LLM智能体在物理世界部署中的空间认知缺陷，特别是长期规划能力完全失败


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在数字领域表现出色，但在物理世界部署中存在显著差距，主要挑战在于形成和维护稳健的空间心理模型。研究者识别出三个阻碍这一转变的核心认知挑战：空间推理、通过心理模拟进行长期状态跟踪，以及在部分观察下的主动探索。

Method: 引入CubeBench，一个基于魔方的新型生成基准测试。采用三层诊断框架：从具有完整符号信息的基础状态跟踪，到仅具有部分视觉数据的主动探索。通过为领先的LLM提供外部求解器工具来隔离认知瓶颈。

Result: 实验显示领先LLM存在严重局限性，在所有长期任务中通过率均为0.00%，暴露了长期规划的根本性失败。通过分析失败模式，为开发更物理基础的智能体提供关键见解。

Conclusion: CubeBench有效揭示了LLM智能体在物理世界部署中的认知缺陷，特别是长期规划能力的根本性不足。该基准测试和诊断框架为开发更物理基础的智能体提供了重要指导。

Abstract: Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.

</details>


### [63] [AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis](https://arxiv.org/abs/2512.23424)
*Jinye Du,Quan Yuan,Zuyao Zhang,Yanzhi Yi,Jiahui Hu,Wangyi Chen,Yiyang Zhu,Qishui Zheng,Wenxiang Zou,Xiangyu Chang,Zuohe Zheng,Zichun Ye,Chao Liu,Shanni Li,Renwei Zhang,Yiping Deng,Xinwei Hu,Xuefeng Jin,Jie Zhao*

Main category: cs.AI

TL;DR: AKG kernel agent是一个多智能体系统，利用LLM代码生成能力自动化AI计算内核的开发、迁移和性能调优，支持多种DSL和硬件后端，在KernelBench上相比PyTorch Eager实现平均加速1.46倍。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型对高性能计算内核需求激增，但LLM、多模态架构、推荐系统等复杂模型结合稀疏化、量化等技术带来了巨大计算挑战。频繁的硬件更新和多样化的芯片架构需要为每个平台定制内核实现，手动优化无法满足需求，成为AI系统开发的关键瓶颈。

Method: 提出AKG kernel agent（AI驱动的内核生成器），这是一个多智能体系统，利用LLM代码生成能力自动化内核生成、迁移和性能调优。系统支持多种领域特定语言（DSL），包括Triton、TileLang、CPP和CUDA-C，能够针对不同硬件后端，同时保持正确性和可移植性。模块化设计允许快速集成新的DSL和硬件目标。

Result: 在KernelBench上使用Triton DSL在GPU和NPU后端进行评估，AKG kernel agent相比PyTorch Eager基线实现平均加速1.46倍，证明了其在加速现代AI工作负载内核开发方面的有效性。

Conclusion: AKG kernel agent通过自动化内核开发流程，有效解决了AI系统开发中的计算内核优化瓶颈问题，展示了LLM代码生成能力在硬件加速器编程中的实际应用价值，为应对日益复杂的AI计算需求提供了可行的解决方案。

Abstract: Modern AI models demand high-performance computation kernels. The growing complexity of LLMs, multimodal architectures, and recommendation systems, combined with techniques like sparsity and quantization, creates significant computational challenges. Moreover, frequent hardware updates and diverse chip architectures further complicate this landscape, requiring tailored kernel implementations for each platform. However, manual optimization cannot keep pace with these demands, creating a critical bottleneck in AI system development. Recent advances in LLM code generation capabilities have opened new possibilities for automating kernel development. In this work, we propose AKG kernel agent (AI-driven Kernel Generator), a multi-agent system that automates kernel generation, migration, and performance tuning. AKG kernel agent is designed to support multiple domain-specific languages (DSLs), including Triton, TileLang, CPP, and CUDA-C, enabling it to target different hardware backends while maintaining correctness and portability. The system's modular design allows rapid integration of new DSLs and hardware targets. When evaluated on KernelBench using Triton DSL across GPU and NPU backends, AKG kernel agent achieves an average speedup of 1.46$\times$ over PyTorch Eager baselines implementations, demonstrating its effectiveness in accelerating kernel development for modern AI workloads.

</details>


### [64] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: HiR是一个高效的强化学习框架，通过"选择-重写"策略将失败尝试重放为成功样本，解决复杂指令跟随任务中的稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在复杂指令跟随任务中面临挑战，因为初始模型难以生成满足所有约束的高质量响应，导致稀疏或难以区分的奖励信号，阻碍学习效率。

Method: 提出Hindsight instruction Replay (HiR)框架，采用选择-重写策略：1）选择失败尝试中已满足的部分约束；2）重写这些尝试为成功样本；3）在原始样本和重放样本上进行RL训练，理论框架为双偏好学习（指令级和响应级）。

Result: 大量实验表明HiR在不同指令跟随任务中取得显著效果，同时需要更少的计算资源。代码和数据集已开源。

Conclusion: HiR通过后见之明重放策略有效解决了复杂指令跟随任务中的稀疏奖励问题，提高了RL训练效率和效果。

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [65] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: 提出CreativeDC方法，通过两阶段提示法解决LLM生成教育问题时存在的"人工蜂群思维"效应，提高问题生成的多样性和新颖性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育问题生成方面潜力巨大，但存在"人工蜂群思维"效应，导致同一模型内生成相似响应，不同模型间产生同质化输出，使学生暴露于过度相似和重复的问题中，损害思维多样性。

Method: 提出CreativeDC两阶段提示方法，受Wallas创造力理论和Guilford发散-收敛思维框架启发，将LLM推理明确分为不同阶段，通过解耦创意探索和约束满足，使LLM在确定最终问题前探索更广泛的想法空间。

Result: 评估显示CreativeDC在多样性、新颖性和实用性方面显著优于基线方法，同时保持高实用性。扩展分析表明，随着采样增加，CreativeDC生成的有效不同问题数量更多，增长速度更快。

Conclusion: CreativeDC方法能有效解决LLM生成教育问题时的同质化问题，显著提高问题多样性和新颖性，为教育材料的大规模生成提供了更有效的解决方案。

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [66] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: 提出I-PERI算法解决联邦因果发现中的客户端未知干预问题，通过恢复客户端图并集的CPDAG，再利用干预引起的结构差异定向边，得到更紧的Φ-Markov等价类


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果发现方法通常假设所有客户端共享相同的因果模型，这在现实中不成立，因为客户端特定策略或协议（如医院间差异）会引入异质且未知的干预

Method: 提出I-PERI算法：1) 恢复客户端图并集的CPDAG；2) 利用干预引起的跨客户端结构差异定向边，得到Φ-CPDAG表示的Φ-Markov等价类

Result: 提供了I-PERI算法的收敛性理论保证和隐私保护特性证明，并在合成数据上进行了实证评估，展示了算法的有效性

Conclusion: I-PERI算法能够处理联邦因果发现中的客户端未知干预问题，通过利用干预引起的结构差异获得更紧的等价类，为实际应用提供了更实用的解决方案

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>
