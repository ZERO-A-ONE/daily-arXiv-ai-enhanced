<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Achieving Productivity Gains with AI-based IDE features: A Journey at Google](https://arxiv.org/abs/2601.19964)
*Maxim Tabachnyk,Xu Shu,Alexander Frömmgen,Pavel Sychev,Vahid Meimand,Ilia Krets,Stanislav Pyatykh,Abner Araujo,Kristóf Molnár,Satish Chandra*

Main category: cs.SE

TL;DR: Google开发AI IDE功能（代码补全和自然语言代码转换）的经验总结，关注延迟、用户体验和代码质量优化


<details>
  <summary>Details</summary>
Motivation: 在IDE中集成AI功能以提升开发者生产力，但需要解决延迟、用户体验和代码质量等实际挑战

Method: 通过用户界面、后端和模型层的综合优化，结合严格的实验验证，改进代码补全和自然语言代码转换功能

Result: 成功开发出能够提供实质性生产力提升的AI开发者工具，在企业环境中验证了其价值

Conclusion: AI开发者工具的优化需要跨多个层面的系统性改进，通过实验驱动的方法可以在企业环境中实现显著的生产力提升

Abstract: We discuss Google's journey in developing and refining two internal AI-based IDE features: code completion and natural-language-driven code transformation (Transform Code). We address challenges in latency, user experience and suggestion quality, all backed by rigorous experimentation. The article serves as an example of how to refine AI developer tools across the user interface, backend, and model layers, to deliver tangible productivity improvements in an enterprise setting.

</details>


### [2] [Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis](https://arxiv.org/abs/2601.20103)
*Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.SE

TL;DR: 论文提出TRACE基准测试，用于评估LLM在代码生成强化学习环境中检测奖励攻击的能力，发现对比性异常检测比孤立分类更有效，GPT-5.2在对比设置中检测率从45%提升到63%。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成强化学习的发展，需要稳健的环境来防止奖励攻击。LLM越来越多地作为代码RL中的评估器，但它们检测奖励攻击的能力尚未得到充分研究。现有工作在孤立分类场景中评估奖励攻击检测，缺乏更现实的评估设置。

Method: 提出包含54个类别的奖励攻击分类法，创建TRACE基准测试（包含517个测试轨迹）。对比孤立分类设置与更现实的对比性异常检测设置。使用GPT-5.2等模型进行实验，分析语义与句法上下文奖励攻击的差异，并进行定性分析和消融研究。

Result: 模型在对比性设置中比孤立分类设置更能有效捕获奖励攻击，GPT-5.2在对比设置中检测率达到63%（孤立设置为45%）。最先进模型在处理语义上下文奖励攻击时比句法上下文奖励攻击困难更大。良性与被攻击轨迹比例和分析簇大小显著影响检测性能。

Conclusion: 对比性异常检测是评估LLM奖励攻击检测能力的更现实方法。TRACE基准测试为社区提供了评估工具，揭示了当前模型在检测语义复杂奖励攻击方面的局限性，为未来研究提供了方向。

Abstract: Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.

</details>


### [3] [Usage, Effects and Requirements for AI Coding Assistants in the Enterprise: An Empirical Study](https://arxiv.org/abs/2601.20112)
*Maja Vukovic,Rangeet Pan,Tin Kam Ho,Rahul Krishna,Raju Pavuluri,Michele Merler*

Main category: cs.SE

TL;DR: 本文通过调查57名开发者和分析35份用户调研，探讨了AI编程助手和代码大语言模型在企业环境中的实际应用情况、对软件开发流程的影响以及用户需求。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程任务中的广泛应用，需要评估AI编程助手和代码LLM是否适合企业级项目，以及它们对现有软件开发流程和用户体验的影响。

Method: 1. 调查了57名来自不同领域、具有不同软件工程技能的开发者，了解他们使用AI编程助手和代码LLM的经验；2. 分析了35份关于专业人士和学生使用AI编程助手和代码LLM的用户调研报告。

Result: 基于研究结果和现有调研分析，本文讨论了AI驱动的编程助手的需求和期望，为实际应用提供了实证依据。

Conclusion: 本文通过实证研究为AI编程助手在企业环境中的适用性评估提供了重要参考，并指出了未来发展的需求和方向。

Abstract: The rise of large language models (LLMs) has accelerated the development of automated techniques and tools for supporting various software engineering tasks, e.g., program understanding, code generation, software testing, and program repair. As CodeLLMs are being employed toward automating these tasks, one question that arises, especially in enterprise settings, is whether these coding assistants and the code LLMs that power them are ready for real-world projects and enterprise use cases, and how do they impact the existing software engineering process and user experience. In this paper we survey 57 developers from different domains and with varying software engineering skill about their experience with AI coding assistants and CodeLLMs. We also reviewed 35 user surveys on the usage, experience and expectations of professionals and students using AI coding assistants and CodeLLMs. Based on our study findings and analysis of existing surveys, we discuss the requirements for AI-powered coding assistants.

</details>


### [4] [Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization](https://arxiv.org/abs/2601.20147)
*Saima Afrin,Zaiyu Cheng,Tushar Sharma,Alexander Serebrenik,Massimiliano Di Penta,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 本文系统评估了系统提示对指令调优语言模型在代码生成任务中性能的影响，发现模型规模越大，系统提示的影响越显著；少样本提示能减少这种影响；不同编程语言对提示变化的敏感性不同。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优语言模型在代码生成方面表现出色，但系统提示对通用ILMs和专用CLMs性能的影响尚未得到充分研究。本研究旨在填补这一空白，系统评估系统提示对代码生成任务的影响。

Method: 通过构建包含120种模型配置的评估框架，系统研究系统提示的详细程度、模型规模、提示策略（零样本vs少样本）和编程语言（Python vs Java）对ILMs和CLMs在代码生成任务中的影响。

Result: 研究发现：(1) 系统提示的影响随模型规模增大而增强；(2) 与零样本相比，少样本提示能减少系统提示的影响；(3) 编程语言影响敏感性，Java比Python对系统提示变化更敏感。

Conclusion: 系统提示对指令调优语言模型的代码生成性能有显著影响，这种影响受模型规模、提示策略和编程语言的调节。这为优化代码生成模型的提示设计提供了重要指导。

Abstract: Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.

</details>


### [5] [LogSieve: Task-Aware CI Log Reduction for Sustainable LLM-Based Analysis](https://arxiv.org/abs/2601.20148)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: LogSieve是一个轻量级的日志缩减技术，专门针对CI工作流中的非结构化、嘈杂日志，通过语义感知过滤保留关键信息，平均减少42%行数和40%token数，同时保持高语义保真度。


<details>
  <summary>Details</summary>
Motivation: CI日志对于理解构建失败和性能回归至关重要，但其不断增长的体积和冗长性使得手动检查和自动分析成本高昂、耗时且环境代价高。现有工作主要针对结构化系统日志，而非CI工作流中典型的非结构化、嘈杂和冗长日志。

Method: LogSieve是一种轻量级、根因分析感知且语义保留的日志缩减技术，通过过滤低信息行同时保留与下游推理相关的内容。使用基于嵌入的分类器自动检测相关性，实现语义感知过滤。

Result: 在20个开源Android项目的GitHub Actions CI日志上评估，LogSieve平均减少42%行数和40%token数，语义损失最小。相比结构优先基线（LogZip和随机行移除），保持了更高的语义和分类保真度（余弦相似度0.93，GPTScore 0.93，80%精确匹配准确率）。

Conclusion: LogSieve桥接了日志管理和LLM推理，通过预推断缩减降低计算成本，可比例减少能源使用和排放，为更绿色、更可解释的CI自动化提供了实用路径。

Abstract: Logs are essential for understanding Continuous Integration (CI) behavior, particularly for diagnosing build failures and performance regressions. Yet their growing volume and verbosity make both manual inspection and automated analysis increasingly costly, time-consuming, and environmentally costly. While prior work has explored log compression, anomaly detection, and LLM-based log analysis, most efforts target structured system logs rather than the unstructured, noisy, and verbose logs typical of CI workflows.
  We present LogSieve, a lightweight, RCA-aware and semantics-preserving log reduction technique that filters low-information lines while retaining content relevant to downstream reasoning. Evaluated on CI logs from 20 open-source Android projects using GitHub Actions, LogSieve achieves an average 42% reduction in lines and 40% reduction in tokens with minimal semantic loss. This pre-inference reduction lowers computational cost and can proportionally reduce energy use (and associated emissions) by decreasing the volume of data processed during LLM inference.
  Compared with structure-first baselines (LogZip and random-line removal), LogSieve preserves much higher semantic and categorical fidelity (Cosine = 0.93, GPTScore = 0.93, 80% exact-match accuracy). Embedding-based classifiers automate relevance detection with near-human accuracy (97%), enabling scalable and sustainable integration of semantics-aware filtering into CI workflows. LogSieve thus bridges log management and LLM reasoning, offering a practical path toward greener and more interpretable CI automation.

</details>


### [6] [Control Models for In-IDE Code Completion](https://arxiv.org/abs/2601.20223)
*Aral de Moor,Yana Hrynevich,Hleb Badzeika,Vladyslav Furda,Marko Kojic,Artem Savelev,Kostadin Cvejoski,Darya Rovdo,Ekaterina Garanina*

Main category: cs.SE

TL;DR: 该研究为JetBrains IDE中的LLM代码补全引入了控制模型，通过机器学习分类器触发推理并过滤生成建议，以更好地适应用户需求并减少不必要的请求。


<details>
  <summary>Details</summary>
Motivation: 在IDE中集成LLM驱动的代码补全功能时，需要更智能地控制何时触发推理以及如何过滤生成建议，以提高用户体验和系统效率。

Method: 使用提升（boosting）和基于Transformer的架构，在包含98名用户的真实代码补全离线数据集上进行评估。进一步在多种语法多样化的语言上评估基于提升方法的离线分类性能，并在生产环境中进行A/B测试。

Result: 在生产环境的A/B研究中，控制模型提高了代码补全的效率和质量指标，证明了辅助模型在IDE中智能集成LLM驱动功能的潜力。

Conclusion: 研究表明使用辅助控制模型可以优化IDE中LLM代码补全的集成，指出了未来研究方向并提出了开放性问题，为更智能的IDE集成提供了可行路径。

Abstract: We introduce control models for LLM-powered code completion in JetBrains IDEs: ML classifiers which trigger inference and filter the generated suggestions to better align them with users and reduce unnecessary requests. To this end, we evaluate boosting- and transformer-based architectures on an offline dataset of real code completions with n=98 users. We further evaluate the offline classification performance of our boosting-based approach on a range of syntactically diverse languages; and perform an A/B study in a production environment where they improve completion efficiency and quality metrics. With this study, we hope to demonstrate the potential in using auxiliary models for smarter in-IDE integration of LLM-driven features, highlight fruitful future directions, and open problems.

</details>


### [7] [Understanding npm Developers' Practices, Challenges, and Recommendations for Secure Package Development](https://arxiv.org/abs/2601.20240)
*Anthony Peruma,Truman Choy,Gerald Lee,Italo De Oliveira Santos*

Main category: cs.SE

TL;DR: npm开发者调查显示：开发者重视安全但认为包仅中等安全，主要担忧供应链攻击和依赖漏洞，对现有安全工具满意度低（仅40%），偏好自动化工具但面临时间限制和误报率高的问题。


<details>
  <summary>Details</summary>
Motivation: npm生态系统在现代软件开发中至关重要，但第三方包的漏洞已导致严重安全漏洞，损害依赖这些包的应用完整性。本研究旨在调查npm包开发者如何看待和处理安全问题。

Method: 对75名npm包开发者进行在线调查，采用混合方法分析他们的回答。

Result: 开发者重视安全但认为自己的包仅中等安全，主要担忧供应链攻击、依赖漏洞和恶意代码。仅40%对当前npm安全工具满意，主要问题是警报疲劳。偏好双因素认证和npm审计等自动化方法而非代码审查。许多开发者因废弃或漏洞而放弃依赖，通常通过快速发布补丁响应漏洞。主要障碍包括时间限制和高误报率。

Conclusion: 研究结果将帮助npm包贡献者和维护者了解普遍存在的安全挑战，促进关于最佳实践的讨论，以加强npm生态系统的安全性和可信度。

Abstract: Background: The Node Package Manager (npm) ecosystem plays a vital role in modern software development by providing a vast repository of packages and tools that developers can use to implement their software systems. However, recent vulnerabilities in third-party packages have led to serious security breaches, compromising the integrity of applications that depend on them. Objective: This study investigates how npm package developers perceive and handle security in their work. We examined developers' understanding of security risks, the practices and tools they use, the barriers to stronger security measures, and their suggestions for improving the npm ecosystem's security. Method: We conducted an online survey with 75 npm package developers and undertook a mixed-methods approach to analyzing their responses. Results: While developers prioritize security, they perceive their packages as only moderately secure, with concerns about supply chain attacks, dependency vulnerabilities, and malicious code. Only 40% are satisfied with the current npm security tools due to issues such as alert fatigue. Automated methods such as two-factor authentication and npm audit are favored over code reviews. Many drop dependencies due to abandonment or vulnerabilities, and typically respond to vulnerabilities in their packages by quickly releasing patches. Key barriers include time constraints and high false-positive rates. To improve npm security, developers seek better detection tools, clearer documentation, stronger account protections, and more education initiatives. Conclusion: Our findings will benefit npm package contributors and maintainers by highlighting prevalent security challenges and promoting discussions on best practices to strengthen security and trustworthiness within the npm landscape.

</details>


### [8] [How Software Engineering Research Overlooks Local Industry: A Smaller Economy Perspective](https://arxiv.org/abs/2601.20382)
*Klara Borowa,Andrzej Zalewski,Lech Madeyski*

Main category: cs.SE

TL;DR: 波兰软件工程研究者通过ICSE FOSE社区调查分析，指出研究-产业鸿沟对小型经济体影响更大，并提出改进建议


<details>
  <summary>Details</summary>
Motivation: 作为来自波兰（小型经济体、非英语国家）的软件工程研究者，希望展示对软件工程社区关键问题的独特视角，特别是研究-产业鸿沟对小型社区和本地公司的影响

Method: 采用反思性主题分析方法分析ICSE FOSE（软件工程未来）社区调查数据，结合自身经验进行定性研究

Result: 识别出研究-产业鸿沟是主要问题，该问题对小型经济体和本地公司影响尤为显著，并基于分析提出了具体改进建议

Conclusion: 小型经济体的软件工程研究者是宝贵少数群体，需要关注研究-产业鸿沟问题，提出的建议有助于增强小型经济体中软件工程研究与产业合作

Abstract: The software engineering researchers from countries with smaller economies, particularly non-English speaking ones, represent valuable minorities within the software engineering community. As researchers from Poland, we represent such a country. We analyzed the ICSE FOSE (Future of Software Engineering) community survey through reflexive thematic analysis to show our viewpoint on key software community issues. We believe that the main problem is the growing research-industry gap, which particularly impacts smaller communities and small local companies. Based on this analysis and our experiences, we present a set of recommendations for improvements that would enhance software engineering research and industrial collaborations in smaller economies.

</details>


### [9] [Comprehension vs. Adoption: Evaluating a Language Workbench Through a Family of Experiments](https://arxiv.org/abs/2601.20394)
*Giovanna Broccia,Maurice H. ter Beek,Walter Cazzola,Luca Favalli,Francesco Bertolotti,Alessio Ferrari*

Main category: cs.SE

TL;DR: 该研究通过实验评估Neverlang语言工作台的元语言可理解性和用户接受度，发现用户能充分理解其语法并认可其有用性，但易用性仍是挑战，且可理解性与接受度之间无显著相关性。


<details>
  <summary>Details</summary>
Motivation: 当前文献在评估语言工作台时往往忽视用户中心化的可理解性和接受度等维度，而这对促进这类相对新工具被语言设计师采用至关重要。研究旨在填补这一空白。

Method: 采用定制版方法评估模型(MEM)，通过三阶段迭代实验评估Neverlang元语言和程序的可理解性，以及用户接受度(感知易用性、感知有用性和使用意向)，并探究这些维度间的关系。

Result: 用户对Neverlang元语言语法有充分理解，对其有用性持积极态度并有使用意向，但易用性仍是挑战。感知易用性和有用性的高低变化影响使用意向，但可理解性与用户接受度之间无显著相关性。

Conclusion: 元语言更高的可理解性不一定带来更高的接受度，表明理解与采纳之间存在复杂关系。研究强调了在评估语言工作台时考虑用户中心化维度的重要性。

Abstract: Language workbenches are tools that enable the definition, reuse, and composition of programming languages and their ecosystems, aiming to streamline language development. To facilitate their adoption by language designers, the comprehensibility of the language used to define other languages is an important aspect to evaluate. Moreover, considering that language workbenches are relatively new tools, user acceptance emerges as a crucial factor to be accounted for during their assessment. Current literature often neglects user-centred aspects like comprehensibility and acceptance in the assessment of this breed of tools. This paper addresses this gap through a family of experiments assessing Neverlang, a modular language workbench. The study adopts a tailored version of the Method Evaluation Model (MEM) to evaluate the comprehensibility of Neverlang's meta-language and programs, as well as user acceptance in terms of perceived ease of use, perceived usefulness, and intention to use. It also investigates the relationships among these dimensions. The experiments were conducted in three iterations involving participants from academia. The results reveal that users demonstrate sufficient comprehension of Neverlang's meta-language, particularly concerning its syntax, express a favourable perception of its usefulness, and indicate their intention to use it. However, the results also indicate that Neverlang's ease of use remains a challenge. Additionally, variations in the perceived ease of use and perceived usefulness, whether low or high, influence the users' intention to use the tool. Surprisingly, no significant correlation is found between comprehensibility and user acceptance. Notably, higher comprehensibility of the meta-language does not necessarily translate into greater acceptance, underscoring the complex interplay between comprehension and adoption.

</details>


### [10] [An Empirical Evaluation of Modern MLOps Frameworks](https://arxiv.org/abs/2601.20415)
*Jon Marcos-Mercadé,Unai Lopez-Novoa,Mikel Egaña Aranguren*

Main category: cs.SE

TL;DR: 本文对四种MLOps工具（MLflow、Metaflow、Apache Airflow、Kubeflow Pipelines）进行了实证评估，通过MNIST数字分类和IMDB情感分类两个常见ML场景，从安装便捷性、配置灵活性、互操作性等六个维度进行比较，为开发者提供工具选择建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI解决方案在专业环境中的日益普及，开发者需要对当前工具生态有清晰的了解，以便做出明智的MLOps工具选择决策。现有研究缺乏对主流MLOps工具的系统性实证比较。

Method: 通过实现两个常见ML场景（MNIST数字分类器和IMDB情感分类器）来评估四种MLOps工具：MLflow、Metaflow、Apache Airflow和Kubeflow Pipelines。评估标准包括：安装便捷性、配置灵活性、互操作性、代码插桩复杂度、结果可解释性和文档质量。

Result: 研究提供了加权评估结果，展示了不同工具在各个评估维度上的表现差异。结果表明不同工具在不同场景下各有优劣，为不同使用场景提供了具体的工具推荐。

Conclusion: 该评估为开发者提供了实用的工具选择指导，帮助根据具体需求（如安装复杂度、配置灵活性、互操作性等）选择最适合的MLOps工具，从而更有效地管理机器学习模型生命周期。

Abstract: Given the increasing adoption of AI solutions in professional environments, it is necessary for developers to be able to make informed decisions about the current tool landscape. This work empirically evaluates various MLOps (Machine Learning Operations) tools to facilitate the management of the ML model lifecycle: MLflow, Metaflow, Apache Airflow, and Kubeflow Pipelines. The tools are evaluated by assessing the criteria of Ease of installation, Configuration flexibility, Interoperability, Code instrumentation complexity, result interpretability, and Documentation when implementing two common ML scenarios: Digit classifier with MNIST and Sentiment classifier with IMDB and BERT. The evaluation is completed by providing weighted results that lead to practical conclusions on which tools are best suited for different scenarios.

</details>


### [11] [Challenges in Android Data Disclosure: An Empirical Study](https://arxiv.org/abs/2601.20459)
*Mugdha Khedkar,Michael Schlichtig,Mohamed Soliman,Eric Bodden*

Main category: cs.SE

TL;DR: 本文通过调查41名Android开发者和分析172个在线讨论，研究了开发者在填写Google Play商店数据安全部分(DSS)表格时的经验和挑战，发现开发者虽然能识别应用收集的数据，但在将其转化为合规披露时缺乏信心。


<details>
  <summary>Details</summary>
Motivation: 虽然法律要求Android开发者准确报告应用收集的数据，但大型代码库使得这一报告过程具有挑战性。本研究旨在了解开发者在填写Google Play商店数据安全部分(DSS)表格时的实际经验。

Method: 采用实证研究方法：1) 调查41名Android开发者，了解他们如何将隐私相关数据分类到DSS类别以及填写表格时的信心程度；2) 分析172个在线开发者讨论，获取642名额外开发者的观点。总共收集了683名开发者的见解。

Result: 研究发现：开发者通常手动将应用收集的隐私数据分类到Google定义的类别中（有时完全省略分类），并严重依赖现有在线资源填写表格。开发者对识别应用收集的数据有信心，但缺乏将其转化为DSS合规披露的信心。主要挑战包括：识别隐私相关数据、对表格理解有限、担心因与Google隐私要求不符而导致应用被拒。

Conclusion: 研究结果强调了需要更清晰的指导和更易用的工具来支持开发者满足隐私意识报告义务。当前的DSS表格填写过程对开发者来说存在显著挑战，需要改进支持机制。

Abstract: Current legal frameworks enforce that Android developers accurately report the data their apps collect. However, large codebases can make this reporting challenging. This paper employs an empirical approach to understand developers' experience with Google Play Store's Data Safety Section (DSS) form.
  We first survey 41 Android developers to understand how they categorize privacy-related data into DSS categories and how confident they feel when completing the DSS form. To gain a broader and more detailed view of the challenges developers encounter during the process, we complement the survey with an analysis of 172 online developer discussions, capturing the perspectives of 642 additional developers. Together, these two data sources represent insights from 683 developers.
  Our findings reveal that developers often manually classify the privacy-related data their apps collect into the data categories defined by Google-or, in some cases, omit classification entirely-and rely heavily on existing online resources when completing the form. Moreover, developers are generally confident in recognizing the data their apps collect, yet they lack confidence in translating this knowledge into DSS-compliant disclosures. Key challenges include issues in identifying privacy-relevant data to complete the form, limited understanding of the form, and concerns about app rejection due to discrepancies with Google's privacy requirements.
  These results underscore the need for clearer guidance and more accessible tooling to support developers in meeting privacy-aware reporting obligations.

</details>


### [12] [DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning](https://arxiv.org/abs/2601.20615)
*Yanlin Wang,Jiadong Wu,Tianyue Jiang,Mingwei Liu,Jiachi Chen,Chong Wang,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: DrainCode是一种针对RAG代码生成系统的对抗攻击，通过污染检索上下文迫使LLM生成更长输出，从而显著增加延迟和能耗


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在代码生成方面表现出色，但LLM推理的计算成本（延迟和能耗）在安全背景下关注不足。现有研究主要关注模型准确性，而忽略了计算效率的安全威胁。

Method: 采用基于突变的对抗攻击方法，通过策略性地污染检索上下文，迫使LLM生成显著更长的输出，从而增加GPU延迟和能耗。攻击针对RAG-based代码生成系统。

Result: 实验显示DrainCode能实现：延迟增加85%，能耗增加49%，输出长度增加3倍以上。攻击在不同提示策略下具有泛化性，且能有效绕过多种防御措施。

Conclusion: DrainCode是首个针对RAG代码生成系统计算效率的对抗攻击，能显著增加LLM的计算开销，为评估资源受限环境下的LLM安全性提供了新方法。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.

</details>


### [13] [Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model](https://arxiv.org/abs/2601.20662)
*Julien Malka,Arnout Engelen*

Main category: cs.SE

TL;DR: Lila是一个针对函数式包管理模型的去中心化可重现构建监控系统，解决了大规模软件构建完整性验证的监控基础设施问题。


<details>
  <summary>Details</summary>
Motivation: 现代软件工程面临构建系统、分发渠道和开发基础设施的复杂攻击，需要确保软件构建产物的完整性。可重现构建为软件分发提供了透明度和信任的基础，但大规模采用面临两个挑战：跨海量软件集合实现高可重现率，以及建立能够大规模运行的可重现性监控基础设施。

Method: 提出了Lila系统，这是一个针对函数式包管理模型的去中心化可重现性评估系统。Lila支持分布式报告构建结果，并将结果聚合到可重现性数据库中。

Result: 虽然论文没有提供具体数据结果，但指出Nix生态系统已经在超过80,000个包上实现了超过90%的可重现性，表明高可重现率在大规模上是可实现的。

Conclusion: Lila系统解决了可重现构建监控的挑战，为实践者和未来的实证构建可重现性研究提供了分布式报告和聚合能力，有助于推动可重现构建的大规模采用。

Abstract: Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.
  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.
  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies.

</details>


### [14] [ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler](https://arxiv.org/abs/2601.20755)
*Bohua Zou,Debayan Roy,Dhimankumar Yogesh Airao,Weihao Xu,Binqi Sun,Yutao Liu,Haibo Chen*

Main category: cs.SE

TL;DR: 开发了一个基于eBPF的细粒度、非侵入式LLM推理引擎性能分析框架，能够在不修改源代码的情况下提供算子级可见性，运行时开销低于4%


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从研究转向生产应用，理解推理引擎在实时环境中的行为变得至关重要但难以实现。当前的LLM推理系统缺乏算子级可见性，开发者无法了解时间和资源消耗的具体分布，甚至无法判断工作负载是内存受限还是计算受限

Method: 基于扩展的伯克利数据包过滤器（eBPF）技术，开发了一个非侵入式性能分析框架。该系统动态地将探针附加到运行时函数的多个层次，无需修改或重新编译源代码。收集的跟踪数据被转换为丰富的可视化图表，包括算子、图、时间线和硬件计数器趋势

Result: 该框架能够暴露密集推理、混合专家路由和算子卸载在实际运行中的行为。系统运行时开销低于4%，具有高保真度的性能分析能力，使LLM推理变得透明且可诊断

Conclusion: 该框架将性能分析转变为实用的优化工具，支持调度和资源感知部署，为LLM推理引擎提供了前所未有的可见性和诊断能力

Abstract: As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama-cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment.

</details>


### [15] [Context-Augmented Code Generation Using Programming Knowledge Graphs](https://arxiv.org/abs/2601.20810)
*Shahd Seddik,Fahd Seddik,Iman Saberi,Fatemeh Fard,Minh Hieu Huynh,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 该论文提出编程知识图谱(PKG)方法，通过语义表示和细粒度检索增强代码生成能力，结合树剪枝和重排序机制解决RAG中的检索不准确和幻觉问题，在HumanEval和MBPP基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但在处理复杂问题时存在困难。检索增强生成(RAG)通过整合外部知识来缓解这一问题，但现有检索模型经常错过相关上下文，生成模型则容易产生与无关数据的幻觉。

Method: 提出编程知识图谱(PKG)方法，用于代码和文本的语义表示和细粒度检索。通过树剪枝技术提高检索精度，并采用重排序机制整合非RAG解决方案来减少幻觉。将外部数据结构化为更细粒度的节点以改善检索粒度。

Result: 在HumanEval和MBPP基准测试中，pass@1准确率提升最高达20%，在MBPP上比基线方法提高了34%。实验结果表明PKG方法能有效处理复杂问题，同时对无需RAG已正确的解决方案保持最小负面影响。

Conclusion: 提出的编程知识图谱方法结合重排序机制能有效解决复杂代码生成问题，提高检索精度并减少幻觉，在保持对已有正确解决方案最小影响的同时显著提升性能。

Abstract: Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [16] [Benchmarking LLAMA Model Security Against OWASP Top 10 For LLM Applications](https://arxiv.org/abs/2601.19970)
*Nourin Shahin,Izzat Alsmadi*

Main category: cs.CR

TL;DR: 该研究评估了Llama模型变体在OWASP LLM安全框架下的表现，发现小型专用安全模型（Llama-Guard-3-1B）在威胁检测率（76%）和延迟（0.165s）方面优于大型通用模型（Llama-3.1-8B检测率为0%，延迟0.754s），揭示了模型大小与安全有效性之间的反比关系。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从研究原型转向企业系统，其安全漏洞对数据隐私和系统完整性构成严重风险。需要评估不同模型的安全性能，为实际部署提供指导。

Method: 使用FABRIC测试平台和NVIDIA A30 GPU，测试了5个标准Llama模型和5个Llama Guard变体，使用100个对抗性提示覆盖10个OWASP LLM漏洞类别，评估威胁检测准确率、响应安全性和计算开销。

Result: 紧凑型Llama-Guard-3-1B模型达到最高检测率76%，延迟最低（0.165s）；而基础模型如Llama-3.1-8B检测率为0%，延迟更高（0.754s）。发现模型大小与安全有效性呈反比关系，小型专用模型在安全任务上优于大型通用模型。

Conclusion: 企业部署LLM时应优先考虑专用安全模型而非仅依赖大型通用模型，并提供了开源基准数据集支持可重复的AI安全研究。

Abstract: As large language models (LLMs) move from research prototypes to enterprise systems, their security vulnerabilities pose serious risks to data privacy and system integrity. This study benchmarks various Llama model variants against the OWASP Top 10 for LLM Applications framework, evaluating threat detection accuracy, response safety, and computational overhead. Using the FABRIC testbed with NVIDIA A30 GPUs, we tested five standard Llama models and five Llama Guard variants on 100 adversarial prompts covering ten vulnerability categories. Our results reveal significant differences in security performance: the compact Llama-Guard-3-1B model achieved the highest detection rate of 76% with minimal latency (0.165s per test), whereas base models such as Llama-3.1-8B failed to detect threats (0% accuracy) despite longer inference times (0.754s). We observe an inverse relationship between model size and security effectiveness, suggesting that smaller, specialized models often outperform larger general-purpose ones in security tasks. Additionally, we provide an open-source benchmark dataset including adversarial prompts, threat labels, and attack metadata to support reproducible research in AI security, [1].

</details>


### [17] [Reference-Free Spectral Analysis of EM Side-Channels for Always-on Hardware Trojan Detection](https://arxiv.org/abs/2601.20163)
*Mahsa Tahghigh,Hassan Salmani*

Main category: cs.CR

TL;DR: 提出了一种无需参考模型的硬件木马检测方法，结合时频电磁分析和高斯混合模型，通过多窗口短时傅里叶变换识别硬件木马的持久性特征


<details>
  <summary>Details</summary>
Motivation: 硬件木马对可信微电子构成严重威胁，但现有的侧信道检测方法大多依赖难以获得的黄金参考模型，因此需要开发无需参考的检测方法

Method: 采用参考自由的方法，结合时频电磁分析和高斯混合模型。通过应用多窗口大小的短时傅里叶变换，分析电磁信号的统计结构。无硬件木马的电路表现出波动的统计结构，而始终开启的硬件木马会留下持久性特征，具有更少、更一致的高斯混合成分

Result: 在AES-128上的实验结果表明，该方法无需参考模型即可实现硬件木马检测，证明了其可行性

Conclusion: 提出的参考自由方法能够有效检测始终开启的硬件木马，通过分析电磁信号的时频特征和高斯混合模型成分差异，为硬件安全提供了一种实用的检测方案

Abstract: Always-on hardware Trojans (HTs) pose a critical risk to trusted microelectronics, yet most side-channel detection methods rely on unavailable golden references. We present a reference-free approach that combines time-frequency EM analysis with Gaussian Mixture Models (GMMs). By applying Short-Time Fourier Transform (STFT) at multiple window sizes, we show that HT-free circuits exhibit fluctuating statistical structure, while always-on HTs leave persistent footprints with fewer, more consistent mixture components. Results on AES-128 demonstrate feasibility without requiring reference models.

</details>


### [18] [Eliciting Least-to-Most Reasoning for Phishing URL Detection](https://arxiv.org/abs/2601.20270)
*Holly Trikilis,Pasindu Marasinghe,Fariza Rashid,Suranga Seneviratne*

Main category: cs.CR

TL;DR: 提出基于Least-to-Most提示框架的钓鱼URL检测方法，通过答案敏感度机制增强推理能力，在少量训练数据下达到与监督模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击持续高发，准确分类钓鱼URL至关重要。虽然大语言模型在钓鱼URL检测中表现出潜力，但其实现高性能的推理能力尚未得到充分探索。

Method: 提出Least-to-Most提示框架，引入"答案敏感度"机制指导迭代推理过程。使用三个URL数据集和四个最先进的大语言模型进行评估，与单次提示方法和监督模型进行对比。

Result: 该框架优于单次提示基线，在显著减少训练数据需求的情况下，性能与监督模型相当。深度分析显示Least-to-Most的迭代推理和答案敏感度机制共同驱动了性能提升。

Conclusion: 这种简单而强大的提示策略在需要最少训练或少量指导的情况下，持续优于单次提示和监督方法，为钓鱼URL检测提供了有效的解决方案。

Abstract: Phishing continues to be one of the most prevalent attack vectors, making accurate classification of phishing URLs essential. Recently, large language models (LLMs) have demonstrated promising results in phishing URL detection. However, their reasoning capabilities that enabled such performance remain underexplored. To this end, in this paper, we propose a Least-to-Most prompting framework for phishing URL detection. In particular, we introduce an "answer sensitivity" mechanism that guides Least-to-Most's iterative approach to enhance reasoning and yield higher prediction accuracy. We evaluate our framework using three URL datasets and four state-of-the-art LLMs, comparing against a one-shot approach and a supervised model. We demonstrate that our framework outperforms the one-shot baseline while achieving performance comparable to that of the supervised model, despite requiring significantly less training data. Furthermore, our in-depth analysis highlights how the iterative reasoning enabled by Least-to-Most, and reinforced by our answer sensitivity mechanism, drives these performance gains. Overall, we show that this simple yet powerful prompting strategy consistently outperforms both one-shot and supervised approaches, despite requiring minimal training or few-shot guidance. Our experimental setup can be found in our Github repository github.sydney.edu.au/htri0928/least-to-most-phishing-detection.

</details>


### [19] [SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks](https://arxiv.org/abs/2601.20310)
*Xin Zhang,Zijin Yang,Kejiang Chen,Linfeng Ma,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: SemBind是首个防御潜在扩散模型中黑盒伪造攻击的框架，通过将潜在水印信号与图像语义绑定来抵抗攻击，同时保持图像质量基本不变。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在水印虽然能简化生成图像的检测和溯源，但面临黑盒伪造攻击的风险，攻击者只需一个带水印的图像和黑盒访问权限就能将提供商的水印嵌入非提供商生成的图像中，严重威胁溯源和信任体系。

Method: 提出SemBind框架，通过学习的语义掩码器将潜在信号与图像语义绑定。使用对比学习训练掩码器，使相同提示词产生近似不变的编码，不同提示词产生近似正交的编码；这些编码经过重塑和置换后调制目标潜在表示，然后应用标准潜在水印。框架包含可调节的掩码率参数，在抗伪造强度和鲁棒性之间提供可调平衡。

Result: 在四种主流潜在水印方法上，SemBind增强的抗伪造变体显著降低了黑盒伪造攻击下的误接受率，同时提供了可控的鲁棒性-安全性平衡，图像质量基本保持不变。

Conclusion: SemBind是首个有效防御潜在水印黑盒伪造攻击的框架，与现有潜在水印方案兼容，通过语义绑定机制在保持图像质量的同时显著提升安全性，为水印系统提供了实用的抗伪造保护。

Abstract: Latent-based watermarks, integrated into the generation process of latent diffusion models (LDMs), simplify detection and attribution of generated images. However, recent black-box forgery attacks, where an attacker needs at least one watermarked image and black-box access to the provider's model, can embed the provider's watermark into images not produced by the provider, posing outsized risk to provenance and trust. We propose SemBind, the first defense framework for latent-based watermarks that resists black-box forgery by binding latent signals to image semantics via a learned semantic masker. Trained with contrastive learning, the masker yields near-invariant codes for the same prompt and near-orthogonal codes across prompts; these codes are reshaped and permuted to modulate the target latent before any standard latent-based watermark. SemBind is generally compatible with existing latent-based watermarking schemes and keeps image quality essentially unchanged, while a simple mask-ratio parameter offers a tunable trade-off between anti-forgery strength and robustness. Across four mainstream latent-based watermark methods, our SemBind-enabled anti-forgery variants markedly reduce false acceptance under black-box forgery while providing a controllable robustness-security balance.

</details>


### [20] [UnlearnShield: Shielding Forgotten Privacy against Unlearning Inversion](https://arxiv.org/abs/2601.20325)
*Lulu Xue,Shengshan Hu,Wei Lu,Ziqi Zhou,Yufei Song,Jianhong Cheng,Minghui Li,Yanjun Zhang,Leo Yu Zhang*

Main category: cs.CR

TL;DR: UnlearnShield：首个针对机器学习遗忘反转攻击的防御机制，通过在余弦表示空间引入定向扰动，在保护隐私的同时保持模型准确性和遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 机器学习遗忘技术旨在从训练模型中移除特定数据的影响以增强隐私保护，但研究发现存在遗忘反转攻击漏洞，攻击者可利用该漏洞重建本应被删除的数据。目前缺乏专门针对这种威胁的防御措施。

Method: 提出UnlearnShield防御机制，在余弦表示空间中引入定向扰动，并通过约束模块进行调节，共同保持模型准确性和遗忘效果，从而降低反转攻击风险同时保持模型实用性。

Result: 实验表明UnlearnShield在隐私保护、模型准确性和遗忘效果之间实现了良好的平衡。

Conclusion: UnlearnShield是首个专门针对遗忘反转攻击的防御方案，有效解决了机器学习遗忘技术中的隐私漏洞问题。

Abstract: Machine unlearning is an emerging technique that aims to remove the influence of specific data from trained models, thereby enhancing privacy protection. However, recent research has uncovered critical privacy vulnerabilities, showing that adversaries can exploit unlearning inversion to reconstruct data that was intended to be erased. Despite the severity of this threat, dedicated defenses remain lacking. To address this gap, we propose UnlearnShield, the first defense specifically tailored to counter unlearning inversion. UnlearnShield introduces directional perturbations in the cosine representation space and regulates them through a constraint module to jointly preserve model accuracy and forgetting efficacy, thereby reducing inversion risk while maintaining utility. Experiments demonstrate that it achieves a good trade-off among privacy protection, accuracy, and forgetting.

</details>


### [21] [A High-Performance Fractal Encryption Framework and Modern Innovations for Secure Image Transmission](https://arxiv.org/abs/2601.20374)
*Sura Khalid Salsal,Eman Shaker Mahmood,Farah Tawfiq Abdul Hussien,Maryam Mahdi Alhusseini,Azhar Naji Alyahya,Nikolai Safiullin*

Main category: cs.CR

TL;DR: 本文提出了一种基于傅里叶变换的分形图像加密方法，旨在解决传统加密算法在安全性、图像保真度和计算效率之间的权衡问题，显著提升了加密/解密速度和图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前数字时代面临日益增长的数据安全威胁，需要强大的图像加密技术。传统加密算法在安全性、图像保真度和计算效率之间存在权衡问题，无法同时满足所有需求。

Method: 提出基于傅里叶变换的分形加密作为新的图像加密方法，利用最先进的技术。该方法包括分形加密的数学公式化，并与传统加密方法进行对比分析。

Result: 该方法在填补先前研究空白的基础上，显著改善了加密/解密时间和图像保真度，相比其他技术表现更优。

Conclusion: 基于傅里叶变换的分形加密方法在图像加密中表现出更好的性能和效率，同时保持了图像质量。论文还指出了未来研究方向和可能的改进空间。

Abstract: The current digital era, driven by growing threats to data security, requires a robust image encryption technique. Classical encryption algorithms suffer from a trade-off among security, image fidelity, and computational efficiency. This paper aims to enhance the performance and efficiency of image encryption. This is done by proposing Fractal encryption based on Fourier transforms as a new method of image encryption, leveraging state-of-the-art technology. The new approach considered here intends to enhance both security and efficiency in image encryption by comparing Fractal Encryption with basic methods. The suggested system also aims to optimise encryption/ decryption times and preserve image quality. This paper provides an introduction to Image Encryption using the fractal-based method, its mathematical formulation, and its comparative efficiency against publicly known traditional encryption methods. As a result, after filling the gaps identified in previous research, it has significantly improved both its encryption/decryption time and image fidelity compared to other techniques. In this paper, directions for future research and possible improvements are outlined for attention.

</details>


### [22] [Towards Quantum-Safe O-RAN -- Experimental Evaluation of ML-KEM-Based IPsec on the E2 Interface](https://arxiv.org/abs/2601.20378)
*Mario Perera,Michael Mackay,Max Hashem Eiza,Alessandro Raschellà,Nathan Shone,Mukesh Kumar Maheshwari*

Main category: cs.CR

TL;DR: 该论文实验评估了在O-RAN E2接口的IKEv2/IPsec中集成后量子密码ML-KEM的影响，发现相比经典IPsec仅增加3-5ms隧道建立延迟，xApp运行保持稳定，证明量子安全迁移可行。


<details>
  <summary>Details</summary>
Motivation: 随着O-RAN部署扩展和攻击者采用"现在存储、稍后解密"策略，运营商需要关于将关键控制接口迁移到后量子密码的实际成本数据。本研究旨在为O-RAN部署提供量子安全迁移策略的实证依据。

Method: 使用开源测试平台（srsRAN、Open5GS、FlexRIC、strongSwan+liboqs）比较三种配置：无IPsec、基于经典ECDH的IPsec、基于ML-KEM的IPsec。重点关注IPsec隧道建立延迟和Near-RT RIC xApps在真实信令负载下的运行时行为。

Result: ML-KEM集成相比经典IPsec仅增加约3-5ms的隧道建立开销，xApp操作和RIC控制回路在实验中保持稳定。重复自动化运行结果验证了ML-KEM在E2接口上的实际可行性。

Conclusion: 基于ML-KEM的IPsec在O-RAN E2接口上是实际可行的，这些发现为O-RAN部署的量子安全迁移策略提供了重要参考依据。

Abstract: As Open Radio Access Network (O-RAN) deployments expand and adversaries adopt 'store-now, decrypt-later' strategies, operators need empirical data on the cost of migrating critical control interfaces to post-quantum cryptography (PQC). This paper experimentally evaluates the impact of integrating a NIST-aligned module-lattice KEM (ML-KEM, CRYSTALS-Kyber) into IKEv2/IPsec protecting the E2 interface between the 5G Node B (gNB) and the Near-Real-Time RAN Intelligent Controller (Near-RT RIC). Using an open-source testbed built from srsRAN, Open5GS, FlexRIC and strongSwan (with liboqs), we compare three configurations: no IPsec, classical ECDH-based IPsec, and ML-KEM-based IPsec. The study focuses on IPsec tunnel-setup latency and the runtime behaviour of Near-RT RIC xApps under realistic signalling workloads. Results from repeated, automated runs show that ML-KEM integration adds a small overhead to tunnel establishment, which is approximately 3~5 ms in comparison to classical IPsec, while xApp operation and RIC control loops remain stable in our experiments. These findings indicate that ML-KEM based IPsec on the E2 interface is practically feasible and inform quantum-safe migration strategies for O-RAN deployments.

</details>


### [23] [Fuzzy Private Set Union via Oblivious Key Homomorphic Encryption Retrieval](https://arxiv.org/abs/2601.20400)
*Jean-Guillaume Dumas,Aude Maignan,Luiza Soezima*

Main category: cs.CR

TL;DR: 本文提出了模糊私有集合并（FPSU）协议，通过引入新的OKHER子协议改进OKVR技术，能够高效处理集合元素的近似匹配，通信量在O(dm log(δn))到O(d^2m log(δ^2n))之间。


<details>
  <summary>Details</summary>
Motivation: 传统私有集合操作要求元素完全相等，但在实际应用中往往需要处理近似匹配的情况。模糊PSI允许元素接近即可视为交集，但缺乏对应的模糊PSU协议。本文旨在填补这一空白，设计能够处理近似匹配的私有集合并协议。

Method: 提出模糊PSU（FPSU）协议，将接收方集合X替换为以X中元素为中心、半径为δ的d维球体并集。引入新的OKHER（Oblivious Key Homomorphic Encryption Retrieval）子协议改进现有OKVR技术，基于l∞距离设计多种协议变体，利用同态加密技术实现。

Result: 成功设计了FPSU协议，能够高效计算模糊集合并。通信量根据接收方数据集结构不同，在O(dm log(δn))到O(d^2m log(δ^2n))之间，显著优于传统方法。

Conclusion: 本文首次提出了模糊私有集合并协议，通过创新的OKHER技术和基于l∞距离的设计，实现了高效的近似集合操作，为处理现实世界中的近似数据匹配问题提供了新的解决方案。

Abstract: Private Set Multi-Party Computations are protocols that allow parties to jointly and securely compute functions: apart from what is deducible from the output of the function, the input sets are kept private. Then, a Private Set Union (PSU), resp. Intersection (PSI), is a protocol that allows parties to jointly compute the union, resp. the intersection, between their private sets. Now a structured PSI, is a PSI where some structure of the sets can allow for more efficient protocols. For instance in Fuzzy PSI, elements only need to be close enough, instead of equal, to be part of the intersection. We present in this paper, Fuzzy PSU protocols (FPSU), able to efficiently take into account approximations in the union. For this, we introduce a new efficient sub-protocol, called Oblivious Key Homomorphic Encryption Retrieval (OKHER), improving on Oblivious Key-Value Retrieval (OKVR) techniques in our setting. In the fuzzy context, the receiver set $X=\{x_i\}_{1..n}$ is replaced by ${\mathcal B}_δ(X)$, the union of $n$ balls of dimension $d$ with radius $δ$, centered at the $x_i$. The sender set is just its $m$ points of dimension $d$. Then the FPSU functionality corresponds to $X \sqcup \{y \in Y, y \notin {\mathcal B}_δ(X)\}$. Thus, we formally define the FPSU functionality and security properties, and propose several protocols tuned to the patterns of the balls using the $l_\infty$ distance. Using our OKHER routine and homomorphic encryption, we are for instance able to obtain a FPSU protocols with an asymptotic communication volume bound ranging from $O(dm\log(δ{n}))$ to $O(d^2m\log(δ^2n))$, depending on the receiver data set structure.

</details>


### [24] [TÄMU: Emulating Trusted Applications at the (GlobalPlatform)-API Layer](https://arxiv.org/abs/2601.20507)
*Philipp Mao,Li Shi,Marcel Busch,Mathias Payer*

Main category: cs.CR

TL;DR: TÄMU是一个针对移动设备可信执行环境(TEE)中可信应用(TA)的动态分析平台，通过API层拦截执行实现模糊测试和调试，发现了17个零日漏洞。


<details>
  <summary>Details</summary>
Motivation: 移动设备TEE中的可信应用(TA)存在漏洞会危及整个系统安全，但由于TEE的闭源性和碎片化，严重阻碍了TA的动态分析，现有测试主要局限于静态分析。

Method: TÄMU通过API层拦截TA执行实现重托管，利用GlobalPlatform规范的标准化API实现跨TEE扩展，对TEE特定API采用贪婪高层模拟技术，根据模糊测试的潜在覆盖率增益优先进行手动重托管。

Result: 实现了TÄMU平台，成功模拟了4个不同TEE中的67个TA，模糊测试发现了11个TA中的17个零日漏洞，表明TEE生态系统缺乏动态分析能力。

Conclusion: TÄMU通过为移动TEE领域带来有效实用的动态分析，有望弥补TEE生态系统中的动态分析能力缺口，即使拥有源代码的厂商也未能解锁这些能力。

Abstract: Mobile devices rely on Trusted Execution Environments (TEEs) to execute security-critical code and protect sensitive assets. This security-critical code is modularized in components known as Trusted Applications (TAs). Vulnerabilities in TAs can compromise the TEE and, thus, the entire system. However, the closed-source nature and fragmentation of mobile TEEs severely hinder dynamic analysis of TAs, limiting testing efforts to mostly static analyses. This paper presents TÄMU, a rehosting platform enabling dynamic analysis of TAs, specifically fuzzing and debugging, by interposing their execution at the API layer. To scale to many TAs across different TEEs, TÄMU leverages the standardization of TEE APIs, driven by the GlobalPlatform specifications. For the remaining TEE-specific APIs not shared across different TEEs, TÄMU introduces the notion of greedy high-level emulation, a technique that allows prioritizing manual rehosting efforts based on the potential coverage gain during fuzzing. We implement TÄMU and use it to emulate 67 TAs across four TEEs. Our fuzzing campaigns yielded 17 zero-day vulnerabilities across 11 TAs. These results indicate a deficit of dynamic analysis capabilities across the TEE ecosystem, where not even vendors with source code unlocked these capabilities for themselves. TÄMU promises to close this gap by bringing effective and practical dynamic analysis to the mobile TEE domain.

</details>


### [25] [IoT Device Identification with Machine Learning: Common Pitfalls and Best Practices](https://arxiv.org/abs/2601.20548)
*Kahraman Kostas,Rabia Yasa Kostas*

Main category: cs.CR

TL;DR: 本文批判性地分析了使用机器学习进行设备识别的过程，指出了现有文献中的常见陷阱，并提供了增强物联网安全模型可重复性和泛化性的指南。


<details>
  <summary>Details</summary>
Motivation: 现有物联网设备识别研究存在方法上的缺陷，包括数据增强不当、会话标识符误导等问题，导致模型的可重复性和泛化性不足，需要系统性的分析和指导。

Method: 通过分析识别方法（唯一识别 vs 类别识别）的权衡、数据异质性、特征提取挑战和评估指标，识别具体错误并建立稳健的研究指南。

Result: 揭示了物联网设备识别研究中常见的错误模式，包括不当的数据增强实践和误导性的会话标识符使用，为研究人员提供了避免这些陷阱的具体指导。

Conclusion: 通过系统分析物联网设备识别研究的常见问题，本文提供了增强模型可重复性和泛化性的实用指南，有助于推动该领域研究的科学严谨性。

Abstract: This paper critically examines the device identification process using machine learning, addressing common pitfalls in existing literature. We analyze the trade-offs between identification methods (unique vs. class based), data heterogeneity, feature extraction challenges, and evaluation metrics. By highlighting specific errors, such as improper data augmentation and misleading session identifiers, we provide a robust guideline for researchers to enhance the reproducibility and generalizability of IoT security models.

</details>


### [26] [Supply Chain Insecurity: Exposing Vulnerabilities in iOS Dependency Management Systems](https://arxiv.org/abs/2601.20638)
*David Schmidt,Sebastian Schrittwieser,Edgar Weippl*

Main category: cs.CR

TL;DR: iOS依赖管理系统存在严重安全风险：CocoaPods等系统暴露内部包名和版本信息，攻击者可利用依赖混淆攻击实现远程代码执行，或通过劫持废弃域名/URL来接管依赖包，影响数百万用户。


<details>
  <summary>Details</summary>
Motivation: iOS软件供应链中依赖管理系统的安全性研究不足。尽管智能手机应用广泛使用，但iOS依赖管理系统（如CocoaPods）的安全风险未得到充分关注，而其中存在的配置错误和恶意行为可能导致严重的供应链攻击。

Method: 研究聚焦于CocoaPods（同时考察Carthage和SwiftPM），分析iOS应用如何暴露内部包名和版本信息。通过分析9,212个应用的数据集，量化易受攻击的应用数量。同时检查公共GitHub仓库中脆弱依赖的使用情况，并比较iOS依赖管理系统与其他主流系统（Cargo、Go模块、Maven、npm、pip）的安全差异。

Result: 研究发现流行iOS应用确实暴露内部依赖信息，使依赖混淆攻击成为可能。通过劫持单个CocoaPod库的废弃域名，可能危及63个iOS应用，影响数百万用户。攻击者可通过注册未声明的依赖实现开发者机器和构建服务器的远程代码执行。

Conclusion: iOS依赖管理系统存在严重安全漏洞，需要采取缓解措施。研究揭示了iOS软件供应链中的实际威胁，并提出了与其他依赖管理系统比较的缓解策略，强调了加强iOS依赖管理安全性的紧迫性。

Abstract: Dependency management systems are a critical component in software development, enabling projects to incorporate existing functionality efficiently. However, misconfigurations and malicious actors in these systems pose severe security risks, leading to supply chain attacks. Despite the widespread use of smartphone apps, the security of dependency management systems in the iOS software supply chain has received limited attention. In this paper, we focus on CocoaPods, one of the most widely used dependency management systems for iOS app development, but also examine the security of Carthage and Swift Package Manager (SwiftPM). We demonstrate that iOS apps expose internal package names and versions. Attackers can exploit this leakage to register previously unclaimed dependencies in CocoaPods, enabling remote code execution (RCE) on developer machines and build servers. Additionally, we show that attackers can compromise dependencies by reclaiming abandoned domains and GitHub URLs. Analyzing a dataset of 9,212 apps, we quantify how many apps are susceptible to these vulnerabilities. Further, we inspect the use of vulnerable dependencies within public GitHub repositories. Our findings reveal that popular apps disclose internal dependency information, enabling dependency confusion attacks. Furthermore, we show that hijacking a single CocoaPod library through an abandoned domain could compromise 63 iOS apps, affecting millions of users. Finally, we compare iOS dependency management systems with Cargo, Go modules, Maven, npm, and pip to discuss mitigation strategies for the identified threats.

</details>


### [27] [Decentralized Identity in Practice: Benchmarking Latency, Cost, and Privacy](https://arxiv.org/abs/2601.20716)
*Abylay Satybaldy,Kamil Tylinski,Jiahua Xu*

Main category: cs.CR

TL;DR: 本文对三种主流分布式账本DID方法（以太坊、Hedera、XRP Ledger）进行了实证基准测试，比较了延迟、交易成本和元数据泄露，揭示了不同架构在性能与隐私之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管去中心化标识符（DID）在分布式账本上的部署日益增多，但缺乏跨平台的系统性实证研究来了解其实际运行行为，需要为DID系统的选择和配置提供基于证据的见解。

Method: 使用统一的实验设置，通过参考软件开发工具包（SDKs）对三种主流账本DID方法（以太坊、Hedera、XRP Ledger）进行实证基准测试，测量延迟、交易成本和链上元数据暴露，并使用基于熵的元数据泄露评分（MLS）量化隐私泄露。

Result: 不同平台展现出明显的架构权衡：以太坊支持近即时、链下DID创建，但链上生命周期操作延迟和成本最高；XRPL提供确定性和稳定的延迟与固定低费用，但交易负载更详细导致元数据泄露更高；Hedera实现最低链上延迟和低费用且元数据泄露最小，但偶尔因SDK端处理和确认管道产生方差。

Conclusion: 账本架构和SDK工作流程对DID延迟、成本和元数据暴露有重要影响，这些发现为在性能和隐私约束下选择和配置DID系统提供了实证依据。

Abstract: Decentralized Identifiers (DIDs) are increasingly deployed on distributed ledgers, yet systematic cross-platform evidence on their operational behavior remains limited. We present an empirical benchmarking study of three prominent ledger-based DID methods - Ethereum, Hedera, and XRP Ledger - using reference Software Development Kits (SDKs) under a unified experimental setup. We measure latency, transaction cost, and on-chain metadata exposure, normalizing latency by each platform's block or consensus interval and cost by its native value transfer fee. Privacy leakage is quantified using a Metadata-Leakage Score (MLS), an entropy-based measure expressed in bits per operation.
  Our results reveal distinct architectural trade-offs. Ethereum enables near-instant, off-chain DID creation, but incurs the highest latency and cost for on-chain lifecycle operations. XRPL delivers deterministic and stable latency with fixed, low fees, yet exhibits higher metadata leakage due to more verbose transaction payloads. Hedera achieves the lowest on-chain latency and low fees with minimal metadata leakage, while occasional variance arises from SDK-side processing and confirmation pipelines.
  Overall, the findings show that ledger architecture and SDK workflows play a major role in shaping DID latency, cost, and metadata exposure, complementing the effects of the underlying consensus mechanism. These results provide evidence-based insights to support informed selection and configuration of DID systems under performance and privacy constraints.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [28] [NeuroAI and Beyond](https://arxiv.org/abs/2601.19955)
*Jean-Marc Fellous,Gert Cauwenberghs,Cornelia Fermüller,Yulia Sandamisrkaya,Terrence Sejnowski*

Main category: cs.AI

TL;DR: 该论文基于2025年8月的研讨会，探讨神经科学与人工智能的交叉领域，提出NeuroAI概念，旨在通过神经科学原理改进AI算法，同时深化对生物神经计算的理解。


<details>
  <summary>Details</summary>
Motivation: 神经科学与人工智能在过去几年都取得了显著进展，但两者之间的连接仍然松散。作者希望通过整合这两个领域，创建神经科学启发的人工智能（NeuroAI），以提升AI算法的范围和效率，同时改变对生物神经计算的理解方式。

Method: 基于2025年8月举办的研讨会，聚焦于具身性、语言与通信、机器人学、人类与机器学习、神经形态工程等子领域，评估现有进展并识别未来有前景的研究方向。收集多位领先研究人员的个人观点，并附上研究人员和学员进行的SWOT分析。

Result: 识别了神经科学与AI之间当前和未来的协同领域，提出了NeuroAI的发展框架。通过SWOT分析描述了NeuroAI的益处和风险，为这一交叉领域的研究提供了系统性的视角和未来发展方向。

Conclusion: 倡导发展NeuroAI——一种神经科学启发的人工智能，认为这种交叉学科方法不仅能够显著提升AI算法的范围和效率，还能改变我们对生物神经计算的理解方式，为两个领域带来互利共赢的发展前景。

Abstract: Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.

</details>


### [29] [Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning](https://arxiv.org/abs/2601.20014)
*Shuhui Qu*

Main category: cs.AI

TL;DR: SQ-BCP是一种在部分可观测环境下进行推理时规划的方法，通过显式表示前提条件状态、针对性自查询和桥接假设来解决LLM在缺失关键前提时产生幻觉或违反约束的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在部分可观测环境下进行推理时规划经常失败：当任务关键前提条件在查询时未指定时，模型容易产生幻觉或生成违反硬约束的计划。

Method: SQ-BCP显式表示前提条件状态（满足/违反/未知），通过(1)针对性自查询向oracle/用户获取信息，(2)桥接假设通过额外动作建立缺失条件。采用双向搜索，使用基于拉回的验证器作为目标兼容性的分类证书，仅使用基于距离的分数进行排序和剪枝。

Result: 在WikiHow和RecipeNLG任务中，当预条件被保留时，SQ-BCP将资源违反率分别降低到14.9%和5.8%（最佳基线为26.0%和15.7%），同时保持竞争力的参考质量。

Conclusion: SQ-BCP通过显式处理部分可观测性，在保证计划兼容性的同时显著减少了约束违反，为LLM在现实世界规划任务中的可靠性提供了理论保证。

Abstract: Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\texttt{Sat}/\texttt{Viol}/\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \textbf{14.9\%} and \textbf{5.8\%} (vs.\ \textbf{26.0\%} and \textbf{15.7\%} for the best baseline), while maintaining competitive reference quality.

</details>


### [30] [Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints](https://arxiv.org/abs/2601.20021)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出模糊范畴论规划(FCP)，通过模糊逻辑处理自然语言规划中的模糊谓词，使用t-范数组合计划质量，同时保持可执行性的精确验证


<details>
  <summary>Details</summary>
Motivation: 自然语言规划常涉及模糊谓词（如"合适的替代品"、"足够稳定"），现有范畴论规划器将其视为二值判断，导致阈值化处理会丢失有意义区别且无法追踪多步计划中的质量退化

Method: FCP为每个动作（态射）标注[0,1]区间内的程度值，使用Lukasiewicz t-范数组合计划质量，通过回拉验证保持可执行性检查，使用LLM进行分级适用性接地，支持基于剩余的后向需求中间相遇搜索

Result: 在PDDL3偏好/超额订阅基准和RecipeNLG-Subs（基于RecipeNLG构建的缺失替代品食谱规划基准）上评估，FCP相比LLM-only和ReAct风格基线提高了成功率并减少了硬约束违反，同时与经典PDDL3规划器保持竞争力

Conclusion: FCP通过模糊逻辑扩展范畴论规划，有效处理自然语言中的模糊谓词，在保持可执行性验证的同时支持分级质量评估，在食谱替代规划等实际应用中表现优异

Abstract: Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.

</details>


### [31] [Towards Intelligent Urban Park Development Monitoring: LLM Agents for Multi-Modal Information Fusion and Analysis](https://arxiv.org/abs/2601.20206)
*Zixuan Xiao,Chunguang Hu,Jun Ma*

Main category: cs.AI

TL;DR: 提出多模态LLM智能体框架，用于城市新建公园发展监测，解决传统遥感变化检测方法在高层次智能分析上的局限性


<details>
  <summary>Details</summary>
Motivation: 城市新建公园发展监测对评估城市规划效果和优化资源配置具有重要意义。传统基于遥感影像的变化检测方法在高层次智能分析方面存在明显局限，难以满足当前城市规划管理的需求，特别是在处理复杂多模态数据分析时缺乏灵活性

Method: 提出多模态LLM智能体框架，利用LLM的语义理解和推理能力。设计了通用的水平和垂直数据对齐机制，确保多模态数据的一致性和有效追踪。构建特定工具包来缓解LLM因缺乏领域专业知识而产生的幻觉问题

Result: 与普通GPT-4o和其他智能体相比，该方法能够实现稳健的多模态信息融合和分析，为城市公园发展监测提供可靠且可扩展的解决方案

Conclusion: 该多模态LLM智能体框架能够充分利用LLM的语义理解能力，有效应对城市公园发展监测中的挑战，为多样化和不断发展的监测需求提供定制化解决方案

Abstract: As an important part of urbanization, the development monitoring of newly constructed parks is of great significance for evaluating the effect of urban planning and optimizing resource allocation. However, traditional change detection methods based on remote sensing imagery have obvious limitations in high-level and intelligent analysis, and thus are difficult to meet the requirements of current urban planning and management. In face of the growing demand for complex multi-modal data analysis in urban park development monitoring, these methods often fail to provide flexible analysis capabilities for diverse application scenarios. This study proposes a multi-modal LLM agent framework, which aims to make full use of the semantic understanding and reasoning capabilities of LLM to meet the challenges in urban park development monitoring. In this framework, a general horizontal and vertical data alignment mechanism is designed to ensure the consistency and effective tracking of multi-modal data. At the same time, a specific toolkit is constructed to alleviate the hallucination issues of LLM due to the lack of domain-specific knowledge. Compared to vanilla GPT-4o and other agents, our approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions tailored to the diverse and evolving demands of urban park development monitoring.

</details>


### [32] [Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2601.20221)
*Hang Zhang,Ruheng Wang,Yuelyu Ji,Mingu Kwak,Xizhi Wu,Chenyu Li,Li Zhang,Wenqi Shi,Yifan Peng,Yanshan Wang*

Main category: cs.AI

TL;DR: 该论文提出了一个名为$\method$的智能框架，通过训练医学推理验证器在评估过程中迭代查询外部医学语料库，解决现有奖励模型只能提供标量奖励值且依赖单次检索的问题，显著提升了医学推理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医学推理基准测试中表现出色，但在临床部署时需要确保事实准确性。现有奖励模型方法存在两个局限：只能产生标量奖励值而缺乏明确理由，以及依赖单次检索无法在验证过程中进行自适应知识访问。

Method: 提出了$\method$框架，结合工具增强验证和迭代强化学习范式，仅需轨迹级监督。该框架训练医学推理验证器在评估过程中迭代查询外部医学语料库，并采用自适应课程机制动态调整训练数据分布。

Result: 在四个医学推理基准测试中，$\method$相比现有方法取得显著提升：MedQA准确率相对基础生成器提高23.5%，MedXpertQA提高32.0%。更重要的是，相比先前奖励模型基线，采样预算需求减少了8倍。

Conclusion: 基于动态检索证据的验证为构建更可靠的医学推理系统提供了原则性路径，证明了在验证过程中迭代访问外部知识库的重要性。

Abstract: Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\method$ demonstrates an $\mathbf{8\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.

</details>


### [33] [AMA: Adaptive Memory via Multi-Agent Collaboration](https://arxiv.org/abs/2601.20352)
*Weiquan Huang,Zixuan Wang,Hehai Lin,Sudong Wang,Bo Xu,Qian Li,Beier Zhu,Linyi Yang,Chengwei Qin*

Main category: cs.AI

TL;DR: AMA框架通过多智能体协作实现自适应记忆管理，显著提升长期记忆一致性和检索精度，同时减少80%的token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体记忆系统存在检索粒度僵化、维护策略积累过重、更新机制粗粒度等问题，导致存储信息与任务推理需求不匹配，以及逻辑不一致性随时间累积。

Method: 提出自适应记忆多智能体协作框架(AMA)，采用分层记忆设计，通过Constructor和Retriever实现多粒度记忆构建和自适应查询路由，Judge验证相关性和一致性，Refresher执行针对性更新或移除过时条目。

Result: 在挑战性长上下文基准测试中，AMA显著优于现有最优基线，相比全上下文方法减少约80%的token消耗，有效保持检索精度和长期记忆一致性。

Conclusion: AMA框架通过多智能体协作和分层记忆设计，成功解决了现有记忆系统的局限性，为LLM智能体提供了更高效、一致的自适应记忆管理方案。

Abstract: The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.

</details>


### [34] [Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution](https://arxiv.org/abs/2601.20379)
*Zhengbo Jiao,Hongyu Xian,Qinglong Wang,Yunpu Ma,Zhebo Wang,Zifan Zhang,Dezhang Kong,Meng Han*

Main category: cs.AI

TL;DR: PoT框架通过在线优化策略，让LLM从执行反馈中学习，显著提升复杂推理能力，4B模型在LiveCodeBench上超越GPT-4o等大模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂长时程推理中存在困难，主要因为其固定策略假设。现有方法仅将执行反馈作为外部信号用于轨迹过滤或重写，未能将其内化以改进底层推理策略。受波普尔"猜想与反驳"认识论启发，作者认为智能需要从失败尝试中实时演化模型策略。

Method: 提出Policy of Thoughts (PoT)框架，将推理重新定义为实例内的在线优化过程。首先通过高效探索机制生成多样候选解，然后使用Group Relative Policy Optimization (GRPO)基于执行反馈更新瞬态LoRA适配器。这种闭环设计实现了模型推理先验的动态、实例特定优化。

Result: 实验显示PoT显著提升性能：4B模型在LiveCodeBench上达到49.71%准确率，超越了GPT-4o和DeepSeek-V3，尽管模型规模小了50倍以上。

Conclusion: PoT框架通过在线策略优化使LLM能够从执行反馈中学习，有效解决了复杂推理中的稳定性问题，为小型模型实现超越大型模型的性能提供了新途径。

Abstract: Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of "conjectures and refutations," we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.

</details>


### [35] [CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning](https://arxiv.org/abs/2601.20467)
*Zhenxuan Fan,Jie Cao,Yang Dai,Zheqi Lv,Wenqiao Zhang,Zhongle Xie,Peng LU,Beng Chin Ooi*

Main category: cs.AI

TL;DR: CtrlCoT是一个双粒度思维链压缩框架，通过语义抽象和令牌级剪枝的协调，在减少30.7%令牌的同时提升7.6个百分点的推理准确率


<details>
  <summary>Details</summary>
Motivation: 思维链提示虽然能提升LLM推理能力，但冗长的推理轨迹导致高延迟和高内存成本，需要压缩但现有方法要么过于保守（语义层面缩短），要么过于激进（令牌级剪枝），容易丢失关键推理线索并降低准确性

Method: 提出CtrlCoT框架，包含三个组件：1）分层推理抽象：生成多粒度语义的思维链；2）逻辑保持蒸馏：训练逻辑感知剪枝器保留关键推理线索（如数字和运算符）；3）分布对齐生成：对齐压缩轨迹与推理风格以避免碎片化

Result: 在MATH-500数据集上使用Qwen2.5-7B-Instruct模型，CtrlCoT比最强基线减少30.7%的令牌使用量，同时准确率提升7.6个百分点，实现了更高效可靠的推理

Conclusion: CtrlCoT通过协调语义抽象和令牌级剪枝，有效解决了思维链压缩中的权衡问题，在保持推理正确性的同时显著降低了计算成本

Abstract: Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.

</details>


### [36] [Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function](https://arxiv.org/abs/2601.20554)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: 该研究将动态风险度量ICVaR应用于部分可观测环境下的风险敏感规划，开发了具有有限时间性能保证的策略评估算法，并扩展了三种在线规划算法以优化ICVaR值函数而非期望回报。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境中，传统的期望回报最大化规划方法可能无法充分控制尾部风险，特别是在安全关键应用中需要更严格的风险管理。现有风险敏感规划方法在部分可观测环境中的应用有限，需要开发新的算法来处理ICVaR这一动态风险度量。

Method: 1. 开发了ICVaR策略评估算法，具有不依赖于动作空间大小的有限时间性能保证；2. 扩展了三种在线规划算法：Sparse Sampling、PFT-DPW和POMCPOW，使其优化ICVaR值函数而非期望回报；3. 引入风险参数α，α=1恢复标准期望规划，α<1增加风险规避程度；4. 为ICVaR Sparse Sampling建立了风险敏感目标下的有限时间性能保证，并设计了针对ICVaR的新探索策略。

Result: 在基准POMDP领域的实验中，提出的ICVaR规划器相比风险中性对应方法实现了更低的尾部风险。ICVaR Sparse Sampling在风险敏感目标下具有理论性能保证，新探索策略能有效优化ICVaR值函数。

Conclusion: 该研究成功将ICVaR动态风险度量集成到部分可观测环境下的在线规划中，提供了理论保证和实际有效的算法，能够在保持期望回报的同时更好地控制尾部风险，适用于需要风险敏感决策的应用场景。

Abstract: We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $α$, where $α= 1$ recovers standard expectation-based planning and $α< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.

</details>


### [37] [Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies](https://arxiv.org/abs/2601.20604)
*Gray Cox*

Main category: cs.AI

TL;DR: 该研究提出了一种通过结构化多模型对话实证测试AI对齐策略的方法框架，基于和平研究传统，将AI对齐重新定义为关系问题而非控制问题，通过实验验证不同大语言模型能否实质性参与复杂对齐框架的对话。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐研究缺乏实证测试方法，需要开发能够评估AI系统是否能够参与复杂对齐对话的框架。研究旨在将AI对齐从控制问题重新定义为关系问题，借鉴和平研究传统中的协商、冲突转化和公共资源治理理念。

Method: 采用结构化多模型对话实验设计，为Claude、Gemini和GPT-4o分配四种不同角色（提议者、响应者、监督者、翻译者），在六种条件下进行72轮对话，总计576,822字符的结构化交流，测试AI系统能否实质性参与复杂对齐框架。

Result: AI系统能够有意义地参与和平研究概念讨论，从不同架构视角提出互补性异议，并产生初始框架中未出现的新见解（如"VCW作为过渡框架"）。不同模型关注点各异：Claude强调验证挑战，Gemini关注偏见和可扩展性，GPT-4o突出实施障碍。

Conclusion: 该框架为研究人员提供了在实施前压力测试对齐提案的可复制方法，初步证据表明AI具备VCW所提出的对话推理能力。但对话更多关注过程元素而非AI本质的基础主张，未来研究可探索人-AI混合协议和扩展对话研究。

Abstract: This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.
  Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.
  Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of "VCW as transitional framework." Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.
  The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.

</details>


### [38] [Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation](https://arxiv.org/abs/2601.20614)
*Yanqi Dai,Yuxiang Ji,Xiao Zhang,Yong Wang,Xiangxiang Chu,Zhiwu Lu*

Main category: cs.AI

TL;DR: MathForge框架通过难度感知组策略优化算法和多方面问题重构策略，针对数学推理中的难题进行改进，显著提升大模型数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习可验证奖励方法在算法和数据层面都缺乏对更难题目的重视，这对提升模型未充分发展的能力至关重要。算法上，广泛使用的组相对策略优化存在隐式不平衡问题；数据上，增强方法主要重述问题而非系统增加内在难度。

Method: 提出MathForge框架，包含两个核心组件：1) 难度感知组策略优化算法，通过难度平衡组优势估计纠正隐式不平衡，并采用难度感知问题级加权优先处理难题；2) 多方面问题重构策略，在保持原答案不变的前提下从多个方面重构问题以增加难度。

Result: 大量实验表明，MathForge在各种数学推理任务上显著优于现有方法。框架形成协同循环：MQR扩展数据前沿，DGPO有效学习增强数据。

Conclusion: MathForge通过算法和数据双重改进，针对数学推理中的难题进行优化，有效提升大模型的数学推理能力。代码和增强数据已开源。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.

</details>


### [39] [Investigating the Development of Task-Oriented Communication in Vision-Language Models](https://arxiv.org/abs/2601.20641)
*Boaz Carmeli,Orr Paradise,Shafi Goldwasser,Yonatan Belinkov,Ron Meir*

Main category: cs.AI

TL;DR: LLM智能体能在协作推理任务中发展出不同于自然语言的任务导向通信协议，这些协议具有高效性和隐蔽性特征。


<details>
  <summary>Details</summary>
Motivation: 研究LLM智能体是否能发展出不同于标准自然语言的任务导向通信协议，特别关注这些协议可能表现出的两个核心特性：高效性（更简洁地传递任务相关信息）和隐蔽性（外部观察者难以解读）。

Method: 使用指称游戏框架，让视觉语言模型智能体进行通信，为评估语言变体提供可控、可测量的实验环境。

Result: 实验表明：1）VLM能够发展出有效、适应任务需求的通信模式；2）能够发展出对人类和外部智能体都难以解读的隐蔽协议；3）观察到相似模型之间无需显式共享协议就能自发协调。

Conclusion: 这些发现凸显了任务导向通信的潜力和风险，并确立了指称游戏作为该领域未来研究的宝贵测试平台。

Abstract: We investigate whether \emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.

</details>


### [40] [Implementing Metric Temporal Answer Set Programming](https://arxiv.org/abs/2601.20735)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Susana Hahn,Javier Romero,Torsten Schaub*

Main category: cs.AI

TL;DR: 该研究提出了一种计算性度量ASP方法，用于处理定量时间约束（如持续时间和截止时间），通过差分约束扩展解决时间粒度带来的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统度量ASP在处理细粒度时间约束时面临可扩展性挑战，特别是时间精度会显著加剧ASP的接地瓶颈问题。

Method: 采用ASP的差分约束扩展（线性约束的简化形式），将时间相关方面外部化处理，使度量ASP与时间粒度解耦。

Result: 该方法有效解决了时间精度对系统的影响，实现了不受时间精度影响的解决方案，保持了系统的可扩展性。

Conclusion: 通过差分约束扩展ASP，成功开发出能够处理定量时间约束的计算方法，解决了细粒度时间约束下的可扩展性问题。

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.

</details>


### [41] [MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents](https://arxiv.org/abs/2601.20831)
*Vishnu Sashank Dorbala,Dinesh Manocha*

Main category: cs.AI

TL;DR: MemCtrl框架使用多模态大语言模型在线修剪记忆，通过可训练的记忆头μ决定保留、更新或丢弃观察和反思，显著提升具身任务完成能力。


<details>
  <summary>Details</summary>
Motivation: 现有记忆压缩和检索系统通常将记忆视为大型离线存储空间，这不适合需要在严格内存和计算约束下在线操作的具身智能体。

Method: 提出MemCtrl框架，为多模态大语言模型添加可训练的记忆头μ作为门控机制，在线决定哪些观察或反思需要保留、更新或丢弃。训练两种类型的μ：1)通过离线专家训练，2)通过在线强化学习训练。

Result: 在EmbodiedBench基准测试的多个子集上，μ增强的MLLMs平均提升约16%，特定指令子集提升超过20%。定性分析显示μ增强的MLLMs在长而复杂的指令类型上表现优异。

Conclusion: MemCtrl框架通过在线记忆修剪有效解决了具身智能体的内存约束问题，显著提升了任务完成能力，特别是在处理复杂指令时表现突出。

Abstract: Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.

</details>


### [42] [SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models](https://arxiv.org/abs/2601.20856)
*Sebastiano Monti,Carlo Nicolini,Gianni Pellegrini,Jacopo Staiano,Bruno Lepri*

Main category: cs.AI

TL;DR: 大型语言模型在复杂推理任务上表现良好，但在需要超过25步的长时程规划任务中性能显著下降，表明其前向规划能力存在根本性限制。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在复杂推理任务上的能力已被广泛测试，但其长时程规划能力尚未得到充分研究。本研究旨在系统评估最先进的大型推理模型在规划和长时程推理方面的能力。

Method: 提出了一个基于Sokoban推箱子游戏的新基准测试，该测试经过特意简化以隔离长时程规划与状态持久性。同时研究了为模型配备PDDL（规划领域定义语言）解析、验证和求解工具的效果。

Result: 研究发现，当解决方案需要超过25步移动时，规划性能出现一致性的下降，这表明前向规划能力存在根本性约束。配备PDDL工具只能带来适度的改进，暗示固有的架构限制可能无法仅通过测试时扩展方法克服。

Conclusion: 大型推理模型在长时程规划方面存在显著限制，特别是在需要超过25步的复杂规划任务中。这表明需要新的架构改进，而不仅仅是规模扩展，才能克服这些规划限制。

Abstract: Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.

</details>
