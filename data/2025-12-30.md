<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.SE](#cs.SE) [Total: 11]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration](https://arxiv.org/abs/2512.21360)
*Shuide Wen,Yu Sun,Beier Ku,Zhi Gao,Lijun Ma,Yang Yang,Can Jiao*

Main category: cs.AI

TL;DR: 本文提出了一种基于多模态大语言模型和多智能体协作的HTP绘画测试自动分析系统，解决了传统HTP测试评分标准不统一、依赖主观经验的问题，实现了专家水平的心理评估。


<details>
  <summary>Details</summary>
Motivation: HTP绘画测试作为临床心理学中广泛使用的投射技术，长期面临评分标准不统一、依赖检查者主观经验、缺乏统一量化编码系统等问题，需要开发标准化、客观化的评估工具。

Method: 采用多模态大语言模型（MLLM）和多智能体协作框架，将特征识别与心理推理解耦，整合社会心理学视角和去污名化叙事，构建自动化的HTP测试分析系统。

Result: 定量实验显示MLLM解释与人类专家解释的平均语义相似度约为0.75（标准差约0.05），在结构化专家数据集中相似度提升至0.85，达到专家级理解水平。定性分析表明系统能有效纠正视觉幻觉，生成具有高生态效度和内在一致性的心理报告。

Conclusion: 研究证实了多模态大模型作为投射评估标准化工具的潜力，提出的多智能体框架通过角色分工为数字心理健康服务提供了新范式。

Abstract: Background: The House-Tree-Person (HTP) drawing test, introduced by John Buck in 1948, remains a widely used projective technique in clinical psychology. However, it has long faced challenges such as heterogeneous scoring standards, reliance on examiners subjective experience, and a lack of a unified quantitative coding system.
  Results: Quantitative experiments showed that the mean semantic similarity between Multimodal Large Language Model (MLLM) interpretations and human expert interpretations was approximately 0.75 (standard deviation about 0.05). In structurally oriented expert data sets, this similarity rose to 0.85, indicating expert-level baseline comprehension. Qualitative analyses demonstrated that the multi-agent system, by integrating social-psychological perspectives and destigmatizing narratives, effectively corrected visual hallucinations and produced psychological reports with high ecological validity and internal coherence.
  Conclusions: The findings confirm the potential of multimodal large models as standardized tools for projective assessment. The proposed multi-agent framework, by dividing roles, decouples feature recognition from psychological inference and offers a new paradigm for digital mental-health services.
  Keywords: House-Tree-Person test; multimodal large language model; multi-agent collaboration; cosine similarity; computational psychology; artificial intelligence

</details>


### [2] [A Study of Solving Life-and-Death Problems in Go Using Relevance-Zone Based Solvers](https://arxiv.org/abs/2512.21365)
*Chung-Chin Shih,Ti-Rong Wu,Ting Han Wei,Yu-Shan Hsu,Hung Guei,I-Chen Wu*

Main category: cs.AI

TL;DR: 该研究分析了使用基于相关区域搜索(RZS)和相关区域模式表的计算机围棋求解器解决死活题的行为，发现求解器能识别关键区域、发现罕见模式，但在某些问题上与人类解法存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索当前最先进的计算机围棋求解器在解决死活题时的表现，特别是使用相关区域搜索(RZS)和相关区域模式表技术时，与人类专家解法进行比较分析。

Method: 使用基于相关区域搜索(RZS)和相关区域模式表的计算机围棋求解器，分析七大本因坊赵治勋《死活辞典》中的死活题，比较求解器解法与人类专家给出的标准解法。

Result: 1. 求解器能成功识别每个死活题的相关区域（关键区域）；2. 发现一系列模式，包括罕见模式；3. 在两个问题上找到与给定解法不同的答案；4. 发现求解器存在两个问题：对罕见模式价值判断错误，以及倾向于直接求活而非最大化地盘。

Conclusion: 基于相关区域的求解器在解决死活题方面表现出色，能识别关键区域和发现罕见模式，但与人类解法存在差异，特别是在价值判断和策略偏好方面，需要进一步改进。

Abstract: This paper analyzes the behavior of solving Life-and-Death (L&D) problems in the game of Go using current state-of-the-art computer Go solvers with two techniques: the Relevance-Zone Based Search (RZS) and the relevance-zone pattern table. We examined the solutions derived by relevance-zone based solvers on seven L&D problems from the renowned book "Life and Death Dictionary" written by Cho Chikun, a Go grandmaster, and found several interesting results. First, for each problem, the solvers identify a relevance-zone that highlights the critical areas for solving. Second, the solvers discover a series of patterns, including some that are rare. Finally, the solvers even find different answers compared to the given solutions for two problems. We also identified two issues with the solver: (a) it misjudges values of rare patterns, and (b) it tends to prioritize living directly rather than maximizing territory, which differs from the behavior of human Go players. We suggest possible approaches to address these issues in future work. Our code and data are available at https://rlg.iis.sinica.edu.tw/papers/study-LD-RZ.

</details>


### [3] [Three-way conflict analysis based on alliance and conflict functions](https://arxiv.org/abs/2512.21419)
*Junfang Luo,Mengjun Hu,Guangming Lang,Xin Yang,Keyun Qin*

Main category: cs.AI

TL;DR: 该论文提出将冲突分析中的辅助函数拆分为联盟和冲突两个独立函数，以解决传统方法中正负值聚合导致语义模糊的问题，并应用于三支冲突分析中的智能体、议题和智能体对的划分。


<details>
  <summary>Details</summary>
Motivation: 传统三支冲突分析中，评分函数或辅助函数将对立方面（如联盟与冲突）合并到单一函数中，导致在群体议题或智能体聚合时语义解释困难。例如，联盟+1和冲突-1的平均值与两个中立0关系的结果相同，但实际态度差异很大，需要更清晰的语义表达。

Method: 将传统辅助函数拆分为独立的联盟函数和冲突函数，分别表示智能体间的联盟和冲突关系。基于这一分离，对智能体、议题和智能体对进行三支划分，并应用于解决冲突分析中的关键问题，特别是探索联盟集和策略的概念。

Result: 提出了新的冲突分析模型，能够更精确地表示和聚合智能体间的态度关系，避免了传统方法中的语义模糊问题。通过实际案例应用验证了模型的有效性。

Conclusion: 将联盟和冲突关系分离为独立函数能够提供更清晰的语义解释，改进三支冲突分析中智能体、议题和智能体对的划分方法，为解决冲突分析中的关键问题提供了更有效的框架。

Abstract: Trisecting agents, issues, and agent pairs are essential topics of three-way conflict analysis. They have been commonly studied based on either a rating or an auxiliary function. A rating function defines the positive, negative, or neutral ratings of agents on issues. An auxiliary function defines the alliance, conflict, and neutrality relations between agents. These functions measure two opposite aspects in a single function, leading to challenges in interpreting their aggregations over a group of issues or agents. For example, when studying agent relations regarding a set of issues, a standard aggregation takes the average of an auxiliary function concerning single issues. Therefore, a pair of alliance +1 and conflict -1 relations will produce the same result as a pair of neutrality 0 relations, although the attitudes represented by the two pairs are very different. To clarify semantics, we separate the two opposite aspects in an auxiliary function into a pair of alliance and conflict functions. Accordingly, we trisect the agents, issues, and agent pairs and investigate their applications in solving a few crucial questions in conflict analysis. Particularly, we explore the concepts of alliance sets and strategies. A real-world application is given to illustrate the proposed models.

</details>


### [4] [Three-way decision with incomplete information based on similarity and satisfiability](https://arxiv.org/abs/2512.21421)
*Junfang Luo,Mengjun Hu,Keyun Qin*

Main category: cs.AI

TL;DR: 该论文将三支决策从完全信息推广到不完全信息场景，提出了计算和概念两种形式化的新方法，包括相似度度量、α-相似类、可逼近性等概念，为处理现实世界中的不完全信息问题提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用中信息往往是不完全的，而现有的三支决策方法主要针对完全信息场景。为了将三支决策应用于更实际的现实问题，需要将其推广到处理不完全信息的情况。

Method: 1. 计算形式化：提出对象相似度度量作为等价关系的推广，基于此讨论使用α-相似类和对象可逼近性的两种三支决策方法。
2. 概念形式化：提出公式满足度度量作为完全信息下满足性的定量推广，基于此研究使用公式α-意义集和公式置信度的两种三支决策方法。

Result: 成功将三支决策从完全信息推广到不完全信息场景，提出了四种新的三支决策方法：基于α-相似类和可逼近性的计算形式化方法，以及基于α-意义集和置信度的概念形式化方法。这些方法为处理现实世界中的不完全信息问题提供了新的理论框架。

Conclusion: 该研究将三支决策理论扩展到不完全信息场景，提出了创新的计算和概念形式化方法。特别是可逼近性概念和概念形式化中的两种方法指出了新的研究方向，为处理现实世界中的不完全信息决策问题提供了有前景的理论基础。

Abstract: Three-way decision is widely applied with rough set theory to learn classification or decision rules. The approaches dealing with complete information are well established in the literature, including the two complementary computational and conceptual formulations. The computational formulation uses equivalence relations, and the conceptual formulation uses satisfiability of logic formulas. In this paper, based on a briefly review of these two formulations, we generalize both formulations into three-way decision with incomplete information that is more practical in real-world applications. For the computational formulation, we propose a new measure of similarity degree of objects as a generalization of equivalence relations. Based on it, we discuss two approaches to three-way decision using alpha-similarity classes and approximability of objects, respectively. For the conceptual formulation, we propose a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability with complete information. Based on it, we study two approaches to three-way decision using alpha-meaning sets of formulas and confidence of formulas, respectively. While using similarity classes is a common method of analyzing incomplete information in the literature, the proposed concept of approximability and the two approaches in conceptual formulation point out new promising directions.

</details>


### [5] [LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis](https://arxiv.org/abs/2512.21482)
*Fanwei Zeng,Changtao Miao,Jing Huang,Zhiya Tan,Shutao Gong,Xiaoming Yu,Yang Wang,Huazhe Tan,Weibin Yao,Jianshu Li*

Main category: cs.AI

TL;DR: LogicLens是一个统一的视觉-文本协同推理框架，用于分析文本中心伪造图像，通过跨线索感知的思维链机制和加权多任务奖励函数优化，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC技术的快速发展，复杂的文本中心伪造对社会安全和信息真实性构成重大威胁。现有方法通常局限于粗粒度的视觉分析，缺乏复杂推理能力，并且将检测、定位和解释视为离散的子任务，忽视了它们之间的内在联系。

Method: 提出LogicLens统一框架，采用跨线索感知的思维链(CCT)机制进行深度推理，通过加权多任务奖励函数进行GRPO优化。同时设计了PR²(感知器、推理器、审查器)流水线生成高质量标注，并构建了包含5,397张图像的RealText数据集。

Result: 在T-IC13的零样本评估中，LogicLens比专门化框架高出41.4%，比GPT-4o高出23.4%的宏观平均F1分数。在密集文本T-SROIE数据集上，在mF1、CSS和宏观平均F1等指标上显著领先其他MLLM方法。

Conclusion: LogicLens通过统一的视觉-文本协同推理框架有效解决了文本中心伪造分析问题，在检测、定位和解释任务上表现出色，为信息真实性验证提供了有力工具。

Abstract: Sophisticated text-centric forgeries, fueled by rapid AIGC advancements, pose a significant threat to societal security and information authenticity. Current methods for text-centric forgery analysis are often limited to coarse-grained visual analysis and lack the capacity for sophisticated reasoning. Moreover, they typically treat detection, grounding, and explanation as discrete sub-tasks, overlooking their intrinsic relationships for holistic performance enhancement. To address these challenges, we introduce LogicLens, a unified framework for Visual-Textual Co-reasoning that reformulates these objectives into a joint task. The deep reasoning of LogicLens is powered by our novel Cross-Cues-aware Chain of Thought (CCT) mechanism, which iteratively cross-validates visual cues against textual logic. To ensure robust alignment across all tasks, we further propose a weighted multi-task reward function for GRPO-based optimization. Complementing this framework, we first designed the PR$^2$ (Perceiver, Reasoner, Reviewer) pipeline, a hierarchical and iterative multi-agent system that generates high-quality, cognitively-aligned annotations. Then, we constructed RealText, a diverse dataset comprising 5,397 images with fine-grained annotations, including textual explanations, pixel-level segmentation, and authenticity labels for model training. Extensive experiments demonstrate the superiority of LogicLens across multiple benchmarks. In a zero-shot evaluation on T-IC13, it surpasses the specialized framework by 41.4% and GPT-4o by 23.4% in macro-average F1 score. Moreover, on the challenging dense-text T-SROIE dataset, it establishes a significant lead over other MLLM-based methods in mF1, CSS, and the macro-average F1. Our dataset, model, and code will be made publicly available.

</details>


### [6] [Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model](https://arxiv.org/abs/2512.21540)
*Yanhao Li,Lu Ma,Jiaran Zhang,Lexiang Tang,Wentao Zhang,Guibo Luo*

Main category: cs.AI

TL;DR: Leash是一个自适应长度惩罚和奖励塑形的强化学习框架，用于优化LLM的推理效率，通过动态调整惩罚系数减少60%推理长度同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定长度惩罚，难以调优且无法适应LLM不断演进的推理能力，导致准确性和简洁性之间的次优权衡。

Method: 将长度控制建模为约束优化问题，采用拉格朗日对偶方法动态调整惩罚系数：当生成超过目标长度时加强惩罚，当生成较短时放松惩罚。

Result: 在Deepseek-R1-Distill-Qwen-1.5B和Qwen3-4B-Thinking-2507上，Leash在多样任务（包括数学推理、编码和指令跟随）中平均减少60%推理长度，同时保持竞争力性能。

Conclusion: Leash为开发可控且高效的LLM提供了一个实用有效的范式，能够在推理能力和计算预算之间取得平衡。

Abstract: Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and reward SHaping), a reinforcement learning framework for efficient reasoning in LLMs. We formulate length control as a constrained optimization problem and employ a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. When generations exceed the target length, the penalty is intensified; when they are shorter, it is relaxed. This adaptive mechanism guides models toward producing concise reasoning without sacrificing task performance. Experiments on Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 show that Leash reduces the average reasoning length by 60% across diverse tasks - including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following - while maintaining competitive performance. Our work thus presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets.

</details>


### [7] [A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning](https://arxiv.org/abs/2512.21583)
*Zelin Zang,Wenyi Gu,Siqi Ma,Dan Yang,Yue Shen,Zhu Zhang,Guohui Fan,Wing-Kuen Ling,Fuji Yang*

Main category: cs.AI

TL;DR: 基于LLaVA构建的诊断框架，结合视觉语言对齐与逻辑正则化推理，提升医学多模态AI的诊断准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着医学领域大语言模型和视觉语言模型的快速发展，简单整合临床文本和医学影像并不能保证可靠的推理。现有多模态模型常产生幻觉或不一致的思维链，限制了临床信任度。

Method: 提出基于LLaVA的诊断框架，包含：文本和图像的输入编码器、跨模态对齐的投影模块、将诊断任务分解为步骤的推理控制器，以及将逐步前提组装成可验证结论的逻辑树生成器。

Result: 在MedXpertQA和其他基准测试上的评估显示，该方法提高了多模态任务的诊断准确性，产生更可解释的推理轨迹，同时在纯文本设置中保持竞争力。

Conclusion: 这些结果表明，该方法朝着可信赖的多模态医学AI迈出了有希望的一步。

Abstract: With the rapid growth of large language models (LLMs) and vision-language models (VLMs) in medicine, simply integrating clinical text and medical imaging does not guarantee reliable reasoning. Existing multimodal models often produce hallucinations or inconsistent chains of thought, limiting clinical trust. We propose a diagnostic framework built upon LLaVA that combines vision-language alignment with logic-regularized reasoning. The system includes an input encoder for text and images, a projection module for cross-modal alignment, a reasoning controller that decomposes diagnostic tasks into steps, and a logic tree generator that assembles stepwise premises into verifiable conclusions. Evaluations on MedXpertQA and other benchmarks show that our method improves diagnostic accuracy and yields more interpretable reasoning traces on multimodal tasks, while remaining competitive on text-only settings. These results suggest a promising step toward trustworthy multimodal medical AI.

</details>


### [8] [Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design](https://arxiv.org/abs/2512.21623)
*Takahide Suzuki,Kazuki Nakanishi,Takashi Fujiwara,Hideyuki Shimizu*

Main category: cs.AI

TL;DR: OrchestRA是一个人类在环的多智能体平台，将生物学、化学和药理学统一为自主发现引擎，通过自主执行模拟和结果推理来驱动迭代优化，将药物发现从随机搜索转变为可编程的基于证据的工程学科。


<details>
  <summary>Details</summary>
Motivation: 当前治疗发现面临专业领域碎片化以及计算设计与生理验证之间的执行鸿沟等挑战。虽然生成式AI有前景，但现有模型通常只是被动助手而非自主执行者。

Method: OrchestRA平台包含：1) Orchestrator协调器；2) Biologist Agent利用大规模知识图谱（>1000万关联）进行深度推理确定高置信度靶点；3) Chemist Agent自主检测结构口袋进行从头设计或药物重定位；4) Pharmacologist Agent通过基于生理的药代动力学(PBPK)模拟评估候选药物。这些智能体建立动态反馈循环，药代动力学和毒性特征直接触发结构重新优化。

Result: 该平台将自主执行与人类指导无缝集成，实现了治疗设计的民主化，建立了从药代动力学和毒性特征到结构重新优化的动态反馈循环。

Conclusion: OrchestRA将药物发现从随机搜索转变为可编程的基于证据的工程学科，通过统一生物学、化学和药理学领域的自主智能体，解决了当前治疗发现中的碎片化和执行鸿沟问题。

Abstract: Therapeutic discovery remains a formidable challenge, impeded by the fragmentation of specialized domains and the execution gap between computational design and physiological validation. Although generative AI offers promise, current models often function as passive assistants rather than as autonomous executors. Here, we introduce OrchestRA, a human-in-the-loop multi-agent platform that unifies biology, chemistry, and pharmacology into an autonomous discovery engine. Unlike static code generators, our agents actively execute simulations and reason the results to drive iterative optimization. Governed by an Orchestrator, a Biologist Agent leverages deep reasoning over a massive knowledge graph (>10 million associations) to pinpoint high-confidence targets; a Chemist Agent autonomously detects structural pockets for de novo design or drug repositioning; and a Pharmacologist Agent evaluates candidates via rigorous physiologically based pharmacokinetic (PBPK) simulations. This architecture establishes a dynamic feedback loop where pharmacokinetic and toxicity profiles directly trigger structural reoptimization. By seamlessly integrating autonomous execution with human guidance, OrchestRA democratizes therapeutic design, transforming drug discovery from a stochastic search to a programmable evidence-based engineering discipline.

</details>


### [9] [Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing](https://arxiv.org/abs/2512.21626)
*Hong Xie,Haoran Gu,Yanying Huang,Tao Tan,Defu Lian*

Main category: cs.AI

TL;DR: 该论文提出了一种针对LLM应用、边缘智能等资源分配问题的多臂赌博机变体，其中每个臂具有随机容量，容量单位与奖励函数关联，每次选择具有优先级权重，容量按优先级分配。论文证明了实例无关和实例相关的遗憾下界，设计了离线最优算法和基于UCB的在线算法，其遗憾上界与下界匹配。


<details>
  <summary>Details</summary>
Motivation: 解决LLM应用、边缘智能等场景中的资源分配问题，这些场景中资源具有随机容量，且分配遵循优先级机制。传统多臂赌博机模型无法直接处理这种具有优先级权重和容量约束的资源分配问题。

Method: 1. 提出MSB-PRS（多臂随机赌博机-优先级资源共享）模型；2. 证明实例无关和实例相关的遗憾下界；3. 设计离线最优算法MSB-PRS-OffOpt，计算复杂度为O(MK³)；4. 以MSB-PRS-OffOpt为子程序，设计基于近似上置信界（UCB）的在线算法。

Result: 1. 证明了遗憾下界：实例无关下界Ω(α₁σ√(KMT))，实例相关下界Ω(α₁σ²(M/Δ)lnT)；2. 离线算法MSB-PRS-OffOpt能定位最优分配策略；3. 在线算法的遗憾上界与下界匹配，实例无关上界相差√(KlnKT)因子，实例相关上界相差α₁K²因子。

Conclusion: 该论文成功解决了优先级资源共享机制下的非线性组合效用函数的优化和学习问题，为LLM应用和边缘智能等场景的资源分配提供了理论框架和有效算法，遗憾上下界基本匹配，算法具有理论保证。

Abstract: This paper proposes a variant of multiple-play stochastic bandits tailored to resource allocation problems arising from LLM applications, edge intelligence, etc. The model is composed of $M$ arms and $K$ plays. Each arm has a stochastic number of capacities, and each unit of capacity is associated with a reward function. Each play is associated with a priority weight. When multiple plays compete for the arm capacity, the arm capacity is allocated in a larger priority weight first manner. Instance independent and instance dependent regret lower bounds of $Ω( α_1 σ\sqrt{KM T} )$ and $Ω(α_1 σ^2 \frac{M}Δ \ln T)$ are proved, where $α_1$ is the largest priority weight and $σ$ characterizes the reward tail. When model parameters are given, we design an algorithm named \texttt{MSB-PRS-OffOpt} to locate the optimal play allocation policy with a computational complexity of $O(MK^3)$. Utilizing \texttt{MSB-PRS-OffOpt} as a subroutine, an approximate upper confidence bound (UCB) based algorithm is designed, which has instance independent and instance dependent regret upper bounds matching the corresponding lower bound up to factors of $ \sqrt{K \ln KT }$ and $α_1 K^2$ respectively. To this end, we address nontrivial technical challenges arising from optimizing and learning under a special nonlinear combinatorial utility function induced by the prioritized resource sharing mechanism.

</details>


### [10] [Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning](https://arxiv.org/abs/2512.21699)
*Eranga Bandara,Tharaka Hewa,Ross Gore,Sachin Shetty,Ravi Mukkamala,Peter Foytik,Abdul Rahman,Safdar H. Bouk,Xueping Liang,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 本文提出了一种基于多模型共识和推理层治理的负责任、可解释AI智能体架构，旨在解决自主AI系统在可解释性、问责制、鲁棒性和治理方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统的发展，智能体AI在推理、规划和执行多步骤任务方面展现出强大能力，但自主性的增加带来了可解释性、问责制、鲁棒性和治理方面的关键挑战。现有实现通常强调功能和可扩展性，但缺乏理解决策原理或跨智能体交互执行责任的机制。

Method: 提出一种基于多模型共识和推理层治理的负责任、可解释AI智能体架构。该架构包含异构LLM和VLM智能体联盟，它们从共享输入上下文独立生成候选输出，显式暴露不确定性、分歧和替代解释。专门的推理智能体对这些输出进行结构化整合，强制执行安全和策略约束，减轻幻觉和偏见，并产生可审计、有证据支持的决策。

Result: 通过在多个真实世界智能体AI工作流中评估该架构，证明共识驱动的推理提高了不同应用领域的鲁棒性、透明度和操作信任。

Conclusion: 这项工作为设计既自主可扩展又负责任可解释的智能体AI系统提供了实用指导，通过构建方式确保系统既强大又可信。

Abstract: Agentic AI represents a major shift in how autonomous systems reason, plan, and execute multi-step tasks through the coordination of Large Language Models (LLMs), Vision Language Models (VLMs), tools, and external services. While these systems enable powerful new capabilities, increasing autonomy introduces critical challenges related to explainability, accountability, robustness, and governance, especially when agent outputs influence downstream actions or decisions. Existing agentic AI implementations often emphasize functionality and scalability, yet provide limited mechanisms for understanding decision rationale or enforcing responsibility across agent interactions. This paper presents a Responsible(RAI) and Explainable(XAI) AI Agent Architecture for production-grade agentic workflows based on multi-model consensus and reasoning-layer governance. In the proposed design, a consortium of heterogeneous LLM and VLM agents independently generates candidate outputs from a shared input context, explicitly exposing uncertainty, disagreement, and alternative interpretations. A dedicated reasoning agent then performs structured consolidation across these outputs, enforcing safety and policy constraints, mitigating hallucinations and bias, and producing auditable, evidence-backed decisions. Explainability is achieved through explicit cross-model comparison and preserved intermediate outputs, while responsibility is enforced through centralized reasoning-layer control and agent-level constraints. We evaluate the architecture across multiple real-world agentic AI workflows, demonstrating that consensus-driven reasoning improves robustness, transparency, and operational trust across diverse application domains. This work provides practical guidance for designing agentic AI systems that are autonomous and scalable, yet responsible and explainable by construction.

</details>


### [11] [Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets](https://arxiv.org/abs/2512.21775)
*Matyas Bohacek,Ignacio Vilanova Echavarri*

Main category: cs.AI

TL;DR: 提出合规评级方案（CRS）框架，用于评估生成式AI数据集在透明度、问责制和安全性方面的合规性，并发布开源Python库实现该框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展依赖于大规模开源数据集，但这些数据集通常采用不受限制和不透明的数据收集方式。现有文献主要关注GAI模型的开发和应用，而忽略了数据集创建的伦理和法律考量。此外，数据集在共享、编辑和复制过程中，其来源、合法性和安全性信息往往丢失。

Method: 引入合规评级方案（CRS）框架，评估数据集在关键透明度、问责制和安全性原则方面的合规性。开发并发布基于数据溯源技术的开源Python库，可无缝集成到现有数据集处理和AI训练流程中。

Result: 提出了一个系统性的合规评估框架，并提供了可操作的实现工具。该库具有反应性和前瞻性双重功能，既能评估现有数据集的CRS，也能指导负责任的新数据集爬取和构建。

Conclusion: CRS框架和开源库填补了生成式AI数据集合规性评估的空白，为解决数据集创建中的伦理和法律问题提供了实用工具，有助于促进更负责任的数据集构建和管理实践。

Abstract: Generative Artificial Intelligence (GAI) has experienced exponential growth in recent years, partly facilitated by the abundance of large-scale open-source datasets. These datasets are often built using unrestricted and opaque data collection practices. While most literature focuses on the development and applications of GAI models, the ethical and legal considerations surrounding the creation of these datasets are often neglected. In addition, as datasets are shared, edited, and further reproduced online, information about their origin, legitimacy, and safety often gets lost. To address this gap, we introduce the Compliance Rating Scheme (CRS), a framework designed to evaluate dataset compliance with critical transparency, accountability, and security principles. We also release an open-source Python library built around data provenance technology to implement this framework, allowing for seamless integration into existing dataset-processing and AI training pipelines. The library is simultaneously reactive and proactive, as in addition to evaluating the CRS of existing datasets, it equally informs responsible scraping and construction of new datasets.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [12] [Composition Theorems for f-Differential Privacy](https://arxiv.org/abs/2512.21358)
*Natasha Fernandes,Annabelle McIver,Parastoo Sadeghi*

Main category: cs.CR

TL;DR: fDP与定量信息流的信道模型等价，通过Galois连接证明，支持改进的隐私机制组合分析


<details>
  <summary>Details</summary>
Motivation: fDP作为新的隐私定义能提供更好的隐私损失预测，但需要建立其理论基础并扩展应用能力

Method: 通过统计假设检验基础建立fDP与定量信息流信道模型的等价性，使用两个偏序集之间的Galois连接证明

Result: 证明了fDP与定量信息流信道模型的等价性，获得了新颖的通用组合定理

Conclusion: fDP与定量信息流信道模型的等价性为复杂隐私设计提供了改进的分析框架，支持更好的隐私机制组合分析

Abstract: "f differential privacy" (fDP) is a recent definition for privacy privacy which can offer improved predictions of "privacy loss". It has been used to analyse specific privacy mechanisms, such as the popular Gaussian mechanism. In this paper we show how fDP's foundation in statistical hypothesis testing implies equivalence to the channel model of Quantitative Information Flow. We demonstrate this equivalence by a Galois connection between two partially ordered sets. This equivalence enables novel general composition theorems for fDP, supporting improved analysis for complex privacy designs.

</details>


### [13] [Power Side-Channel Analysis of the CVA6 RISC-V Core at the RTL Level Using VeriSide](https://arxiv.org/abs/2512.21362)
*Behnam Farnaghinejad,Antonio Porsia,Annachiara Ruospo,Alessandro Savino,Stefano Di Carlo,Ernesto Sanchez*

Main category: cs.CR

TL;DR: 该论文评估了CVA6 RISC-V核心的侧信道漏洞，通过VeriSide RTL级功耗分析框架分析软件AES加密，发现相关功耗分析(CPA)存在显著泄漏，能够恢复密钥。


<details>
  <summary>Details</summary>
Motivation: 现代RISC-V处理器不仅需要功能正确性，还需要抵抗侧信道攻击。该研究旨在评估CVA6 RISC-V核心的侧信道漏洞，强调早期RTL评估对未来安全RISC-V设计的重要性。

Method: 使用VeriSide RTL级功耗分析框架，对CVA6 RISC-V核心进行侧信道分析，特别关注软件实现的AES加密，采用相关功耗分析(CPA)技术来评估密钥泄漏情况。

Result: 研究发现CVA6设计的相关功耗分析(CPA)存在显著泄漏，能够成功恢复加密密钥，表明该RISC-V核心在侧信道攻击面前存在安全漏洞。

Conclusion: 这些发现强调了在早期RTL阶段进行侧信道评估的重要性，对于未来开发更安全的RISC-V处理器设计具有指导意义，需要在设计初期就考虑侧信道防护措施。

Abstract: Security in modern RISC-V processors demands more than functional correctness: It requires resilience to side-channel attacks. This paper evaluates the vulnerability of the side channel of the CVA6 RISC-V core by analyzing software-based AES encryption uses an RTL-level power profiling framework called VeriSide. This work represents that this design's Correlation Power Analysis (CPA) reveals significant leakage, enabling key recovery. These findings underscore the importance of early-stage RTL assessments in shaping future secure RISC-V designs.

</details>


### [14] [Key Length-Oriented Classification of Lightweight Cryptographic Algorithms for IoT Security](https://arxiv.org/abs/2512.21368)
*Arsalan Vahi*

Main category: cs.CR

TL;DR: 该研究对物联网中常用的对称轻量级密码进行安全评估调查，提出基于应用特性和密钥大小的分类法，发现密钥长度小于128位的密码安全性不足。


<details>
  <summary>Details</summary>
Motivation: 现有调查主要关注轻量级密码的硬件/软件实现和性能评估，缺乏针对物联网环境特定安全方面的分析，本研究旨在填补这一空白。

Method: 对物联网系统中常用的对称轻量级密码进行全面的安全评估调查，提出两种分类法：基于物联网应用特性的分类和基于密钥大小的安全等级评估。

Result: 研究发现密钥大小是轻量级密码安全性的关键参数，使用短于128位密钥的密码被认为安全性较低，甚至不足以保护敏感数据。

Conclusion: 该研究为物联网应用中的轻量级密码安全评估提供了系统框架，强调了密钥长度对安全性的重要性，为资源受限的实时应用选择合适密码提供了指导。

Abstract: The successful deployment of the Internet of Things (IoT) applications relies heavily on their robust security, and lightweight cryptography is considered an emerging solution in this context. While existing surveys have been examining lightweight cryptographic techniques from the perspective of hardware and software implementations or performance evaluation, there is a significant gap in addressing different security aspects specific to the IoT environment. This study aims to bridge this gap. This research presents a thorough survey focused on the security evaluation of symmetric lightweight ciphers commonly used in IoT systems. The objective of this study is to provide a holistic understanding of lightweight ciphers, emphasizing their security strength, which is an essential consideration for real-time and resource-constrained applications. Furthermore, we propose two taxonomies: one for classifying IoT applications based on their inherent characteristics, and another for evaluating security levels based on key size. Our findings indicate that key size is a critical parameter in the security of lightweight ciphers. Ciphers employing keys shorter than 128 bits are considered less secure or even insecure for protecting sensitive data

</details>


### [15] [A Systematic Review of Technical Defenses Against Software-Based Cheating in Online Multiplayer Games](https://arxiv.org/abs/2512.21377)
*Adwa Alangari,Ohoud Alharbi*

Main category: cs.CR

TL;DR: 这篇系统文献综述调查了在线多人游戏中针对软件作弊的技术防御措施，将现有方法分为服务器端检测、客户端反篡改、内核级反作弊驱动和硬件辅助TEE四类，并评估了它们在检测效果、性能开销、隐私影响和可扩展性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 在线多人游戏中软件作弊问题日益严重，破坏了游戏公平性和玩家体验，需要系统性地分析和评估现有的技术防御方案，为游戏开发者提供有效的反作弊设计指导。

Method: 采用系统文献综述方法，对现有反作弊技术进行分类：1) 服务器端检测方法；2) 客户端反篡改技术；3) 内核级反作弊驱动程序；4) 硬件辅助的可信执行环境(TEEs)。对每类方法从检测效果、性能开销、隐私影响和可扩展性四个维度进行评估。

Result: 分析揭示了反作弊技术中的关键权衡：内核级解决方案具有高可见性但存在隐私和稳定性风险，而服务器端方法侵入性低但洞察能力有限。硬件辅助TEEs在安全性和性能之间提供了较好的平衡，但部署成本较高。

Conclusion: 反作弊是一场持续的技术军备竞赛，需要设计鲁棒且能抵抗对手攻击的反作弊系统。未来的反作弊设计需要在检测能力、性能开销、隐私保护和可扩展性之间找到最佳平衡点。

Abstract: This systematic literature review surveys technical defenses against software-based cheating in online multiplayer games. Categorizing existing approach-es into server-side detection, client-side anti-tamper, kernel-level anti-cheat drivers, and hardware-assisted TEEs. Each category is evaluated in terms of detection effectiveness, perfor-mance overhead, privacy im-pact, and scalability. The analy-sis highlights key trade-offs, particularly between the high visibility of kernel-level solutions and their privacy and stability risks, versus the low intrusive-ness but limited insight of server-side methods. Overall, the re-view emphasizes the ongoing arms race with cheaters and the need for robust, adversary-resistant anti-cheat designs.

</details>


### [16] [LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors](https://arxiv.org/abs/2512.21404)
*Tianwei Lan,Farid Naït-Abdesselam*

Main category: cs.CR

TL;DR: LAMLAD是一个利用大语言模型生成对抗样本攻击Android恶意软件检测系统的框架，通过双智能体架构实现高成功率的规避攻击。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习技术在Android恶意软件检测中广泛应用且有效，但这些模型仍然容易受到对抗攻击，攻击者可以通过精心设计的特征扰动来逃避检测同时保持恶意功能。

Method: LAMLAD采用双智能体架构：LLM操纵器生成真实且保持功能性的特征扰动，LLM分析器指导扰动过程实现成功规避。框架集成了检索增强生成技术以提高效率和上下文感知能力，专注于Drebin风格的特征表示。

Result: 实验结果显示，LAMLAD对三种代表性Android恶意软件检测器的攻击成功率高达97%，每个对抗样本平均仅需3次尝试。同时提出了基于对抗训练的防御策略，能将攻击成功率平均降低30%以上。

Conclusion: LAMLAD展示了利用大语言模型进行对抗攻击的有效性、效率和适应性，同时提出的防御策略显著增强了模型对这类攻击的鲁棒性。

Abstract: The rapid growth in both the scale and complexity of Android malware has driven the widespread adoption of machine learning (ML) techniques for scalable and accurate malware detection. Despite their effectiveness, these models remain vulnerable to adversarial attacks that introduce carefully crafted feature-level perturbations to evade detection while preserving malicious functionality. In this paper, we present LAMLAD, a novel adversarial attack framework that exploits the generative and reasoning capabilities of large language models (LLMs) to bypass ML-based Android malware classifiers. LAMLAD employs a dual-agent architecture composed of an LLM manipulator, which generates realistic and functionality-preserving feature perturbations, and an LLM analyzer, which guides the perturbation process toward successful evasion. To improve efficiency and contextual awareness, LAMLAD integrates retrieval-augmented generation (RAG) into the LLM pipeline. Focusing on Drebin-style feature representations, LAMLAD enables stealthy and high-confidence attacks against widely deployed Android malware detection systems. We evaluate LAMLAD against three representative ML-based Android malware detectors and compare its performance with two state-of-the-art adversarial attack methods. Experimental results demonstrate that LAMLAD achieves an attack success rate (ASR) of up to 97%, requiring on average only three attempts per adversarial sample, highlighting its effectiveness, efficiency, and adaptability in practical adversarial settings. Furthermore, we propose an adversarial training-based defense strategy that reduces the ASR by more than 30% on average, significantly enhancing model robustness against LAMLAD-style attacks.

</details>


### [17] [GoldenFuzz: Generative Golden Reference Hardware Fuzzing](https://arxiv.org/abs/2512.21524)
*Lichao Wu,Mohamadreza Rostami,Huimin Li,Nikhilesh Singh,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: GoldenFuzz是一个两阶段硬件模糊测试框架，通过使用快速的金色参考模型作为被测设备的数字孪生体，实现了测试用例精炼与覆盖探索的部分解耦，显著提升了硬件漏洞发现的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现代硬件系统日益复杂，存在大量bug和安全漏洞。现有硬件模糊测试工具存在语义感知有限、测试精炼效率低、计算开销大等问题，主要依赖于缓慢的设备仿真。

Method: 1. 使用快速、符合ISA标准的金色参考模型作为被测设备的数字孪生体；2. 采用两阶段框架，先在GRM上进行模糊测试实现快速低成本的测试用例精炼；3. 通过连接精心选择的指令块构建测试用例，平衡指令间和指令内质量；4. 利用高覆盖和低覆盖样本的反馈驱动机制增强硬件状态探索能力。

Result: 在三个RISC-V处理器（RocketChip、BOOM、CVA6）上的评估显示：GoldenFuzz在达到最高覆盖率的同时，测试用例长度和计算开销最小；发现了所有已知漏洞和五个新漏洞，其中四个CVSS v3严重性评分超过7/10；在商业BA51-H核心扩展中发现了两个先前未知的漏洞。

Conclusion: GoldenFuzz通过部分解耦测试用例精炼与覆盖探索，利用金色参考模型作为快速数字孪生体，显著提升了硬件模糊测试的效率和效果，能够有效发现高严重性漏洞。

Abstract: Modern hardware systems, driven by demands for high performance and application-specific functionality, have grown increasingly complex, introducing large surfaces for bugs and security-critical vulnerabilities. Fuzzing has emerged as a scalable solution for discovering such flaws. Yet, existing hardware fuzzers suffer from limited semantic awareness, inefficient test refinement, and high computational overhead due to reliance on slow device simulation.
  In this paper, we present GoldenFuzz, a novel two-stage hardware fuzzing framework that partially decouples test case refinement from coverage and vulnerability exploration. GoldenFuzz leverages a fast, ISA-compliant Golden Reference Model (GRM) as a ``digital twin'' of the Device Under Test (DUT). It fuzzes the GRM first, enabling rapid, low-cost test case refinement, accelerating deep architectural exploration and vulnerability discovery on DUT. During the fuzzing pipeline, GoldenFuzz iteratively constructs test cases by concatenating carefully chosen instruction blocks that balance the subtle inter- and intra-instructions quality. A feedback-driven mechanism leveraging insights from both high- and low-coverage samples further enhances GoldenFuzz's capability in hardware state exploration. Our evaluation of three RISC-V processors, RocketChip, BOOM, and CVA6, demonstrates that GoldenFuzz significantly outperforms existing fuzzers in achieving the highest coverage with minimal test case length and computational overhead. GoldenFuzz uncovers all known vulnerabilities and discovers five new ones, four of which are classified as highly severe with CVSS v3 severity scores exceeding seven out of ten. It also identifies two previously unknown vulnerabilities in the commercial BA51-H core extension.

</details>


### [18] [Enhancing Distributed Authorization With Lagrange Interpolation And Attribute-Based Encryption](https://arxiv.org/abs/2512.21525)
*Keshav Sinha,Sumitra,Richa Kumari,Akashdeep Bhardwaj,Shawon Rahman*

Main category: cs.CR

TL;DR: 该论文提出了一种结合流密码加密和Shamir秘密共享的多方执行方案，旨在减少服务器计算开销和响应时间，实现安全数据访问。


<details>
  <summary>Details</summary>
Motivation: 当前安全环境中，用户需要访问大量机密数据，但传统访问控制列表增加了服务器计算开销和响应时间。为了解决这两个问题，需要一种更高效的安全数据访问机制。

Method: 采用双方法：1) 使用基于对合函数的流密码加密文件数据；2) 使用Shamir秘密共享方案将对称密钥分割分发给用户。解密时通过二阶拉格朗日插值从隐藏点重建密钥。

Result: 评估了加密解密时间、吞吐量、计算开销和安全性分析。结果表明该方法能有效减少服务器计算开销。

Conclusion: 提出的多方执行机制能够减少服务器计算负担，未来可用于组织内大规模安全数据共享。

Abstract: In todays security landscape, every user wants to access large amounts of data with confidentiality and authorization. To maintain confidentiality, various researchers have proposed several techniques. However, to access secure data, researchers use access control lists to grant authentication and provide authorization. The above several steps will increase the server's computation overhead and response time. To cope with these two problems, we proposed multiparty execution on the server. In this paper, we introduce two different approaches. The first approach is encryption, utilizing the Involution Function Based Stream Cipher to encrypt the file data. The second approach is key distribution, using the Shamir secret sharing scheme to divide and distribute the symmetric key to every user. The decryption process required key reconstruction, which used second order Lagrange interpolation to reconstruct the secret keys from the hidden points. The process will reduce the server's computational overhead. The results are evaluated based on the encryption and decryption time, throughput, computational overhead, and security analysis. In the future, the proposed mechanism will be used to share large-scale, secure data within the organization.

</details>


### [19] [Verifiable Passkey: The Decentralized Authentication Standard](https://arxiv.org/abs/2512.21663)
*Aditya Mitra,Sibi Chakkaravarthy Sethuraman*

Main category: cs.CR

TL;DR: 本文提出了一种名为"可验证通行密钥"的新标准，旨在解决FIDO2通行密钥存储空间有限和联邦认证隐私风险的问题，允许用户在跨平台使用通行密钥时保护隐私和避免用户追踪。


<details>
  <summary>Details</summary>
Motivation: FIDO2通行密钥虽然提供了防钓鱼的身份验证，但存在两个主要问题：1）安全存储模块（如TPM或物理安全密钥）的存储空间有限，限制了用户可以创建的通行密钥数量；2）使用联邦认证和单点登录（SSO）虽然可以解决存储限制，但身份提供商（IdP）可能追踪用户在不同服务间的活动，带来显著的隐私风险。

Method: 本文引入了一种名为"可验证通行密钥"的新标准。该标准允许用户使用为可验证凭证颁发者创建的通行密钥，在无需牺牲隐私或面临用户追踪风险的情况下，跨任何平台进行身份验证。

Result: 通过提出的可验证通行密钥标准，用户能够克服传统通行密钥的存储限制，同时避免联邦认证带来的隐私风险，实现在保护隐私的前提下跨平台使用通行密钥。

Conclusion: 可验证通行密钥标准为密码身份验证系统提供了一种创新的解决方案，既解决了存储空间限制问题，又保护了用户隐私，避免了身份提供商对用户活动的追踪。

Abstract: Passwordless authentication has revolutionized the way we authenticate across various websites and services. FIDO2 Passkeys, is one of the most-widely adopted standards of passwordless authentication that promises phishing-resistance. However, like any other authentication system, passkeys require the user details to be saved on a centralized server, also known as Relying Party (RP) Server. This has led users to create a new passkey for every new online account. While this just works for a limited number of online accounts, the limited storage space of secure storage modules like TPM or a physical security key limits the number of passkeys a user can have. For example, Yubico Yubikey 5 (firmware 5.0 - 5.6) offers to store only 25 passkeys, while firmware 5.7+ allows to store upto 100 [1]. To overcome this problem, one of the widely adopted approaches is to use Federated Authentication with Single Sign On (SSO). This allows the user to create a passkey for the Identity Provider (IdP) and use the IdP to authenticate to all service providers. This proves to be a significant privacy risk since the IdP can potentially track users across different services. To overcome these limitations, this paper introduces a novel standard 'Verifiable Passkey' that allows the user to use Passkeys created for a Verifiable Credential issuer across any platform without risking privacy or user tracking.

</details>


### [20] [Exploring the Security Threats of Retriever Backdoors in Retrieval-Augmented Code Generation](https://arxiv.org/abs/2512.21681)
*Tian Li,Bo Lin,Shangwen Wang,Yusong Tan*

Main category: cs.CR

TL;DR: 本文首次系统性地研究了检索增强代码生成中的后门攻击威胁，开发了难以检测的VenomRACG攻击方法，发现仅需注入相当于知识库0.05%的漏洞代码就能在51.29%的情况下使后门检索器将漏洞代码排在前5位，导致GPT-4o等模型在超过40%的目标场景中生成漏洞代码。


<details>
  <summary>Details</summary>
Motivation: 检索增强代码生成技术虽然被广泛采用以增强大语言模型的软件开发能力，但其安全影响尚未得到充分探索。本文旨在研究一个关键且隐蔽的威胁：针对检索器组件的后门攻击，这代表了供应链中的重大安全漏洞。

Method: 首先开发了VenomRACG这一新型强大且隐蔽的攻击方法，使中毒样本在统计上与良性代码无法区分，从而在所有评估的防御机制中保持低检测率。然后使用这种方法探索了检索器后门攻击的实际威胁。

Result: 研究发现，仅注入相当于整个知识库大小0.05%的漏洞代码，攻击者就能在51.29%的情况下成功操纵后门检索器将漏洞代码排在前5位。这导致GPT-4o等模型在超过40%的目标场景中生成漏洞代码，而系统的整体性能不受影响。

Conclusion: 检索器后门攻击不是理论上的担忧，而是对软件开发生态系统的实际威胁，现有防御机制对此视而不见，迫切需要采取强有力的安全措施。

Abstract: Retrieval-Augmented Code Generation (RACG) is increasingly adopted to enhance Large Language Models for software development, yet its security implications remain dangerously underexplored. This paper conducts the first systematic exploration of a critical and stealthy threat: backdoor attacks targeting the retriever component, which represents a significant supply-chain vulnerability. It is infeasible to assess this threat realistically, as existing attack methods are either too ineffective to pose a real danger or are easily detected by state-of-the-art defense mechanisms spanning both latent-space analysis and token-level inspection, which achieve consistently high detection rates. To overcome this barrier and enable a realistic analysis, we first developed VenomRACG, a new class of potent and stealthy attack that serves as a vehicle for our investigation. Its design makes poisoned samples statistically indistinguishable from benign code, allowing the attack to consistently maintain low detectability across all evaluated defense mechanisms. Armed with this capability, our exploration reveals a severe vulnerability: by injecting vulnerable code equivalent to only 0.05% of the entire knowledge base size, an attacker can successfully manipulate the backdoored retriever to rank the vulnerable code in its top-5 results in 51.29% of cases. This translates to severe downstream harm, causing models like GPT-4o to generate vulnerable code in over 40% of targeted scenarios, while leaving the system's general performance intact. Our findings establish that retriever backdooring is not a theoretical concern but a practical threat to the software development ecosystem that current defenses are blind to, highlighting the urgent need for robust security measures.

</details>


### [21] [Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding](https://arxiv.org/abs/2512.21698)
*A V Uday Kiran Kandala*

Main category: cs.CR

TL;DR: 提出了一种统一的栅格域隐写框架GPC，能够在渲染文本字形像素空间中嵌入文本、图像、音频和视频等异构数据，通过最小扰动内部墨水像素的基数来表达有效载荷值。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言或结构文本的隐写方法存在局限性，需要一种能够在普通文本中嵌入多种类型数据的视觉隐蔽方法，且操作在字体栅格化之后，仅修改确定性文本渲染管道产生的位图。

Method: 提出Glyph Perturbation Cardinality (GPC)框架，每个字形作为隐蔽编码单元，通过最小扰动内部墨水像素的基数来表达有效载荷值。将多模态输入（图像强度、音频特征、视频帧值）归一化为有界整数序列，分布在多个字形中。解码时重新栅格化覆盖文本，减去规范字形栅格，通过像素计数分析恢复有效载荷值。

Result: 该方法在文本到文本嵌入中进行了演示，并推广到多模态输入。强度增量保持视觉不可感知，同时形成稳定可解码的信号。该方法计算轻量，基于确定性栅格行为，使普通文本能够作为多模态数据嵌入的视觉隐蔽媒介。

Conclusion: GPC框架提供了一种统一的栅格域隐写方法，能够在普通文本中嵌入多种类型数据，具有视觉隐蔽性、计算轻量和基于确定性渲染行为的优势，为多模态数据嵌入提供了新的可能性。

Abstract: This work introduces a unified raster domain steganographic framework, termed as the Glyph Perturbation Cardinality (GPC) framework, capable of embedding heterogeneous data such as text, images, audio, and video directly into the pixel space of rendered textual glyphs. Unlike linguistic or structural text based steganography, the proposed method operates exclusively after font rasterization, modifying only the bitmap produced by a deterministic text rendering pipeline. Each glyph functions as a covert encoding unit, where a payload value is expressed through the cardinality of minimally perturbed interior ink pixels. These minimal intensity increments remain visually imperceptible while forming a stable and decodable signal. The framework is demonstrated for text to text embedding and generalized to multimodal inputs by normalizing image intensities, audio derived scalar features, and video frame values into bounded integer sequences distributed across glyphs. Decoding is achieved by re-rasterizing the cover text, subtracting canonical glyph rasters, and recovering payload values via pixel count analysis. The approach is computationally lightweight, and grounded in deterministic raster behavior, enabling ordinary text to serve as a visually covert medium for multimodal data embedding.

</details>


### [22] [Assessing the Effectiveness of Membership Inference on Generative Music](https://arxiv.org/abs/2512.21762)
*Kurtis Chow,Omar Samiullah,Vinesh Sridhar,Hewen Zhang*

Main category: cs.CR

TL;DR: 该论文首次研究了成员推理攻击在生成音乐领域的应用，发现现有攻击方法对MuseGAN模型效果有限，音乐数据对成员推理攻击具有较强抵抗力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI快速发展引发了对用户隐私和版权训练的担忧。成员推理攻击处于这两个关切的交叉点，但尚无研究探讨该攻击在生成音乐领域的应用。考虑到音乐产业价值数十亿美元且艺术家需要确定其作品是否被未经许可使用，这是一个迫切需要研究的问题。

Method: 对流行的生成音乐模型MuseGAN应用多种现有成员推理攻击方法，进行初步研究，评估这些攻击在生成音乐领域的有效性。

Result: 研究发现音乐数据对已知的成员推理技术具有相当强的抵抗力，与先前生成音频成员推理攻击的研究结果相似，现有攻击方法对生成音乐模型效果有限。

Conclusion: 这是首次针对生成音乐的成员推理攻击研究，发现音乐数据对现有攻击方法具有较强抵抗力，但考虑到音乐产业的重要性和版权问题，这一领域仍需进一步深入研究。

Abstract: Generative AI systems are quickly improving, now able to produce believable output in several modalities including images, text, and audio. However, this fast development has prompted increased scrutiny concerning user privacy and the use of copyrighted works in training. A recent attack on machine-learning models called membership inference lies at the crossroads of these two concerns. The attack is given as input a set of records and a trained model and seeks to identify which of those records may have been used to train the model. On one hand, this attack can be used to identify user data used to train a model, which may violate their privacy especially in sensitive applications such as models trained on medical data. On the other hand, this attack can be used by rights-holders as evidence that a company used their works without permission to train a model.
  Remarkably, it appears that no work has studied the effect of membership inference attacks (MIA) on generative music. Given that the music industry is worth billions of dollars and artists would stand to gain from being able to determine if their works were being used without permission, we believe this is a pressing issue to study. As such, in this work we begin a preliminary study into whether MIAs are effective on generative music. We study the effect of several existing attacks on MuseGAN, a popular and influential generative music model. Similar to prior work on generative audio MIAs, our findings suggest that music data is fairly resilient to known membership inference techniques.

</details>


### [23] [Securing Cross-Domain Internet of Drones: An RFF-PUF Allied Authenticated Key Exchange Protocol With Over-the-Air Enrollment](https://arxiv.org/abs/2512.21827)
*Xuanyu Chen,Yue Zheng,Junqing Zhang,Guanxiong Shen,Chip-Hong Chang*

Main category: cs.CR

TL;DR: 提出一种轻量级无人机物联网相互认证机制，结合射频指纹和物理不可克隆函数技术，实现安全的无人机间及无人机与地面站通信，无需第三方参与和秘密存储。


<details>
  <summary>Details</summary>
Motivation: 无人机物联网面临跨异构不信任域的安全通信挑战，现有解决方案存在计算开销大、依赖第三方、需要秘密存储、要求严格注册环境等问题，不适用于动态跨域部署。

Method: 整合射频指纹技术和物理不可克隆函数：RFF用于空中注册实现设备识别，PUF作为信任根建立相互认证，并结合一次性密码本加密实现即时密钥生成，无需在无人机中存储秘密。

Result: 非正式安全分析和ProVerif形式化验证证明协议能抵抗常见安全攻击，在安全特性、计算开销、通信开销和存储开销方面均优于现有无人机物联网认证方案。

Conclusion: 提出的轻量级相互认证机制解决了无人机物联网跨域部署的安全挑战，通过RFF和PUF技术的整合实现了安全、高效、无需秘密存储的认证方案。

Abstract: The Internet of Drones (IoD) is an emerging and crucial paradigm enabling advanced applications that require seamless, secure communication across heterogeneous and untrusted domains. In such environments, access control and the transmission of sensitive data pose significant security challenges for IoD systems, necessitating the design of lightweight mutual authentication and key exchange protocols. Existing solutions are often hampered by high computation overhead, reliance on third parties, the requirement for secret storage in resource-constrained drones, and the need for a strictly controlled enrollment environment. These limitations make them impractical for dynamic cross-domain deployment. To address these limitations, we propose a lightweight mutual authentication mechanism that integrates Radio Frequency Fingerprint (RFF) and Physical Unclonable Function (PUF) technologies for secure drone-to-drone (D2D) and drone-to-ground station server (D2G) communication. RFF-based device identification is used to achieve over-the-air (OTA) enrollment, while the PUF serves as the root of trust for establishing mutual authentication among communication parties. Additionally, the on-the-fly key generation capability of the PUF is co-designed with One-Time-Pad (OTP) encryption to realize ephemeral keying and eliminate the need for storing secrets within drones. Both informal security analysis and ProVerif-based formal security verification comprehensively demonstrate the resilience of our protocol against common security attacks. The proposed protocol also outperforms existing IoD authentication schemes in terms of security features, as well as computation, communication, and storage overhead.

</details>


### [24] [Abstraction of Trusted Execution Environments as the Missing Layer for Broad Confidential Computing Adoption: A Systematization of Knowledge](https://arxiv.org/abs/2512.22090)
*Quentin Michaud,Sara Ramezanian,Dhouha Ayed,Olivier Levillain,Joaquin Garcia-Alfaro*

Main category: cs.CR

TL;DR: 该论文系统化分析了可信执行环境（TEEs）技术，对现有TEE解决方案进行分类，并提出基于不同设计选择的抽象层知识体系，指出WebAssembly是支持最多特性的有前景方法，同时讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 可信执行环境（TEEs）技术存在多种解决方案，各有不同特性，需要抽象层来统一生态系统，使应用开发者和系统管理员能够尽可能广泛和高效地利用机密计算。

Method: 首先概述代表性TEE技术，描述和总结每个TEE生态系统，根据主要设计选择进行分类。然后提出围绕每个设计选择的抽象层知识体系，描述每个设计的基础技术以及每个抽象层的内部工作原理和特性。

Result: 研究揭示了改进现有抽象层解决方案的机会，并突出显示WebAssembly是支持最大特性集的有前景方法。研究还识别了现有抽象层解决方案的改进空间。

Conclusion: 论文以未来研究方向讨论结束，包括未来抽象层如何演进以及与机密计算生态系统集成的方式。WebAssembly作为支持最多特性的方法显示出巨大潜力，为TEE生态系统的统一提供了重要方向。

Abstract: Trusted Execution Environments (TEEs) protect sensitive code and data from the operating system, hypervisor, or other untrusted software. Different solutions exist, each proposing different features. Abstraction layers aim to unify the ecosystem, allowing application developers and system administrators to leverage confidential computing as broadly and efficiently as possible. We start with an overview of representative available TEE technologies. We describe and summarize each TEE ecosystem, classifying them in different categories depending on their main design choices. Then, we propose a systematization of knowledge focusing on different abstraction layers around each design choice. We describe the underlying technologies of each design, as well as the inner workings and features of each abstraction layer. Our study reveals opportunities for improving existing abstraction layer solutions. It also highlights WebAssembly, a promising approach that supports the largest set of features. We close with a discussion on future directions for research, such as how future abstraction layers may evolve and integrate with the confidential computing ecosystem.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [25] [Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey](https://arxiv.org/abs/2512.21347)
*Vítor Mateus de Brito,Kleinner Farias*

Main category: cs.SE

TL;DR: 该研究通过调查46名行业专业人士，实证分析了LLM在软件工程中的采用情况，发现开发者对LLM持积极态度，但也担忧认知依赖、安全风险和技术自主性侵蚀等问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在软件工程中的快速发展和广泛应用，这些工具已深度嵌入开发者的日常工作流程。理解LLMs在行业中的实际使用情况、开发者对其的认知和态度变得至关重要。本研究旨在通过实证调查填补学术讨论与真实软件开发实践之间的差距。

Method: 本研究采用实证研究方法，对46名具有不同教育背景和经验水平的行业专业人士进行了问卷调查。通过收集和分析这些软件开发从业者的实际使用经验和观点，来了解LLM在软件工程中的采用情况和影响。

Result: 调查结果显示：1）开发者对LLM持积极态度，特别是在快速解决技术问题、改进文档支持和增强源代码标准化方面；2）同时存在对认知依赖、安全风险和技术自主性可能被侵蚀的担忧；3）LLM在软件工程中的应用带来了效率提升，但也引发了新的挑战。

Conclusion: 研究强调需要批判性和有监督地使用基于LLM的工具。研究结果为开发者和研究人员提供了可操作的见解，帮助他们以更有效、负责任和安全的方式采用和发展LLM技术。同时，研究也呼吁未来对LLM的认知、伦理和组织影响进行深入研究。

Abstract: The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.

</details>


### [26] [CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation](https://arxiv.org/abs/2512.21351)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: CosmoCore-Evo在CosmoCore基础上引入进化算法，将RL轨迹视为"基因组"进行变异和选择，提升代码生成任务的适应性和新颖性


<details>
  <summary>Details</summary>
Motivation: 受人类进化中自然选择和适应的启发，旨在让智能体突破训练模式限制，在分布偏移环境中（如API变更、新库引入）实现更好的适应性和新颖性

Method: 将RL轨迹视为基因组，在夜间回放阶段进行变异和选择；增强Dream Queue，包含高适应度轨迹变异和企业调优的适应度函数（考虑效率、合规性、可扩展性）

Result: 在HumanEval变体、BigCodeBench和自定义PySpark管道模拟等基准测试中，相比原始CosmoCore和PPO、REAMER等基线，实现高达35%的解决方案新颖性提升和25%的适应速度提升

Conclusion: 进化组件在弥合LLM智能体的感知差距方面发挥关键作用，CosmoCore-Evo通过进化算法显著提升了代码生成任务的适应性和新颖性

Abstract: Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.

</details>


### [27] [Multi-Agent LLM Committees for Autonomous Software Beta Testing](https://arxiv.org/abs/2512.21352)
*Sumanth Bharadwaj Hachalli Karanam,Dhiwahar Adhithya Kennady*

Main category: cs.SE

TL;DR: 提出多智能体委员会框架，通过视觉增强的LLM协作进行软件测试，相比单智能体基线显著提升任务成功率


<details>
  <summary>Details</summary>
Motivation: 传统软件beta测试成本高、耗时长，单智能体LLM方法存在幻觉和行为不一致问题，需要更可靠的自动化测试方案

Method: 采用多智能体委员会框架，结合视觉增强的多样化LLM，通过三轮投票协议达成共识，系统探索Web应用

Result: 多智能体委员会在84次实验运行中达到89.5%总体任务成功率，2-4智能体配置达到91.7-100%，相比单智能体基线提升13.7-22.0个百分点

Conclusion: 多智能体委员会框架能有效解决单智能体LLM的局限性，实现高效、可靠的软件测试，适用于CI/CD流水线部署

Abstract: Manual software beta testing is costly and time-consuming, while single-agent large language model (LLM) approaches suffer from hallucinations and inconsistent behavior. We propose a multi-agent committee framework in which diverse vision-enabled LLMs collaborate through a three-round voting protocol to reach consensus on testing actions. The framework combines model diversity, persona-driven behavioral variation, and visual user interface understanding to systematically explore web applications. Across 84 experimental runs with 9 testing personas and 4 scenarios, multi-agent committees achieve an 89.5 percent overall task success rate. Configurations with 2 to 4 agents reach 91.7 to 100 percent success, compared to 78.0 percent for single-agent baselines, yielding improvements of 13.7 to 22.0 percentage points. At the action level, the system attains a 93.1 percent success rate with a median per-action latency of 0.71 seconds, enabling real-time and continuous integration testing. Vision-enabled agents successfully identify user interface elements, with navigation and reporting achieving 100 percent success and form filling achieving 99.2 percent success. We evaluate the framework on WebShop and OWASP benchmarks, achieving 74.7 percent success on WebShop compared to a 50.1 percent published GPT-3 baseline, and 82.0 percent success on OWASP Juice Shop security testing with coverage of 8 of the 10 OWASP Top 10 vulnerability categories. Across 20 injected regressions, the committee achieves an F1 score of 0.91 for bug detection, compared to 0.78 for single-agent baselines. The open-source implementation enables reproducible research and practical deployment of LLM-based software testing in CI/CD pipelines.

</details>


### [28] [Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors](https://arxiv.org/abs/2512.21431)
*Hridya Dhulipala,Xiaokai Rong,Tien N. Nguyen*

Main category: cs.SE

TL;DR: Cerberus是一个无需实际执行的预测性覆盖引导测试框架，使用LLMs生成触发运行时错误的输入，并通过代码覆盖预测和错误检测来发现代码片段中的运行时异常。


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，需要在未实际执行的情况下检测代码片段中的运行时错误和异常，特别是在将在线代码片段集成到代码库之前进行安全检测。

Method: Cerberus采用两阶段反馈循环：第一阶段同时提高代码覆盖率和检测运行时错误；当覆盖率达到100%或最大值时，第二阶段专注于检测运行时错误。框架使用LLMs生成触发错误的输入，并进行代码覆盖预测和错误检测，无需实际执行代码。

Result: 实证评估表明，Cerberus在（不）完整代码片段测试中表现优于传统和基于学习的测试框架，能更高效地生成高覆盖率测试用例，从而发现更多运行时错误。

Conclusion: Cerberus通过结合LLMs的预测能力和两阶段反馈机制，实现了无需执行的代码测试，有效提高了代码覆盖率和运行时错误检测能力。

Abstract: In several software development scenarios, it is desirable to detect runtime errors and exceptions in code snippets without actual execution. A typical example is to detect runtime exceptions in online code snippets before integrating them into a codebase. In this paper, we propose Cerberus, a novel predictive, execution-free coverage-guided testing framework. Cerberus uses LLMs to generate the inputs that trigger runtime errors and to perform code coverage prediction and error detection without code execution. With a two-phase feedback loop, Cerberus first aims to both increasing code coverage and detecting runtime errors, then shifts to focus only detecting runtime errors when the coverage reaches 100% or its maximum, enabling it to perform better than prompting the LLMs for both purposes. Our empirical evaluation demonstrates that Cerberus performs better than conventional and learning-based testing frameworks for (in)complete code snippets by generating high-coverage test cases more efficiently, leading to the discovery of more runtime errors.

</details>


### [29] [Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing](https://arxiv.org/abs/2512.21440)
*Hridya Dhulipala,Xiaokai Rong,Aashish Yadavally,Tien N. Nguyen*

Main category: cs.SE

TL;DR: FuzzWise：基于LLM的多智能体框架，将种子生成与最小化集成到单一流程中，通过预测性代码覆盖评估生成高质量初始种子语料库


<details>
  <summary>Details</summary>
Motivation: 在基于突变的灰盒模糊测试中，生成高质量的初始输入种子对有效模糊测试至关重要。传统方法需要分别进行大规模语料库生成和后续最小化两个阶段，效率较低且资源消耗大。

Method: FuzzWise采用基于大语言模型的多智能体框架：第一个LLM智能体为目标程序生成测试用例；第二个LLM智能体作为预测性代码覆盖模块，评估每个生成的测试用例是否能提升当前语料库的整体覆盖率。这种流线化流程允许立即评估每个新生成测试种子的贡献。

Result: 实证评估表明，FuzzWise生成的测试用例数量显著少于基线方法。尽管测试用例数量较少，但FuzzWise实现了更高的代码覆盖率，并触发了更多的运行时错误。此外，在生成捕获更多错误的初始语料库方面，FuzzWise更具时间效率和覆盖效率。

Conclusion: FuzzWise通过LLM驱动的预测方法，将种子生成与最小化集成到单一流程中，无需实际执行程序即可评估测试用例价值，节省计算资源和时间，特别适用于不希望或无法实际执行的场景，能够生成高质量的初始种子语料库。

Abstract: In mutation-based greybox fuzzing, generating high-quality input seeds for the initial corpus is essential for effective fuzzing. Rather than conducting separate phases for generating a large corpus and subsequently minimizing it, we propose FuzzWise which integrates them into one process to generate the optimal initial corpus of seeds (ICS). FuzzWise leverages a multi-agent framework based on Large Language Models (LLMs). The first LLM agent generates test cases for the target program. The second LLM agent, which functions as a predictive code coverage module, assesses whether each generated test case will enhance the overall coverage of the current corpus. The streamlined process allows each newly generated test seed to be immediately evaluated for its contribution to the overall coverage. FuzzWise employs a predictive approach using an LLM and eliminates the need for actual execution, saving computational resources and time, particularly in scenarios where the execution is not desirable or even impossible. Our empirical evaluation demonstrates that FuzzWise generates significantly fewer test cases than baseline methods. Despite the lower number of test cases, FuzzWise achieves high code coverage and triggers more runtime errors compared to the baselines. Moreover, it is more time-efficient and coverage-efficient in producing an initial corpus catching more errors.

</details>


### [30] [Code Clone Refactoring in C# with Lambda Expressions](https://arxiv.org/abs/2512.21511)
*Takuto Kawamoto,Yoshiki Higo*

Main category: cs.SE

TL;DR: 提出一种针对C#语言的代码克隆重构方法，使用lambda表达式参数化行为差异，评估显示35%的克隆对适合重构，其中28.9%成功重构


<details>
  <summary>Details</summary>
Motivation: 现有"提取方法"重构技术主要针对Java程序，使用参数化方法处理代码克隆中的差异，但不同编程语言的特性（特别是lambda表达式规范）影响该技术的适用性，需要针对C#语言开发专门的克隆重构方法

Method: 提出C#特定的技术，使用lambda表达式分析和合并代码克隆。通过NiCad克隆检测器检测克隆对，然后应用提出的方法评估哪些克隆对可以成功重构。对确定为可重构的克隆对实际尝试重构操作

Result: 从22个项目的2,217个克隆对中，提出的方法确定35.0%的克隆对适合重构。在这些适合重构的克隆对中，28.9%成功完成重构。这表明C#特定的lambda表达式方法在代码克隆重构中具有实际可行性

Conclusion: 不同编程语言的最优"提取方法"重构技术确实存在差异，针对C#语言的lambda表达式方法能够有效处理代码克隆重构问题。该方法在C#项目中具有实际应用价值，能够成功重构相当比例的代码克隆

Abstract: "Extract Method" refactoring is a technique for consolidating code clones. Parameterization approaches are used to extract a single method from multiple code clones that contain differences. This approach parameterizes expressions and behaviors within a method. In particular, behavior parameterization has been extensively studied in Java programs, but little research has been conducted on other programming languages.
  Lambda expressions can be used to parameterize behaviors, but the specifications of each programming language significantly affect the applicability of this technique. Therefore, the optimal "Extract Method" approach may vary depending on the programming language.
  In this study, we propose a C#-specific technique that uses lambda expressions to analyze and consolidate code clones. We evaluated our proposed method by applying it to code clones detected by the NiCad clone detector and measuring how many of them could be successfully consolidated.
  In total, 2,217 clone pairs from 22 projects were included in our evaluation. For the clone pairs determined to be refactorable, we also attempted refactoring actually. The proposed approach determined that 35.0% of all clone pairs were suitable for refactoring. Among these, 28.9% were successfully refactored.

</details>


### [31] [Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code](https://arxiv.org/abs/2512.21591)
*Shuo Sun,Shixin Zhang,Jiwei Yan,Jun Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 提出了一种基于LLM的仓库级Python类型推断方法，通过类型与依赖关系的协同演化实现准确推断


<details>
  <summary>Details</summary>
Motivation: Python的动态类型机制虽然灵活，但容易导致运行时类型错误，现有类型推断工具主要处理孤立代码片段，难以应对仓库级别的复杂跨过程依赖关系

Method: 构建实体依赖图(EDG)建模仓库中的对象和类型依赖关系，采用迭代式类型推断方法，让类型和依赖关系在每次迭代中协同演化，并引入类型检查器循环策略实时验证和修正推断结果

Result: 在12个复杂Python仓库上的评估显示，TypeSim得分0.89，TypeExact得分0.84，分别比最强基线提升27%和40%，同时减少了92.7%的工具引入的新类型错误

Conclusion: 该方法在仓库级Python类型推断方面取得了显著进展，为实际Python开发提供了自动化、可靠的类型标注解决方案

Abstract: Python's dynamic typing mechanism, while promoting flexibility, is a significant source of runtime type errors that plague large-scale software, which inspires the automatic type inference techniques. Existing type inference tools have achieved advances in type inference within isolated code snippets. However, repository-level type inference remains a significant challenge, primarily due to the complex inter-procedural dependencies that are difficult to model and resolve. To fill this gap, we present \methodName, a novel approach based on LLMs that achieves repository-level type inference through the co-evolution of types and dependencies. \methodName~constructs an Entity Dependency Graph (EDG) to model the objects and type dependencies across the repository. During the inference process, it iteratively refines types and dependencies in EDG for accurate type inference. Our key innovations are: (1) an EDG model designed to capture repository-level type dependencies; (2) an iterative type inference approach where types and dependencies co-evolve in each iteration; and (3) a type-checker-in-the-loop strategy that validates and corrects inferences on-the-fly, thereby reducing error propagation. When evaluated on 12 complex Python repositories, \methodName~significantly outperformed prior works, achieving a \textit{TypeSim} score of 0.89 and a \textit{TypeExact} score of 0.84, representing a 27\% and 40\% relative improvement over the strongest baseline. More importantly, \methodName~removed new type errors introduced by the tool by 92.7\%. This demonstrates a significant leap towards automated, reliable type annotation for real-world Python development.

</details>


### [32] [The State of the SBOM Tool Ecosystems: A Comparative Analysis of SPDX and CycloneDX](https://arxiv.org/abs/2512.21781)
*Abdul Ali Bangash,Tongxu Ge,Zhimin Zhao,Arshdeep Singh,Zitao Wang,Bram Adams*

Main category: cs.SE

TL;DR: 该研究对SBOM（软件物料清单）的两种主流格式SPDX和CycloneDX的工具生态系统进行了定量比较，分析了170个公开SBOM工具、36,990个问题报告以及各自生态系统的健康指标，揭示了两种格式在开发者参与度、工具成熟度和行业采用方面的互补优势。


<details>
  <summary>Details</summary>
Motivation: SBOM的采用依赖于工具生态系统，而SPDX和CycloneDX作为两种主流格式，其生态系统在成熟度、工具支持和社区参与方面存在显著差异。研究旨在通过定量比较为开发者和从业者提供关于这两种生态系统互补优势的见解，并识别相互增强的机会。

Method: 1. 对170个公开宣传的SBOM工具进行定量比较，识别每种格式的增强领域；2. 比较两种生态系统的健康指标（171个CycloneDX工具 vs 470个SPDX工具），评估稳健性和成熟度；3. 分析开源工具的36,990个问题报告，识别挑战和开发机会；4. 调查使用每种工具生态系统的前250个开源项目，比较它们的健康指标。

Result: 研究发现两种生态系统具有明显不同的特征：使用CycloneDX工具的项目表现出更高的开发者参与度和某些健康指标，而SPDX工具则受益于更成熟的生态系统，拥有更广泛的工具可用性和已建立的行业采用。两种格式在开发者参与度、工具成熟度和行业采用方面展现出互补优势。

Conclusion: 这项研究为开发者、贡献者和从业者提供了关于SPDX和CycloneDX生态系统互补优势的见解，识别了相互增强的机会。研究结果表明两种格式各有优势，为SBOM生态系统的进一步发展提供了有价值的参考。

Abstract: A Software Bill of Materials (SBOM) provides transparency by documenting software component metadata and dependencies. However, SBOM adoption depends on tool ecosystems. With two dominant formats: SPDX and CycloneDX - the ecosystems vary significantly in maturity, tool support, and community engagement. We conduct a quantitative comparison of use cases for 170 publicly advertised SBOM tools, identifying enhancement areas for each format. We compare health metrics of both ecosystems (171 CycloneDX versus 470 SPDX tools) to evaluate robustness and maturity. We quantitatively compare 36,990 issue reports from open-source tools to identify challenges and development opportunities. Finally, we investigate the top 250 open-source projects using each tool ecosystem and compare their health metrics. Our findings reveal distinct characteristics: projects using CycloneDX tools demonstrate higher developer engagement and certain health indicators, while SPDX tools benefit from a more mature ecosystem with broader tool availability and established industry adoption. This research provides insights for developers, contributors, and practitioners regarding complementary strengths of these ecosystems and identifies opportunities for mutual enhancement.

</details>


### [33] [A Story About Cohesion and Separation: Label-Free Metric for Log Parser Evaluation](https://arxiv.org/abs/2512.21811)
*Qiaolin Qin,Jianchen Zhao,Heng Li,Weiyi Shang,Ettore Merlo*

Main category: cs.SE

TL;DR: 提出PMSS——一种无需标签的日志解析器评估指标，通过聚类质量和模板相似度分析来评估解析器性能，解决了现有评估方法依赖标注数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有日志解析器评估指标严重依赖标注数据，限制了在工业场景中的应用；同时不同版本的真实标签会导致不一致的性能结论，需要一种无需标签的评估方法。

Method: 提出PMSS指标，结合中位数轮廓分析和Levenshtein距离来评估解析器的分组质量和模板质量，具有接近线性的时间复杂度。

Result: PMSS与基于标签的FGA和FTA指标显著正相关（p<1e-8），Spearman相关系数分别为0.648和0.587；PMSS最佳解析器与FGA最佳解析器性能差异仅2.1%。

Conclusion: PMSS为日志解析器评估提供了无需标签的替代方案，在真实标签不一致或不可用时具有重要价值，并提供了使用该指标进行解析器选择的指南。

Abstract: Log parsing converts log messages into structured event templates, allowing for automated log analysis and reducing manual inspection effort. To select the most compatible parser for a specific system, multiple evaluation metrics are commonly used for performance comparisons. However, existing evaluation metrics heavily rely on labeled log data, which limits prior studies to a fixed set of datasets and hinders parser evaluations and selections in the industry. Further, we discovered that different versions of ground-truth used in existing studies can lead to inconsistent performance conclusions. Motivated by these challenges, we propose a novel label-free template-level metric, PMSS (parser medoid silhouette score), to evaluate log parser performance. PMSS evaluates both parser grouping and template quality with medoid silhouette analysis and Levenshtein distance within a near-linear time complexity in general. To understand its relationship with label-based template-level metrics, FGA and FTA, we compared their evaluation outcomes for six log parsers on the standard corrected Loghub 2.0 dataset. Our results indicate that log parsers achieving the highest PMSS or FGA exhibit comparable performance, differing by only 2.1% on average in terms of the FGA score; the difference is 9.8% for FTA. PMSS is also significantly (p<1e-8) and positively correlated to both FGA and FTA: the Spearman's rho correlation coefficient of PMSS-FGA and PMSS-FTA are respectively 0.648 and 0.587, close to the coefficient between FGA and FTA (0.670). We further extended our discussion on how to interpret the conclusions from different metrics, identifying challenges in using PMSS, and provided guidelines on conducting parser selections with our metric. PMSS provides a valuable evaluation alternative when ground-truths are inconsistent or labels are unavailable.

</details>


### [34] [HALF: Process Hollowing Analysis Framework for Binary Programs with the Assistance of Kernel Modules](https://arxiv.org/abs/2512.22043)
*Zhangbo Long,Letian Sha,Jiaye Pan,Dongpeng Xu,Yifei Huang,Fu Xiao*

Main category: cs.SE

TL;DR: 提出一个新的二进制程序分析框架，通过内核模块扩展传统动态二进制插桩能力，采用解耦分析思想，在容器进程中构建分析环境，提高细粒度分析的可用性和性能。


<details>
  <summary>Details</summary>
Motivation: 二进制程序分析在系统安全中非常重要，但细粒度分析（如动态污点分析）存在部署性差、内存占用高、性能开销大等问题，需要更好地适应内存破坏利用和沙箱规避恶意软件等新分析场景。

Method: 1. 使用内核模块扩展传统动态二进制插桩的分析能力；2. 基于解耦分析思想，通过进程空洞技术在容器进程中构建分析环境；3. 复用现有动态二进制插桩平台功能，减少对目标程序执行的影响。

Result: 在Windows平台上实现了原型系统，通过大量基准测试和实际程序的实验验证了框架的有效性和性能，并通过分析实际利用程序和恶意代码验证了框架的实用性价值。

Conclusion: 该框架提高了细粒度二进制程序分析的可用性和性能，能够有效分析内存破坏利用和沙箱规避恶意软件，具有实际应用价值。

Abstract: Binary program analysis is still very important in system security. There are many practical achievements in binary code analysis, but fine-grained analysis such as dynamic taint analysis, is constantly studied due to the problem of deployability, high memory usage, and performance overhead, so as to better adapt to the new analysis scenario, such as memory corruption exploits and sandbox evasion malware. This paper presents a new binary program analysis framework, in order to improve the usability and performance of fine-grained analysis. The framework mainly uses the kernel module to further expand the analysis capability of the traditional dynamic binary instrumentation. Then, based on the idea of decoupling analysis, the analysis environment is constructed in the container process through process hollowing techniques in a new way. It can reuse the functions of the existing dynamic binary instrumentation platforms and also reduce the impact on the execution of the target program. The prototype is implemented on the Windows platform. The validity and performance of the framework are verified by a large number of experiments with benchmark and actual programs. The effectiveness of the framework is also verified by the analysis of actual exploit programs and malicious code, demonstrating the value of the practical application.

</details>


### [35] [Proceedings First Workshop on Adaptable Cloud Architectures](https://arxiv.org/abs/2512.22054)
*Giuseppe De Palma,Saverio Giallorenzo*

Main category: cs.SE

TL;DR: WACA 2025研讨会论文集，聚焦可适应云架构，与DisCoTec 2025联合举办


<details>
  <summary>Details</summary>
Motivation: 随着云计算技术的快速发展，构建能够适应不同工作负载、资源需求和环境变化的云架构变得越来越重要。WACA 2025旨在探讨可适应云架构的设计、实现和应用挑战。

Method: 通过研讨会形式汇集学术界和工业界专家，分享最新研究成果和实践经验，包括论文展示、讨论和同行评审过程。

Result: 形成了包含多篇高质量论文的论文集，涵盖了可适应云架构的各个关键方面，为相关领域的研究和实践提供了重要参考。

Conclusion: 可适应云架构是云计算发展的重要方向，需要学术界和工业界的持续合作与创新，WACA 2025为此领域的知识交流和技术进步做出了贡献。

Abstract: This volume contains the post-proceedings of the Workshop on Adaptable Cloud Architectures (WACA 2025), held on June 20, 2025, in Lille, France, co-located with DisCoTec 2025 - 20th International Federated Conference on Distributed Computing Techniques.

</details>
