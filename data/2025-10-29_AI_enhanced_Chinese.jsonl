{"id": "2510.23627", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23627", "abs": "https://arxiv.org/abs/2510.23627", "authors": ["Fred Zimmerman"], "title": "AI-Driven Development of a Publishing Imprint: Xynapse Traces", "comment": null, "summary": "Xynapse Traces is an experimental publishing imprint created via a fusion of\nhuman and algorithmic methods using a configuration-driven architecture and a\nmulti-model AI integration framework. The system achieved a remarkable 90%\nreduction in time-to-market (from a typical 6-12 months to just 2-4 weeks),\nwith 80% cost reduction compared to traditional imprint development, while\npublishing 52 books in its first year and maintaining exceptional quality\nmetrics, including 99% citation accuracy and 100% validation success after\ninitial corrections. Key technical innovations include a continuous ideation\npipeline with tournament-style evaluation, a novel codex design for\ntranscriptive meditation practice, comprehensive automation spanning from\nideation through production and distribution, and publisher personas that\ndefine and guide the imprint's mission. The system also integrates automated\nverification with human oversight, ensuring that gains in speed do not\ncompromise publishing standards. This effort has significant implications for\nthe future of book publishing, suggesting new paradigms for human-AI\ncollaboration that democratize access to sophisticated publishing capabilities\nand make previously unviable niche markets accessible.", "AI": {"tldr": "Xynapse Traces\u662f\u4e00\u4e2a\u5b9e\u9a8c\u6027\u51fa\u7248\u54c1\u724c\uff0c\u901a\u8fc7\u4eba\u673a\u878d\u5408\u65b9\u6cd5\u5b9e\u73b0\u4e86\u51fa\u7248\u6d41\u7a0b\u7684\u663e\u8457\u4f18\u5316\uff0c\u5c06\u4e0a\u5e02\u65f6\u95f4\u4ece6-12\u4e2a\u6708\u7f29\u77ed\u81f32-4\u5468\uff0c\u6210\u672c\u964d\u4f4e80%\uff0c\u7b2c\u4e00\u5e74\u51fa\u724852\u672c\u4e66\u5e76\u4fdd\u6301\u9ad8\u8d28\u91cf\u6807\u51c6\u3002", "motivation": "\u63a2\u7d22\u4eba\u673a\u534f\u4f5c\u7684\u65b0\u51fa\u7248\u8303\u5f0f\uff0c\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u89e3\u51b3\u4f20\u7edf\u51fa\u7248\u6d41\u7a0b\u8017\u65f6\u957f\u3001\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4f7f\u539f\u672c\u4e0d\u53ef\u884c\u7684\u7ec6\u5206\u5e02\u573a\u53d8\u5f97\u53ef\u884c\u3002", "method": "\u91c7\u7528\u914d\u7f6e\u9a71\u52a8\u67b6\u6784\u548c\u591a\u6a21\u578bAI\u96c6\u6210\u6846\u67b6\uff0c\u5305\u62ec\u6301\u7eed\u521b\u610f\u7ba1\u9053\u3001\u9526\u6807\u8d5b\u5f0f\u8bc4\u4f30\u3001\u65b0\u9896\u7684\u4ee3\u7801\u8bbe\u8ba1\u3001\u5168\u9762\u81ea\u52a8\u5316\u6d41\u7a0b\u4ee5\u53ca\u5b9a\u4e49\u51fa\u7248\u4f7f\u547d\u7684\u51fa\u7248\u8005\u89d2\u8272\u3002", "result": "\u5b9e\u73b0\u4e8690%\u7684\u4e0a\u5e02\u65f6\u95f4\u7f29\u51cf\u548c80%\u7684\u6210\u672c\u964d\u4f4e\uff0c\u7b2c\u4e00\u5e74\u51fa\u724852\u672c\u4e66\uff0c\u8d28\u91cf\u6307\u6807\u4f18\u5f02\uff1a99%\u7684\u5f15\u7528\u51c6\u786e\u7387\u548c100%\u7684\u9a8c\u8bc1\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u5728\u51fa\u7248\u9886\u57df\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u56fe\u4e66\u51fa\u7248\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u6c11\u4e3b\u5316\u9ad8\u7ea7\u51fa\u7248\u80fd\u529b\u5e76\u6fc0\u6d3b\u7ec6\u5206\u5e02\u573a\u3002"}}
{"id": "2510.23642", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.23642", "abs": "https://arxiv.org/abs/2510.23642", "authors": ["Yuansheng Ni", "Songcheng Cai", "Xiangchao Chen", "Jiarong Liang", "Zhiheng Lyu", "Jiaqi Deng", "Kai Zou", "Ping Nie", "Fei Yuan", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder2: Building Multi-Language Visualization Coding Agents", "comment": null, "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.", "AI": {"tldr": "\u63d0\u51fa\u4e86VisCode-Multi-679K\u6570\u636e\u96c6\u3001VisPlotBench\u57fa\u51c6\u548cVisCoder2\u6a21\u578b\u7cfb\u5217\uff0c\u7528\u4e8e\u6539\u8fdb\u53ef\u89c6\u5316\u7f16\u7801\u4ee3\u7406\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u591a\u8f6e\u8c03\u8bd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709LLM\u5728\u53ef\u89c6\u5316\u4ee3\u7801\u751f\u6210\u4e2d\u5b58\u5728\u8bed\u8a00\u8986\u76d6\u6709\u9650\u3001\u6267\u884c\u4e0d\u53ef\u9760\u3001\u7f3a\u4e4f\u8fed\u4ee3\u4fee\u6b63\u673a\u5236\u7b49\u95ee\u9898\uff0c\u4e14\u53d7\u9650\u4e8e\u7a84\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u9a8c\u8bc1\u6570\u636e\u96c6VisCode-Multi-679K\uff08679K\u6837\u672c\uff0c12\u79cd\u8bed\u8a00\uff09\uff0c\u5f00\u53d1\u8bc4\u4f30\u57fa\u51c6VisPlotBench\uff0c\u8bad\u7ec3\u591a\u8bed\u8a00\u53ef\u89c6\u5316\u6a21\u578bVisCoder2\u3002", "result": "VisCoder2\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u57fa\u7ebf\uff0c\u63a5\u8fd1GPT-4.1\u6027\u80fd\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u8c03\u8bd5\u572832B\u89c4\u6a21\u8fbe\u523082.4%\u603b\u4f53\u6267\u884c\u901a\u8fc7\u7387\uff0c\u5c24\u5176\u5728\u7b26\u53f7\u6216\u7f16\u8bd1\u5668\u4f9d\u8d56\u8bed\u8a00\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d44\u6e90\u548c\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u53ef\u89c6\u5316\u7f16\u7801\u4ee3\u7406\u7684\u73b0\u6709\u9650\u5236\uff0c\u4e3a\u591a\u8bed\u8a00\u3001\u591a\u8f6e\u4fee\u6b63\u7684\u53ef\u89c6\u5316\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2510.23664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23664", "abs": "https://arxiv.org/abs/2510.23664", "authors": ["Eranga Bandara", "Ross Gore", "Xueping Liang", "Sachini Rajapakse", "Isurunima Kularathne", "Pramoda Karunarathna", "Peter Foytik", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Amin Hass", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "Agentsway -- Software Development Methodology for AI Agents-based Teams", "comment": null, "summary": "The emergence of Agentic AI is fundamentally transforming how software is\ndesigned, developed, and maintained. Traditional software development\nmethodologies such as Agile, Kanban, ShapeUp, etc, were originally designed for\nhuman-centric teams and are increasingly inadequate in environments where\nautonomous AI agents contribute to planning, coding, testing, and continuous\nlearning. To address this methodological gap, we present \"Agentsway\" a novel\nsoftware development framework designed for ecosystems where AI agents operate\nas first-class collaborators. Agentsway introduces a structured lifecycle\ncentered on human orchestration, and privacy-preserving collaboration among\nspecialized AI agents. The framework defines distinct roles for planning,\nprompting, coding, testing, and fine-tuning agents, each contributing to\niterative improvement and adaptive learning throughout the development process.\nBy integrating fine-tuned LLMs that leverage outputs and feedback from\ndifferent agents throughout the development cycle as part of a retrospective\nlearning process, Agentsway enhances domain-specific reasoning, and explainable\ndecision-making across the entire software development lifecycle. Responsible\nAI principles are further embedded across the agents through the coordinated\nuse of multiple fine-tuned LLMs and advanced reasoning models, ensuring\nbalanced, transparent, and accountable decision-making. This work advances\nsoftware engineering by formalizing agent-centric collaboration, integrating\nprivacy-by-design principles, and defining measurable metrics for productivity\nand trust. Agentsway represents a foundational step toward the next generation\nof AI-native, self-improving software development methodologies. To the best of\nour knowledge, this is the first research effort to introduce a dedicated\nmethodology explicitly designed for AI agent-based software engineering teams.", "AI": {"tldr": "\u63d0\u51fa\u4e86\"Agentsway\"\u6846\u67b6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3aAI\u4ee3\u7406\u534f\u4f5c\u7684\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\u8bba\uff0c\u901a\u8fc7\u5b9a\u4e49\u4e0d\u540c\u89d2\u8272\u7684AI\u4ee3\u7406\u548c\u96c6\u6210\u5fae\u8c03LLM\u6765\u589e\u5f3a\u8f6f\u4ef6\u5f00\u53d1\u7684\u63a8\u7406\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\u5982Agile\u3001Kanban\u7b49\u662f\u4e3a\u4eba\u7c7b\u56e2\u961f\u8bbe\u8ba1\u7684\uff0c\u5728\u81ea\u4e3bAI\u4ee3\u7406\u53c2\u4e0e\u89c4\u5212\u3001\u7f16\u7801\u3001\u6d4b\u8bd5\u548c\u6301\u7eed\u5b66\u4e60\u7684\u73af\u5883\u4e2d\u8d8a\u6765\u8d8a\u4e0d\u9002\u7528\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u8bba\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "Agentsway\u6846\u67b6\u56f4\u7ed5\u4eba\u7c7b\u7f16\u6392\u548c\u9690\u79c1\u4fdd\u62a4\u534f\u4f5c\u6784\u5efa\uff0c\u5b9a\u4e49\u4e86\u89c4\u5212\u3001\u63d0\u793a\u3001\u7f16\u7801\u3001\u6d4b\u8bd5\u548c\u5fae\u8c03\u7b49\u4e0d\u540c\u89d2\u8272\u7684\u4e13\u4e1aAI\u4ee3\u7406\uff0c\u901a\u8fc7\u96c6\u6210\u5fae\u8c03LLM\u548c\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u5b9e\u73b0\u8fed\u4ee3\u6539\u8fdb\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u3002", "result": "\u8be5\u6846\u67b6\u901a\u8fc7\u534f\u8c03\u4f7f\u7528\u591a\u4e2a\u5fae\u8c03LLM\u548c\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\uff0c\u5728\u8f6f\u4ef6\u5f00\u53d1\u5168\u751f\u547d\u5468\u671f\u4e2d\u589e\u5f3a\u4e86\u9886\u57df\u7279\u5b9a\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\uff0c\u5e76\u5d4c\u5165\u4e86\u8d1f\u8d23\u4efb\u7684AI\u539f\u5219\u3002", "conclusion": "Agentsway\u4ee3\u8868\u4e86\u5411\u4e0b\u4e00\u4ee3AI\u539f\u751f\u3001\u81ea\u6211\u6539\u8fdb\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\u8bba\u8fc8\u51fa\u7684\u57fa\u7840\u6027\u4e00\u6b65\uff0c\u9996\u6b21\u4e3a\u57fa\u4e8eAI\u4ee3\u7406\u7684\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u5f15\u5165\u4e86\u4e13\u7528\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2510.23619", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23619", "abs": "https://arxiv.org/abs/2510.23619", "authors": ["Yuyang Miao", "Huijun Xing", "Danilo P. Mandic", "Tony G. Constantinides"], "title": "Short Ticketing Detection Framework Analysis Report", "comment": null, "summary": "This report presents a comprehensive analysis of an unsupervised multi-expert\nmachine learning framework for detecting short ticketing fraud in railway\nsystems. The study introduces an A/B/C/D station classification system that\nsuccessfully identifies suspicious patterns across 30 high-risk stations. The\nframework employs four complementary algorithms: Isolation Forest, Local\nOutlier Factor, One-Class SVM, and Mahalanobis Distance. Key findings include\nthe identification of five distinct short ticketing patterns and potential for\nshort ticketing recovery in transportation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u591a\u4e13\u5bb6\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u94c1\u8def\u7cfb\u7edf\u4e2d\u7684\u77ed\u7968\u6b3a\u8bc8\uff0c\u901a\u8fc7A/B/C/D\u8f66\u7ad9\u5206\u7c7b\u7cfb\u7edf\u8bc6\u522b\u4e8630\u4e2a\u9ad8\u98ce\u9669\u8f66\u7ad9\u7684\u53ef\u7591\u6a21\u5f0f\u3002", "motivation": "\u94c1\u8def\u7cfb\u7edf\u4e2d\u77ed\u7968\u6b3a\u8bc8\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u9700\u8981\u6709\u6548\u7684\u65e0\u76d1\u7763\u68c0\u6d4b\u65b9\u6cd5\u6765\u8bc6\u522b\u6b3a\u8bc8\u6a21\u5f0f\uff0c\u51cf\u5c11\u7ecf\u6d4e\u635f\u5931\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u4e92\u8865\u7b97\u6cd5\uff1a\u9694\u79bb\u68ee\u6797\u3001\u5c40\u90e8\u79bb\u7fa4\u56e0\u5b50\u3001\u4e00\u7c7bSVM\u548c\u9a6c\u6c0f\u8ddd\u79bb\uff0c\u7ed3\u5408A/B/C/D\u8f66\u7ad9\u5206\u7c7b\u7cfb\u7edf\u8fdb\u884c\u591a\u4e13\u5bb6\u5206\u6790\u3002", "result": "\u6210\u529f\u8bc6\u522b\u4e8630\u4e2a\u9ad8\u98ce\u9669\u8f66\u7ad9\uff0c\u53d1\u73b0\u4e86\u4e94\u79cd\u4e0d\u540c\u7684\u77ed\u7968\u6b3a\u8bc8\u6a21\u5f0f\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4ea4\u901a\u7cfb\u7edf\u4e2d\u6062\u590d\u77ed\u7968\u635f\u5931\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65e0\u76d1\u7763\u591a\u4e13\u5bb6\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4b\u94c1\u8def\u77ed\u7968\u6b3a\u8bc8\uff0c\u4e3a\u4ea4\u901a\u7cfb\u7edf\u6b3a\u8bc8\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23674", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23674", "abs": "https://arxiv.org/abs/2510.23674", "authors": ["Bin Wang", "Hui Li", "AoFan Liu", "BoTao Yang", "Ao Yang", "YiLu Zhong", "Weixiang Huang", "Yanping Zhang", "Runhuai Huang", "Weimin Zeng"], "title": "RefleXGen:The unexamined code is not worth using", "comment": null, "summary": "Security in code generation remains a pivotal challenge when applying large\nlanguage models (LLMs). This paper introduces RefleXGen, an innovative method\nthat significantly enhances code security by integrating Retrieval-Augmented\nGeneration (RAG) techniques with guided self-reflection mechanisms inherent in\nLLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing\nspecialized secure code datasets - processes that can be resource-intensive -\nRefleXGen iteratively optimizes the code generation process through\nself-assessment and reflection without the need for extensive resources. Within\nthis framework, the model continuously accumulates and refines its knowledge\nbase, thereby progressively improving the security of the generated code.\nExperimental results demonstrate that RefleXGen substantially enhances code\nsecurity across multiple models, achieving a 13.6% improvement with GPT-3.5\nTurbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a\n5.8% improvement with Gemini. Our findings highlight that improving the quality\nof model self-reflection constitutes an effective and practical strategy for\nstrengthening the security of AI-generated code.", "AI": {"tldr": "RefleXGen\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u81ea\u53cd\u601d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u7684\u5b89\u5168\u6027\uff0c\u65e0\u9700\u5927\u91cf\u8d44\u6e90\u5373\u53ef\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u662f\u5e94\u7528\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u5173\u952e\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8d44\u6e90\u8fdb\u884c\u5fae\u8c03\u6216\u6784\u5efa\u4e13\u7528\u6570\u636e\u96c6\u3002", "method": "\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u548cLLMs\u7684\u5f15\u5bfc\u81ea\u53cd\u601d\u673a\u5236\uff0c\u901a\u8fc7\u81ea\u6211\u8bc4\u4f30\u548c\u53cd\u601d\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\uff0c\u6301\u7eed\u79ef\u7d2f\u548c\u7cbe\u70bc\u77e5\u8bc6\u5e93\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u4e2d\u663e\u8457\u63d0\u5347\u4ee3\u7801\u5b89\u5168\u6027\uff1aGPT-3.5 Turbo\u63d0\u534713.6%\uff0cGPT-4o\u63d0\u53476.7%\uff0cCodeQwen\u63d0\u53474.5%\uff0cGemini\u63d0\u53475.8%\u3002", "conclusion": "\u63d0\u9ad8\u6a21\u578b\u81ea\u53cd\u601d\u8d28\u91cf\u662f\u589e\u5f3aAI\u751f\u6210\u4ee3\u7801\u5b89\u5168\u6027\u7684\u6709\u6548\u5b9e\u7528\u7b56\u7565\u3002"}}
{"id": "2510.23643", "categories": ["cs.CR", "cs.AI", "cs.LG", "I.2.6; D.4.6"], "pdf": "https://arxiv.org/pdf/2510.23643", "abs": "https://arxiv.org/abs/2510.23643", "authors": ["Zhixin Pan", "Ziyu Shu", "Linh Nguyen", "Amberbir Alemayoh"], "title": "SAND: A Self-supervised and Adaptive NAS-Driven Framework for Hardware Trojan Detection", "comment": null, "summary": "The globalized semiconductor supply chain has made Hardware Trojans (HT) a\nsignificant security threat to embedded systems, necessitating the design of\nefficient and adaptable detection mechanisms. Despite promising machine\nlearning-based HT detection techniques in the literature, they suffer from ad\nhoc feature selection and the lack of adaptivity, all of which hinder their\neffectiveness across diverse HT attacks. In this paper, we propose SAND, a\nselfsupervised and adaptive NAS-driven framework for efficient HT detection.\nSpecifically, this paper makes three key contributions. (1) We leverage\nself-supervised learning (SSL) to enable automated feature extraction,\neliminating the dependency on manually engineered features. (2) SAND integrates\nneural architecture search (NAS) to dynamically optimize the downstream\nclassifier, allowing for seamless adaptation to unseen benchmarks with minimal\nfine-tuning. (3) Experimental results show that SAND achieves a significant\nimprovement in detection accuracy (up to 18.3%) over state-of-the-art methods,\nexhibits high resilience against evasive Trojans, and demonstrates strong\ngeneralization.", "AI": {"tldr": "\u63d0\u51faSAND\u6846\u67b6\uff0c\u4e00\u79cd\u81ea\u76d1\u7763\u548c\u81ea\u9002\u5e94NAS\u9a71\u52a8\u7684\u786c\u4ef6\u6728\u9a6c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7279\u5f81\u63d0\u53d6\u548c\u52a8\u6001\u4f18\u5316\u5206\u7c7b\u5668\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5168\u7403\u5316\u7684\u534a\u5bfc\u4f53\u4f9b\u5e94\u94fe\u4f7f\u786c\u4ef6\u6728\u9a6c\u6210\u4e3a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u91cd\u5927\u5b89\u5168\u5a01\u80c1\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u9009\u62e9\u968f\u610f\u548c\u7f3a\u4e4f\u9002\u5e94\u6027\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u81ea\u52a8\u5316\u7279\u5f81\u63d0\u53d6\uff0c\u96c6\u6210\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u52a8\u6001\u4f18\u5316\u4e0b\u6e38\u5206\u7c7b\u5668\uff0c\u53ea\u9700\u5c11\u91cf\u5fae\u8c03\u5373\u53ef\u9002\u5e94\u65b0\u57fa\u51c6\u3002", "result": "SAND\u5728\u68c0\u6d4b\u51c6\u786e\u7387\u4e0a\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe18.3%\uff0c\u5bf9\u89c4\u907f\u6027\u6728\u9a6c\u5177\u6709\u9ad8\u5f39\u6027\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SAND\u6846\u67b6\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u786c\u4ef6\u6728\u9a6c\u68c0\u6d4b\u4e2d\u7684\u7279\u5f81\u5de5\u7a0b\u548c\u9002\u5e94\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u68c0\u6d4b\u3002"}}
{"id": "2510.23691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23691", "abs": "https://arxiv.org/abs/2510.23691", "authors": ["Zihao Wang", "Xujing Li", "Yining Ye", "Junjie Fang", "Haoming Wang", "Longxiang Liu", "Shihao Liang", "Junting Lu", "Zhiyong Wu", "Jiazhan Feng", "Wanjun Zhong", "Zili Li", "Yu Wang", "Yu Miao", "Bo Zhou", "Yuanfan Li", "Hao Wang", "Zhongkai Zhao", "Faming Wu", "Zhengxuan Jiang", "Weihao Tan", "Heyuan Yao", "Shi Yan", "Xiangyang Li", "Yitao Liang", "Yujia Qin", "Guang Shi"], "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents", "comment": null, "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.", "AI": {"tldr": "Game-TARS\u662f\u4e00\u4e2a\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u4f7f\u7528\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u952e\u76d8\u9f20\u6807\u52a8\u4f5c\u7a7a\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5728\u5f02\u6784\u6e38\u620f\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u8de8\u64cd\u4f5c\u7cfb\u7edf\u3001\u7f51\u9875\u548c\u6a21\u62df\u6e38\u620f\u7b49\u5f02\u6784\u9886\u57df\u8fdb\u884c\u5927\u89c4\u6a21\u6301\u7eed\u9884\u8bad\u7ec3\u7684\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u907f\u514dAPI\u6216GUI\u65b9\u6cd5\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4eba\u7c7b\u952e\u76d8\u9f20\u6807\u8f93\u5165\u7684\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\uff0c\u7ed3\u5408500B token\u7684\u591a\u6837\u5316\u8f68\u8ff9\u548c\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u91c7\u7528\u8870\u51cf\u6301\u7eed\u635f\u5931\u51cf\u5c11\u56e0\u679c\u6df7\u6dc6\uff0c\u4ee5\u53ca\u7a00\u758f\u601d\u7ef4\u7b56\u7565\u5e73\u8861\u63a8\u7406\u6df1\u5ea6\u548c\u63a8\u7406\u6210\u672c\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754cMinecraft\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u6bd4\u4e4b\u524d\u6700\u4f73\u6a21\u578b\u63d0\u9ad8\u7ea62\u500d\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u7f51\u98753D\u6e38\u620f\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u65b0\u624b\u6c34\u5e73\uff0c\u5728FPS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aGPT-5\u3001Gemini-2.5-Pro\u548cClaude-4-Sonnet\u3002", "conclusion": "\u7b80\u5355\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u8868\u793a\u7ed3\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e3a\u5f00\u53d1\u5177\u6709\u5e7f\u6cdb\u8ba1\u7b97\u673a\u4f7f\u7528\u80fd\u529b\u7684\u901a\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2510.23761", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23761", "abs": "https://arxiv.org/abs/2510.23761", "authors": ["Kevin Han", "Siddharth Maddikayala", "Tim Knappe", "Om Patel", "Austen Liao", "Amir Barati Farimani"], "title": "TDFlow: Agentic Workflows for Test Driven Software Engineering", "comment": null, "summary": "We introduce TDFlow, a novel test-driven agentic workflow that frames\nrepository-scale software engineering as a test-resolution task, specifically\ndesigned to solve human-written tests. Given a set of tests, TDFlow repeatedly\nproposes, revises, and debugs repository-scale patches using precisely\nengineered sub-agents and tightly constrained tools. The workflow decomposes\nsoftware engineering program repair into four components governed by respective\nsub-agents. This simple, forced decoupling of patch proposing, debugging, patch\nrevision, and optional test generation (1) reduces long-context burden on any\nindividual sub-agent, (2) focuses each sub-agent on specific, pre-defined\nsub-tasks, and (3) allows for specialized performance improvement on specific\nsub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on\nSWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and\n94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within\nSWE-Bench Lite and Verified uncover only 7 instances of test hacking, which\nwere subsequently counted as failures. Furthermore, we show that the primary\nobstacle to human-level software engineering performance lies within writing\nsuccessful reproduction tests. We envision a human-LLM interactive system\npowered by TDFlow where human developers write tests solved by LLM systems.\nTogether, these results indicate that modern LLMs, when embedded in a narrowly\nengineered, test-driven workflow, already achieve human-level test resolution\n-- with the final frontier for fully autonomous repository repair being the\naccurate generation of valid reproduction tests.", "AI": {"tldr": "TDFlow\u662f\u4e00\u4e2a\u57fa\u4e8e\u6d4b\u8bd5\u9a71\u52a8\u7684\u667a\u80fd\u5de5\u4f5c\u6d41\uff0c\u5c06\u4ed3\u5e93\u7ea7\u8f6f\u4ef6\u5de5\u7a0b\u6784\u5efa\u4e3a\u6d4b\u8bd5\u89e3\u51b3\u4efb\u52a1\uff0c\u901a\u8fc7\u5206\u89e3\u4e3a\u56db\u4e2a\u4e13\u95e8\u5b50\u4ee3\u7406\u6765\u63d0\u5347\u7a0b\u5e8f\u4fee\u590d\u6027\u80fd\uff0c\u5728SWE-Bench\u6d4b\u8bd5\u4e2d\u8fbe\u523088.8%\u901a\u8fc7\u7387\u3002", "motivation": "\u89e3\u51b3\u4ed3\u5e93\u7ea7\u8f6f\u4ef6\u5de5\u7a0b\u4fee\u590d\u95ee\u9898\uff0c\u901a\u8fc7\u6d4b\u8bd5\u9a71\u52a8\u65b9\u6cd5\u51cf\u5c11\u957f\u4e0a\u4e0b\u6587\u8d1f\u62c5\uff0c\u4e13\u6ce8\u4e8e\u7279\u5b9a\u5b50\u4efb\u52a1\uff0c\u63d0\u5347LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u5c06\u8f6f\u4ef6\u5de5\u7a0b\u7a0b\u5e8f\u4fee\u590d\u5206\u89e3\u4e3a\u56db\u4e2a\u7ec4\u4ef6\uff1a\u8865\u4e01\u63d0\u8bae\u3001\u8c03\u8bd5\u3001\u8865\u4e01\u4fee\u8ba2\u548c\u53ef\u9009\u6d4b\u8bd5\u751f\u6210\uff0c\u6bcf\u4e2a\u7ec4\u4ef6\u7531\u4e13\u95e8\u7684\u5b50\u4ee3\u7406\u8d1f\u8d23\uff0c\u4f7f\u7528\u7cbe\u786e\u8bbe\u8ba1\u7684\u5b50\u4ee3\u7406\u548c\u4e25\u683c\u7ea6\u675f\u7684\u5de5\u5177\u3002", "result": "\u5728SWE-Bench Lite\u4e0a\u8fbe\u523088.8%\u901a\u8fc7\u7387\uff08\u6bd4\u6b21\u4f18\u7cfb\u7edf\u63d0\u534727.8%\uff09\uff0c\u5728SWE-Bench Verified\u4e0a\u8fbe\u523094.3%\u901a\u8fc7\u7387\uff0c800\u6b21\u8fd0\u884c\u4e2d\u4ec5\u53d1\u73b07\u6b21\u6d4b\u8bd5\u9ed1\u5ba2\u884c\u4e3a\u3002", "conclusion": "\u73b0\u4ee3LLM\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u9a71\u52a8\u5de5\u4f5c\u6d41\u4e2d\u5df2\u80fd\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u7684\u6d4b\u8bd5\u89e3\u51b3\u80fd\u529b\uff0c\u5b8c\u5168\u81ea\u4e3b\u4ed3\u5e93\u4fee\u590d\u7684\u6700\u540e\u969c\u788d\u5728\u4e8e\u51c6\u786e\u751f\u6210\u6709\u6548\u7684\u590d\u73b0\u6d4b\u8bd5\u3002"}}
{"id": "2510.23673", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23673", "abs": "https://arxiv.org/abs/2510.23673", "authors": ["Bin Wang", "Zexin Liu", "Hao Yu", "Ao Yang", "Yenan Huang", "Jing Guo", "Huangsheng Cheng", "Hui Li", "Huiyu Wu"], "title": "MCPGuard : Automatically Detecting Vulnerabilities in MCP Servers", "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a standardized interface\nenabling seamless integration between Large Language Models (LLMs) and external\ndata sources and tools. While MCP significantly reduces development complexity\nand enhances agent capabilities, its openness and extensibility introduce\ncritical security vulnerabilities that threaten system trustworthiness and user\ndata protection. This paper systematically analyzes the security landscape of\nMCP-based systems, identifying three principal threat categories: (1) agent\nhijacking attacks stemming from protocol design deficiencies; (2) traditional\nweb vulnerabilities in MCP servers; and (3) supply chain security. To address\nthese challenges, we comprehensively survey existing defense strategies,\nexamining both proactive server-side scanning approaches, ranging from layered\ndetection pipelines and agentic auditing frameworks to zero-trust registry\nsystems, and runtime interaction monitoring solutions that provide continuous\noversight and policy enforcement. Our analysis reveals that MCP security\nfundamentally represents a paradigm shift where the attack surface extends from\ntraditional code execution to semantic interpretation of natural language\nmetadata, necessitating novel defense mechanisms tailored to this unique threat\nmodel.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86MCP\u534f\u8bae\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u8bc6\u522b\u4e86\u4e09\u5927\u5a01\u80c1\u7c7b\u522b\uff0c\u5e76\u8c03\u67e5\u4e86\u73b0\u6709\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u63ed\u793a\u4e86MCP\u5b89\u5168\u9700\u8981\u9488\u5bf9\u81ea\u7136\u8bed\u8a00\u5143\u6570\u636e\u8bed\u4e49\u89e3\u91ca\u7684\u65b0\u9632\u5fa1\u673a\u5236\u3002", "motivation": "MCP\u4f5c\u4e3a\u6807\u51c6\u5316\u63a5\u53e3\u867d\u7136\u7b80\u5316\u4e86LLM\u4e0e\u5916\u90e8\u6570\u636e\u6e90\u7684\u96c6\u6210\uff0c\u4f46\u5176\u5f00\u653e\u6027\u548c\u53ef\u6269\u5c55\u6027\u5f15\u5165\u4e86\u4e25\u91cd\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5a01\u80c1\u7cfb\u7edf\u53ef\u4fe1\u5ea6\u548c\u7528\u6237\u6570\u636e\u4fdd\u62a4\u3002", "method": "\u7cfb\u7edf\u5206\u6790MCP\u5b89\u5168\u6001\u52bf\uff0c\u8bc6\u522b\u4e09\u7c7b\u4e3b\u8981\u5a01\u80c1\uff1a\u4ee3\u7406\u52ab\u6301\u653b\u51fb\u3001MCP\u670d\u52a1\u5668\u4f20\u7edfWeb\u6f0f\u6d1e\u548c\u4f9b\u5e94\u94fe\u5b89\u5168\uff0c\u5e76\u5168\u9762\u8c03\u67e5\u73b0\u6709\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u53d1\u73b0MCP\u5b89\u5168\u4ee3\u8868\u4e86\u4e00\u4e2a\u8303\u5f0f\u8f6c\u53d8\uff0c\u653b\u51fb\u9762\u4ece\u4f20\u7edf\u4ee3\u7801\u6267\u884c\u6269\u5c55\u5230\u81ea\u7136\u8bed\u8a00\u5143\u6570\u636e\u7684\u8bed\u4e49\u89e3\u91ca\uff0c\u9700\u8981\u9488\u5bf9\u8fd9\u4e00\u72ec\u7279\u5a01\u80c1\u6a21\u578b\u7684\u65b0\u9632\u5fa1\u673a\u5236\u3002", "conclusion": "MCP\u5b89\u5168\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u5176\u72ec\u7279\u5a01\u80c1\u6a21\u578b\u7684\u521b\u65b0\u9632\u5fa1\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u81ea\u7136\u8bed\u8a00\u5143\u6570\u636e\u7684\u8bed\u4e49\u89e3\u91ca\u5b89\u5168\u3002"}}
{"id": "2510.23734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23734", "abs": "https://arxiv.org/abs/2510.23734", "authors": ["Eamon Duede"], "title": "AI and the Decentering of Disciplinary Creativity", "comment": null, "summary": "This paper examines the role of artificial intelligence in scientific\nproblem-solving, with a focus on its implications for disciplinary creativity.\nDrawing on recent work in the philosophy of creativity, I distinguish between\ncreative approaches and creative products, and introduce the concept of\ndisciplinary creativity -the creative application of discipline-specific\nexpertise to a valued problem within that field. Through two cases in\nmathematics, I show that while computation can extend disciplinary creativity,\ncertain approaches involving AI can serve to displace it. This displacement has\nthe potential to alter (and, perhaps, diminish) the value of scientific\npursuit.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8AI\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u4f5c\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u6570\u5b66\u6848\u4f8b\u8868\u660e\uff0c\u867d\u7136\u8ba1\u7b97\u80fd\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9bAI\u65b9\u6cd5\u53ef\u80fd\u53d6\u4ee3\u5b83\uff0c\u4ece\u800c\u6539\u53d8\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002", "motivation": "\u7814\u7a76\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u5f71\u54cd\u79d1\u5b66\u9886\u57df\u7684\u521b\u9020\u529b\uff0c\u7279\u522b\u662f\u5b66\u79d1\u521b\u9020\u529b\u7684\u6982\u5ff5\u2014\u2014\u5728\u7279\u5b9a\u5b66\u79d1\u5185\u8fd0\u7528\u4e13\u4e1a\u77e5\u8bc6\u89e3\u51b3\u6709\u4ef7\u503c\u95ee\u9898\u7684\u521b\u9020\u6027\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u521b\u9020\u529b\u54f2\u5b66\u7406\u8bba\uff0c\u533a\u5206\u521b\u9020\u6027\u65b9\u6cd5\u548c\u521b\u9020\u6027\u4ea7\u54c1\uff0c\u901a\u8fc7\u4e24\u4e2a\u6570\u5b66\u6848\u4f8b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8ba1\u7b97\u53ef\u4ee5\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9bAI\u65b9\u6cd5\u4f1a\u53d6\u4ee3\u5b66\u79d1\u521b\u9020\u529b\uff0c\u8fd9\u53ef\u80fd\u6539\u53d8\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002", "conclusion": "AI\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u5177\u6709\u53cc\u91cd\u4f5c\u7528\uff1a\u65e2\u80fd\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4e5f\u53ef\u80fd\u901a\u8fc7\u53d6\u4ee3\u5b83\u800c\u6539\u53d8\u79d1\u5b66\u8ffd\u6c42\u7684\u672c\u8d28\u548c\u4ef7\u503c\u3002"}}
{"id": "2510.23893", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23893", "abs": "https://arxiv.org/abs/2510.23893", "authors": ["Rodrigo Falc\u00e3o", "Stefan Schweitzer", "Julien Siebert", "Emily Calvet", "Frank Elberzhager"], "title": "Evaluating the effectiveness of LLM-based interoperability", "comment": null, "summary": "Background: Systems of systems are becoming increasingly dynamic and\nheterogeneous, and this adds pressure on the long-standing challenge of\ninteroperability. Besides its technical aspect, interoperability has also an\neconomic side, as development time efforts are required to build the\ninteroperability artifacts. Objectives: With the recent advances in the field\nof large language models (LLMs), we aim at analyzing the effectiveness of\nLLM-based strategies to make systems interoperate autonomously, at runtime,\nwithout human intervention. Method: We selected 13 open source LLMs and curated\nfour versions of a dataset in the agricultural interoperability use case. We\nperformed three runs of each model with each version of the dataset, using two\ndifferent strategies. Then we compared the effectiveness of the models and the\nconsistency of their results across multiple runs. Results: qwen2.5-coder:32b\nwas the most effective model using both strategies DIRECT (average pass@1 >=\n0.99) and CODEGEN (average pass@1 >= 0.89) in three out of four dataset\nversions. In the fourth dataset version, which included an unit conversion, all\nmodels using the strategy DIRECT failed, whereas using CODEGEN\nqwen2.5-coder:32b succeeded with an average pass@1 = 0.75. Conclusion: Some\nLLMs can make systems interoperate autonomously. Further evaluation in\ndifferent domains is recommended, and further research on reliability\nstrategies should be conducted.", "AI": {"tldr": "\u8bc4\u4f3013\u4e2a\u5f00\u6e90LLM\u5728\u519c\u4e1a\u4e92\u64cd\u4f5c\u6027\u7528\u4f8b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0qwen2.5-coder:32b\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u591f\u5b9e\u73b0\u7cfb\u7edf\u95f4\u7684\u81ea\u4e3b\u4e92\u64cd\u4f5c\u3002", "motivation": "\u968f\u7740\u7cfb\u7edf\u53d8\u5f97\u8d8a\u6765\u8d8a\u52a8\u6001\u548c\u5f02\u6784\uff0c\u4e92\u64cd\u4f5c\u6027\u6210\u4e3a\u957f\u671f\u6311\u6218\u3002\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7814\u7a76LLM\u80fd\u5426\u5728\u8fd0\u884c\u65f6\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5b9e\u73b0\u7cfb\u7edf\u81ea\u4e3b\u4e92\u64cd\u4f5c\u3002", "method": "\u9009\u62e913\u4e2a\u5f00\u6e90LLM\uff0c\u5728\u519c\u4e1a\u4e92\u64cd\u4f5c\u6027\u7528\u4f8b\u4e2d\u7b56\u5212\u56db\u4e2a\u7248\u672c\u7684\u6570\u636e\u96c6\u3002\u4f7f\u7528\u4e24\u79cd\u7b56\u7565\uff08DIRECT\u548cCODEGEN\uff09\u5bf9\u6bcf\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u7248\u672c\u8fdb\u884c\u4e09\u6b21\u8fd0\u884c\uff0c\u6bd4\u8f83\u6a21\u578b\u6709\u6548\u6027\u548c\u7ed3\u679c\u4e00\u81f4\u6027\u3002", "result": "qwen2.5-coder:32b\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u7248\u672c\u4e2d\u4f7f\u7528DIRECT\u7b56\u7565\uff08\u5e73\u5747pass@1\u22650.99\uff09\u548cCODEGEN\u7b56\u7565\uff08\u5e73\u5747pass@1\u22650.89\uff09\u8868\u73b0\u6700\u4f73\u3002\u5728\u5305\u542b\u5355\u4f4d\u8f6c\u6362\u7684\u7b2c\u56db\u4e2a\u7248\u672c\u4e2d\uff0c\u6240\u6709\u4f7f\u7528DIRECT\u7b56\u7565\u7684\u6a21\u578b\u90fd\u5931\u8d25\uff0c\u4f46\u4f7f\u7528CODEGEN\u7b56\u7565\u7684qwen2.5-coder:32b\u4ecd\u80fd\u8fbe\u5230\u5e73\u5747pass@1=0.75\u3002", "conclusion": "\u67d0\u4e9bLLM\u80fd\u591f\u5b9e\u73b0\u7cfb\u7edf\u7684\u81ea\u4e3b\u4e92\u64cd\u4f5c\u3002\u5efa\u8bae\u5728\u4e0d\u540c\u9886\u57df\u8fdb\u884c\u8fdb\u4e00\u6b65\u8bc4\u4f30\uff0c\u5e76\u5bf9\u53ef\u9760\u6027\u7b56\u7565\u8fdb\u884c\u66f4\u591a\u7814\u7a76\u3002"}}
{"id": "2510.23675", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23675", "abs": "https://arxiv.org/abs/2510.23675", "authors": ["Yuchong Xie", "Zesen Liu", "Mingyu Luo", "Zhixiang Zhang", "Kaikai Zhang", "Zongjie Li", "Ping Chen", "Shuai Wang", "Dongdong She"], "title": "QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents", "comment": null, "summary": "Modern coding agents integrated into IDEs combine powerful tools and\nsystem-level actions, exposing a high-stakes attack surface. Existing Indirect\nPrompt Injection (IPI) studies focus mainly on query-specific behaviors,\nleading to unstable attacks with lower success rates. We identify a more\nsevere, query-agnostic threat that remains effective across diverse user\ninputs. This challenge can be overcome by exploiting a common vulnerability:\nleakage of the agent's internal prompt, which turns the attack into a\nconstrained white-box optimization problem. We present QueryIPI, the first\nquery-agnostic IPI method for coding agents. QueryIPI refines malicious tool\ndescriptions through an iterative, prompt-based process informed by the leaked\ninternal prompt. Experiments on five simulated agents show that QueryIPI\nachieves up to 87 percent success, outperforming baselines, and the generated\nmalicious descriptions also transfer to real-world systems, highlighting a\npractical security risk to modern LLM-based coding agents.", "AI": {"tldr": "QueryIPI\u662f\u4e00\u79cd\u67e5\u8be2\u65e0\u5173\u7684\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u7f16\u7801\u4ee3\u7406\u5185\u90e8\u63d0\u793a\u6cc4\u9732\u6f0f\u6d1e\uff0c\u5c06\u653b\u51fb\u8f6c\u5316\u4e3a\u7ea6\u675f\u767d\u76d2\u4f18\u5316\u95ee\u9898\uff0c\u5728\u6a21\u62df\u4ee3\u7406\u4e2d\u8fbe\u523087%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u67e5\u8be2\u7279\u5b9a\u884c\u4e3a\uff0c\u5bfc\u81f4\u653b\u51fb\u4e0d\u7a33\u5b9a\u4e14\u6210\u529f\u7387\u4f4e\u3002\u4f5c\u8005\u53d1\u73b0\u66f4\u4e25\u91cd\u7684\u67e5\u8be2\u65e0\u5173\u5a01\u80c1\uff0c\u53ef\u901a\u8fc7\u5229\u7528\u4ee3\u7406\u5185\u90e8\u63d0\u793a\u6cc4\u9732\u6f0f\u6d1e\u6765\u89e3\u51b3\u3002", "method": "QueryIPI\u901a\u8fc7\u8fed\u4ee3\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u8fc7\u7a0b\uff0c\u6839\u636e\u6cc4\u9732\u7684\u5185\u90e8\u63d0\u793a\u6765\u4f18\u5316\u6076\u610f\u5de5\u5177\u63cf\u8ff0\uff0c\u5b9e\u73b0\u67e5\u8be2\u65e0\u5173\u7684\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002", "result": "\u5728\u4e94\u4e2a\u6a21\u62df\u4ee3\u7406\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cQueryIPI\u8fbe\u5230\u6700\u9ad887%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u751f\u6210\u7684\u6076\u610f\u63cf\u8ff0\u53ef\u8fc1\u79fb\u5230\u771f\u5b9e\u7cfb\u7edf\u3002", "conclusion": "QueryIPI\u63ed\u793a\u4e86\u73b0\u4ee3\u57fa\u4e8eLLM\u7684\u7f16\u7801\u4ee3\u7406\u9762\u4e34\u7684\u5b9e\u9645\u5b89\u5168\u98ce\u9669\uff0c\u5176\u67e5\u8be2\u65e0\u5173\u7684\u653b\u51fb\u65b9\u6cd5\u5177\u6709\u9ad8\u6210\u529f\u7387\u548c\u53ef\u8fc1\u79fb\u6027\u3002"}}
{"id": "2510.23744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23744", "abs": "https://arxiv.org/abs/2510.23744", "authors": ["Eline M. Bovy", "Caleb Probine", "Marnix Suilen", "Ufuk Topcu", "Nils Jansen"], "title": "Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability", "comment": "Accepted at NeurIPS 2025", "summary": "Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete\nmodel uncertainty. ME-POMDPs represent a finite set of POMDPs that share the\nsame state, action, and observation spaces, but may arbitrarily vary in their\ntransition, observation, and reward models. Such models arise, for instance,\nwhen multiple domain experts disagree on how to model a problem. The goal is to\nfind a single policy that is robust against any choice of POMDP within the set,\ni.e., a policy that maximizes the worst-case reward across all POMDPs. We\ngeneralize and expand on existing work in the following way. First, we show\nthat ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which\nwe call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any\narbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its\ntransition and reward functions or only in its observation and reward\nfunctions, while preserving (optimal) policies. We then devise exact and\napproximate (point-based) algorithms to compute robust policies for AB-POMDPs,\nand thus ME-POMDPs. We demonstrate that we can compute policies for standard\nPOMDP benchmarks extended to the multi-environment setting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u73af\u5883POMDPs\uff08ME-POMDPs\uff09\u53ca\u5176\u6269\u5c55\u5f62\u5f0fAB-POMDPs\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u79bb\u6563\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684POMDP\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u7cbe\u786e\u548c\u8fd1\u4f3c\u7b97\u6cd5\u6765\u8ba1\u7b97\u9c81\u68d2\u7b56\u7565\u3002", "motivation": "\u5f53\u591a\u4e2a\u9886\u57df\u4e13\u5bb6\u5bf9\u95ee\u9898\u5efa\u6a21\u5b58\u5728\u5206\u6b67\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u6846\u67b6\u3002ME-POMDPs\u6269\u5c55\u4e86\u6807\u51c6POMDPs\uff0c\u80fd\u591f\u8868\u793a\u4e00\u7ec4\u5171\u4eab\u72b6\u6001\u3001\u52a8\u4f5c\u548c\u89c2\u6d4b\u7a7a\u95f4\u4f46\u53ef\u80fd\u4efb\u610f\u53d8\u5316\u5176\u8f6c\u79fb\u3001\u89c2\u6d4b\u548c\u5956\u52b1\u6a21\u578b\u7684POMDPs\u96c6\u5408\u3002", "method": "1. \u5c06ME-POMDPs\u63a8\u5e7f\u5230\u5177\u6709\u521d\u59cb\u4fe1\u5ff5\u96c6\u5408\u7684POMDPs\uff08AB-POMDPs\uff09\n2. \u8bc1\u660e\u4efb\u610fME-POMDP\u53ef\u4ee5\u7b80\u5316\u4e3a\u4ec5\u5728\u8f6c\u79fb\u548c\u5956\u52b1\u51fd\u6570\u6216\u4ec5\u5728\u89c2\u6d4b\u548c\u5956\u52b1\u51fd\u6570\u4e0a\u53d8\u5316\u7684ME-POMDP\n3. \u5f00\u53d1\u7cbe\u786e\u548c\u8fd1\u4f3c\uff08\u57fa\u4e8e\u70b9\uff09\u7b97\u6cd5\u6765\u8ba1\u7b97AB-POMDPs\u7684\u9c81\u68d2\u7b56\u7565", "result": "\u6210\u529f\u5c06\u6807\u51c6POMDP\u57fa\u51c6\u6269\u5c55\u5230\u591a\u73af\u5883\u8bbe\u7f6e\uff0c\u5e76\u80fd\u591f\u8ba1\u7b97\u76f8\u5e94\u7684\u7b56\u7565\u3002\u7b97\u6cd5\u80fd\u591f\u5904\u7406\u5177\u6709\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u590d\u6742\u51b3\u7b56\u95ee\u9898\u3002", "conclusion": "ME-POMDPs\u548cAB-POMDPs\u4e3a\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u8ba1\u7b97\u5728\u591a\u4e2a\u53ef\u80fd\u73af\u5883\u4e2d\u6700\u574f\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f18\u7684\u9c81\u68d2\u7b56\u7565\u3002"}}
{"id": "2510.23970", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23970", "abs": "https://arxiv.org/abs/2510.23970", "authors": ["Maria C. Borges", "Julian Legler", "Lucca Di Benedetto"], "title": "Validating Alerts in Cloud-Native Observability", "comment": "16th Symposium on Software Performance (SSP'25)", "summary": "Observability and alerting form the backbone of modern reliability\nengineering. Alerts help teams catch faults early before they turn into\nproduction outages and serve as first clues for troubleshooting. However,\ndesigning effective alerts is challenging. They need to strike a fine balance\nbetween catching issues early and minimizing false alarms. On top of this,\nalerts often cover uncommon faults, so the code is rarely executed and\ntherefore rarely checked. To address these challenges, several industry\npractitioners advocate for testing alerting code with the same rigor as\napplication code. Still, there's a lack of tools that support such systematic\ndesign and validation of alerts.\n  This paper introduces a new alerting extension for the observability\nexperimentation tool OXN. It lets engineers experiment with alerts early during\ndevelopment. With OXN, engineers can now tune rules at design time and\nroutinely validate the firing behavior of their alerts, avoiding future\nproblems at runtime.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86OXN\u89c2\u6d4b\u5b9e\u9a8c\u5de5\u5177\u7684\u8b66\u62a5\u6269\u5c55\uff0c\u8ba9\u5de5\u7a0b\u5e08\u80fd\u5728\u5f00\u53d1\u65e9\u671f\u5b9e\u9a8c\u548c\u6d4b\u8bd5\u8b66\u62a5\u89c4\u5219\uff0c\u907f\u514d\u8fd0\u884c\u65f6\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u53ef\u9760\u6027\u5de5\u7a0b\u4e2d\u8b66\u62a5\u8bbe\u8ba1\u9762\u4e34\u6311\u6218\uff1a\u9700\u8981\u5728\u53ca\u65e9\u53d1\u73b0\u95ee\u9898\u4e0e\u51cf\u5c11\u8bef\u62a5\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e14\u8b66\u62a5\u4ee3\u7801\u5f88\u5c11\u6267\u884c\u548c\u68c0\u67e5\u3002\u7f3a\u4e4f\u7cfb\u7edf\u5316\u8bbe\u8ba1\u548c\u9a8c\u8bc1\u8b66\u62a5\u7684\u5de5\u5177\u3002", "method": "\u5728OXN\u89c2\u6d4b\u5b9e\u9a8c\u5de5\u5177\u4e2d\u65b0\u589e\u8b66\u62a5\u6269\u5c55\u529f\u80fd\uff0c\u8ba9\u5de5\u7a0b\u5e08\u53ef\u4ee5\u5728\u8bbe\u8ba1\u9636\u6bb5\u8c03\u6574\u89c4\u5219\u5e76\u5e38\u89c4\u9a8c\u8bc1\u8b66\u62a5\u89e6\u53d1\u884c\u4e3a\u3002", "result": "\u5de5\u7a0b\u5e08\u73b0\u5728\u53ef\u4ee5\u5728\u5f00\u53d1\u65e9\u671f\u5b9e\u9a8c\u8b66\u62a5\uff0c\u5728\u8bbe\u8ba1\u65f6\u8c03\u6574\u89c4\u5219\uff0c\u5e76\u5e38\u89c4\u9a8c\u8bc1\u8b66\u62a5\u7684\u89e6\u53d1\u884c\u4e3a\u3002", "conclusion": "OXN\u7684\u8b66\u62a5\u6269\u5c55\u89e3\u51b3\u4e86\u7cfb\u7edf\u5316\u8bbe\u8ba1\u548c\u9a8c\u8bc1\u8b66\u62a5\u7684\u6311\u6218\uff0c\u5e2e\u52a9\u907f\u514d\u672a\u6765\u8fd0\u884c\u65f6\u95ee\u9898\u3002"}}
{"id": "2510.23847", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23847", "abs": "https://arxiv.org/abs/2510.23847", "authors": ["Joel Poncha Lemayian", "Ghyslain Gagnon", "Kaiwen Zhang", "Pascal Giard"], "title": "EthVault: A Secure and Resource-Conscious FPGA-Based Ethereum Cold Wallet", "comment": "Under review for publication", "summary": "Cryptocurrency blockchain networks safeguard digital assets using\ncryptographic keys, with wallets playing a critical role in generating,\nstoring, and managing these keys. Wallets, typically categorized as hot and\ncold, offer varying degrees of security and convenience. However, they are\ngenerally software-based applications running on microcontrollers.\nConsequently, they are vulnerable to malware and side-channel attacks, allowing\nperpetrators to extract private keys by targeting critical algorithms, such as\nECC, which processes private keys to generate public keys and authorize\ntransactions. To address these issues, this work presents EthVault, the first\nhardware architecture for an Ethereum hierarchically deterministic cold wallet,\nfeaturing hardware implementations of key algorithms for secure key generation.\nAlso, an ECC architecture resilient to side-channel and timing attacks is\nproposed. Moreover, an architecture of the child key derivation function, a\nfundamental component of cryptocurrency wallets, is proposed. The design\nminimizes resource usage, meeting market demand for small, portable\ncryptocurrency wallets. FPGA implementation results validate the feasibility of\nthe proposed approach. The ECC architecture exhibits uniform execution behavior\nacross varying inputs, while the complete design utilizes only 27%, 7%, and 6%\nof LUTs, registers, and RAM blocks, respectively, on a Xilinx Zynq UltraScale+\nFPGA.", "AI": {"tldr": "EthVault\u662f\u9996\u4e2a\u7528\u4e8e\u4ee5\u592a\u574a\u5206\u5c42\u786e\u5b9a\u6027\u51b7\u94b1\u5305\u7684\u786c\u4ef6\u67b6\u6784\uff0c\u901a\u8fc7\u786c\u4ef6\u5b9e\u73b0\u5173\u952e\u7b97\u6cd5\u6765\u5b89\u5168\u751f\u6210\u5bc6\u94a5\uff0c\u5e76\u63d0\u4f9b\u6297\u4fa7\u4fe1\u9053\u548c\u65f6\u5e8f\u653b\u51fb\u7684ECC\u67b6\u6784\u3002", "motivation": "\u73b0\u6709\u7684\u8f6f\u4ef6\u94b1\u5305\u8fd0\u884c\u5728\u5fae\u63a7\u5236\u5668\u4e0a\uff0c\u5bb9\u6613\u53d7\u5230\u6076\u610f\u8f6f\u4ef6\u548c\u4fa7\u4fe1\u9053\u653b\u51fb\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u901a\u8fc7\u9488\u5bf9ECC\u7b49\u5173\u952e\u7b97\u6cd5\u6765\u63d0\u53d6\u79c1\u94a5\u3002", "method": "\u63d0\u51faEthVault\u786c\u4ef6\u67b6\u6784\uff0c\u5305\u62ec\u6297\u4fa7\u4fe1\u9053\u548c\u65f6\u5e8f\u653b\u51fb\u7684ECC\u67b6\u6784\u3001\u5b50\u5bc6\u94a5\u6d3e\u751f\u51fd\u6570\u67b6\u6784\uff0c\u5e76\u6700\u5c0f\u5316\u8d44\u6e90\u4f7f\u7528\u4ee5\u6ee1\u8db3\u5c0f\u578b\u4fbf\u643a\u94b1\u5305\u7684\u5e02\u573a\u9700\u6c42\u3002", "result": "FPGA\u5b9e\u73b0\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0cECC\u67b6\u6784\u5728\u4e0d\u540c\u8f93\u5165\u4e0b\u8868\u73b0\u51fa\u7edf\u4e00\u7684\u6267\u884c\u884c\u4e3a\uff0c\u5b8c\u6574\u8bbe\u8ba1\u4ec5\u4f7f\u7528Xilinx Zynq UltraScale+ FPGA\u4e0a27%\u7684LUT\u30017%\u7684\u5bc4\u5b58\u5668\u548c6%\u7684RAM\u5757\u3002", "conclusion": "EthVault\u4e3a\u52a0\u5bc6\u8d27\u5e01\u94b1\u5305\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u62b5\u5fa1\u4fa7\u4fe1\u9053\u548c\u65f6\u5e8f\u653b\u51fb\uff0c\u540c\u65f6\u6ee1\u8db3\u4fbf\u643a\u6027\u9700\u6c42\u3002"}}
{"id": "2510.23746", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23746", "abs": "https://arxiv.org/abs/2510.23746", "authors": ["Laura Mismetti", "Marvin Alberts", "Andreas Krause", "Mara Graziani"], "title": "Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra", "comment": null, "summary": "Tandem Mass Spectrometry enables the identification of unknown compounds in\ncrucial fields such as metabolomics, natural product discovery and\nenvironmental analysis. However, current methods rely on database matching from\npreviously observed molecules, or on multi-step pipelines that require\nintermediate fragment or fingerprint prediction. This makes finding the correct\nmolecule highly challenging, particularly for compounds absent from reference\ndatabases. We introduce a framework that, by leveraging test-time tuning,\nenhances the learning of a pre-trained transformer model to address this gap,\nenabling end-to-end de novo molecular structure generation directly from the\ntandem mass spectra and molecular formulae, bypassing manual annotations and\nintermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on\ntwo popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.\nTest-time tuning on experimental spectra allows the model to dynamically adapt\nto novel spectra, and the relative performance gain over conventional\nfine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground\ntruth, the generated molecular candidates remain structurally accurate,\nproviding valuable guidance for human interpretation and more reliable\nidentification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d4b\u8bd5\u65f6\u8c03\u4f18\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u751f\u6210\u5206\u5b50\u7ed3\u6784\uff0c\u65e0\u9700\u6570\u636e\u5e93\u5339\u914d\u6216\u4e2d\u95f4\u6b65\u9aa4\uff0c\u5728NPLIB1\u548cMassSpecGym\u57fa\u51c6\u4e0a\u5206\u522b\u6bd4DiffMS\u65b9\u6cd5\u63d0\u5347100%\u548c20%\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u5e93\u5339\u914d\u6216\u591a\u6b65\u6d41\u7a0b\uff0c\u96be\u4ee5\u8bc6\u522b\u672a\u5728\u53c2\u8003\u6570\u636e\u5e93\u4e2d\u51fa\u73b0\u7684\u5316\u5408\u7269\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u4ece\u8d28\u8c31\u6570\u636e\u751f\u6210\u5206\u5b50\u7ed3\u6784\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6d4b\u8bd5\u65f6\u8c03\u4f18\u589e\u5f3a\u9884\u8bad\u7ec3transformer\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u8fdb\u884c\u7aef\u5230\u7aef\u7684\u4ece\u5934\u5206\u5b50\u7ed3\u6784\u751f\u6210\uff0c\u7ed5\u8fc7\u624b\u52a8\u6ce8\u91ca\u548c\u4e2d\u95f4\u6b65\u9aa4\u3002", "result": "\u5728NPLIB1\u548cMassSpecGym\u57fa\u51c6\u4e0a\u5206\u522b\u6bd4DiffMS\u65b9\u6cd5\u63d0\u5347100%\u548c20%\uff0c\u6d4b\u8bd5\u65f6\u8c03\u4f18\u76f8\u6bd4\u4f20\u7edf\u5fae\u8c03\u5728MassSpecGym\u4e0a\u5e26\u676562%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u52a8\u6001\u9002\u5e94\u65b0\u8d28\u8c31\u6570\u636e\uff0c\u5373\u4f7f\u9884\u6d4b\u504f\u79bb\u771f\u5b9e\u503c\uff0c\u751f\u6210\u7684\u5206\u5b50\u5019\u9009\u4ecd\u4fdd\u6301\u7ed3\u6784\u51c6\u786e\u6027\uff0c\u4e3a\u4eba\u5de5\u89e3\u91ca\u548c\u53ef\u9760\u8bc6\u522b\u63d0\u4f9b\u6709\u4ef7\u503c\u6307\u5bfc\u3002"}}
{"id": "2510.24019", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24019", "abs": "https://arxiv.org/abs/2510.24019", "authors": ["Xing Xing", "Wei Wang", "Lipeng Ma", "Weidong Yang", "Junjie Zheng"], "title": "Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs", "comment": null, "summary": "Recent progress in large language models (LLMs) has advanced automatic code\ngeneration, yet most approaches rely on direct, single-step translation from\nproblem descriptions to code, disregarding structured software engineering\npractices. We introduce a lifecycle-aware framework that systematically\nincorporates intermediate artifacts such as requirements analysis, state\nmachine modeling, and pseudocode into both the training and inference stages.\nThis design aligns code generation with standard software development phases\nand enables more structured reasoning. Experiments show that lifecycle-level\nfine-tuning improves code correctness by up to 75% over the same model before\nfine-tuning, with performance gains compounding across intermediate stages.\nMulti-step inference consistently surpasses single-step generation,\ndemonstrating the effectiveness of intermediate scaffolding. Notably,\nopen-source LLMs, once fine-tuned under our framework, match or slightly\noutperform models pretrained on code. When applied to DeepSeek-Coder-1.3B, our\nframework yields relative CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and\n22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B,\nrespectively. Our pipeline also proves robust with up to 80\\% less training\ndata, confirming its resilience. Ablation studies further reveal that each\nintermediate artifact contributes distinctly to final code quality, with state\nmachine modeling yielding the most substantial impact. Our source code and\ndetailed experimental data are available at\nhttps://anonymous.4open.science/r/Lifecycle-Aware-3CCB.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u751f\u547d\u5468\u671f\u611f\u77e5\u7684\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u9700\u6c42\u5206\u6790\u3001\u72b6\u6001\u673a\u5efa\u6a21\u548c\u4f2a\u4ee3\u7801\u7b49\u4e2d\u95f4\u4ea7\u7269\uff0c\u5c06\u4ee3\u7801\u751f\u6210\u4e0e\u6807\u51c6\u8f6f\u4ef6\u5f00\u53d1\u9636\u6bb5\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u6b63\u786e\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u4e3b\u8981\u4f9d\u8d56\u4ece\u95ee\u9898\u63cf\u8ff0\u5230\u4ee3\u7801\u7684\u5355\u6b65\u7ffb\u8bd1\uff0c\u5ffd\u89c6\u4e86\u7ed3\u6784\u5316\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u4e2d\u95f4\u4ea7\u7269\u7cfb\u7edf\u6027\u5730\u878d\u5165\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u3002", "method": "\u6784\u5efa\u751f\u547d\u5468\u671f\u611f\u77e5\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u9700\u6c42\u5206\u6790\u3001\u72b6\u6001\u673a\u5efa\u6a21\u548c\u4f2a\u4ee3\u7801\u7b49\u4e2d\u95f4\u4ea7\u7269\uff0c\u91c7\u7528\u591a\u6b65\u63a8\u7406\u800c\u975e\u5355\u6b65\u751f\u6210\uff0c\u5e76\u5bf9\u5f00\u6e90LLM\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u751f\u547d\u5468\u671f\u7ea7\u5fae\u8c03\u4f7f\u4ee3\u7801\u6b63\u786e\u6027\u63d0\u5347\u9ad8\u8fbe75%\uff1b\u5728DeepSeek-Coder-1.3B\u4e0a\uff0cCodeBLEU\u76f8\u5bf9ChatGPT-3.5\u3001ChatGPT-4o-mini\u3001DeepSeek-R1\u548cLLaMA-8B\u5206\u522b\u63d0\u534734.3%\u300120.0%\u300111.2%\u548c22.3%\uff1b\u8bad\u7ec3\u6570\u636e\u51cf\u5c1180%\u4ecd\u4fdd\u6301\u7a33\u5065\u3002", "conclusion": "\u4e2d\u95f4\u4ea7\u7269\u5bf9\u6700\u7ec8\u4ee3\u7801\u8d28\u91cf\u6709\u663e\u8457\u8d21\u732e\uff0c\u72b6\u6001\u673a\u5efa\u6a21\u5f71\u54cd\u6700\u5927\uff1b\u5fae\u8c03\u540e\u7684\u5f00\u6e90LLM\u53ef\u5339\u914d\u6216\u8d85\u8d8a\u9884\u8bad\u7ec3\u6a21\u578b\uff1b\u591a\u6b65\u63a8\u7406\u59cb\u7ec8\u4f18\u4e8e\u5355\u6b65\u751f\u6210\uff0c\u8bc1\u5b9e\u4e86\u4e2d\u95f4\u652f\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.23891", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23891", "abs": "https://arxiv.org/abs/2510.23891", "authors": ["Jiaqi Xue", "Yifei Zhao", "Mansour Al Ghanim", "Shangqian Gao", "Ruimin Sun", "Qian Lou", "Mengxin Zheng"], "title": "PRO: Enabling Precise and Robust Text Watermark for Open-Source LLMs", "comment": null, "summary": "Text watermarking for large language models (LLMs) enables model owners to\nverify text origin and protect intellectual property. While watermarking\nmethods for closed-source LLMs are relatively mature, extending them to\nopen-source models remains challenging, as developers cannot control the\ndecoding process. Consequently, owners of open-source LLMs lack practical means\nto verify whether text was generated by their models. A core difficulty lies in\nembedding watermarks directly into model weights without hurting detectability.\nA promising idea is to distill watermarks from a closed-source model into an\nopen one, but this suffers from (i) poor detectability due to mismatch between\nlearned and predefined patterns, and (ii) fragility to downstream modifications\nsuch as fine-tuning or model merging. To overcome these limitations, we propose\nPRO, a Precise and Robust text watermarking method for open-source LLMs. PRO\njointly trains a watermark policy model with the LLM, producing patterns that\nare easier for the model to learn and more consistent with detection criteria.\nA regularization term further simulates downstream perturbations and penalizes\ndegradation in watermark detectability, ensuring robustness under model edits.\nExperiments on open-source LLMs (e.g., LLaMA-3.2, LLaMA-3, Phi-2) show that PRO\nsubstantially improves both watermark detectability and resilience to model\nmodifications.", "AI": {"tldr": "PRO\u662f\u4e00\u79cd\u9488\u5bf9\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cbe\u786e\u9c81\u68d2\u6587\u672c\u6c34\u5370\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u6c34\u5370\u7b56\u7565\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5f00\u6e90\u6a21\u578b\u6c34\u5370\u68c0\u6d4b\u6027\u5dee\u548c\u6297\u5e72\u6270\u80fd\u529b\u5f31\u7684\u95ee\u9898\u3002", "motivation": "\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u5b9e\u7528\u7684\u6587\u672c\u6765\u6e90\u9a8c\u8bc1\u624b\u6bb5\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u6027\u548c\u6297\u4e0b\u6e38\u4fee\u6539\uff08\u5982\u5fae\u8c03\u3001\u6a21\u578b\u5408\u5e76\uff09\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u8054\u5408\u8bad\u7ec3\u6c34\u5370\u7b56\u7565\u6a21\u578b\u4e0eLLM\uff0c\u751f\u6210\u6613\u4e8e\u5b66\u4e60\u4e14\u4e0e\u68c0\u6d4b\u6807\u51c6\u4e00\u81f4\u7684\u6a21\u5f0f\uff1b\u4f7f\u7528\u6b63\u5219\u5316\u9879\u6a21\u62df\u4e0b\u6e38\u6270\u52a8\u5e76\u60e9\u7f5a\u6c34\u5370\u68c0\u6d4b\u6027\u9000\u5316\u3002", "result": "\u5728LLaMA-3.2\u3001LLaMA-3\u3001Phi-2\u7b49\u5f00\u6e90\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPRO\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u68c0\u6d4b\u6027\u548c\u6a21\u578b\u4fee\u6539\u540e\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "PRO\u4e3a\u5f00\u6e90LLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6c34\u5370\u65b9\u6848\uff0c\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6297\u5e72\u6270\u80fd\u529b\u65b9\u9762\u5747\u6709\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2510.23772", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23772", "abs": "https://arxiv.org/abs/2510.23772", "authors": ["Vivek Veeriah", "Federico Barbero", "Marcus Chiam", "Xidong Feng", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Johan Obando-Ceron", "Jiaxin Shi", "Shaobo Hou", "Satinder Singh", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions", "comment": "Accepted at the Creative AI Track, NeurIPS 2025", "summary": "The rapid advancement of Generative AI has raised significant questions\nregarding its ability to produce creative and novel outputs. Our recent work\ninvestigates this question within the domain of chess puzzles and presents an\nAI system designed to generate puzzles characterized by aesthetic appeal,\nnovelty, counter-intuitive and unique solutions. We briefly discuss our method\nbelow and refer the reader to the technical paper for more details. To assess\nour system's creativity, we presented a curated booklet of AI-generated puzzles\nto three world-renowned experts: International Master for chess compositions\nAmatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All\nthree are noted authors on chess aesthetics and the evolving role of computers\nin the game. They were asked to select their favorites and explain what made\nthem appealing, considering qualities such as their creativity, level of\nchallenge, or aesthetic design.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u8c61\u68cb\u8c1c\u9898\u9886\u57df\u7684\u521b\u9020\u529b\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u751f\u6210\u5177\u6709\u7f8e\u5b66\u5438\u5f15\u529b\u3001\u65b0\u9896\u6027\u3001\u53cd\u76f4\u89c9\u548c\u72ec\u7279\u89e3\u51b3\u65b9\u6848\u7684\u8c1c\u9898\u7cfb\u7edf\uff0c\u5e76\u7531\u4e09\u4f4d\u56fd\u9645\u8c61\u68cb\u4e13\u5bb6\u8bc4\u4f30\u5176\u521b\u9020\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4eba\u4eec\u5bf9\u5176\u4ea7\u751f\u521b\u9020\u6027\u65b0\u9896\u8f93\u51fa\u7684\u80fd\u529b\u5b58\u5728\u7591\u95ee\uff0c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1AI\u5728\u8c61\u68cb\u8c1c\u9898\u521b\u4f5c\u65b9\u9762\u7684\u521b\u9020\u529b\u3002", "method": "\u5f00\u53d1AI\u7cfb\u7edf\u751f\u6210\u8c61\u68cb\u8c1c\u9898\uff0c\u9080\u8bf7\u4e09\u4f4d\u56fd\u9645\u8c61\u68cb\u4e13\u5bb6\uff08\u56fd\u9645\u5927\u5e08Amatzia Avni\u3001\u7279\u7ea7\u5927\u5e08Jonathan Levitt\u548cMatthew Sadler\uff09\u8bc4\u4f30AI\u751f\u6210\u7684\u8c1c\u9898\u9009\u96c6\uff0c\u57fa\u4e8e\u521b\u9020\u529b\u3001\u6311\u6218\u6027\u548c\u7f8e\u5b66\u8bbe\u8ba1\u7b49\u6807\u51c6\u8bc4\u9009\u6700\u4f73\u8c1c\u9898\u3002", "result": "\u4e09\u4f4d\u56fd\u9645\u8c61\u68cb\u4e13\u5bb6\u6210\u529f\u4eceAI\u751f\u6210\u7684\u8c1c\u9898\u4e2d\u6311\u9009\u51fa\u4ed6\u4eec\u6700\u559c\u6b22\u7684\u8c1c\u9898\uff0c\u5e76\u89e3\u91ca\u4e86\u8fd9\u4e9b\u8c1c\u9898\u7684\u5438\u5f15\u529b\u6240\u5728\u3002", "conclusion": "\u7814\u7a76\u8868\u660eAI\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u5177\u6709\u521b\u9020\u6027\u548c\u7f8e\u5b66\u4ef7\u503c\u7684\u8c61\u68cb\u8c1c\u9898\uff0c\u8bc1\u660e\u4e86\u751f\u6210\u5f0fAI\u5728\u7279\u5b9a\u9886\u57df\u5177\u5907\u521b\u9020\u6f5c\u529b\u3002"}}
{"id": "2510.24142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24142", "abs": "https://arxiv.org/abs/2510.24142", "authors": ["Joran Leest", "Ilias Gerostathopoulos", "Patricia Lago", "Claudia Raibulet"], "title": "Monitoring and Observability of Machine Learning Systems: Current Practices and Gaps", "comment": null, "summary": "Production machine learning (ML) systems fail silently -- not with crashes,\nbut through wrong decisions. While observability is recognized as critical for\nML operations, there is a lack empirical evidence of what practitioners\nactually capture. This study presents empirical results on ML observability in\npractice through seven focus group sessions in several domains. We catalog the\ninformation practitioners systematically capture across ML systems and their\nenvironment and map how they use it to validate models, detect and diagnose\nfaults, and explain observed degradations. Finally, we identify gaps in current\npractice and outline implications for tooling design and research to establish\nML observability practices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e03\u4e2a\u7126\u70b9\u5c0f\u7ec4\u4f1a\u8bae\uff0c\u5b9e\u8bc1\u5206\u6790\u4e86\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u89c2\u6d4b\u6027\u5b9e\u8df5\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u5b9e\u8df5\u4e2d\u7684\u5dee\u8ddd\uff0c\u5e76\u4e3a\u5de5\u5177\u8bbe\u8ba1\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002", "motivation": "\u751f\u4ea7\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u901a\u5e38\u4ee5\u9519\u8bef\u51b3\u7b56\u800c\u975e\u5d29\u6e83\u7684\u65b9\u5f0f\u5931\u8d25\uff0c\u867d\u7136\u53ef\u89c2\u6d4b\u6027\u5bf9ML\u8fd0\u7ef4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u5b9e\u8df5\u8005\u5b9e\u9645\u6355\u83b7\u5185\u5bb9\u7684\u5b9e\u8bc1\u8bc1\u636e\u3002", "method": "\u901a\u8fc7\u5728\u4e0d\u540c\u9886\u57df\u7684\u4e03\u4e2a\u7126\u70b9\u5c0f\u7ec4\u4f1a\u8bae\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u7cfb\u7edf\u5316\u5730\u7f16\u76ee\u5b9e\u8df5\u8005\u5728ML\u7cfb\u7edf\u53ca\u5176\u73af\u5883\u4e2d\u6355\u83b7\u7684\u4fe1\u606f\uff0c\u5e76\u5206\u6790\u4ed6\u4eec\u5982\u4f55\u4f7f\u7528\u8fd9\u4e9b\u4fe1\u606f\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86ML\u53ef\u89c2\u6d4b\u6027\u7684\u5b9e\u8bc1\u7ed3\u679c\uff0c\u5305\u62ec\u5b9e\u8df5\u8005\u5982\u4f55\u9a8c\u8bc1\u6a21\u578b\u3001\u68c0\u6d4b\u548c\u8bca\u65ad\u6545\u969c\u4ee5\u53ca\u89e3\u91ca\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u8bc6\u522b\u4e86\u5f53\u524d\u5b9e\u8df5\u4e2d\u7684\u5dee\u8ddd\uff0c\u5e76\u4e3a\u5efa\u7acbML\u53ef\u89c2\u6d4b\u6027\u5b9e\u8df5\u7684\u5de5\u5177\u8bbe\u8ba1\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2510.23927", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23927", "abs": "https://arxiv.org/abs/2510.23927", "authors": ["Daniel Spokoyny", "Nikolai Vogler", "Xin Gao", "Tianyi Zheng", "Yufei Weng", "Jonghyun Park", "Jiajun Jiao", "Geoffrey M. Voelker", "Stefan Savage", "Taylor Berg-Kirkpatrick"], "title": "Victim as a Service: Designing a System for Engaging with Interactive Scammers", "comment": null, "summary": "Pig butchering, and similar interactive online scams, lower their victims'\ndefenses by building trust over extended periods of conversation - sometimes\nweeks or months. They have become increasingly public losses (at least $75B by\none recent study). However, because of their long-term conversational nature,\nthey are extremely challenging to investigate at scale. In this paper, we\ndescribe the motivation, design, implementation, and experience with\nCHATTERBOX, an LLM-based system that automates long-term engagement with online\nscammers, making large-scale investigations of their tactics possible. We\ndescribe the techniques we have developed to attract scam attempts, the system\nand LLM-engineering required to convincingly engage with scammers, and the\nnecessary capabilities required to satisfy or evade \"milestones\" in scammers'\nworkflow.", "AI": {"tldr": "\u5f00\u53d1\u4e86CHATTERBOX\u7cfb\u7edf\uff0c\u4f7f\u7528LLM\u81ea\u52a8\u4e0e\u5728\u7ebf\u8bc8\u9a97\u8005\u8fdb\u884c\u957f\u671f\u4e92\u52a8\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u8c03\u67e5\u8bc8\u9a97\u7b56\u7565", "motivation": "\u6740\u732a\u76d8\u7b49\u5728\u7ebf\u8bc8\u9a97\u901a\u8fc7\u957f\u671f\u5bf9\u8bdd\u5efa\u7acb\u4fe1\u4efb\uff0c\u9020\u6210\u5de8\u5927\u7ecf\u6d4e\u635f\u5931\uff08\u81f3\u5c11750\u4ebf\u7f8e\u5143\uff09\uff0c\u4f46\u7531\u4e8e\u5176\u957f\u671f\u5bf9\u8bdd\u7279\u6027\u96be\u4ee5\u8fdb\u884c\u5927\u89c4\u6a21\u8c03\u67e5", "method": "\u8bbe\u8ba1\u57fa\u4e8eLLM\u7684CHATTERBOX\u7cfb\u7edf\uff0c\u5f00\u53d1\u5438\u5f15\u8bc8\u9a97\u5c1d\u8bd5\u7684\u6280\u672f\uff0c\u4f7f\u7528LLM\u5de5\u7a0b\u4e0e\u8bc8\u9a97\u8005\u8fdb\u884c\u53ef\u4fe1\u4e92\u52a8\uff0c\u6ee1\u8db3\u6216\u89c4\u907f\u8bc8\u9a97\u8005\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\"\u91cc\u7a0b\u7891\"", "result": "\u6210\u529f\u6784\u5efa\u4e86\u80fd\u591f\u81ea\u52a8\u5316\u957f\u671f\u53c2\u4e0e\u8bc8\u9a97\u5bf9\u8bdd\u7684\u7cfb\u7edf\uff0c\u4f7f\u5927\u89c4\u6a21\u8c03\u67e5\u6210\u4e3a\u53ef\u80fd", "conclusion": "CHATTERBOX\u7cfb\u7edf\u4e3a\u8c03\u67e5\u957f\u671f\u5728\u7ebf\u8bc8\u9a97\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.23807", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23807", "abs": "https://arxiv.org/abs/2510.23807", "authors": ["Hamid R. Tizhoosh"], "title": "Why Foundation Models in Pathology Are Failing", "comment": null, "summary": "In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.", "AI": {"tldr": "\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u764c\u75c7\u8bca\u65ad\u3001\u9884\u540e\u548c\u591a\u6a21\u6001\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u8bca\u65ad\u51c6\u786e\u6027\u4f4e\u3001\u9c81\u68d2\u6027\u5dee\u3001\u51e0\u4f55\u4e0d\u7a33\u5b9a\u3001\u8ba1\u7b97\u9700\u6c42\u5927\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u95ee\u9898\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u8be5\u8303\u5f0f\u3002", "motivation": "\u975e\u533b\u5b66\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u548c\u591a\u6a21\u6001\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u8bed\u8a00\u5904\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4f46\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u5feb\u901f\u5e94\u7528\u5e76\u672a\u5e26\u6765\u9884\u671f\u7684\u7a81\u7834\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u8bc6\u522b\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u6839\u672c\u5f31\u70b9\uff0c\u5e76\u5206\u6790\u8fd9\u4e9b\u5f31\u70b9\u6e90\u4e8e\u901a\u7528\u57fa\u7840\u5efa\u6a21\u5047\u8bbe\u4e0e\u4eba\u4f53\u7ec4\u7ec7\u5185\u5728\u590d\u6742\u6027\u4e4b\u95f4\u7684\u6982\u5ff5\u4e0d\u5339\u914d\u3002", "result": "\u8bc6\u522b\u51fa\u4e03\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u539f\u56e0\uff1a\u751f\u7269\u590d\u6742\u6027\u3001\u65e0\u6548\u7684\u81ea\u76d1\u7763\u3001\u8fc7\u5ea6\u6cdb\u5316\u3001\u8fc7\u5ea6\u67b6\u6784\u590d\u6742\u6027\u3001\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u521b\u65b0\u3001\u6570\u636e\u4e0d\u8db3\u4ee5\u53ca\u4e0e\u7ec4\u7ec7\u5757\u5927\u5c0f\u76f8\u5173\u7684\u57fa\u672c\u8bbe\u8ba1\u7f3a\u9677\u3002", "conclusion": "\u5f53\u524d\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u6982\u5ff5\u4e0a\u4e0e\u7ec4\u7ec7\u5f62\u6001\u5b66\u6027\u8d28\u4e0d\u5339\u914d\uff0c\u9700\u8981\u4ece\u6839\u672c\u4e0a\u91cd\u65b0\u601d\u8003\u8be5\u8303\u5f0f\u672c\u8eab\u3002"}}
{"id": "2510.24188", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24188", "abs": "https://arxiv.org/abs/2510.24188", "authors": ["C\u00e9sar Santos", "Ermeson Andrade", "Roberto Natella"], "title": "Investigating Software Aging in LLM-Generated Software Systems", "comment": "Presented at the 17th International Workshop on Software Aging and\n  Rejuvenation (WoSAR), 2025", "summary": "Automatically generated software, especially code produced by Large Language\nModels (LLMs), is increasingly adopted to accelerate development and reduce\nmanual effort. However, little is known about the long-term reliability of such\nsystems under sustained execution. In this paper, we experimentally investigate\nthe phenomenon of software aging in applications generated by LLM-based tools.\nUsing the Bolt platform and standardized prompts from Baxbench, we generated\nfour service-oriented applications and subjected them to 50-hour load tests.\nResource usage, response time, and throughput were continuously monitored to\ndetect degradation patterns. The results reveal significant evidence of\nsoftware aging, including progressive memory growth, increased response time,\nand performance instability across all applications. Statistical analyzes\nconfirm these trends and highlight variability in the severity of aging\naccording to the type of application. Our findings show the need to consider\naging in automatically generated software and provide a foundation for future\nstudies on mitigation strategies and long-term reliability evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLM\u751f\u6210\u8f6f\u4ef6\u4e2d\u7684\u8f6f\u4ef6\u8001\u5316\u73b0\u8c61\uff0c\u901a\u8fc750\u5c0f\u65f6\u8d1f\u8f7d\u6d4b\u8bd5\u53d1\u73b0\u6240\u6709\u5e94\u7528\u90fd\u51fa\u73b0\u4e86\u663e\u8457\u7684\u5185\u5b58\u589e\u957f\u3001\u54cd\u5e94\u65f6\u95f4\u589e\u52a0\u548c\u6027\u80fd\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u7684\u8f6f\u4ef6\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u9700\u8981\u4e86\u89e3\u8fd9\u7c7b\u7cfb\u7edf\u5728\u6301\u7eed\u6267\u884c\u4e0b\u7684\u957f\u671f\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u8f6f\u4ef6\u8001\u5316\u73b0\u8c61\u3002", "method": "\u4f7f\u7528Bolt\u5e73\u53f0\u548cBaxbench\u6807\u51c6\u5316\u63d0\u793a\u751f\u6210\u56db\u4e2a\u9762\u5411\u670d\u52a1\u7684\u5e94\u7528\uff0c\u8fdb\u884c50\u5c0f\u65f6\u8d1f\u8f7d\u6d4b\u8bd5\uff0c\u6301\u7eed\u76d1\u63a7\u8d44\u6e90\u4f7f\u7528\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u541e\u5410\u91cf\u3002", "result": "\u6240\u6709\u5e94\u7528\u90fd\u663e\u793a\u51fa\u663e\u8457\u7684\u8f6f\u4ef6\u8001\u5316\u8bc1\u636e\uff0c\u5305\u62ec\u6e10\u8fdb\u5f0f\u5185\u5b58\u589e\u957f\u3001\u54cd\u5e94\u65f6\u95f4\u589e\u52a0\u548c\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u7edf\u8ba1\u5206\u6790\u786e\u8ba4\u4e86\u8fd9\u4e9b\u8d8b\u52bf\u3002", "conclusion": "\u81ea\u52a8\u751f\u6210\u8f6f\u4ef6\u9700\u8981\u8003\u8651\u8001\u5316\u95ee\u9898\uff0c\u8fd9\u4e3a\u672a\u6765\u7814\u7a76\u7f13\u89e3\u7b56\u7565\u548c\u957f\u671f\u53ef\u9760\u6027\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23938", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23938", "abs": "https://arxiv.org/abs/2510.23938", "authors": ["Marcin Spoczynski", "Marcela S. Melara"], "title": "Scalable GPU-Based Integrity Verification for Large Machine Learning Models", "comment": null, "summary": "We present a security framework that strengthens distributed machine learning\nby standardizing integrity protections across CPU and GPU platforms and\nsignificantly reducing verification overheads. Our approach co-locates\nintegrity verification directly with large ML model execution on GPU\naccelerators, resolving the fundamental mismatch between how large ML workloads\ntypically run (primarily on GPUs) and how security verifications traditionally\noperate (on separate CPU-based processes), delivering both immediate\nperformance benefits and long-term architectural consistency. By performing\ncryptographic operations natively on GPUs using dedicated compute units (e.g.,\nIntel Arc's XMX units, NVIDIA's Tensor Cores), our solution eliminates the\npotential architectural bottlenecks that could plague traditional CPU-based\nverification systems when dealing with large models. This approach leverages\nthe same GPU-based high-memory bandwidth and parallel processing primitives\nthat power ML workloads ensuring integrity checks keep pace with model\nexecution even for massive models exceeding 100GB. This framework establishes a\ncommon integrity verification mechanism that works consistently across\ndifferent GPU vendors and hardware configurations. By anticipating future\ncapabilities for creating secure channels between trusted execution\nenvironments and GPU accelerators, we provide a hardware-agnostic foundation\nthat enterprise teams can deploy regardless of their underlying CPU and GPU\ninfrastructures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b89\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u5728GPU\u4e0a\u539f\u751f\u6267\u884c\u5b8c\u6574\u6027\u9a8c\u8bc1\u6765\u89e3\u51b3\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u74f6\u9888\u95ee\u9898", "motivation": "\u89e3\u51b3\u5927\u578bML\u5de5\u4f5c\u8d1f\u8f7d\u4e3b\u8981\u5728GPU\u4e0a\u8fd0\u884c\uff0c\u800c\u5b89\u5168\u9a8c\u8bc1\u4f20\u7edf\u4e0a\u5728\u5355\u72ec\u7684CPU\u8fdb\u7a0b\u4e2d\u8fd0\u884c\u7684\u6839\u672c\u4e0d\u5339\u914d\u95ee\u9898", "method": "\u5728GPU\u52a0\u901f\u5668\u4e0a\u76f4\u63a5\u8fdb\u884c\u5b8c\u6574\u6027\u9a8c\u8bc1\uff0c\u5229\u7528GPU\u4e13\u7528\u8ba1\u7b97\u5355\u5143\uff08\u5982Intel Arc\u7684XMX\u5355\u5143\u3001NVIDIA\u7684Tensor Cores\uff09\u539f\u751f\u6267\u884c\u52a0\u5bc6\u64cd\u4f5c", "result": "\u663e\u8457\u964d\u4f4e\u9a8c\u8bc1\u5f00\u9500\uff0c\u786e\u4fdd\u5b8c\u6574\u6027\u68c0\u67e5\u80fd\u591f\u8ddf\u4e0a\u6a21\u578b\u6267\u884c\u901f\u5ea6\uff0c\u5373\u4f7f\u5bf9\u4e8e\u8d85\u8fc7100GB\u7684\u5927\u578b\u6a21\u578b\u4e5f\u80fd\u4fdd\u6301\u540c\u6b65", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u8de8\u4e0d\u540cGPU\u5382\u5546\u548c\u786c\u4ef6\u914d\u7f6e\u7684\u901a\u7528\u5b8c\u6574\u6027\u9a8c\u8bc1\u673a\u5236\uff0c\u4e3a\u4f01\u4e1a\u56e2\u961f\u63d0\u4f9b\u4e86\u786c\u4ef6\u65e0\u5173\u7684\u5b89\u5168\u57fa\u7840\u67b6\u6784"}}
{"id": "2510.23822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23822", "abs": "https://arxiv.org/abs/2510.23822", "authors": ["Zhenyu Zhang", "Tianyi Chen", "Weiran Xu", "Alex Pentland", "Jiaxin Pei"], "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "comment": null, "summary": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning\nremain challenging for large language models (LLMs). Sequential prompting\nmethods are prone to context drift, loss of goal information, and recurrent\nfailure cycles, while hierarchical prompting methods often weaken cross-level\ncontinuity or incur substantial runtime overhead. We introduce ReCAP (Recursive\nContext-Aware Reasoning and Planning), a hierarchical framework with shared\ncontext for reasoning and planning in LLMs. ReCAP combines three key\nmechanisms: (i) plan-ahead decomposition, in which the model generates a full\nsubtask list, executes the first item, and refines the remainder; (ii)\nstructured re-injection of parent plans, maintaining consistent multi-level\ncontext during recursive return; and (iii) memory-efficient execution, bounding\nthe active prompt so costs scale linearly with task depth. Together these\nmechanisms align high-level goals with low-level actions, reduce redundant\nprompting, and preserve coherent context updates across recursion. Experiments\ndemonstrate that ReCAP substantially improves subgoal alignment and success\nrates on various long-horizon reasoning benchmarks, achieving a 32% gain on\nsynchronous Robotouille and a 29% improvement on asynchronous Robotouille under\nthe strict pass@1 protocol.", "AI": {"tldr": "ReCAP\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9012\u5f52\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u548c\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u5212\u5206\u89e3\u3001\u7ed3\u6784\u5316\u7236\u8ba1\u5212\u91cd\u6ce8\u5165\u548c\u5185\u5b58\u9ad8\u6548\u6267\u884c\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u5b50\u76ee\u6807\u5bf9\u9f50\u548c\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u591a\u6b65\u63a8\u7406\u548c\u52a8\u6001\u91cd\u89c4\u5212\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u4e0a\u4e0b\u6587\u6f02\u79fb\u3001\u76ee\u6807\u4fe1\u606f\u4e22\u5931\u548c\u5faa\u73af\u5931\u8d25\u7b49\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u5bfc\u81f4\u7684\u8de8\u5c42\u7ea7\u8fde\u7eed\u6027\u51cf\u5f31\u6216\u8fd0\u884c\u65f6\u5f00\u9500\u8fc7\u5927\u3002", "method": "\u7ed3\u5408\u4e09\u4e2a\u5173\u952e\u673a\u5236\uff1a(1) \u8ba1\u5212\u5206\u89e3\uff1a\u751f\u6210\u5b8c\u6574\u5b50\u4efb\u52a1\u5217\u8868\uff0c\u6267\u884c\u7b2c\u4e00\u9879\u5e76\u7ec6\u5316\u5269\u4f59\u4efb\u52a1\uff1b(2) \u7ed3\u6784\u5316\u7236\u8ba1\u5212\u91cd\u6ce8\u5165\uff1a\u5728\u9012\u5f52\u8fd4\u56de\u65f6\u4fdd\u6301\u591a\u5c42\u7ea7\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff1b(3) \u5185\u5b58\u9ad8\u6548\u6267\u884c\uff1a\u9650\u5236\u6d3b\u52a8\u63d0\u793a\uff0c\u4f7f\u6210\u672c\u968f\u4efb\u52a1\u6df1\u5ea6\u7ebf\u6027\u6269\u5c55\u3002", "result": "\u5728\u5404\u79cd\u957f\u65f6\u7a0b\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u6539\u5584\u4e86\u5b50\u76ee\u6807\u5bf9\u9f50\u548c\u6210\u529f\u7387\uff0c\u5728\u4e25\u683cpass@1\u534f\u8bae\u4e0b\uff0c\u540c\u6b65Robotouille\u4efb\u52a1\u63d0\u5347\u4e8632%\uff0c\u5f02\u6b65Robotouille\u4efb\u52a1\u63d0\u5347\u4e8629%\u3002", "conclusion": "ReCAP\u6846\u67b6\u901a\u8fc7\u5c06\u9ad8\u5c42\u76ee\u6807\u4e0e\u4f4e\u5c42\u52a8\u4f5c\u5bf9\u9f50\u3001\u51cf\u5c11\u5197\u4f59\u63d0\u793a\u548c\u4fdd\u6301\u8de8\u9012\u5f52\u7684\u8fde\u8d2f\u4e0a\u4e0b\u6587\u66f4\u65b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u63a8\u7406\u548c\u89c4\u5212\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2510.24241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24241", "abs": "https://arxiv.org/abs/2510.24241", "authors": ["Zixian Zhang", "Takfarinas Saber"], "title": "MAGNET: A Multi-Graph Attentional Network for Code Clone Detection", "comment": null, "summary": "Code clone detection is a fundamental task in software engineering that\nunderpins refactoring, debugging, plagiarism detection, and vulnerability\nanalysis. Existing methods often rely on singular representations such as\nabstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs\n(DFGs), which capture only partial aspects of code semantics. Hybrid approaches\nhave emerged, but their fusion strategies are typically handcrafted and\nineffective. In this study, we propose MAGNET, a multi-graph attentional\nframework that jointly leverages AST, CFG, and DFG representations to capture\nsyntactic and semantic features of source code. MAGNET integrates residual\ngraph neural networks with node-level self-attention to learn both local and\nlong-range dependencies, introduces a gated cross-attention mechanism for\nfine-grained inter-graph interactions, and employs Set2Set pooling to fuse\nmulti-graph embeddings into unified program-level representations. Extensive\nexperiments on BigCloneBench and Google Code Jam demonstrate that MAGNET\nachieves state-of-the-art performance with an overall F1 score of 96.5\\% and\n99.2\\% on the two datasets, respectively. Ablation studies confirm the critical\ncontributions of multi-graph fusion and each attentional component. Our code is\navailable at https://github.com/ZixianReid/Multigraph_match", "AI": {"tldr": "MAGNET\u662f\u4e00\u4e2a\u591a\u56fe\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5229\u7528AST\u3001CFG\u548cDFG\u8868\u793a\u6765\u6355\u83b7\u6e90\u4ee3\u7801\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u5728\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u8868\u793a\uff08\u5982AST\u3001CFG\u3001DFG\uff09\uff0c\u53ea\u80fd\u6355\u83b7\u4ee3\u7801\u8bed\u4e49\u7684\u90e8\u5206\u65b9\u9762\u3002\u6df7\u5408\u65b9\u6cd5\u867d\u7136\u51fa\u73b0\uff0c\u4f46\u5176\u878d\u5408\u7b56\u7565\u901a\u5e38\u662f\u624b\u5de5\u8bbe\u8ba1\u4e14\u6548\u679c\u4e0d\u4f73\u3002", "method": "MAGNET\u6574\u5408\u6b8b\u5dee\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0e\u8282\u70b9\u7ea7\u81ea\u6ce8\u610f\u529b\u6765\u5b66\u4e60\u5c40\u90e8\u548c\u957f\u7a0b\u4f9d\u8d56\uff0c\u5f15\u5165\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u7ec6\u7c92\u5ea6\u56fe\u95f4\u4ea4\u4e92\uff0c\u5e76\u4f7f\u7528Set2Set\u6c60\u5316\u5c06\u591a\u56fe\u5d4c\u5165\u878d\u5408\u4e3a\u7edf\u4e00\u7684\u7a0b\u5e8f\u7ea7\u8868\u793a\u3002", "result": "\u5728BigCloneBench\u548cGoogle Code Jam\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMAGNET\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8696.5%\u548c99.2%\u7684\u603b\u4f53F1\u5206\u6570\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u591a\u56fe\u878d\u5408\u548c\u6bcf\u4e2a\u6ce8\u610f\u529b\u7ec4\u4ef6\u7684\u91cd\u8981\u8d21\u732e\u3002MAGNET\u4e3a\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u591a\u56fe\u878d\u5408\u65b9\u6cd5\u3002"}}
{"id": "2510.24072", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.24072", "abs": "https://arxiv.org/abs/2510.24072", "authors": ["Austin Shouli", "Yulia Bobkova", "Ajay Kumar Shrestha"], "title": "Covert Surveillance in Smart Devices: A SCOUR Framework Analysis of Youth Privacy Implications", "comment": "To appear in the IEEE UEMCON 2025 proceedings", "summary": "This paper investigates how smart devices covertly capture private\nconversations and discusses in more in-depth the implications of this for youth\nprivacy. Using a structured review guided by the PRISMA methodology, the\nanalysis focuses on privacy concerns, data capture methods, data storage and\nsharing practices, and proposed technical mitigations. To structure and\nsynthesize findings, we introduce the SCOUR framework, encompassing\nSurveillance mechanisms, Consent and awareness, Operational data flow, Usage\nand exploitation, and Regulatory and technical safeguards. Findings reveal that\nsmart devices have been covertly capturing personal data, especially with smart\ntoys and voice-activated smart gadgets built for youth. These issues are\nworsened by unclear data collection practices and insufficient transparency in\nsmart device applications. Balancing privacy and utility in smart devices is\ncrucial, as youth are becoming more aware of privacy breaches and value their\npersonal data more. Strategies to improve regulatory and technical safeguards\nare also provided. The review identifies research gaps and suggests future\ndirections. The limitations of this literature review are also explained. The\nfindings have significant implications for policy development and the\ntransparency of data collection for smart devices.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7PRISMA\u65b9\u6cd5\u7efc\u8ff0\u4e86\u667a\u80fd\u8bbe\u5907\u5982\u4f55\u9690\u853d\u91c7\u96c6\u9752\u5c11\u5e74\u9690\u79c1\u5bf9\u8bdd\uff0c\u63d0\u51fa\u4e86SCOUR\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u9690\u79c1\u62c5\u5fe7\u3001\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\u3001\u5b58\u50a8\u5171\u4eab\u5b9e\u8df5\u53ca\u6280\u672f\u7f13\u89e3\u63aa\u65bd\u3002", "motivation": "\u8c03\u67e5\u667a\u80fd\u8bbe\u5907\u5bf9\u9752\u5c11\u5e74\u9690\u79c1\u7684\u9690\u853d\u4fb5\u72af\u95ee\u9898\uff0c\u7531\u4e8e\u6570\u636e\u91c7\u96c6\u5b9e\u8df5\u4e0d\u660e\u786e\u548c\u900f\u660e\u5ea6\u4e0d\u8db3\uff0c\u9752\u5c11\u5e74\u9690\u79c1\u9762\u4e34\u4e25\u91cd\u5a01\u80c1\u3002", "method": "\u91c7\u7528PRISMA\u65b9\u6cd5\u8fdb\u884c\u7ed3\u6784\u5316\u6587\u732e\u7efc\u8ff0\uff0c\u5f15\u5165SCOUR\u6846\u67b6\u5206\u6790\u76d1\u63a7\u673a\u5236\u3001\u540c\u610f\u4e0e\u610f\u8bc6\u3001\u64cd\u4f5c\u6570\u636e\u6d41\u3001\u4f7f\u7528\u4e0e\u5229\u7528\u3001\u76d1\u7ba1\u4e0e\u6280\u672f\u4fdd\u969c\u4e94\u4e2a\u7ef4\u5ea6\u3002", "result": "\u53d1\u73b0\u667a\u80fd\u8bbe\u5907\u7279\u522b\u662f\u667a\u80fd\u73a9\u5177\u548c\u8bed\u97f3\u6fc0\u6d3b\u8bbe\u5907\u9690\u853d\u91c7\u96c6\u4e2a\u4eba\u6570\u636e\uff0c\u9752\u5c11\u5e74\u9690\u79c1\u610f\u8bc6\u589e\u5f3a\u4f46\u4fdd\u62a4\u63aa\u65bd\u4e0d\u8db3\uff0c\u9700\u8981\u5e73\u8861\u9690\u79c1\u4e0e\u5b9e\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u6539\u5584\u76d1\u7ba1\u548c\u6280\u672f\u4fdd\u969c\u7684\u7b56\u7565\uff0c\u8bc6\u522b\u4e86\u7814\u7a76\u7a7a\u767d\uff0c\u5bf9\u667a\u80fd\u8bbe\u5907\u6570\u636e\u91c7\u96c6\u900f\u660e\u5ea6\u548c\u653f\u7b56\u5236\u5b9a\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2510.23824", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23824", "abs": "https://arxiv.org/abs/2510.23824", "authors": ["Murad Ismayilov", "Edwin Meriaux", "Shuo Wen", "Gregory Dudek"], "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models", "comment": "Accepted at MIT URTC 2025", "summary": "Coordinating multiple autonomous agents in shared environments under\ndecentralized conditions is a long-standing challenge in robotics and\nartificial intelligence. This work addresses the problem of decentralized goal\nassignment for multi-agent path planning, where agents independently generate\nranked preferences over goals based on structured representations of the\nenvironment, including grid visualizations and scenario data. After this\nreasoning phase, agents exchange their goal rankings, and assignments are\ndetermined by a fixed, deterministic conflict-resolution rule (e.g., agent\nindex ordering), without negotiation or iterative coordination. We\nsystematically compare greedy heuristics, optimal assignment, and large\nlanguage model (LLM)-based agents in fully observable grid-world settings. Our\nresults show that LLM-based agents, when provided with well-designed prompts\nand relevant quantitative information, can achieve near-optimal makespans and\nconsistently outperform traditional heuristics. These findings underscore the\npotential of language models for decentralized goal assignment in multi-agent\npath planning and highlight the importance of information structure in such\nsystems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u8d2a\u5fc3\u542f\u53d1\u5f0f\u3001\u6700\u4f18\u5206\u914d\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0LLM\u667a\u80fd\u4f53\u5728\u826f\u597d\u63d0\u793a\u4e0b\u80fd\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u6761\u4ef6\u4e0b\u591a\u667a\u80fd\u4f53\u5728\u5171\u4eab\u73af\u5883\u4e2d\u7684\u534f\u8c03\u6311\u6218\uff0c\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u5206\u914d\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u667a\u80fd\u4f53\u57fa\u4e8e\u73af\u5883\u7ed3\u6784\u5316\u8868\u793a\u72ec\u7acb\u751f\u6210\u76ee\u6807\u504f\u597d\u6392\u5e8f\uff0c\u901a\u8fc7\u56fa\u5b9a\u51b2\u7a81\u89e3\u51b3\u89c4\u5219\u8fdb\u884c\u76ee\u6807\u5206\u914d\uff0c\u65e0\u9700\u534f\u5546\u6216\u8fed\u4ee3\u534f\u8c03\u3002\u7cfb\u7edf\u6bd4\u8f83\u4e86\u8d2a\u5fc3\u542f\u53d1\u5f0f\u3001\u6700\u4f18\u5206\u914d\u548cLLM\u667a\u80fd\u4f53\u65b9\u6cd5\u3002", "result": "LLM\u667a\u80fd\u4f53\u5728\u63d0\u4f9b\u826f\u597d\u63d0\u793a\u548c\u5b9a\u91cf\u4fe1\u606f\u65f6\uff0c\u80fd\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u5b8c\u6210\u65f6\u95f4\uff0c\u4e14\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4fe1\u606f\u7ed3\u6784\u8bbe\u8ba1\u5bf9\u6b64\u7c7b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.24265", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24265", "abs": "https://arxiv.org/abs/2510.24265", "authors": ["Sadia Afroz", "Zixuan Feng", "Katie Kimura", "Bianca Trinkenreich", "Igor Steinmacher", "Anita Sarma"], "title": "Developer Productivity with GenAI", "comment": null, "summary": "Generative AI (GenAI) tools are increasingly being adopted in software\ndevelopment as productivity aids. However, evidence regarding where and when\nthese tools actually enhance productivity is unclear. In this paper, we\ninvestigate how GenAI adoption affects different dimensions of developer\nproductivity. We surveyed 415 software practitioners to capture their\nperceptions of productivity changes associated with AI-assisted development\nusing the SPACE framework - Satisfaction and well-being, Performance, Activity,\nCommunication and collaboration, and Efficiency and flow. Our results,\ndisaggregated by frequency of AI usage, reveal limited overall productivity\nchange, highlighting the productivity paradox in which developers become faster\nbut do not necessarily create better software or feel more fulfilled.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u4e86\u751f\u6210\u5f0fAI\u5de5\u5177\u5bf9\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u867d\u7136\u5f00\u53d1\u901f\u5ea6\u63d0\u5347\uff0c\u4f46\u8f6f\u4ef6\u8d28\u91cf\u6539\u5584\u6709\u9650\uff0c\u5f00\u53d1\u8005\u6ee1\u610f\u5ea6\u53d8\u5316\u4e0d\u5927\uff0c\u5b58\u5728\u751f\u4ea7\u529b\u6096\u8bba\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u5bf9\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u7684\u5b9e\u9645\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u5b9e\u8bc1\u7814\u7a76\u6765\u4e86\u89e3\u8fd9\u4e9b\u5de5\u5177\u5728\u4f55\u65f6\u4f55\u5730\u771f\u6b63\u63d0\u5347\u751f\u4ea7\u529b\u3002", "method": "\u5bf9415\u540d\u8f6f\u4ef6\u4ece\u4e1a\u8005\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u4f7f\u7528SPACE\u6846\u67b6\uff08\u6ee1\u610f\u5ea6\u4e0e\u5e78\u798f\u611f\u3001\u7ee9\u6548\u3001\u6d3b\u52a8\u3001\u6c9f\u901a\u534f\u4f5c\u3001\u6548\u7387\u4e0e\u6d41\u7545\u6027\uff09\u8bc4\u4f30AI\u8f85\u52a9\u5f00\u53d1\u5bf9\u751f\u4ea7\u529b\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6574\u4f53\u751f\u4ea7\u529b\u53d8\u5316\u6709\u9650\uff0c\u5b58\u5728\u751f\u4ea7\u529b\u6096\u8bba\uff1a\u5f00\u53d1\u8005\u901f\u5ea6\u53d8\u5feb\u4f46\u672a\u5fc5\u521b\u9020\u66f4\u597d\u7684\u8f6f\u4ef6\u6216\u611f\u5230\u66f4\u6ee1\u610f\uff0c\u7ed3\u679c\u6309AI\u4f7f\u7528\u9891\u7387\u8fdb\u884c\u4e86\u7ec6\u5206\u5206\u6790\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u91c7\u7528\u5e26\u6765\u4e86\u901f\u5ea6\u63d0\u5347\uff0c\u4f46\u5e76\u672a\u663e\u8457\u6539\u5584\u8f6f\u4ef6\u8d28\u91cf\u6216\u5f00\u53d1\u8005\u6ee1\u610f\u5ea6\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3AI\u5de5\u5177\u5bf9\u751f\u4ea7\u529b\u7684\u591a\u7ef4\u5f71\u54cd\u3002"}}
{"id": "2510.24101", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24101", "abs": "https://arxiv.org/abs/2510.24101", "authors": ["Nam Tran", "Khoa Nguyen", "Dongxi Liu", "Josef Pieprzyk", "Willy Susilo"], "title": "Traceable Signatures from Lattices", "comment": "45 pages", "summary": "Traceable signatures (Kiayas et al., EUROCRYPT 2004) is an anonymous digital\nsignature system that extends the tracing power of the opening authority in\ngroup signatures. There are many known constructions of traceable signatures,\nbut all are based on number-theoretic/pairing assumptions. For such reason,\nthey may not be secure in the presence of quantum computers. This work revisits\nthe notion of traceable signatures and presents a lattice-based construction\nprovably secure in the quantum random oracle model (QROM).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u683c\u7684\u53ef\u8ffd\u8e2a\u7b7e\u540d\u65b9\u6848\uff0c\u5728\u91cf\u5b50\u968f\u673a\u9884\u8a00\u673a\u6a21\u578b\u4e0b\u53ef\u8bc1\u660e\u5b89\u5168\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u4e8e\u6570\u8bba/\u914d\u5bf9\u5047\u8bbe\u7684\u65b9\u6848\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u9762\u524d\u4e0d\u5b89\u5168\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u8ffd\u8e2a\u7b7e\u540d\u65b9\u6848\u90fd\u57fa\u4e8e\u6570\u8bba/\u914d\u5bf9\u5047\u8bbe\uff0c\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u9762\u524d\u53ef\u80fd\u4e0d\u5b89\u5168\uff0c\u9700\u8981\u6784\u5efa\u91cf\u5b50\u5b89\u5168\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u683c\u5bc6\u7801\u5b66\u6784\u9020\u53ef\u8ffd\u8e2a\u7b7e\u540d\u65b9\u6848\uff0c\u5728\u91cf\u5b50\u968f\u673a\u9884\u8a00\u673a\u6a21\u578b(QROM)\u4e0b\u8fdb\u884c\u5b89\u5168\u6027\u8bc1\u660e\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u683c\u7684\u53ef\u8ffd\u8e2a\u7b7e\u540d\u65b9\u6848\uff0c\u5728QROM\u4e0b\u53ef\u8bc1\u660e\u5b89\u5168\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u53ef\u8ffd\u8e2a\u7b7e\u540d\u63d0\u4f9b\u4e86\u91cf\u5b50\u5b89\u5168\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u683c\u5bc6\u7801\u5b66\u5728\u8be5\u9886\u57df\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.23856", "categories": ["cs.AI", "68Txx"], "pdf": "https://arxiv.org/pdf/2510.23856", "abs": "https://arxiv.org/abs/2510.23856", "authors": ["Segev Shlomov", "Alon Oved", "Sami Marreed", "Ido Levy", "Offer Akrabi", "Avi Yaeli", "\u0141ukasz Str\u0105k", "Elizabeth Koumpan", "Yinon Goldshtein", "Eilam Shapira", "Nir Mashkif", "Asaf Adi"], "title": "From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production", "comment": "AAAI Conference on Artificial Intelligence", "summary": "Agents are rapidly advancing in automating digital work, but enterprises face\na harder challenge: moving beyond prototypes to deployed systems that deliver\nmeasurable business value. This path is complicated by fragmented frameworks,\nslow development, and the absence of standardized evaluation practices.\nGeneralist agents have emerged as a promising direction, excelling on academic\nbenchmarks and offering flexibility across task types, applications, and\nmodalities. Yet, evidence of their use in production enterprise settings\nremains limited. This paper reports IBM's experience developing and piloting\nthe Computer Using Generalist Agent (CUGA), which has been open-sourced for the\ncommunity (https://github.com/cuga-project/cuga-agent). CUGA adopts a\nhierarchical planner--executor architecture with strong analytical foundations,\nachieving state-of-the-art performance on AppWorld and WebArena. Beyond\nbenchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing\ntalent acquisition domain, addressing enterprise requirements for scalability,\nauditability, safety, and governance. To support assessment, we introduce\nBPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary\nevaluations, CUGA approached the accuracy of specialized agents while\nindicating potential for reducing development time and cost. Our contribution\nis twofold: presenting early evidence of generalist agents operating at\nenterprise scale, and distilling technical and organizational lessons from this\ninitial pilot. We outline requirements and next steps for advancing\nresearch-grade architectures like CUGA into robust, enterprise-ready systems.", "AI": {"tldr": "IBM\u5f00\u53d1\u4e86\u901a\u7528\u667a\u80fd\u4f53CUGA\uff0c\u91c7\u7528\u5206\u5c42\u89c4\u5212-\u6267\u884c\u67b6\u6784\uff0c\u5728\u5b66\u672f\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u4f01\u4e1a\u4e1a\u52a1\u6d41\u7a0b\u5916\u5305\u4eba\u624d\u62db\u8058\u9886\u57df\u8fdb\u884c\u4e86\u8bd5\u70b9\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5e94\u7528\u7684\u6f5c\u529b\u3002", "motivation": "\u4f01\u4e1a\u9762\u4e34\u5c06AI\u4ee3\u7406\u4ece\u539f\u578b\u8f6c\u5411\u5b9e\u9645\u90e8\u7f72\u7684\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u6846\u67b6\u788e\u7247\u5316\u3001\u5f00\u53d1\u7f13\u6162\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u7b49\u95ee\u9898\u3002\u901a\u7528\u667a\u80fd\u4f53\u5728\u5b66\u672f\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4f01\u4e1a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5e94\u7528\u8bc1\u636e\u6709\u9650\u3002", "method": "CUGA\u91c7\u7528\u5206\u5c42\u89c4\u5212-\u6267\u884c\u67b6\u6784\uff0c\u5177\u6709\u5f3a\u5927\u7684\u5206\u6790\u57fa\u7840\u3002\u5728\u4e1a\u52a1\u6d41\u7a0b\u5916\u5305\u4eba\u624d\u62db\u8058\u9886\u57df\u8fdb\u884c\u8bd5\u70b9\u8bc4\u4f30\uff0c\u5e76\u5f15\u5165\u4e86\u5305\u542b26\u4e2a\u4efb\u52a1\u7684BPO-TA\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "CUGA\u5728AppWorld\u548cWebArena\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u5728\u521d\u6b65\u8bc4\u4f30\u4e2d\uff0cCUGA\u63a5\u8fd1\u4e13\u7528\u667a\u80fd\u4f53\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u793a\u51fa\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u548c\u6210\u672c\u7684\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u901a\u7528\u667a\u80fd\u4f53\u5728\u4f01\u4e1a\u89c4\u6a21\u8fd0\u884c\u7684\u65e9\u671f\u8bc1\u636e\uff0c\u5e76\u603b\u7ed3\u4e86\u6280\u672f\u548c\u7ec4\u7ec7\u7ecf\u9a8c\u3002\u4e3a\u5c06\u7814\u7a76\u7ea7\u67b6\u6784\u53d1\u5c55\u4e3a\u7a33\u5065\u7684\u4f01\u4e1a\u5c31\u7eea\u7cfb\u7edf\u63d0\u51fa\u4e86\u8981\u6c42\u548c\u4e0b\u4e00\u6b65\u8ba1\u5212\u3002"}}
{"id": "2510.24358", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24358", "abs": "https://arxiv.org/abs/2510.24358", "authors": ["Lingyue Fu", "Bolun Zhang", "Hao Guan", "Yaoming Zhu", "Lin Qiu", "Weiwen Liu", "Xuezhi Cao", "Xunliang Cai", "Weinan Zhang", "Yong Yu"], "title": "Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation", "comment": null, "summary": "Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86PRDBench\u57fa\u51c6\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u6d41\u6c34\u7ebf\u6784\u5efa\u5305\u542b50\u4e2a\u771f\u5b9ePython\u9879\u76ee\u7684\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4ee3\u7801\u667a\u80fd\u4f53\u8bc4\u4f30\u57fa\u51c6\u7684\u9ad8\u6807\u6ce8\u6210\u672c\u548c\u50f5\u5316\u8bc4\u4f30\u6307\u6807\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u667a\u80fd\u4f53\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u9ad8\u6807\u6ce8\u6210\u672c\u548c\u4e13\u4e1a\u8981\u6c42\uff0c\u4ee5\u53ca\u4e3b\u8981\u4f9d\u8d56\u5355\u5143\u6d4b\u8bd5\u7684\u50f5\u5316\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u91c7\u7528\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u57fa\u51c6\u6784\u5efa\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u4eba\u5de5\u76d1\u7763\u751f\u6210\u591a\u6837\u5316\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u9879\u76ee\u7ea7\u4efb\u52a1\uff0c\u5e76\u5f15\u5165Agent-as-a-Judge\u8303\u5f0f\u6765\u8bc4\u5206\u667a\u80fd\u4f53\u8f93\u51fa\u3002", "result": "\u6784\u5efa\u4e86PRDBench\u57fa\u51c6\uff0c\u5305\u542b50\u4e2a\u8de820\u4e2a\u9886\u57df\u7684\u771f\u5b9ePython\u9879\u76ee\uff0c\u6bcf\u4e2a\u9879\u76ee\u90fd\u6709\u7ed3\u6784\u5316\u4ea7\u54c1\u9700\u6c42\u6587\u6863\u3001\u7efc\u5408\u8bc4\u4f30\u6807\u51c6\u548c\u53c2\u8003\u5b9e\u73b0\u3002", "conclusion": "PRDBench\u4e3a\u4ee3\u7801\u667a\u80fd\u4f53\u548c\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u51c6\uff0c\u4e3a\u6807\u6ce8\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u6846\u67b6\u3002"}}
{"id": "2510.24141", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24141", "abs": "https://arxiv.org/abs/2510.24141", "authors": ["Miao Zhang", "Shenao Wang", "Guilin Zheng", "Yanjie Zhao", "Haoyu Wang"], "title": "Demystifying Cookie Sharing Risks in WebView-based Mobile App-in-app Ecosystems", "comment": "To appear in the 40th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE'25)", "summary": "Mini-programs, an emerging mobile application paradigm within super-apps,\noffer a seamless and installation-free experience. However, the adoption of the\nweb-view component has disrupted their isolation mechanisms, exposing new\nattack surfaces and vulnerabilities. In this paper, we introduce a novel\nvulnerability called Cross Mini-program Cookie Sharing (CMCS), which arises\nfrom the shared web-view environment across mini-programs. This vulnerability\nallows unauthorized data exchange across mini-programs by enabling one\nmini-program to access cookies set by another within the same web-view context,\nviolating isolation principles. As a preliminary step, we analyzed the web-view\nmechanisms of four major platforms, including WeChat, AliPay, TikTok, and\nBaidu, and found that all of them are affected by CMCS vulnerabilities.\nFurthermore, we demonstrate the collusion attack enabled by CMCS, where\nprivileged mini-programs exfiltrate sensitive user data via cookies accessible\nto unprivileged mini-programs. To measure the impact of collusion attacks\nenabled by CMCS vulnerabilities in the wild, we developed MiCoScan, a static\nanalysis tool that detects mini-programs affected by CMCS vulnerabilities.\nMiCoScan employs web-view context modeling to identify clusters of\nmini-programs sharing the same web-view domain and cross-webview data flow\nanalysis to detect sensitive data transmissions to/from web-views. Using\nMiCoScan, we conducted a large-scale analysis of 351,483 mini-programs,\nidentifying 45,448 clusters sharing web-view domains, 7,965 instances of\nprivileged data transmission, and 9,877 mini-programs vulnerable to collusion\nattacks. Our findings highlight the widespread prevalence and significant\nsecurity risks posed by CMCS vulnerabilities, underscoring the urgent need for\nimproved isolation mechanisms in mini-program ecosystems.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u4e86\u4e00\u79cd\u540d\u4e3a\u8de8\u5c0f\u7a0b\u5e8fCookie\u5171\u4eab(CMCS)\u7684\u65b0\u578b\u6f0f\u6d1e\uff0c\u8be5\u6f0f\u6d1e\u6e90\u4e8e\u5c0f\u7a0b\u5e8f\u4e2d\u5171\u4eab\u7684web-view\u73af\u5883\uff0c\u5141\u8bb8\u672a\u7ecf\u6388\u6743\u7684\u8de8\u5c0f\u7a0b\u5e8f\u6570\u636e\u4ea4\u6362\uff0c\u8fdd\u53cd\u4e86\u9694\u79bb\u539f\u5219\u3002", "motivation": "\u5c0f\u7a0b\u5e8f\u91c7\u7528web-view\u7ec4\u4ef6\u7834\u574f\u4e86\u539f\u6709\u7684\u9694\u79bb\u673a\u5236\uff0c\u66b4\u9732\u4e86\u65b0\u7684\u653b\u51fb\u9762\u548c\u6f0f\u6d1e\u3002\u4f5c\u8005\u65e8\u5728\u7814\u7a76\u8fd9\u79cd\u5171\u4eabweb-view\u73af\u5883\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u5206\u6790\u4e86\u56db\u5927\u5e73\u53f0(\u5fae\u4fe1\u3001\u652f\u4ed8\u5b9d\u3001\u6296\u97f3\u3001\u767e\u5ea6)\u7684web-view\u673a\u5236\uff0c\u5f00\u53d1\u4e86MiCoScan\u9759\u6001\u5206\u6790\u5de5\u5177\u8fdb\u884c\u5927\u89c4\u6a21\u68c0\u6d4b\uff0c\u8be5\u5de5\u5177\u91c7\u7528web-view\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u8de8web-view\u6570\u636e\u6d41\u5206\u6790\u3002", "result": "\u6240\u6709\u56db\u4e2a\u5e73\u53f0\u90fd\u53d7\u5230CMCS\u6f0f\u6d1e\u5f71\u54cd\u3002\u5728351,483\u4e2a\u5c0f\u7a0b\u5e8f\u7684\u5927\u89c4\u6a21\u5206\u6790\u4e2d\uff0c\u53d1\u73b045,448\u4e2a\u5171\u4eabweb-view\u57df\u7684\u96c6\u7fa4\uff0c7,965\u4e2a\u7279\u6743\u6570\u636e\u4f20\u8f93\u5b9e\u4f8b\uff0c9,877\u4e2a\u5c0f\u7a0b\u5e8f\u6613\u53d7\u5408\u8c0b\u653b\u51fb\u3002", "conclusion": "CMCS\u6f0f\u6d1e\u5728\u5c0f\u7a0b\u5e8f\u751f\u6001\u4e2d\u5e7f\u6cdb\u5b58\u5728\u4e14\u5e26\u6765\u91cd\u5927\u5b89\u5168\u98ce\u9669\uff0c\u8feb\u5207\u9700\u8981\u6539\u8fdb\u9694\u79bb\u673a\u5236\u3002"}}
{"id": "2510.23881", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23881", "abs": "https://arxiv.org/abs/2510.23881", "authors": ["Xidong Feng", "Vivek Veeriah", "Marcus Chiam", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Federico Barbero", "Johan Obando-Ceron", "Jiaxin Shi", "Satinder Singh", "Shaobo Hou", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Generating Creative Chess Puzzles", "comment": null, "summary": "While Generative AI rapidly advances in various domains, generating truly\ncreative, aesthetic, and counter-intuitive outputs remains a challenge. This\npaper presents an approach to tackle these difficulties in the domain of chess\npuzzles. We start by benchmarking Generative AI architectures, and then\nintroduce an RL framework with novel rewards based on chess engine search\nstatistics to overcome some of those shortcomings. The rewards are designed to\nenhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.\nOur RL approach dramatically increases counter-intuitive puzzle generation by\n10x, from 0.22\\% (supervised) to 2.5\\%, surpassing existing dataset rates\n(2.1\\%) and the best Lichess-trained model (0.4\\%). Our puzzles meet novelty\nand diversity benchmarks, retain aesthetic themes, and are rated by human\nexperts as more creative, enjoyable, and counter-intuitive than composed book\npuzzles, even approaching classic compositions. Our final outcome is a curated\nbooklet of these AI-generated puzzles, which is acknowledged for creativity by\nthree world-renowned experts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8c61\u68cb\u8c1c\u9898\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u65b0\u9896\u7684\u5956\u52b1\u673a\u5236\u6765\u63d0\u5347\u8c1c\u9898\u7684\u72ec\u7279\u6027\u3001\u53cd\u76f4\u89c9\u6027\u548c\u591a\u6837\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53cd\u76f4\u89c9\u8c1c\u9898\u7684\u751f\u6210\u7387\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0fAI\u5728\u5404\u4e2a\u9886\u57df\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u751f\u6210\u771f\u6b63\u5177\u6709\u521b\u9020\u6027\u3001\u7f8e\u5b66\u4ef7\u503c\u548c\u53cd\u76f4\u89c9\u6027\u7684\u8f93\u51fa\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8c61\u68cb\u8c1c\u9898\u9886\u57df\u3002", "method": "\u9996\u5148\u5bf9\u751f\u6210\u5f0fAI\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7136\u540e\u5f15\u5165\u57fa\u4e8e\u8c61\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u5956\u52b1\u673a\u5236\u6765\u589e\u5f3a\u8c1c\u9898\u7684\u72ec\u7279\u6027\u3001\u53cd\u76f4\u89c9\u6027\u3001\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u53cd\u76f4\u89c9\u8c1c\u9898\u751f\u6210\u7387\u4ece0.22%\uff08\u76d1\u7763\u5b66\u4e60\uff09\u5927\u5e45\u63d0\u5347\u81f32.5%\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u6570\u636e\u96c6\uff082.1%\uff09\u548c\u6700\u4f73Lichess\u8bad\u7ec3\u6a21\u578b\uff080.4%\uff09\u3002\u751f\u6210\u7684\u8c1c\u9898\u7b26\u5408\u65b0\u9896\u6027\u548c\u591a\u6837\u6027\u6807\u51c6\uff0c\u4fdd\u7559\u4e86\u7f8e\u5b66\u4e3b\u9898\uff0c\u5e76\u88ab\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4e3a\u66f4\u5177\u521b\u9020\u6027\u3001\u8da3\u5473\u6027\u548c\u53cd\u76f4\u89c9\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684AI\u8c61\u68cb\u8c1c\u9898\uff0c\u6700\u7ec8\u6210\u679c\u5f97\u5230\u4e86\u4e09\u4f4d\u4e16\u754c\u77e5\u540d\u4e13\u5bb6\u7684\u8ba4\u53ef\uff0c\u8bc1\u660e\u4e86AI\u5728\u521b\u9020\u6027\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24367", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24367", "abs": "https://arxiv.org/abs/2510.24367", "authors": ["Junda He", "Jieke Shi", "Terry Yue Zhuo", "Christoph Treude", "Jiamou Sun", "Zhenchang Xing", "Xiaoning Du", "David Lo"], "title": "LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and the Road Ahead", "comment": null, "summary": "The rapid integration of Large Language Models (LLMs) into software\nengineering (SE) has revolutionized tasks like code generation, producing a\nmassive volume of software artifacts. This surge has exposed a critical\nbottleneck: the lack of scalable, reliable methods to evaluate these outputs.\nHuman evaluation is costly and time-consuming, while traditional automated\nmetrics like BLEU fail to capture nuanced quality aspects. In response, the\nLLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.\nThis approach leverages the advanced reasoning of LLMs, offering a path toward\nhuman-like nuance at automated scale. However, LLM-as-a-Judge research in SE is\nstill in its early stages. This forward-looking SE 2030 paper aims to steer the\ncommunity toward advancing LLM-as-a-Judge for evaluating LLM-generated software\nartifacts. We provide a literature review of existing SE studies, analyze their\nlimitations, identify key research gaps, and outline a detailed roadmap. We\nenvision these frameworks as reliable, robust, and scalable human surrogates\ncapable of consistent, multi-faceted artifact evaluation by 2030. Our work aims\nto foster research and adoption of LLM-as-a-Judge frameworks, ultimately\nimproving the scalability of software artifact evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLLM-as-a-Judge\u8303\u5f0f\uff0c\u65e8\u5728\u89e3\u51b3LLM\u751f\u6210\u8f6f\u4ef6\u5236\u54c1\u8bc4\u4f30\u7684\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u3001\u5c40\u9650\u6027\u5206\u6790\u548c\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u63a8\u52a8\u8be5\u8303\u5f0f\u6210\u4e3a\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u4eba\u7c7b\u8bc4\u4f30\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5feb\u901f\u96c6\u6210\u4ea7\u751f\u4e86\u5927\u91cf\u8f6f\u4ef6\u5236\u54c1\uff0c\u4f46\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u53ef\u9760\u8bc4\u4f30\u65b9\u6cd5\u3002\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\uff0c\u4f20\u7edf\u81ea\u52a8\u5316\u6307\u6807\u65e0\u6cd5\u6355\u6349\u8d28\u91cf\u7ec6\u8282\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u8303\u5f0f\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\u5206\u6790\u73b0\u6709SE\u7814\u7a76\uff0c\u8bc6\u522b\u5c40\u9650\u6027\uff0c\u627e\u51fa\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u5236\u5b9a\u8be6\u7ec6\u7684\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "result": "LLM-as-a-Judge\u7814\u7a76\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u4f46\u5177\u6709\u901a\u8fc7LLM\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u5b9e\u73b0\u7c7b\u4eba\u8bc4\u4f30\u7684\u6f5c\u529b\u3002", "conclusion": "\u52302030\u5e74\uff0cLLM-as-a-Judge\u6846\u67b6\u6709\u671b\u6210\u4e3a\u53ef\u9760\u3001\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u7684\u4eba\u7c7b\u8bc4\u4f30\u66ff\u4ee3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e00\u81f4\u7684\u591a\u7ef4\u5ea6\u5236\u54c1\u8bc4\u4f30\uff0c\u63d0\u5347\u8f6f\u4ef6\u5236\u54c1\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.24317", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24317", "abs": "https://arxiv.org/abs/2510.24317", "authors": ["Mar\u00eda Sanz-G\u00f3mez", "V\u00edctor Mayoral-Vilches", "Francesco Balassone", "Luis Javier Navarrete-Lozano", "Crist\u00f3bal R. J. Veas Chavez", "Maite del Mundo de Torres"], "title": "Cybersecurity AI Benchmark (CAIBench): A Meta-Benchmark for Evaluating Cybersecurity AI Agents", "comment": null, "summary": "Cybersecurity spans multiple interconnected domains, complicating the\ndevelopment of meaningful, labor-relevant benchmarks. Existing benchmarks\nassess isolated skills rather than integrated performance. We find that\npre-trained knowledge of cybersecurity in LLMs does not imply attack and\ndefense abilities, revealing a gap between knowledge and capability. To address\nthis limitation, we present the Cybersecurity AI Benchmark (CAIBench), a\nmodular meta-benchmark framework that allows evaluating LLM models and agents\nacross offensive and defensive cybersecurity domains, taking a step towards\nmeaningfully measuring their labor-relevance. CAIBench integrates five\nevaluation categories, covering over 10,000 instances: Jeopardy-style CTFs,\nAttack and Defense CTFs, Cyber Range exercises, knowledge benchmarks, and\nprivacy assessments. Key novel contributions include systematic simultaneous\noffensive-defensive evaluation, robotics-focused cybersecurity challenges\n(RCTF2), and privacy-preserving performance assessment (CyberPII-Bench).\nEvaluation of state-of-the-art AI models reveals saturation on security\nknowledge metrics (~70\\% success) but substantial degradation in multi-step\nadversarial (A\\&D) scenarios (20-40\\% success), or worse in robotic targets\n(22\\% success). The combination of framework scaffolding and LLM model choice\nsignificantly impacts performance; we find that proper matches improve up to\n2.6$\\times$ variance in Attack and Defense CTFs. These results demonstrate a\npronounced gap between conceptual knowledge and adaptive capability,\nemphasizing the need for a meta-benchmark.", "AI": {"tldr": "CAIBench\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u7f51\u7edc\u5b89\u5168AI\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u653b\u9632\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u7efc\u5408\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u77e5\u8bc6\u638c\u63e1\u4e0e\u5b9e\u9645\u80fd\u529b\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u53ea\u8bc4\u4f30\u5b64\u7acb\u6280\u80fd\u800c\u975e\u7efc\u5408\u8868\u73b0\uff0c\u9884\u8bad\u7ec3LLM\u7684\u7f51\u7edc\u5b89\u5168\u77e5\u8bc6\u5e76\u4e0d\u4ee3\u8868\u5176\u653b\u9632\u80fd\u529b\uff0c\u5b58\u5728\u77e5\u8bc6\u4e0e\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5f00\u53d1\u4e86CAIBench\u6a21\u5757\u5316\u5143\u57fa\u51c6\u6846\u67b6\uff0c\u6574\u5408\u4e94\u4e2a\u8bc4\u4f30\u7c7b\u522b\uff1aJeopardy\u5f0fCTF\u3001\u653b\u9632CTF\u3001\u7f51\u7edc\u9776\u573a\u7ec3\u4e60\u3001\u77e5\u8bc6\u57fa\u51c6\u548c\u9690\u79c1\u8bc4\u4f30\uff0c\u8986\u76d610,000\u591a\u4e2a\u5b9e\u4f8b\u3002", "result": "\u6700\u5148\u8fdbAI\u6a21\u578b\u5728\u5b89\u5168\u77e5\u8bc6\u6307\u6807\u4e0a\u8fbe\u5230\u9971\u548c\uff08\u7ea670%\u6210\u529f\u7387\uff09\uff0c\u4f46\u5728\u591a\u6b65\u5bf9\u6297\u573a\u666f\u4e2d\u8868\u73b0\u5927\u5e45\u4e0b\u964d\uff0820-40%\u6210\u529f\u7387\uff09\uff0c\u5728\u673a\u5668\u4eba\u76ee\u6807\u4e0a\u66f4\u5dee\uff0822%\u6210\u529f\u7387\uff09\u3002\u6846\u67b6\u642d\u5efa\u4e0e\u6a21\u578b\u9009\u62e9\u7684\u5339\u914d\u53ef\u63d0\u5347\u653b\u9632CTF\u6027\u80fd\u8fbe2.6\u500d\u3002", "conclusion": "\u6982\u5ff5\u77e5\u8bc6\u4e0e\u9002\u5e94\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u5143\u57fa\u51c6\u6d4b\u8bd5\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.23882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23882", "abs": "https://arxiv.org/abs/2510.23882", "authors": ["Adil Rasheed", "Oscar Ravik", "Omer San"], "title": "Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins", "comment": null, "summary": "This work investigates the use of digital twins for dynamical system modeling\nand control, integrating physics-based, data-driven, and hybrid approaches with\nboth traditional and AI-driven controllers. Using a miniature greenhouse as a\ntest platform, four predictive models Linear, Physics-Based Modeling (PBM),\nLong Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are\ndeveloped and compared under interpolation and extrapolation scenarios. Three\ncontrol strategies Model Predictive Control (MPC), Reinforcement Learning (RL),\nand Large Language Model (LLM) based control are also implemented to assess\ntrade-offs in precision, adaptability, and implementation effort. Results show\nthat in modeling HAM provides the most balanced performance across accuracy,\ngeneralization, and computational efficiency, while LSTM achieves high\nprecision at greater resource cost. Among controllers, MPC delivers robust and\npredictable performance, RL demonstrates strong adaptability, and LLM-based\ncontrollers offer flexible human-AI interaction when coupled with predictive\ntools.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff08\u7ebf\u6027\u3001\u7269\u7406\u5efa\u6a21\u3001LSTM\u3001\u6df7\u5408\u5efa\u6a21\uff09\u548c\u4e09\u79cd\u63a7\u5236\u7b56\u7565\uff08MPC\u3001RL\u3001LLM\u63a7\u5236\uff09\u5728\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0HAM\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u5747\u8861\uff0c\u800cMPC\u63a7\u5236\u5668\u7a33\u5065\uff0cRL\u9002\u5e94\u6027\u5f3a\uff0cLLM\u63a7\u5236\u5668\u652f\u6301\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u7814\u7a76\u6570\u5b57\u5b6a\u751f\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u548c\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u6574\u5408\u7269\u7406\u57fa\u7840\u3001\u6570\u636e\u9a71\u52a8\u548c\u6df7\u5408\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4f20\u7edf\u4e0eAI\u9a71\u52a8\u63a7\u5236\u5668\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u4ee5\u5fae\u578b\u6e29\u5ba4\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5f00\u53d1\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff08\u7ebf\u6027\u3001PBM\u3001LSTM\u3001HAM\uff09\u548c\u4e09\u79cd\u63a7\u5236\u7b56\u7565\uff08MPC\u3001RL\u3001LLM\u63a7\u5236\uff09\uff0c\u5728\u63d2\u503c\u548c\u5916\u63a8\u573a\u666f\u4e0b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "HAM\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u5747\u8861\uff1bLSTM\u7cbe\u5ea6\u9ad8\u4f46\u8d44\u6e90\u6d88\u8017\u5927\uff1bMPC\u63a7\u5236\u5668\u7a33\u5065\u53ef\u9884\u6d4b\uff1bRL\u9002\u5e94\u6027\u5f3a\uff1bLLM\u63a7\u5236\u5668\u7ed3\u5408\u9884\u6d4b\u5de5\u5177\u53ef\u5b9e\u73b0\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "conclusion": "HAM\u6a21\u578b\u5728\u5efa\u6a21\u4e2d\u63d0\u4f9b\u6700\u4f73\u5e73\u8861\u6027\u80fd\uff0cMPC\u63a7\u5236\u5668\u7a33\u5065\uff0cRL\u9002\u5e94\u6027\u5f3a\uff0cLLM\u63a7\u5236\u5668\u652f\u6301\u4eba\u673a\u4ea4\u4e92\uff0c\u4e3a\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u7684\u5efa\u6a21\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2510.24428", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24428", "abs": "https://arxiv.org/abs/2510.24428", "authors": ["Nguyen Hoang Anh", "Minh Le-Anh", "Bach Le", "Nghi D. Q. Bui"], "title": "CodeWiki: Automated Repository-Level Documentation at Scale", "comment": null, "summary": "Developers spend nearly 58% of their time understanding codebases, yet\nmaintaining comprehensive documentation remains challenging due to complexity\nand manual effort. While recent Large Language Models (LLMs) show promise for\nfunction-level documentation, they fail at the repository level, where\ncapturing architectural patterns and cross-module interactions is essential. We\nintroduce CodeWiki, the first open-source framework for holistic\nrepository-level documentation across seven programming languages. CodeWiki\nemploys three innovations: (i) hierarchical decomposition that preserves\narchitectural context, (ii) recursive agentic processing with dynamic\ndelegation, and (iii) synthesis of textual and visual artifacts including\narchitecture diagrams and data flows. We also present CodeWikiBench, the first\nrepository-level documentation benchmark with multi-level rubrics and agentic\nassessment. CodeWiki achieves 68.79% quality score with proprietary models and\n64.80% with open-source alternatives, outperforming existing closed-source\nsystems and demonstrating scalable, accurate documentation for real-world\nrepositories.", "AI": {"tldr": "CodeWiki\u662f\u9996\u4e2a\u5f00\u6e90\u7684\u5168\u4ed3\u5e93\u7ea7\u4ee3\u7801\u6587\u6863\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u6b21\u5206\u89e3\u3001\u9012\u5f52\u4ee3\u7406\u5904\u7406\u548c\u6587\u672c\u89c6\u89c9\u5408\u6210\uff0c\u57287\u79cd\u7f16\u7a0b\u8bed\u8a00\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4ed3\u5e93\u7ea7\u6587\u6863\u751f\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u4eba\u545858%\u65f6\u95f4\u7528\u4e8e\u7406\u89e3\u4ee3\u7801\u5e93\uff0c\u4f46\u73b0\u6709LLM\u53ea\u80fd\u5728\u51fd\u6570\u7ea7\u522b\u751f\u6210\u6587\u6863\uff0c\u65e0\u6cd5\u5904\u7406\u4ed3\u5e93\u7ea7\u522b\u7684\u67b6\u6784\u6a21\u5f0f\u548c\u8de8\u6a21\u5757\u4ea4\u4e92\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u6587\u6863\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e09\u4e2a\u521b\u65b0\uff1a\u5c42\u6b21\u5206\u89e3\u4fdd\u6301\u67b6\u6784\u4e0a\u4e0b\u6587\u3001\u9012\u5f52\u4ee3\u7406\u5904\u7406\u4e0e\u52a8\u6001\u59d4\u6258\u3001\u6587\u672c\u548c\u89c6\u89c9\u5de5\u4ef6\u7684\u5408\u6210\uff08\u5305\u62ec\u67b6\u6784\u56fe\u548c\u6570\u636e\u6d41\uff09\u3002", "result": "\u5728CodeWikiBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCodeWiki\u4f7f\u7528\u4e13\u6709\u6a21\u578b\u83b7\u5f9768.79%\u8d28\u91cf\u5206\u6570\uff0c\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u83b7\u5f9764.80%\uff0c\u4f18\u4e8e\u73b0\u6709\u95ed\u6e90\u7cfb\u7edf\u3002", "conclusion": "CodeWiki\u5c55\u793a\u4e86\u4e3a\u771f\u5b9e\u4e16\u754c\u4ed3\u5e93\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u51c6\u786e\u6587\u6863\u7684\u80fd\u529b\uff0c\u662f\u9996\u4e2a\u6709\u6548\u7684\u4ed3\u5e93\u7ea7\u6587\u6863\u751f\u6210\u6846\u67b6\u3002"}}
{"id": "2510.24393", "categories": ["cs.CR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24393", "abs": "https://arxiv.org/abs/2510.24393", "authors": ["Yan Meng", "Jiachun Li", "Matthew Pillari", "Arjun Deopujari", "Liam Brennan", "Hafsah Shamsie", "Haojin Zhu", "Yuan Tian"], "title": "Your Microphone Array Retains Your Identity: A Robust Voice Liveness Detection System for Smart Speakers", "comment": "This is a paper accepted by USENIX Security 2022. See:\n  https://www.usenix.org/conference/usenixsecurity22/presentation/meng", "summary": "Though playing an essential role in smart home systems, smart speakers are\nvulnerable to voice spoofing attacks. Passive liveness detection, which\nutilizes only the collected audio rather than the deployed sensors to\ndistinguish between live-human and replayed voices, has drawn increasing\nattention. However, it faces the challenge of performance degradation under the\ndifferent environmental factors as well as the strict requirement of the fixed\nuser gestures.\n  In this study, we propose a novel liveness feature, array fingerprint, which\nutilizes the microphone array inherently adopted by the smart speaker to\ndetermine the identity of collected audios. Our theoretical analysis\ndemonstrates that by leveraging the circular layout of microphones, compared\nwith existing schemes, array fingerprint achieves a more robust performance\nunder the environmental change and user's movement. Then, to leverage such a\nfingerprint, we propose ARRAYID, a lightweight passive detection scheme, and\nelaborate a series of features working together with array fingerprint. Our\nevaluation on the dataset containing 32,780 audio samples and 14 spoofing\ndevices shows that ARRAYID achieves an accuracy of 99.84%, which is superior to\nexisting passive liveness detection schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9635\u5217\u6307\u7eb9\u7684\u65b0\u578b\u6d3b\u4f53\u68c0\u6d4b\u7279\u5f81\uff0c\u5229\u7528\u667a\u80fd\u97f3\u7bb1\u5185\u7f6e\u7684\u9ea6\u514b\u98ce\u9635\u5217\u6765\u533a\u5206\u771f\u4eba\u8bed\u97f3\u548c\u91cd\u653e\u8bed\u97f3\u653b\u51fb\uff0c\u5e76\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u65b9\u6848ARRAYID\uff0c\u5728\u5305\u542b32,780\u4e2a\u97f3\u9891\u6837\u672c\u7684\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.84%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u667a\u80fd\u97f3\u7bb1\u867d\u7136\u5728\u5bb6\u5c45\u7cfb\u7edf\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u8bed\u97f3\u6b3a\u9a97\u653b\u51fb\u3002\u73b0\u6709\u7684\u88ab\u52a8\u6d3b\u4f53\u68c0\u6d4b\u65b9\u6cd5\u9762\u4e34\u73af\u5883\u56e0\u7d20\u53d8\u5316\u548c\u7528\u6237\u59ff\u52bf\u56fa\u5b9a\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u667a\u80fd\u97f3\u7bb1\u56fa\u6709\u7684\u9ea6\u514b\u98ce\u9635\u5217\uff0c\u63d0\u51fa\u9635\u5217\u6307\u7eb9\u7279\u5f81\uff0c\u901a\u8fc7\u5206\u6790\u9ea6\u514b\u98ce\u7684\u5706\u5f62\u5e03\u5c40\u6765\u8bc6\u522b\u97f3\u9891\u6765\u6e90\u3002\u5f00\u53d1\u4e86ARRAYID\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u65b9\u6848\uff0c\u7ed3\u5408\u4e00\u7cfb\u5217\u4e0e\u9635\u5217\u6307\u7eb9\u534f\u540c\u5de5\u4f5c\u7684\u7279\u5f81\u3002", "result": "\u5728\u5305\u542b32,780\u4e2a\u97f3\u9891\u6837\u672c\u548c14\u79cd\u6b3a\u9a97\u8bbe\u5907\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cARRAYID\u8fbe\u5230\u4e8699.84%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u88ab\u52a8\u6d3b\u4f53\u68c0\u6d4b\u65b9\u6848\u3002", "conclusion": "\u9635\u5217\u6307\u7eb9\u7279\u5f81\u548cARRAYID\u65b9\u6848\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u73af\u5883\u53d8\u5316\u548c\u7528\u6237\u79fb\u52a8\uff0c\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u8bed\u97f3\u6d3b\u4f53\u68c0\u6d4b\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u97f3\u7bb1\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2510.23883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23883", "abs": "https://arxiv.org/abs/2510.23883", "authors": ["Shrestha Datta", "Shahriar Kabir Nahin", "Anshuman Chhabra", "Prasant Mohapatra"], "title": "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges", "comment": null, "summary": "Agentic AI systems powered by large language models (LLMs) and endowed with\nplanning, tool use, memory, and autonomy, are emerging as powerful, flexible\nplatforms for automation. Their ability to autonomously execute tasks across\nweb, software, and physical environments creates new and amplified security\nrisks, distinct from both traditional AI safety and conventional software\nsecurity. This survey outlines a taxonomy of threats specific to agentic AI,\nreviews recent benchmarks and evaluation methodologies, and discusses defense\nstrategies from both technical and governance perspectives. We synthesize\ncurrent research and highlight open challenges, aiming to support the\ndevelopment of secure-by-design agent systems.", "AI": {"tldr": "\u8be5\u8c03\u67e5\u8bba\u6587\u5206\u6790\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5e26\u6765\u7684\u65b0\u578b\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bc4\u4f30\u65b9\u6cd5\u548c\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5177\u6709\u81ea\u4e3b\u6267\u884c\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5728Web\u3001\u8f6f\u4ef6\u548c\u7269\u7406\u73af\u5883\u4e2d\u521b\u9020\u4e86\u4e0e\u4f20\u7edfAI\u5b89\u5168\u548c\u8f6f\u4ef6\u5b89\u5168\u4e0d\u540c\u7684\u65b0\u578b\u653e\u5927\u5b89\u5168\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u56de\u987e\u73b0\u6709\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ece\u6280\u672f\u548c\u6cbb\u7406\u89d2\u5ea6\u5206\u6790\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u4e86\u667a\u80fd\u4f53AI\u7279\u6709\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u5e76\u7efc\u5408\u4e86\u5f53\u524d\u7814\u7a76\u6210\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u652f\u6301\u5f00\u53d1\u5b89\u5168\u8bbe\u8ba1\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5e76\u5f3a\u8c03\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2510.24483", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24483", "abs": "https://arxiv.org/abs/2510.24483", "authors": ["Michele Lanza"], "title": "The Divine Software Engineering Comedy -- Inferno: The Okinawa Files", "comment": null, "summary": "In June 2024 I co-organized the FUture of Software Engineering symposium in\nOkinawa, Japan. Me, Andrian Marcus, Takashi Kobayashi and Shinpei Hayashi were\ngeneral chairs, Nicole Novielli, Kevin Moran, Yutaro Kashiwa and Masanari Kondo\nwere program chairs, some members of my group, Carmen Armenti, Stefano\nCampanella, Roberto Minelli, were the tables, can't have a room with only\nchairs, after all. We invited a crowd of people to discuss what future software\nengineering has. FUSE became a 3-day marathon on whether there is actually a\nfuture at all for SE. This essay is a slightly dark take about what I saw at\nthat event, very loosely based on the discussions that took place, adding some\nhealthy sarcasm and cynicism, the intellectual salt and pepper I never seem to\nrun out of. I listened to the brilliant people who gathered to talk about where\nwe're headed, and distilled three nightmares headed in our direction: software\nmakers who don't know what they're doing, but get the job done anyway, a field\nmoving so fast it can't remember its own lessons, and technologies multiplying\nlike rabbits in Spring. So, let's start. The future, eh? The future of software\nengineering looks like a car crash in slow motion: you can see it coming but\nyou can't look away. The thing is...", "AI": {"tldr": "\u4f5c\u8005\u57fa\u4e8e2024\u5e746\u6708\u5728\u65e5\u672c\u51b2\u7ef3\u4e3e\u529e\u7684\u8f6f\u4ef6\u5de5\u7a0b\u672a\u6765\u7814\u8ba8\u4f1a\uff0c\u63d0\u51fa\u4e86\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u672a\u6765\u53d1\u5c55\u7684\u4e09\u4e2a\u5669\u68a6\uff1a\u8f6f\u4ef6\u5f00\u53d1\u8005\u4e0d\u77e5\u9053\u81ea\u5df1\u5728\u505a\u4ec0\u4e48\u4f46\u4f9d\u7136\u5b8c\u6210\u4efb\u52a1\u3001\u9886\u57df\u53d1\u5c55\u592a\u5feb\u65e0\u6cd5\u8bb0\u4f4f\u6559\u8bad\u3001\u6280\u672f\u50cf\u6625\u5929\u5154\u5b50\u822c\u5feb\u901f\u7e41\u6b96\u3002", "motivation": "\u4f5c\u8005\u53c2\u52a0FUSE\u7814\u8ba8\u4f1a\u540e\uff0c\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u611f\u5230\u62c5\u5fe7\uff0c\u5e0c\u671b\u901a\u8fc7\u8fd9\u7bc7\u6587\u7ae0\u8868\u8fbe\u5bf9\u5f53\u524d\u8d8b\u52bf\u7684\u6279\u5224\u6027\u89c2\u5bdf\u548c\u8bbd\u523a\u6027\u601d\u8003\u3002", "method": "\u57fa\u4e8e\u7814\u8ba8\u4f1a\u8ba8\u8bba\u5185\u5bb9\uff0c\u7ed3\u5408\u4e2a\u4eba\u89c2\u5bdf\u548c\u8bbd\u523a\u6027\u5206\u6790\uff0c\u63d0\u70bc\u51fa\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\u4f5c\u4e3a\u5bf9\u672a\u6765\u8f6f\u4ef6\u5de5\u7a0b\u7684\u8b66\u793a\u3002", "result": "\u8bc6\u522b\u51fa\u8f6f\u4ef6\u5de5\u7a0b\u9762\u4e34\u7684\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u5f00\u53d1\u8005\u80fd\u529b\u4e0e\u8d23\u4efb\u8131\u8282\u3001\u9886\u57df\u77e5\u8bc6\u4f20\u627f\u65ad\u88c2\u3001\u6280\u672f\u7206\u70b8\u5f0f\u589e\u957f\u5e26\u6765\u7684\u6df7\u4e71\u3002", "conclusion": "\u4f5c\u8005\u8ba4\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u7684\u672a\u6765\u5c31\u50cf\u6162\u52a8\u4f5c\u7684\u8f66\u7978\uff0c\u867d\u7136\u80fd\u770b\u5230\u95ee\u9898\u6765\u4e34\u5374\u65e0\u6cd5\u963b\u6b62\uff0c\u8868\u8fbe\u4e86\u5bf9\u8be5\u9886\u57df\u53d1\u5c55\u65b9\u5411\u7684\u60b2\u89c2\u770b\u6cd5\u3002"}}
{"id": "2510.24408", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.24408", "abs": "https://arxiv.org/abs/2510.24408", "authors": ["Yifan Wu", "Xuewei Feng", "Yuxiang Yang", "Ke Xu"], "title": "Uncovering Gaps Between RFC Updates and TCP/IP Implementations: LLM-Facilitated Differential Checks on Intermediate Representations", "comment": "15 pages, 7 figures", "summary": "As the core of the Internet infrastructure, the TCP/IP protocol stack\nundertakes the task of network data transmission. However, due to the\ncomplexity of the protocol and the uncertainty of cross-layer interaction,\nthere are often inconsistencies between the implementation of the protocol\nstack code and the RFC standard. This inconsistency may not only lead to\ndifferences in protocol functions but also cause serious security\nvulnerabilities. At present, with the continuous expansion of protocol stack\nfunctions and the rapid iteration of RFC documents, it is increasingly\nimportant to detect and fix these inconsistencies. With the rise of large\nlanguage models, researchers have begun to explore how to extract protocol\nspecifications from RFC documents through these models, including protocol\nstack modeling, state machine extraction, text ambiguity analysis, and other\nrelated content. However, existing methods rely on predefined patterns or\nrule-based approaches that fail to generalize across different protocol\nspecifications. Automated and scalable detection of these inconsistencies\nremains a significant challenge. In this study, we propose an automated\nanalysis framework based on LLM and differential models. By modeling the\niterative relationship of the protocol and based on the iterative update\nrelationship of the RFC standard, we perform incremental code function analysis\non different versions of kernel code implementations to automatically perform\ncode detection and vulnerability analysis. We conduct extensive evaluations to\nvalidate the effectiveness of our framework, demonstrating its effectiveness in\nidentifying potential vulnerabilities caused by RFC code inconsistencies.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u548c\u5dee\u5206\u6a21\u578b\u7684\u81ea\u52a8\u5316\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bTCP/IP\u534f\u8bae\u6808\u5b9e\u73b0\u4e0eRFC\u6807\u51c6\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u8bc6\u522b\u6f5c\u5728\u5b89\u5168\u6f0f\u6d1e", "motivation": "\u534f\u8bae\u6808\u4ee3\u7801\u5b9e\u73b0\u4e0eRFC\u6807\u51c6\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u53ef\u80fd\u5bfc\u81f4\u529f\u80fd\u5dee\u5f02\u548c\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u8de8\u534f\u8bae\u89c4\u8303\u6cdb\u5316\uff0c\u81ea\u52a8\u5316\u68c0\u6d4b\u4ecd\u5177\u6311\u6218", "method": "\u57fa\u4e8eLLM\u548c\u5dee\u5206\u6a21\u578b\u7684\u81ea\u52a8\u5316\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u534f\u8bae\u8fed\u4ee3\u5173\u7cfb\u548cRFC\u6807\u51c6\u66f4\u65b0\u5173\u7cfb\uff0c\u5bf9\u4e0d\u540c\u7248\u672c\u5185\u6838\u4ee3\u7801\u5b9e\u73b0\u8fdb\u884c\u589e\u91cf\u4ee3\u7801\u529f\u80fd\u5206\u6790", "result": "\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u5728\u8bc6\u522b\u7531RFC\u4ee3\u7801\u4e0d\u4e00\u81f4\u5f15\u8d77\u7684\u6f5c\u5728\u6f0f\u6d1e\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u5206\u6790\u6846\u67b6\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u534f\u8bae\u6808\u5b9e\u73b0\u4e0eRFC\u6807\u51c6\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u8bc6\u522b\u6f5c\u5728\u5b89\u5168\u6f0f\u6d1e"}}
{"id": "2510.23925", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23925", "abs": "https://arxiv.org/abs/2510.23925", "authors": ["Guohao Sun", "Hang Hua", "Jian Wang", "Jiebo Luo", "Sohail Dianat", "Majid Rabbani", "Raghuveer Rao", "Zhiqiang Tao"], "title": "Latent Chain-of-Thought for Visual Reasoning", "comment": "NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u644a\u9500\u53d8\u5206\u63a8\u7406\u7684\u53ef\u6269\u5c55\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u6837\u6027\u5bfb\u6c42\u5f3a\u5316\u5b66\u4e60\u6539\u8fdbLVLMs\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u80fd\u529b\uff0c\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u6548\u679c\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u8bad\u7ec3\u7b97\u6cd5\uff08SFT\u3001PPO\u3001GRPO\uff09\u5728\u672a\u89c1\u63a8\u7406\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u6709\u504f\u89c1\u7684\u5956\u52b1\u6a21\u578b\uff0c\u9650\u5236\u4e86\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06LVLMs\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u540e\u9a8c\u63a8\u7406\uff0c\u91c7\u7528\u644a\u9500\u53d8\u5206\u63a8\u7406\u548c\u591a\u6837\u6027\u5bfb\u6c42\u5f3a\u5316\u5b66\u4e60\uff0c\u5f15\u5165\u7a00\u758f\u5956\u52b1\u51fd\u6570\u8fdb\u884ctoken\u7ea7\u5b66\u4e60\uff0c\u5e76\u5b9e\u65bd\u8d1d\u53f6\u65af\u63a8\u7406\u7f29\u653e\u7b56\u7565\u3002", "result": "\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u6700\u5148\u8fdbLVLMs\u7684\u6548\u679c\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u57fa\u4e8e\u644a\u9500\u53d8\u5206\u63a8\u7406\u548c\u591a\u6837\u6027\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u7b97\u6cd5\u80fd\u6709\u6548\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347LVLMs\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.24459", "categories": ["cs.AI", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24459", "abs": "https://arxiv.org/abs/2510.24459", "authors": ["Habtom Kahsay Gidey", "Niklas Huber", "Alexander Lenz", "Alois Knoll"], "title": "Affordance Representation and Recognition for Autonomous Agents", "comment": null, "summary": "The autonomy of software agents is fundamentally dependent on their ability\nto construct an actionable internal world model from the structured data that\ndefines their digital environment, such as the Document Object Model (DOM) of\nweb pages and the semantic descriptions of web services. However, constructing\nthis world model from raw structured data presents two critical challenges: the\nverbosity of raw HTML makes it computationally intractable for direct use by\nfoundation models, while the static nature of hardcoded API integrations\nprevents agents from adapting to evolving services.\n  This paper introduces a pattern language for world modeling from structured\ndata, presenting two complementary architectural patterns. The DOM Transduction\nPattern addresses the challenge of web page complexity by distilling} a\nverbose, raw DOM into a compact, task-relevant representation or world model\noptimized for an agent's reasoning core. Concurrently, the Hypermedia\nAffordances Recognition Pattern enables the agent to dynamically enrich its\nworld model by parsing standardized semantic descriptions to discover and\nintegrate the capabilities of unknown web services at runtime. Together, these\npatterns provide a robust framework for engineering agents that can efficiently\nconstruct and maintain an accurate world model, enabling scalable, adaptive,\nand interoperable automation across the web and its extended resources.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u67b6\u6784\u6a21\u5f0f\uff1aDOM\u8f6c\u6362\u6a21\u5f0f\u7528\u4e8e\u7b80\u5316\u7f51\u9875DOM\u4e3a\u7d27\u51d1\u7684\u4efb\u52a1\u76f8\u5173\u8868\u793a\uff0c\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\u7528\u4e8e\u52a8\u6001\u53d1\u73b0\u548c\u96c6\u6210\u672a\u77e5Web\u670d\u52a1\u7684\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u8f6f\u4ef6\u4ee3\u7406\u4ece\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u53ef\u64cd\u4f5c\u5185\u90e8\u4e16\u754c\u6a21\u578b\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u539f\u59cbHTML\u7684\u5197\u957f\u6027\u4f7f\u5176\u96be\u4ee5\u88ab\u57fa\u7840\u6a21\u578b\u76f4\u63a5\u4f7f\u7528\uff0c\u4ee5\u53ca\u786c\u7f16\u7801API\u96c6\u6210\u7684\u9759\u6001\u6027\u963b\u788d\u4ee3\u7406\u9002\u5e94\u4e0d\u65ad\u6f14\u5316\u7684\u670d\u52a1\u3002", "method": "\u5f15\u5165\u4e16\u754c\u5efa\u6a21\u7684\u6a21\u5f0f\u8bed\u8a00\uff0c\u5305\u542bDOM\u8f6c\u6362\u6a21\u5f0f\u548c\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\u3002DOM\u8f6c\u6362\u6a21\u5f0f\u5c06\u5197\u957f\u7684\u539f\u59cbDOM\u63d0\u70bc\u4e3a\u7d27\u51d1\u7684\u4efb\u52a1\u76f8\u5173\u8868\u793a\uff1b\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\u901a\u8fc7\u89e3\u6790\u6807\u51c6\u5316\u8bed\u4e49\u63cf\u8ff0\u6765\u52a8\u6001\u53d1\u73b0\u548c\u96c6\u6210Web\u670d\u52a1\u80fd\u529b\u3002", "result": "\u8fd9\u4e24\u79cd\u6a21\u5f0f\u5171\u540c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u6846\u67b6\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u9ad8\u6548\u6784\u5efa\u548c\u7ef4\u62a4\u51c6\u786e\u7684\u4e16\u754c\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u5f0f\u8bed\u8a00\u4e3a\u5de5\u7a0b\u5316\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u53ef\u4e92\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5728Web\u53ca\u5176\u6269\u5c55\u8d44\u6e90\u4e0a\u6709\u6548\u8fd0\u4f5c\u3002"}}
{"id": "2510.24422", "categories": ["cs.CR", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24422", "abs": "https://arxiv.org/abs/2510.24422", "authors": ["Bijeet Basak", "Nupur Patil", "Kurian Polachan", "Srinivas Vivek"], "title": "Attack on a PUF-based Secure Binary Neural Network", "comment": "Accepted at VLSID 2026. To be published in IEEE Xplore", "summary": "Binarized Neural Networks (BNNs) deployed on memristive crossbar arrays\nprovide energy-efficient solutions for edge computing but are susceptible to\nphysical attacks due to memristor nonvolatility. Recently, Rajendran et al.\n(IEEE Embedded Systems Letter 2025) proposed a Physical Unclonable Function\n(PUF)-based scheme to secure BNNs against theft attacks. Specifically, the\nweight and bias matrices of the BNN layers were secured by swapping columns\nbased on device's PUF key bits.\n  In this paper, we demonstrate that this scheme to secure BNNs is vulnerable\nto PUF-key recovery attack. As a consequence of our attack, we recover the\nsecret weight and bias matrices of the BNN. Our approach is motivated by\ndifferential cryptanalysis and reconstructs the PUF key bit-by-bit by observing\nthe change in model accuracy, and eventually recovering the BNN model\nparameters. Evaluated on a BNN trained on the MNIST dataset, our attack could\nrecover 85% of the PUF key, and recover the BNN model up to 93% classification\naccuracy compared to the original model's 96% accuracy. Our attack is very\nefficient and it takes a couple of minutes to recovery the PUF key and the\nmodel parameters.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u5bf9\u57fa\u4e8ePUF\u4fdd\u62a4\u7684BNN\u6a21\u578b\u7684\u5bc6\u94a5\u6062\u590d\u653b\u51fb\uff0c\u901a\u8fc7\u5dee\u5206\u5bc6\u7801\u5206\u6790\u65b9\u6cd5\u80fd\u591f\u6062\u590d85%\u7684PUF\u5bc6\u94a5\uff0c\u5e76\u91cd\u5efa\u51fa\u5177\u670993%\u51c6\u786e\u7387\u7684\u539f\u59cbBNN\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u5fc6\u963b\u5668\u7684\u975e\u6613\u5931\u6027\u7279\u6027\uff0c\u90e8\u7f72\u5728\u5fc6\u963b\u4ea4\u53c9\u9635\u5217\u4e0a\u7684BNN\u5bb9\u6613\u53d7\u5230\u7269\u7406\u653b\u51fb\u3002\u867d\u7136\u5df2\u6709PUF\u65b9\u6848\u7528\u4e8e\u4fdd\u62a4BNN\uff0c\u4f46\u672c\u6587\u53d1\u73b0\u8be5\u65b9\u6848\u5b58\u5728\u6f0f\u6d1e\u3002", "method": "\u91c7\u7528\u5dee\u5206\u5bc6\u7801\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c2\u5bdf\u6a21\u578b\u51c6\u786e\u7387\u7684\u53d8\u5316\u6765\u9010\u4f4d\u91cd\u5efaPUF\u5bc6\u94a5\uff0c\u6700\u7ec8\u6062\u590dBNN\u7684\u6743\u91cd\u548c\u504f\u7f6e\u77e9\u9635\u3002", "result": "\u5728MNIST\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u653b\u51fb\u80fd\u591f\u6062\u590d85%\u7684PUF\u5bc6\u94a5\uff0c\u91cd\u5efa\u7684BNN\u6a21\u578b\u8fbe\u523093%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff08\u539f\u6a21\u578b\u4e3a96%\uff09\uff0c\u6574\u4e2a\u653b\u51fb\u8fc7\u7a0b\u4ec5\u9700\u51e0\u5206\u949f\u3002", "conclusion": "\u73b0\u6709\u7684PUF\u4fdd\u62a4\u65b9\u6848\u5bf9BNN\u7684\u5b89\u5168\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u5f3a\u5927\u7684\u4fdd\u62a4\u673a\u5236\u6765\u62b5\u5fa1\u6b64\u7c7b\u5dee\u5206\u5206\u6790\u653b\u51fb\u3002"}}
{"id": "2510.23942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23942", "abs": "https://arxiv.org/abs/2510.23942", "authors": ["Sridhar Mahadevan"], "title": "Decentralized Causal Discovery using Judo Calculus", "comment": "54 pages", "summary": "We describe a theory and implementation of an intuitionistic decentralized\nframework for causal discovery using judo calculus, which is formally defined\nas j-stable causal inference using j-do-calculus in a topos of sheaves. In\nreal-world applications -- from biology to medicine and social science --\ncausal effects depend on regime (age, country, dose, genotype, or lab\nprotocol). Our proposed judo calculus formalizes this context dependence\nformally as local truth: a causal claim is proven true on a cover of regimes,\nnot everywhere at once. The Lawvere-Tierney modal operator j chooses which\nregimes are relevant; j-stability means the claim holds constructively and\nconsistently across that family. We describe an algorithmic and implementation\nframework for judo calculus, combining it with standard score-based,\nconstraint-based, and gradient-based causal discovery methods. We describe\nexperimental results on a range of domains, from synthetic to real-world\ndatasets from biology and economics. Our experimental results show the\ncomputational efficiency gained by the decentralized nature of sheaf-theoretic\ncausal discovery, as well as improved performance over classical causal\ndiscovery methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u89c9\u4e3b\u4e49\u5206\u6563\u5f0f\u6846\u67b6\u7684\u56e0\u679c\u53d1\u73b0\u7406\u8bba\u2014\u2014judo\u6f14\u7b97\uff0c\u4f7f\u7528j-\u7a33\u5b9a\u56e0\u679c\u63a8\u65ad\u548cj-do\u6f14\u7b97\u5728\u5c42\u62d3\u6251\u4e2d\u5f62\u5f0f\u5316\u5904\u7406\u56e0\u679c\u5173\u7cfb\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff08\u4ece\u751f\u7269\u5b66\u5230\u533b\u5b66\u548c\u793e\u4f1a\u79d1\u5b66\uff09\uff0c\u56e0\u679c\u6548\u5e94\u4f9d\u8d56\u4e8e\u5177\u4f53\u60c5\u5883\uff08\u5982\u5e74\u9f84\u3001\u56fd\u5bb6\u3001\u5242\u91cf\u3001\u57fa\u56e0\u578b\u6216\u5b9e\u9a8c\u534f\u8bae\uff09\uff0c\u9700\u8981\u5f62\u5f0f\u5316\u5904\u7406\u8fd9\u79cd\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u3002", "method": "\u4f7f\u7528judo\u6f14\u7b97\uff0c\u7ed3\u5408Lawvere-Tierney\u6a21\u6001\u7b97\u5b50j\u9009\u62e9\u76f8\u5173\u60c5\u5883\uff0c\u5c06\u56e0\u679c\u58f0\u660e\u5f62\u5f0f\u5316\u4e3a\u5728\u60c5\u5883\u8986\u76d6\u4e0a\u7684\u5c40\u90e8\u771f\u503c\u3002\u7ed3\u5408\u6807\u51c6\u7684\u57fa\u4e8e\u5206\u6570\u3001\u7ea6\u675f\u548c\u68af\u5ea6\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u5c42\u7406\u8bba\u7684\u5206\u6563\u5f0f\u56e0\u679c\u53d1\u73b0\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08\u751f\u7269\u5b66\u548c\u7ecf\u6d4e\u5b66\uff09\u4e0a\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "judo\u6f14\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u6784\u9020\u6027\u548c\u4e00\u81f4\u6027\u7684\u6846\u67b6\u6765\u5904\u7406\u56e0\u679c\u5173\u7cfb\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u901a\u8fc7\u5206\u6563\u5f0f\u65b9\u6cd5\u63d0\u9ad8\u4e86\u56e0\u679c\u53d1\u73b0\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2510.24498", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24498", "abs": "https://arxiv.org/abs/2510.24498", "authors": ["Tejaswini Bollikonda"], "title": "Design and Optimization of Cloud Native Homomorphic Encryption Workflows for Privacy-Preserving ML Inference", "comment": "6 pages 2 figures, 2 tABLES", "summary": "As machine learning (ML) models become increasingly deployed through cloud\ninfrastructures, the confidentiality of user data during inference poses a\nsignificant security challenge. Homomorphic Encryption (HE) has emerged as a\ncompelling cryptographic technique that enables computation on encrypted data,\nallowing predictions to be generated without decrypting sensitive inputs.\nHowever, the integration of HE within large scale cloud native pipelines\nremains constrained by high computational overhead, orchestration complexity,\nand model compatibility issues.\n  This paper presents a systematic framework for the design and optimization of\ncloud native homomorphic encryption workflows that support privacy-preserving\nML inference. The proposed architecture integrates containerized HE modules\nwith Kubernetes-based orchestration, enabling elastic scaling and parallel\nencrypted computation across distributed environments. Furthermore,\noptimization strategies including ciphertext packing, polynomial modulus\nadjustment, and operator fusion are employed to minimize latency and resource\nconsumption while preserving cryptographic integrity. Experimental results\ndemonstrate that the proposed system achieves up to 3.2times inference\nacceleration and 40% reduction in memory utilization compared to conventional\nHE pipelines. These findings illustrate a practical pathway for deploying\nsecure ML-as-a-Service (MLaaS) systems that guarantee data confidentiality\nunder zero-trust cloud conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e91\u539f\u751f\u540c\u6001\u52a0\u5bc6\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u9690\u79c1\u4fdd\u62a4\u7684\u673a\u5668\u5b66\u4e60\u63a8\u7406\u670d\u52a1\uff0c\u901a\u8fc7\u5bb9\u5668\u5316\u548cKubernetes\u7f16\u6392\u5b9e\u73b0\u5206\u5e03\u5f0f\u52a0\u5bc6\u8ba1\u7b97\uff0c\u76f8\u6bd4\u4f20\u7edfHE\u6d41\u6c34\u7ebf\u5b9e\u73b0\u4e863.2\u500d\u63a8\u7406\u52a0\u901f\u548c40%\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6a21\u578b\u901a\u8fc7\u4e91\u57fa\u7840\u8bbe\u65bd\u90e8\u7f72\uff0c\u7528\u6237\u6570\u636e\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4fdd\u5bc6\u6027\u6210\u4e3a\u91cd\u8981\u5b89\u5168\u6311\u6218\u3002\u540c\u6001\u52a0\u5bc6\u867d\u7136\u80fd\u5728\u52a0\u5bc6\u6570\u636e\u4e0a\u6267\u884c\u8ba1\u7b97\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u4e91\u539f\u751f\u6d41\u6c34\u7ebf\u4e2d\u4ecd\u9762\u4e34\u9ad8\u8ba1\u7b97\u5f00\u9500\u3001\u7f16\u6392\u590d\u6742\u6027\u548c\u6a21\u578b\u517c\u5bb9\u6027\u95ee\u9898\u3002", "method": "\u96c6\u6210\u5bb9\u5668\u5316HE\u6a21\u5757\u4e0eKubernetes\u7f16\u6392\uff0c\u652f\u6301\u5f39\u6027\u6269\u5c55\u548c\u5206\u5e03\u5f0f\u5e76\u884c\u52a0\u5bc6\u8ba1\u7b97\uff1b\u91c7\u7528\u5bc6\u6587\u6253\u5305\u3001\u591a\u9879\u5f0f\u6a21\u6570\u8c03\u6574\u548c\u7b97\u5b50\u878d\u5408\u7b49\u4f18\u5316\u7b56\u7565\u6765\u964d\u4f4e\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u4f20\u7edfHE\u6d41\u6c34\u7ebf\uff0c\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u6700\u9ad83.2\u500d\u7684\u63a8\u7406\u52a0\u901f\u548c40%\u7684\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u96f6\u4fe1\u4efb\u4e91\u6761\u4ef6\u4e0b\u90e8\u7f72\u4fdd\u8bc1\u6570\u636e\u673a\u5bc6\u6027\u7684\u5b89\u5168MLaaS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2510.23965", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23965", "abs": "https://arxiv.org/abs/2510.23965", "authors": ["Aymane El Gadarri", "Ali Aouad", "Vivek F. Farias"], "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity", "comment": null, "summary": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3asign estimator\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u66ff\u6362\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\u5728\u4eba\u7c7b\u504f\u597d\u5f02\u8d28\u6027\u4e0b\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\u5728\u9762\u5bf9\u4eba\u7c7b\u504f\u597d\u5f02\u8d28\u6027\u65f6\u5b58\u5728\u8106\u5f31\u6027\uff0c\u62df\u5408\u7b80\u5355\u7684\u6982\u7387\u6a21\u578b\u4f1a\u5bfc\u81f4\u5bf9\u7fa4\u4f53\u5e73\u5747\u6548\u7528\u7684\u4e0d\u4e00\u81f4\u4f30\u8ba1\u3002", "method": "\u4f7f\u7528sign estimator\u65b9\u6cd5\uff0c\u5728\u805a\u5408\u6b65\u9aa4\u4e2d\u7528\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u66ff\u4ee3\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u5b9e\u73b0\u4e00\u81f4\u7684\u5e8f\u6570\u5bf9\u9f50\u3002", "result": "\u5728\u6570\u5b57\u5b6a\u751f\u7684LLM\u5bf9\u9f50\u6a21\u62df\u4e2d\uff0csign estimator\u663e\u8457\u51cf\u5c11\u4e86\u504f\u597d\u626d\u66f2\uff0c\u5c06\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u4e86\u8fd135%\uff0c\u4e0e\u771f\u5b9e\u7fa4\u4f53\u504f\u597d\u7684\u4e0d\u4e00\u81f4\u4ece12%\u964d\u81f38%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u73b0\u6709LLM\u5bf9\u9f50\u7ba1\u9053\u5b9e\u73b0\u7b80\u5355\u6027\u7684\u540c\u65f6\uff0c\u4f18\u4e8e\u663e\u5f0f\u5efa\u6a21\u7528\u6237\u5f02\u8d28\u6027\u7684\u9762\u677f\u6570\u636e\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u7684\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u4f30\u8ba1\u5668\u3002"}}
{"id": "2510.23989", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23989", "abs": "https://arxiv.org/abs/2510.23989", "authors": ["Shangde Gao", "Zelin Xu", "Zhe Jiang"], "title": "Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance", "comment": null, "summary": "Shifts in individual movement patterns following disruptive events can reveal\nchanging demands for community resources. However, predicting such shifts\nbefore disruptive events remains challenging for several reasons. First,\nmeasures are lacking for individuals' heterogeneous social infrastructure\nresilience (SIR), which directly influences their movement patterns, and\ncommonly used features are often limited or unavailable at scale, e.g.,\nsociodemographic characteristics. Second, the complex interactions between\nindividual movement patterns and spatial contexts have not been sufficiently\ncaptured. Third, individual-level movement may be spatially sparse and not\nwell-suited to traditional decision-making methods for movement predictions.\nThis study incorporates individuals' SIR into a conditioned deep learning model\nto capture the complex relationships between individual movement patterns and\nlocal spatial context using large-scale, sparse individual-level data. Our\nexperiments demonstrate that incorporating individuals' SIR and spatial context\ncan enhance the model's ability to predict post-event individual movement\npatterns. The conditioned model can capture the divergent shifts in movement\npatterns among individuals who exhibit similar pre-event patterns but differ in\nSIR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e2a\u4f53\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027(SIR)\u7684\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\u3002", "motivation": "\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u524d\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u53d8\u5316\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u8861\u91cf\u4e2a\u4f53\u5f02\u8d28\u6027\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027\u7684\u65b9\u6cd5\uff0c\u4f20\u7edf\u7279\u5f81\u6709\u9650\uff0c\u4e14\u4e2a\u4f53\u79fb\u52a8\u4e0e\u7a7a\u95f4\u73af\u5883\u7684\u590d\u6742\u4ea4\u4e92\u672a\u88ab\u5145\u5206\u6355\u6349\u3002", "method": "\u5c06\u4e2a\u4f53\u7684SIR\u6574\u5408\u5230\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u5229\u7528\u5927\u89c4\u6a21\u7a00\u758f\u4e2a\u4f53\u7ea7\u6570\u636e\u6355\u6349\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u4e0e\u5c40\u90e8\u7a7a\u95f4\u73af\u5883\u7684\u590d\u6742\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u4e2a\u4f53SIR\u548c\u7a7a\u95f4\u73af\u5883\u80fd\u589e\u5f3a\u6a21\u578b\u9884\u6d4b\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u8be5\u6a21\u578b\u80fd\u6355\u6349\u5177\u6709\u76f8\u4f3c\u4e8b\u4ef6\u524d\u6a21\u5f0f\u4f46SIR\u4e0d\u540c\u7684\u4e2a\u4f53\u5728\u79fb\u52a8\u6a21\u5f0f\u4e0a\u7684\u5dee\u5f02\u53d8\u5316\u3002", "conclusion": "\u8be5\u6761\u4ef6\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u4e2a\u4f53\u5728\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u7684\u79fb\u52a8\u6a21\u5f0f\u53d8\u5316\uff0c\u7279\u522b\u662f\u80fd\u533a\u5206\u5177\u6709\u76f8\u4f3c\u5386\u53f2\u6a21\u5f0f\u4f46\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027\u4e0d\u540c\u7684\u4e2a\u4f53\u7684\u79fb\u52a8\u884c\u4e3a\u5dee\u5f02\u3002"}}
{"id": "2510.24031", "categories": ["cs.AI", "cs.CR", "H.3.3, I.2.7, I.5.3, I.2.5,"], "pdf": "https://arxiv.org/pdf/2510.24031", "abs": "https://arxiv.org/abs/2510.24031", "authors": ["Peng Cai", "Reza Ryan", "Nickson M. Karie"], "title": "LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models", "comment": "33 pages, 10 figures", "summary": "System logs are a cornerstone of cybersecurity, supporting proactive breach\nprevention and post-incident investigations. However, analyzing vast amounts of\ndiverse log data remains significantly challenging, as high costs, lack of\nin-house expertise, and time constraints make even basic analysis difficult for\nmany organizations. This study introduces LLMLogAnalyzer, a clustering-based\nlog analysis chatbot that leverages Large Language Models (LLMs) and Machine\nLearning (ML) algorithms to simplify and streamline log analysis processes.\nThis innovative approach addresses key LLM limitations, including context\nwindow constraints and poor structured text handling capabilities, enabling\nmore effective summarization, pattern extraction, and anomaly detection tasks.\nLLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.\nResults demonstrate significant performance improvements over state-of-the-art\nLLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent\ngains ranging from 39% to 68% across different tasks. The system also exhibits\nstrong robustness, achieving a 93% reduction in interquartile range (IQR) when\nusing ROUGE-1 scores, indicating significantly lower result variability. The\nframework's effectiveness stems from its modular architecture comprising a\nrouter, log recognizer, log parser, and search tools. This design enhances LLM\ncapabilities for structured text analysis while improving accuracy and\nrobustness, making it a valuable resource for both cybersecurity experts and\nnon-technical users.", "AI": {"tldr": "LLMLogAnalyzer\u662f\u4e00\u4e2a\u57fa\u4e8e\u805a\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65e5\u5fd7\u5206\u6790\u804a\u5929\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7b80\u5316\u65e5\u5fd7\u5206\u6790\u6d41\u7a0b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u6bd4\u73b0\u6709LLM\u804a\u5929\u673a\u5668\u4eba\u6027\u80fd\u63d0\u534739%-68%\u3002", "motivation": "\u7cfb\u7edf\u65e5\u5fd7\u662f\u7f51\u7edc\u5b89\u5168\u7684\u6838\u5fc3\uff0c\u4f46\u5206\u6790\u5927\u91cf\u591a\u6837\u5316\u65e5\u5fd7\u6570\u636e\u9762\u4e34\u9ad8\u6210\u672c\u3001\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\u548c\u65f6\u95f4\u9650\u5236\u7b49\u6311\u6218\uff0c\u8bb8\u591a\u7ec4\u7ec7\u96be\u4ee5\u8fdb\u884c\u57fa\u672c\u5206\u6790\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5305\u62ec\u8def\u7531\u5668\u3001\u65e5\u5fd7\u8bc6\u522b\u5668\u3001\u65e5\u5fd7\u89e3\u6790\u5668\u548c\u641c\u7d22\u5de5\u5177\uff0c\u7ed3\u5408\u805a\u7c7b\u7b97\u6cd5\u548cLLM\u6280\u672f\uff0c\u89e3\u51b3\u4e86LLM\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u7ed3\u6784\u5316\u6587\u672c\u5904\u7406\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\u65e5\u5fd7\u548c\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4ChatGPT\u3001ChatPDF\u548cNotebookLM\u7b49\u6700\u5148\u8fdb\u7684LLM\u804a\u5929\u673a\u5668\u4eba\uff0c\u6027\u80fd\u63d0\u534739%-68%\uff0c\u9c81\u68d2\u6027\u663e\u8457\u589e\u5f3a\uff0cROUGE-1\u5206\u6570\u7684\u56db\u5206\u4f4d\u8ddd\u51cf\u5c1193%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u589e\u5f3aLLM\u5728\u7ed3\u6784\u5316\u6587\u672c\u5206\u6790\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u7f51\u7edc\u5b89\u5168\u4e13\u5bb6\u548c\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2510.24013", "categories": ["cs.AI", "cs.LG", "cs.NE", "math.CO", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.24013", "abs": "https://arxiv.org/abs/2510.24013", "authors": ["\u0130brahim O\u011fuz \u00c7etinkaya", "\u0130. Esra B\u00fcy\u00fcktahtak\u0131n", "Parshin Shojaee", "Chandan K. Reddy"], "title": "Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling", "comment": null, "summary": "Our study contributes to the scheduling and combinatorial optimization\nliterature with new heuristics discovered by leveraging the power of Large\nLanguage Models (LLMs). We focus on the single-machine total tardiness (SMTT)\nproblem, which aims to minimize total tardiness by sequencing n jobs on a\nsingle processor without preemption, given processing times and due dates. We\ndevelop and benchmark two novel LLM-discovered heuristics, the EDD Challenger\n(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date\n(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that\nemployed simpler rule-based heuristics, we evaluate our LLM-discovered\nalgorithms using rigorous criteria, including optimality gaps and solution time\nderived from a mixed-integer programming (MIP) formulation of SMTT. We compare\ntheir performance against state-of-the-art heuristics and exact methods across\nvarious job sizes (20, 100, 200, and 500 jobs). For instances with more than\n100 jobs, exact methods such as MIP and dynamic programming become\ncomputationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD\nrule and another widely used algorithm in the literature. MDDC consistently\noutperforms traditional heuristics and remains competitive with exact\napproaches, particularly on larger and more complex instances. This study shows\nthat human-LLM collaboration can produce scalable, high-performing heuristics\nfor NP-hard constrained combinatorial optimization, even under limited\nresources when effectively configured.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u53d1\u73b0\u65b0\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u6765\u89e3\u51b3\u5355\u673a\u603b\u5ef6\u8fdf\u95ee\u9898\uff0c\u63d0\u51fa\u4e86EDDC\u548cMDDC\u4e24\u79cd\u7b97\u6cd5\uff0c\u5728\u5927\u578b\u5b9e\u4f8b\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u5355\u673a\u603b\u5ef6\u8fdf\u95ee\u9898\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u800c\u7cbe\u786e\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u8ba1\u7b97\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u9ad8\u6548\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b-LLM\u534f\u4f5c\u53d1\u73b0\u65b0\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5EDDC\u548cMDDC\uff0c\u57fa\u4e8e\u7ecf\u5178\u7684EDD\u548cMDD\u89c4\u5219\uff0c\u5e76\u4e0e\u6df7\u5408\u6574\u6570\u89c4\u5212\u548c\u52a8\u6001\u89c4\u5212\u7b49\u7cbe\u786e\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5bf9\u4e8e\u8d85\u8fc7100\u4e2a\u4f5c\u4e1a\u7684\u5b9e\u4f8b\uff0cEDDC\u6539\u8fdb\u4e86\u7ecf\u5178EDD\u89c4\u5219\uff0cMDDC\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5728\u5927\u578b\u590d\u6742\u5b9e\u4f8b\u4e0a\u4e0e\u7cbe\u786e\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u4eba\u7c7b-LLM\u534f\u4f5c\u53ef\u4ee5\u6709\u6548\u4ea7\u751f\u53ef\u6269\u5c55\u7684\u9ad8\u6027\u80fd\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8eNP\u96be\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2510.24028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24028", "abs": "https://arxiv.org/abs/2510.24028", "authors": ["Tingyue Pan", "Mingyue Cheng", "Shilong Zhang", "Zhiding Liu", "Xiaoyu Tao", "Yucong Luo", "Jintao Zhang", "Qi Liu"], "title": "OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting", "comment": null, "summary": "Cross-domain time series forecasting is a valuable task in various web\napplications. Despite its rapid advancement, achieving effective generalization\nacross heterogeneous time series data remains a significant challenge. Existing\nmethods have made progress by extending single-domain models, yet often fall\nshort when facing domain-specific trend shifts and inconsistent periodic\npatterns. We argue that a key limitation lies in treating temporal series as\nundifferentiated sequence, without explicitly decoupling their inherent\nstructural components. To address this, we propose OneCast, a structured and\nmodular forecasting framework that decomposes time series into seasonal and\ntrend components, each modeled through tailored generative pathways.\nSpecifically, the seasonal component is captured by a lightweight projection\nmodule that reconstructs periodic patterns via interpretable basis functions.\nIn parallel, the trend component is encoded into discrete tokens at segment\nlevel via a semantic-aware tokenizer, and subsequently inferred through a\nmasked discrete diffusion mechanism. The outputs from both branches are\ncombined to produce a final forecast that captures seasonal patterns while\ntracking domain-specific trends. Extensive experiments across eight domains\ndemonstrate that OneCast mostly outperforms state-of-the-art baselines.", "AI": {"tldr": "OneCast\u662f\u4e00\u4e2a\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5206\u91cf\uff0c\u5206\u522b\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6295\u5f71\u6a21\u5757\u548c\u57fa\u4e8e\u6269\u6563\u7684\u6807\u8bb0\u5316\u673a\u5236\u8fdb\u884c\u5efa\u6a21\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u8de8\u57df\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u57df\u7279\u5b9a\u8d8b\u52bf\u53d8\u5316\u548c\u4e0d\u4e00\u81f4\u5468\u671f\u6027\u6a21\u5f0f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u5c06\u65f6\u95f4\u5e8f\u5217\u89c6\u4e3a\u672a\u5206\u5316\u7684\u5e8f\u5217\u800c\u672a\u663e\u5f0f\u89e3\u8026\u5176\u56fa\u6709\u7ed3\u6784\u7ec4\u4ef6\u3002", "method": "\u63d0\u51faOneCast\u6846\u67b6\uff1a1\uff09\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5206\u91cf\uff1b2\uff09\u5b63\u8282\u6027\u5206\u91cf\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6295\u5f71\u6a21\u5757\u4f7f\u7528\u53ef\u89e3\u91ca\u57fa\u51fd\u6570\u91cd\u5efa\u5468\u671f\u6027\u6a21\u5f0f\uff1b3\uff09\u8d8b\u52bf\u5206\u91cf\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u6807\u8bb0\u5668\u5728\u5206\u6bb5\u7ea7\u522b\u7f16\u7801\u4e3a\u79bb\u6563\u6807\u8bb0\uff0c\u5e76\u901a\u8fc7\u63a9\u7801\u79bb\u6563\u6269\u6563\u673a\u5236\u63a8\u65ad\uff1b4\uff09\u4e24\u4e2a\u5206\u652f\u8f93\u51fa\u7ed3\u5408\u751f\u6210\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5728\u516b\u4e2a\u9886\u57df\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cOneCast\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u89e3\u8026\u65f6\u95f4\u5e8f\u5217\u7684\u7ed3\u6784\u7ec4\u4ef6\u5e76\u91c7\u7528\u6a21\u5757\u5316\u5efa\u6a21\u65b9\u6cd5\uff0cOneCast\u80fd\u591f\u6709\u6548\u6355\u6349\u5b63\u8282\u6027\u6a21\u5f0f\u540c\u65f6\u8ddf\u8e2a\u57df\u7279\u5b9a\u8d8b\u52bf\uff0c\u5728\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.24085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "AI": {"tldr": "\u6bd4\u8f83\u7ecf\u5178\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7535\u52a8\u6c7d\u8f66\u8ddf\u8f66\u884c\u4e3a\u5efa\u6a21\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u4f18\u4e8e\u7269\u7406\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u8ddf\u8f66\u95f4\u8ddd\u6761\u4ef6\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u7684\u666e\u53ca\uff0c\u9700\u8981\u7406\u89e3\u5176\u9a7e\u9a76\u884c\u4e3a\u4ee5\u63d0\u5347\u4ea4\u901a\u5b89\u5168\u548c\u5f00\u53d1\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u4e0e\u4f20\u7edf\u5185\u71c3\u673a\u8f66\u8f86\u6df7\u5408\u7684\u4ea4\u901a\u73af\u5883\u4e2d\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86IDM\u3001OVM\u3001OVRV\u548c\u7b80\u5316CACC\u7b49\u7ecf\u5178\u7269\u7406\u6a21\u578b\u4e0e\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\u3002\u901a\u8fc7\u6700\u5c0f\u5316\u9884\u6d4b\u503c\u4e0e\u5b9e\u9645\u6570\u636e\u7684RMSE\u6765\u6821\u51c6\u7ecf\u5178\u6a21\u578b\u53c2\u6570\uff0c\u968f\u673a\u68ee\u6797\u4f7f\u7528\u95f4\u8ddd\u3001\u901f\u5ea6\u548c\u95f4\u9699\u7c7b\u578b\u4f5c\u4e3a\u8f93\u5165\u6765\u9884\u6d4b\u52a0\u901f\u5ea6\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e0b\u8868\u73b0\u6700\u4f18\uff0cRMSE\u5206\u522b\u4e3a0.0046\uff08\u4e2d\u7b49\u95f4\u8ddd\uff09\u30010.0016\uff08\u957f\u95f4\u8ddd\uff09\u548c0.0025\uff08\u8d85\u957f\u95f4\u8ddd\uff09\u3002\u5728\u7269\u7406\u6a21\u578b\u4e2d\uff0cCACC\u8868\u73b0\u6700\u597d\uff0c\u957f\u95f4\u8ddd\u7684RMSE\u4e3a2.67\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\u884c\u4e3a\u548c\u5206\u6790\u6df7\u5408\u81ea\u52a8\u9a7e\u9a76\u4ea4\u901a\u52a8\u6001\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u7535\u52a8\u6c7d\u8f66\u96c6\u6210\u73af\u5883\u4e2d\u3002"}}
{"id": "2510.24115", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24115", "abs": "https://arxiv.org/abs/2510.24115", "authors": ["Sandeep Vissapragada", "Vikrant Sahu", "Gagan Raj Gupta", "Vandita Singh"], "title": "HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology", "comment": null, "summary": "For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.", "AI": {"tldr": "\u5f00\u53d1\u4e86HistoLens\u7cfb\u7edf\uff0c\u8ba9\u75c5\u7406\u5b66\u5bb6\u80fd\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u95ee\u7ec4\u7ec7\u5207\u7247\u95ee\u9898\uff0cAI\u63d0\u4f9b\u7ed3\u6784\u5316\u62a5\u544a\u548c\u53ef\u89c6\u5316\u8bc1\u636e\uff0c\u786e\u4fdd\u533b\u751f\u4fdd\u6301\u4e3b\u5bfc\u5730\u4f4d\u7684\u540c\u65f6\u83b7\u5f97\u53ef\u4fe1\u7684AI\u8f85\u52a9\u8bca\u65ad\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u533b\u751f\u771f\u6b63\u4fe1\u4efb\u4eba\u5de5\u667a\u80fd\uff0c\u9700\u8981\u89e3\u51b3AI\u9ed1\u76d2\u95ee\u9898\uff0c\u8ba9\u533b\u751f\u80fd\u591f\u7406\u89e3AI\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c31\u50cf\u54a8\u8be2\u540c\u4e8b\u4e00\u6837\u900f\u660e\u53ef\u4fe1\u3002", "method": "\u521b\u5efaHistoLens\u7cfb\u7edf\uff0c\u5c06\u75c5\u7406\u5b66\u5bb6\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3a\u7cbe\u786e\u7684AI\u67e5\u8be2\uff0c\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\u5e76\u63d0\u4f9b\u53ef\u89c6\u5316\u70ed\u56fe\u8bc1\u636e\uff0c\u540c\u65f6\u8bad\u7ec3AI\u4e13\u6ce8\u4e8e\u60a3\u8005\u7ec4\u7ec7\u800c\u5ffd\u7565\u80cc\u666f\u566a\u97f3\u3002", "result": "\u5b9e\u73b0\u4e86\u900f\u660e\u534f\u4f5c\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u75c5\u7406\u5b66\u5bb6\u4fdd\u6301\u4e13\u5bb6\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f7f\u7528\u53ef\u4fe1\u7684AI\u52a9\u624b\u9a8c\u8bc1\u89c1\u89e3\uff0c\u505a\u51fa\u66f4\u5feb\u66f4\u81ea\u4fe1\u7684\u8bca\u65ad\u3002", "conclusion": "HistoLens\u901a\u8fc7\u63d0\u4f9b\u900f\u660e\u63a8\u7406\u548c\u53ef\u89c6\u5316\u8bc1\u636e\uff0c\u5efa\u7acb\u4e86\u533b\u751f\u4e0eAI\u4e4b\u95f4\u7684\u4fe1\u4efb\u5173\u7cfb\uff0c\u4f7fAI\u6210\u4e3a\u53ef\u9760\u7684\u8bca\u65ad\u52a9\u624b\u800c\u975e\u9ed1\u76d2\u5de5\u5177\u3002"}}
{"id": "2510.24145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24145", "abs": "https://arxiv.org/abs/2510.24145", "authors": ["Yu Luo", "Jiamin Jiang", "Jingfei Feng", "Lei Tao", "Qingliang Zhang", "Xidao Wen", "Yongqian Sun", "Shenglin Zhang", "Jielong Huang", "Nan Qi", "Dan Pei"], "title": "From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems", "comment": null, "summary": "Incident management (IM) is central to the reliability of large-scale cloud\nsystems. Yet manual IM, where on-call engineers examine metrics, logs, and\ntraces is labor-intensive and error-prone in the face of massive and\nheterogeneous observability data. Existing automated IM approaches often\nstruggle to generalize across systems, provide limited interpretability, and\nincur high deployment costs, which hinders adoption in practice. In this paper,\nwe present OpsAgent, a lightweight, self-evolving multi-agent system for IM\nthat employs a training-free data processor to convert heterogeneous\nobservability data into structured textual descriptions, along with a\nmulti-agent collaboration framework that makes diagnostic inference transparent\nand auditable. To support continual capability growth, OpsAgent also introduces\na dual self-evolution mechanism that integrates internal model updates with\nexternal experience accumulation, thereby closing the deployment loop.\nComprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art\nperformance and show that OpsAgent is generalizable, interpretable,\ncost-efficient, and self-evolving, making it a practically deployable and\nsustainable solution for long-term operation in real-world cloud systems.", "AI": {"tldr": "OpsAgent\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u81ea\u6f14\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e91\u7cfb\u7edf\u4e8b\u4ef6\u7ba1\u7406\uff0c\u901a\u8fc7\u514d\u8bad\u7ec3\u6570\u636e\u5904\u7406\u548c\u900f\u660e\u8bca\u65ad\u63a8\u7406\uff0c\u5728OPENRCA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u624b\u52a8\u4e8b\u4ef6\u7ba1\u7406\u52b3\u52a8\u5bc6\u96c6\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u96be\u4ee5\u8de8\u7cfb\u7edf\u6cdb\u5316\u3001\u53ef\u89e3\u91ca\u6027\u6709\u9650\u4e14\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u514d\u8bad\u7ec3\u6570\u636e\u5904\u7406\u5668\u5c06\u5f02\u6784\u53ef\u89c2\u6d4b\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u5b9e\u73b0\u900f\u660e\u8bca\u65ad\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u53cc\u81ea\u6f14\u5316\u673a\u5236\u652f\u6301\u6301\u7eed\u80fd\u529b\u589e\u957f\u3002", "result": "\u5728OPENRCA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660eOpsAgent\u5177\u6709\u53ef\u6cdb\u5316\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u81ea\u6f14\u5316\u7279\u6027\u3002", "conclusion": "OpsAgent\u662f\u4e00\u4e2a\u5b9e\u9645\u53ef\u90e8\u7f72\u4e14\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e91\u7cfb\u7edf\u7684\u957f\u671f\u8fd0\u7ef4\u3002"}}
{"id": "2510.24151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24151", "abs": "https://arxiv.org/abs/2510.24151", "authors": ["Bingsen Qiu", "Zijian Liu", "Xiao Liu", "Haoshen Yang", "Zeren Gao", "Bingjie Wang", "Feier Zhang", "Yixuan Qin", "Chunyan Li"], "title": "BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data", "comment": null, "summary": "Building training-ready multi-hop question answering (QA) datasets that truly\nstress a model's retrieval and reasoning abilities remains highly challenging\nrecently. While there have been a few recent evaluation datasets that capture\nthe characteristics of hard-to-search but easy-to-verify problems -- requiring\nthe integration of ambiguous, indirect, and cross-domain cues -- these data\nresources remain scarce and are mostly designed for evaluation, making them\nunsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).\nMeanwhile, manually curating non-trivially retrievable questions -- where\nanswers cannot be found through a single direct query but instead require\nmulti-hop reasoning over oblique and loosely connected evidence -- incurs\nprohibitive human costs and fails to scale, creating a critical data bottleneck\nfor training high-capability retrieval-and-reasoning agents.\n  To address this, we present an automated framework for generating\nhigh-difficulty, training-ready multi-hop questions from semi-structured\nknowledge sources. The system (i) grows diverse, logically labeled evidence\nclusters through Natural Language Inference (NLI)-based relation typing and\ndiversity-aware expansion; (ii) applies reverse question construction to\ncompose oblique cues so that isolated signals are underinformative but their\ncombination uniquely identifies the target entity; and (iii) enforces quality\nwith a two-step evaluation pipeline that combines multi-model consensus\nfiltering with structured constraint decomposition and evidence-based matching.\nThe result is a scalable process that yields complex, retrieval-resistant yet\nverifiable questions suitable for SFT/RL training as well as challenging\nevaluation, substantially reducing human curation effort while preserving the\ndifficulty profile of strong evaluation benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u6e90\u751f\u6210\u9ad8\u96be\u5ea6\u3001\u53ef\u7528\u4e8e\u8bad\u7ec3\u7684\u591a\u8df3\u95ee\u7b54\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u4e0d\u9002\u5408\u76d1\u7763\u5fae\u8c03\u6216\u5f3a\u5316\u5b66\u4e60\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u8d44\u6e90\u7a00\u7f3a\uff0c\u5927\u591a\u4ec5\u7528\u4e8e\u8bc4\u4f30\u800c\u975e\u8bad\u7ec3\uff0c\u4e14\u4eba\u5de5\u6784\u5efa\u975e\u5e73\u51e1\u53ef\u68c0\u7d22\u95ee\u9898\u6210\u672c\u9ad8\u6602\uff0c\u65e0\u6cd5\u89c4\u6a21\u5316\uff0c\u8fd9\u6210\u4e3a\u8bad\u7ec3\u9ad8\u80fd\u529b\u68c0\u7d22\u63a8\u7406\u667a\u80fd\u4f53\u7684\u5173\u952e\u6570\u636e\u74f6\u9888\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7NLI\u5173\u7cfb\u7c7b\u578b\u5316\u548c\u591a\u6837\u6027\u611f\u77e5\u6269\u5c55\u6784\u5efa\u8bc1\u636e\u7c07\uff0c\u5e94\u7528\u53cd\u5411\u95ee\u9898\u6784\u5efa\u6765\u7ec4\u5408\u95f4\u63a5\u7ebf\u7d22\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u578b\u5171\u8bc6\u8fc7\u6ee4\u4e0e\u7ed3\u6784\u5316\u7ea6\u675f\u5206\u89e3\u7684\u4e24\u6b65\u8bc4\u4f30\u6d41\u7a0b\u786e\u4fdd\u8d28\u91cf\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u89c4\u6a21\u5316\u751f\u6210\u590d\u6742\u3001\u68c0\u7d22\u62b5\u6297\u4f46\u53ef\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8eSFT/RL\u8bad\u7ec3\u548c\u6311\u6218\u6027\u8bc4\u4f30\uff0c\u5927\u5e45\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u540c\u65f6\u4fdd\u6301\u5f3a\u8bc4\u4f30\u57fa\u51c6\u7684\u96be\u5ea6\u7279\u5f81\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8df3\u95ee\u7b54\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4e3a\u8bad\u7ec3\u9ad8\u80fd\u529b\u68c0\u7d22\u63a8\u7406\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24161", "categories": ["cs.AI", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24161", "abs": "https://arxiv.org/abs/2510.24161", "authors": ["Wentao Tan", "Bowen Wang", "Heng Zhi", "Chenyu Liu", "Zhe Li", "Jian Liu", "Zengrong Lin", "Yukun Dai", "Yipeng Chen", "Wenjie Yang", "Enci Xie", "Hao Xue", "Baixu Ji", "Chen Xu", "Zhibin Wang", "Tianshi Wang", "Lei Zhu", "Heng Tao Shen"], "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.", "AI": {"tldr": "BLM\u2081\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u8de8\u7a7a\u95f4\u4f20\u8f93\u3001\u8de8\u4efb\u52a1\u5b66\u4e60\u548c\u8de8\u5177\u8eab\u6cdb\u5316\uff0c\u5728\u6570\u5b57\u548c\u7269\u7406\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u548c\u5177\u8eab\u7cfb\u7edf\u95f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0cVLAs\u7f3a\u4e4f\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\uff0cELLMs\u5c40\u9650\u4e8e\u6570\u5b57\u7a7a\u95f4\uff0c\u9700\u8981\u7edf\u4e00\u6a21\u578b\u5b9e\u73b0\u8de8\u7a7a\u95f4\u548c\u5177\u8eab\u7684\u65e0\u7f1d\u64cd\u4f5c\u3002", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9636\u6bb5I\u901a\u8fc7\u6570\u5b57\u8bed\u6599\u6ce8\u5165\u5177\u8eab\u77e5\u8bc6\u5e76\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\uff1b\u9636\u6bb5II\u901a\u8fc7\u610f\u56fe\u6865\u63a5\u63a5\u53e3\u8bad\u7ec3\u7b56\u7565\u6a21\u5757\uff0c\u63d0\u53d6MLLM\u7684\u9ad8\u7ea7\u8bed\u4e49\u6307\u5bfc\u63a7\u5236\uff0c\u65e0\u9700\u5fae\u8c03MLLM\u4e3b\u5e72\u3002", "result": "\u5355\u4e2aBLM\u2081\u5b9e\u4f8b\u5728\u6570\u5b57\u4efb\u52a1\u4e2d\u63d0\u5347\u7ea66%\uff0c\u5728\u7269\u7406\u4efb\u52a1\u4e2d\u63d0\u5347\u7ea63%\uff0c\u4f18\u4e8eMLLMs\u3001ELLMs\u3001VLAs\u548cGMLMs\u56db\u4e2a\u6a21\u578b\u5bb6\u65cf\u3002", "conclusion": "BLM\u2081\u6210\u529f\u5b9e\u73b0\u4e86\u8de8\u7a7a\u95f4\u3001\u8de8\u4efb\u52a1\u548c\u8de8\u5177\u8eab\u7684\u7edf\u4e00\u5efa\u6a21\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24166", "abs": "https://arxiv.org/abs/2510.24166", "authors": ["Xin Yang", "Yuhang Zhang", "Wei Li", "Xin Lin", "Wenbin Zou", "Chen Xu"], "title": "UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration", "comment": null, "summary": "Motion planning is a critical component of autonomous vehicle decision-making\nsystems, directly determining trajectory safety and driving efficiency. While\ndeep learning approaches have advanced planning capabilities, existing methods\nremain confined to single-dataset training, limiting their robustness in\nplanning.\n  Through systematic analysis, we discover that vehicular trajectory\ndistributions and history-future correlations demonstrate remarkable\nconsistency across different datasets. Based on these findings, we propose\nUniPlanner, the first planning framework designed for multi-dataset integration\nin autonomous vehicle decision-making. UniPlanner achieves unified\ncross-dataset learning through three synergistic innovations.\n  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates\nhistory-future trajectory pairs from multiple datasets, using historical\ntrajectory similarity to retrieve relevant futures and generate cross-dataset\nplanning guidance.\n  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust\nhistory-future correlations from multiple datasets, transforming historical\ntrajectories into universal planning priors. Its gradient-free design ensures\nthe introduction of valuable priors while preventing shortcut learning, making\nthe planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)\nparadigm implements adaptive dropout to selectively suppress planning priors\nduring training for robust learning, while enabling full prior utilization\nduring inference to maximize planning performance.", "AI": {"tldr": "UniPlanner\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684\u591a\u6570\u636e\u96c6\u96c6\u6210\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5386\u53f2-\u672a\u6765\u8f68\u8ff9\u5b57\u5178\u7f51\u7edc\u3001\u68af\u5ea6\u81ea\u7531\u8f68\u8ff9\u6620\u5c04\u5668\u548c\u7a00\u758f\u5230\u5bc6\u96c6\u8303\u5f0f\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u7edf\u4e00\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u89c4\u5212\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6570\u636e\u96c6\u7684\u8f66\u8f86\u8f68\u8ff9\u5206\u5e03\u548c\u5386\u53f2-\u672a\u6765\u76f8\u5173\u6027\u5177\u6709\u663e\u8457\u4e00\u81f4\u6027\uff0c\u8fd9\u4e3a\u591a\u6570\u636e\u96c6\u96c6\u6210\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "method": "1. HFTDN\uff1a\u805a\u5408\u591a\u6570\u636e\u96c6\u7684\u5386\u53f2-\u672a\u6765\u8f68\u8ff9\u5bf9\uff0c\u57fa\u4e8e\u5386\u53f2\u8f68\u8ff9\u76f8\u4f3c\u6027\u68c0\u7d22\u76f8\u5173\u672a\u6765\u8f68\u8ff9\u751f\u6210\u8de8\u6570\u636e\u96c6\u89c4\u5212\u6307\u5bfc\uff1b2. GFTM\uff1a\u4ece\u591a\u6570\u636e\u96c6\u5b66\u4e60\u9c81\u68d2\u7684\u5386\u53f2-\u672a\u6765\u76f8\u5173\u6027\uff0c\u5c06\u5386\u53f2\u8f68\u8ff9\u8f6c\u6362\u4e3a\u901a\u7528\u89c4\u5212\u5148\u9a8c\uff1b3. S2D\u8303\u5f0f\uff1a\u8bad\u7ec3\u65f6\u9009\u62e9\u6027\u6291\u5236\u89c4\u5212\u5148\u9a8c\u5b9e\u73b0\u9c81\u68d2\u5b66\u4e60\uff0c\u63a8\u7406\u65f6\u5145\u5206\u5229\u7528\u5148\u9a8c\u6700\u5927\u5316\u89c4\u5212\u6027\u80fd\u3002", "result": "UniPlanner\u5b9e\u73b0\u4e86\u8de8\u6570\u636e\u96c6\u7edf\u4e00\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u89c4\u5212\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "conclusion": "UniPlanner\u662f\u9996\u4e2a\u9762\u5411\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684\u591a\u6570\u636e\u96c6\u96c6\u6210\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u534f\u540c\u521b\u65b0\u89e3\u51b3\u4e86\u5355\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24168", "abs": "https://arxiv.org/abs/2510.24168", "authors": ["Weihua Cheng", "Ersheng Ni", "Wenlong Wang", "Yifei Sun", "Junming Liu", "Wangyu Shen", "Yirong Chen", "Botian Shi", "Ding Wang"], "title": "MGA: Memory-Driven GUI Agent for Observation-Centric Interaction", "comment": "Submitted to WWW2025", "summary": "The rapid progress of Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) has enabled agentic systems capable of perceiving and acting\nacross diverse environments. A challenging yet impactful frontier is the\ndevelopment of GUI agents, which must navigate complex desktop and web\ninterfaces while maintaining robustness and generalization. Existing paradigms\ntypically model tasks as long-chain executions, concatenating historical\ntrajectories into the context. While approaches such as Mirage and GTA1 refine\nplanning or introduce multi-branch action selection, they remain constrained by\ntwo persistent issues: Dependence on historical trajectories, which amplifies\nerror propagation. And Local exploration bias, where \"decision-first,\nobservation-later\" mechanisms overlook critical interface cues. We introduce\nthe Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the\nprinciple of observe first, then decide. MGA models each step as an\nindependent, context-rich environment state represented by a triad: current\nscreenshot, task-agnostic spatial information, and a dynamically updated\nstructured memory. Experiments on OSworld benchmarks, real desktop applications\n(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves\nsubstantial gains in robustness, generalization, and efficiency compared to\nstate-of-the-art baselines. The code is publicly available at:\n{https://anonymous.4open.science/r/MGA-3571}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8bb0\u5fc6\u9a71\u52a8\u7684GUI\u4ee3\u7406(MGA)\uff0c\u91c7\u7528\u201c\u5148\u89c2\u5bdf\u540e\u51b3\u7b56\u201d\u539f\u5219\u89e3\u51b3\u73b0\u6709GUI\u4ee3\u7406\u7684\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5386\u53f2\u8f68\u8ff9\u4f9d\u8d56\u548c\u5c40\u90e8\u63a2\u7d22\u504f\u5dee\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4f9d\u8d56\u5386\u53f2\u8f68\u8ff9\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\u653e\u5927\uff0c\u4ee5\u53ca\u201c\u5148\u51b3\u7b56\u540e\u89c2\u5bdf\u201d\u673a\u5236\u5ffd\u89c6\u5173\u952e\u754c\u9762\u7ebf\u7d22\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684GUI\u4ea4\u4e92\u65b9\u6cd5\u3002", "method": "MGA\u5c06GUI\u4ea4\u4e92\u91cd\u6784\u4e3a\u201c\u5148\u89c2\u5bdf\u540e\u51b3\u7b56\u201d\u539f\u5219\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u5efa\u6a21\u4e3a\u72ec\u7acb\u7684\u73af\u5883\u72b6\u6001\u4e09\u5143\u7ec4\uff1a\u5f53\u524d\u622a\u56fe\u3001\u4efb\u52a1\u65e0\u5173\u7684\u7a7a\u95f4\u4fe1\u606f\u548c\u52a8\u6001\u66f4\u65b0\u7684\u7ed3\u6784\u5316\u8bb0\u5fc6\u3002", "result": "\u5728OSworld\u57fa\u51c6\u6d4b\u8bd5\u3001\u771f\u5b9e\u684c\u9762\u5e94\u7528(Chrome\u3001VSCode\u3001VLC)\u548c\u8de8\u4efb\u52a1\u8fc1\u79fb\u5b9e\u9a8c\u4e2d\uff0cMGA\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MGA\u901a\u8fc7\u89c2\u5bdf\u4f18\u5148\u7684\u65b9\u6cd5\u548c\u7ed3\u6784\u5316\u8bb0\u5fc6\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709GUI\u4ee3\u7406\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3aGUI\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24284", "abs": "https://arxiv.org/abs/2510.24284", "authors": ["Wenhao Wang", "Peizhi Niu", "Zhao Xu", "Zhaoyu Chen", "Jian Du", "Yaxin Du", "Xianghe Pang", "Keduan Huang", "Yanfeng Wang", "Qiang Yan", "Siheng Chen"], "title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on external tools to perform\ncomplex, realistic tasks, yet their ability to utilize the rapidly expanding\nModel Contextual Protocol (MCP) ecosystem remains limited. Existing MCP\nresearch covers few servers, depends on costly manual curation, and lacks\ntraining support, hindering progress toward real-world deployment. To overcome\nthese limitations, we introduce MCP-Flow, an automated web-agent-driven\npipeline for large-scale server discovery, data synthesis, and model training.\nMCP-Flow collects and filters data from 1166 servers and 11536 tools, producing\n68733 high-quality instruction-function call pairs and 6439 trajectories, far\nexceeding prior work in scale and diversity. Extensive experiments demonstrate\nMCP-Flow's effectiveness in driving superior MCP tool selection, function-call\ngeneration, and enhanced agentic task performance. MCP-Flow thus provides a\nscalable foundation for advancing LLM agents' proficiency in real-world MCP\nenvironments. MCP-Flow is publicly available at\n\\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.", "AI": {"tldr": "MCP-Flow\u662f\u4e00\u4e2a\u81ea\u52a8\u5316web-agent\u9a71\u52a8\u7684\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u670d\u52a1\u5668\u53d1\u73b0\u3001\u6570\u636e\u5408\u6210\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728MCP\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MCP\u7814\u7a76\u8986\u76d6\u670d\u52a1\u5668\u5c11\u3001\u4f9d\u8d56\u6602\u8d35\u4eba\u5de5\u6574\u7406\u4e14\u7f3a\u4e4f\u8bad\u7ec3\u652f\u6301\uff0c\u963b\u788d\u4e86LLM\u5728\u771f\u5b9e\u4e16\u754cMCP\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316web-agent\u9a71\u52a8\u6d41\u6c34\u7ebf\uff0c\u4ece1166\u4e2a\u670d\u52a1\u5668\u548c11536\u4e2a\u5de5\u5177\u4e2d\u6536\u96c6\u7b5b\u9009\u6570\u636e\uff0c\u751f\u621068733\u4e2a\u9ad8\u8d28\u91cf\u6307\u4ee4-\u51fd\u6570\u8c03\u7528\u5bf9\u548c6439\u6761\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMCP-Flow\u5728MCP\u5de5\u5177\u9009\u62e9\u3001\u51fd\u6570\u8c03\u7528\u751f\u6210\u548c\u4ee3\u7406\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8fdc\u8d85\u5148\u524d\u5de5\u4f5c\u3002", "conclusion": "MCP-Flow\u4e3a\u63d0\u5347LLM\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754cMCP\u73af\u5883\u4e2d\u7684\u719f\u7ec3\u5ea6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.24297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24297", "abs": "https://arxiv.org/abs/2510.24297", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms", "comment": null, "summary": "One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which\ncan be addressed by building and using state and/or action abstractions in\nparallel to the tree search such that information can be shared among nodes of\nthe same layer. The primary usage of abstractions for MCTS is to enhance the\nUpper Confidence Bound (UCB) value during the tree policy by aggregating visits\nand returns of an abstract node. However, this direct usage of abstractions\ndoes not take the case into account where multiple actions with the same parent\nmight be in the same abstract node, as these would then all have the same UCB\nvalue, thus requiring a tiebreak rule. In state-of-the-art abstraction\nalgorithms such as pruned On the Go Abstractions (pruned OGA), this case has\nnot been noticed, and a random tiebreak rule was implicitly chosen. In this\npaper, we propose and empirically evaluate several alternative\nintra-abstraction policies, several of which outperform the random policy\nacross a majority of environments and parameter settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u51e0\u79cd\u66ff\u4ee3\u968f\u673a\u7b56\u7565\u7684\u62bd\u8c61\u5185\u90e8\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3MCTS\u4e2d\u5f53\u591a\u4e2a\u52a8\u4f5c\u5c5e\u4e8e\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u65f6UCB\u503c\u76f8\u540c\u7684\u95ee\u9898\uff0c\u591a\u6570\u7b56\u7565\u5728\u591a\u4e2a\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "motivation": "MCTS\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u72b6\u6001/\u52a8\u4f5c\u62bd\u8c61\u6765\u89e3\u51b3\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982pruned OGA\u5728\u591a\u4e2a\u52a8\u4f5c\u5c5e\u4e8e\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u65f6UCB\u503c\u76f8\u540c\uff0c\u53ea\u80fd\u4f7f\u7528\u968f\u673a\u6253\u7834\u5e73\u5c40\u89c4\u5219\uff0c\u8fd9\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u591a\u79cd\u66ff\u4ee3\u968f\u673a\u7b56\u7565\u7684\u62bd\u8c61\u5185\u90e8\u7b56\u7565\uff0c\u7528\u4e8e\u5904\u7406\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u5185\u591a\u4e2a\u52a8\u4f5c\u7684\u9009\u62e9\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u591a\u79cd\u7b56\u7565\u5728\u5927\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u90fd\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u8bbe\u8ba1\u66f4\u667a\u80fd\u7684\u62bd\u8c61\u5185\u90e8\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347MCTS\u5728\u62bd\u8c61\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u9690\u542b\u7684\u968f\u673a\u6253\u7834\u5e73\u5c40\u95ee\u9898\u3002"}}
{"id": "2510.24299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24299", "abs": "https://arxiv.org/abs/2510.24299", "authors": ["Jiayu Liu", "Wei Dai", "Zhenya Huang", "Ning Miao", "Enhong Chen"], "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank", "comment": null, "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u5185\u90e8\u884c\u4e3a\u7684\u76f8\u5173\u77e9\u9635\u79e9\u4f5c\u4e3a\u63a8\u7406\u8def\u5f84\u53ef\u4fe1\u5ea6\u6307\u6807\u7684\u81ea\u6307\u793a\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u5373\u53ef\u6709\u6548\u9a8c\u8bc1LLM\u63a8\u7406\u7684\u6b63\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\uff08\u5982\u8bad\u7ec3\u9a8c\u8bc1\u5668\u6216\u590d\u6742\u63d0\u793a\uff09\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u901a\u7528\u7684LLM\u8f93\u51fa\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u8f93\u5165\u95ee\u9898\u4e0e\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u4e4b\u95f4\u7684\u76f8\u5173\u77e9\u9635\u79e9\u4f5c\u4e3a\u63a8\u7406\u6b63\u786e\u6027\u7684\u7a33\u5065\u6307\u6807\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u81ea\u6307\u793a\u65b9\u6cd5\u6765\u91cd\u65b0\u52a0\u6743\u5019\u9009\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728\u591a\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u5bb6\u65cf\u7684LLM\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u8d85\u8fc775%\u7684\u51c6\u786e\u7387\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u5c06\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc78%\u3002", "conclusion": "LLM\u7684\u5185\u90e8\u884c\u4e3a\u5df2\u9690\u542b\u5176\u63a8\u7406\u8def\u5f84\u7684\u53ef\u4fe1\u5ea6\u4fe1\u606f\uff0c\u57fa\u4e8e\u76f8\u5173\u77e9\u9635\u79e9\u7684\u81ea\u6307\u793a\u65b9\u6cd5\u80fd\u6709\u6548\u9a8c\u8bc1\u63a8\u7406\u6b63\u786e\u6027\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u4f4e\u3002"}}
{"id": "2510.24303", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24303", "abs": "https://arxiv.org/abs/2510.24303", "authors": ["Deniz Gorur", "Antoni Rago", "Francesca Toni"], "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting", "comment": null, "summary": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u8bba\u65ad\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u540c\u667a\u80fd\u4f53\u5bf9\u8bba\u65ad\u771f\u5b9e\u6027\u8fdb\u884c\u8fa9\u8bba\u5e76\u751f\u6210\u8bc1\u636e\uff0c\u4f7f\u7528\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\u8868\u793a\uff0c\u5b9e\u9a8c\u8868\u660e\u591a\u667a\u80fd\u4f53\u7ec4\u5408\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5c06\u5224\u65ad\u6027\u9884\u6d4b\u89c6\u4e3a\u8bba\u65ad\u9a8c\u8bc1\u4efb\u52a1\uff0c\u9700\u8981\u8bc4\u4f30\u672a\u6765\u4e8b\u4ef6\u7684\u53ef\u80fd\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u591a\u89d2\u5ea6\u8bc1\u636e\u6574\u5408\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u62ecArgLLM\u3001RbAM\u548cRAG-ArgLLM\u4e09\u79cdLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\u8868\u793a\u652f\u6301\u548c\u53cd\u5bf9\u8bc1\u636e\u3002", "result": "\u5728\u6807\u51c6\u5224\u65ad\u6027\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u7279\u522b\u662f\u4e09\u4e2a\u667a\u80fd\u4f53\u7ec4\u5408\u65f6\u80fd\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u6709\u6548\u6574\u5408\u4e0d\u540c\u89d2\u5ea6\u7684\u8bc1\u636e\uff0c\u63d0\u9ad8\u8bba\u65ad\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5224\u65ad\u6027\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.24337", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24337", "abs": "https://arxiv.org/abs/2510.24337", "authors": ["Daria Kravets-Meinke", "Hannah Schmid-Petri", "Sonja Niemann", "Ute Schmid"], "title": "Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research", "comment": null, "summary": "Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly\nbeing used in communication research for content analysis. Studies show that\ngLLMs can outperform both crowd workers and trained coders, such as research\nassistants, on various coding tasks relevant to communication science, often at\na fraction of the time and cost. Additionally, gLLMs can decode implicit\nmeanings and contextual information, be instructed using natural language,\ndeployed with only basic programming skills, and require little to no annotated\ndata beyond a validation dataset - constituting a paradigm shift in automated\ncontent analysis. Despite their potential, the integration of gLLMs into the\nmethodological toolkit of communication research remains underdeveloped. In\ngLLM-assisted quantitative content analysis, researchers must address at least\nseven critical challenges that impact result quality: (1) codebook development,\n(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)\niterative refinement, (6) validation of the model's reliability, and\noptionally, (7) performance enhancement. This paper synthesizes emerging\nresearch on gLLM-assisted quantitative content analysis and proposes a\ncomprehensive best-practice guide to navigate these challenges. Our goal is to\nmake gLLM-based content analysis more accessible to a broader range of\ncommunication researchers and ensure adherence to established disciplinary\nquality standards of validity, reliability, reproducibility, and research\nethics.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f20\u64ad\u5b66\u7814\u7a76\u5185\u5bb9\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u5e94\u5bf97\u4e2a\u5173\u952e\u6311\u6218\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\u3002", "motivation": "\u5c3d\u7ba1gLLMs\u5728\u4f20\u64ad\u5b66\u5185\u5bb9\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u8d85\u8d8a\u4f17\u5305\u5de5\u4f5c\u8005\u548c\u8bad\u7ec3\u6709\u7d20\u7684\u7f16\u7801\u5458\uff0c\u4f46\u5176\u5728\u7814\u7a76\u65b9\u6cd5\u8bba\u4e2d\u7684\u6574\u5408\u4ecd\u7136\u4e0d\u8db3\uff0c\u9700\u8981\u7cfb\u7edf\u6307\u5bfc\u3002", "method": "\u7efc\u5408\u65b0\u5174\u7814\u7a76\uff0c\u63d0\u51fa\u5305\u542b\u4ee3\u7801\u672c\u5f00\u53d1\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u3001\u53c2\u6570\u8c03\u4f18\u3001\u8fed\u4ee3\u4f18\u5316\u3001\u53ef\u9760\u6027\u9a8c\u8bc1\u548c\u6027\u80fd\u63d0\u5347\u76847\u4e2a\u5173\u952e\u6311\u6218\u5e94\u5bf9\u6846\u67b6\u3002", "result": "\u5efa\u7acb\u4e86gLLM\u8f85\u52a9\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\u7684\u7efc\u5408\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u4f7f\u8be5\u65b9\u6cd5\u66f4\u6613\u88ab\u4f20\u64ad\u5b66\u7814\u7a76\u8005\u91c7\u7528\u3002", "conclusion": "\u8be5\u6307\u5357\u65e8\u5728\u8ba9\u57fa\u4e8egLLM\u7684\u5185\u5bb9\u5206\u6790\u66f4\u6613\u88ab\u5e7f\u6cdb\u4f20\u64ad\u5b66\u7814\u7a76\u8005\u4f7f\u7528\uff0c\u5e76\u786e\u4fdd\u7b26\u5408\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u7814\u7a76\u4f26\u7406\u7b49\u5b66\u79d1\u8d28\u91cf\u6807\u51c6\u3002"}}
{"id": "2510.24339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24339", "abs": "https://arxiv.org/abs/2510.24339", "authors": ["Yunxuan Jiang", "Silan Hu", "Xiaoning Wang", "Yuanyuan Zhang", "Xiangyu Chang"], "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation", "comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents", "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.", "AI": {"tldr": "VDSAgents\u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u9884\u6d4b\u6027-\u53ef\u8ba1\u7b97\u6027-\u7a33\u5b9a\u6027(PCS)\u539f\u5219\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u5347LLM\u9a71\u52a8\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u4ec5\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u63a8\u7406\uff0c\u7f3a\u4e4f\u79d1\u5b66\u548c\u7406\u8bba\u539f\u5219\u6307\u5bfc\uff0c\u5728\u5904\u7406\u566a\u58f0\u548c\u590d\u6742\u73b0\u5b9e\u6570\u636e\u96c6\u65f6\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8ePCS\u539f\u5219\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u91c7\u7528\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\u5904\u7406\u6570\u636e\u6e05\u6d17\u3001\u7279\u5f81\u5de5\u7a0b\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\uff0c\u6bcf\u4e2a\u9636\u6bb5\u7531\u4e13\u95e8\u667a\u80fd\u4f53\u8d1f\u8d23\uff0c\u5e76\u6574\u5408\u6270\u52a8\u5206\u6790\u3001\u5355\u5143\u6d4b\u8bd5\u548c\u6a21\u578b\u9a8c\u8bc1\u3002", "result": "\u57289\u4e2a\u4e0d\u540c\u7279\u5f81\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528DeepSeek-V3\u548cGPT-4o\u4f5c\u4e3a\u540e\u7aef\uff0cVDSAgents\u6301\u7eed\u4f18\u4e8eAutoKaggle\u548cDataInterpreter\u7b49\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u5c06PCS\u539f\u5219\u5d4c\u5165LLM\u9a71\u52a8\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\u7684\u53ef\u884c\u6027\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u79d1\u5b66\u53ef\u5ba1\u8ba1\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.24342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24342", "abs": "https://arxiv.org/abs/2510.24342", "authors": ["Silin Chen", "Yuzhong Chen", "Zifan Wang", "Junhao Wang", "Zifeng Jia", "Keith M Kendrick", "Tuo Zhang", "Lin Zhao", "Dezhong Yao", "Tianming Liu", "Xi Jiang"], "title": "A Unified Geometric Space Bridging AI Models and the Human Brain", "comment": null, "summary": "For decades, neuroscientists and computer scientists have pursued a shared\nambition: to understand intelligence and build it. Modern artificial neural\nnetworks now rival humans in language, perception, and reasoning, yet it is\nstill largely unknown whether these artificial systems organize information as\nthe brain does. Existing brain-AI alignment studies have shown the striking\ncorrespondence between the two systems, but such comparisons remain bound to\nspecific inputs and tasks, offering no common ground for comparing how AI\nmodels with different kinds of modalities-vision, language, or multimodal-are\nintrinsically organized. Here we introduce a groundbreaking concept of\nBrain-like Space: a unified geometric space in which every AI model can be\nprecisely situated and compared by mapping its intrinsic spatial attention\ntopological organization onto canonical human functional brain networks,\nregardless of input modality, task, or sensory domain. Our extensive analysis\nof 151 Transformer-based models spanning state-of-the-art large vision models,\nlarge language models, and large multimodal models uncovers a continuous\narc-shaped geometry within this space, reflecting a gradual increase of\nbrain-likeness; different models exhibit distinct distribution patterns within\nthis geometry associated with different degrees of brain-likeness, shaped not\nmerely by their modality but by whether the pretraining paradigm emphasizes\nglobal semantic abstraction and whether the positional encoding scheme\nfacilitates deep fusion across different modalities. Moreover, the degree of\nbrain-likeness for a model and its downstream task performance are not\n\"identical twins\". The Brain-like Space provides the first unified framework\nfor situating, quantifying, and comparing intelligence across domains,\nrevealing the deep organizational principles that bridge machines and the\nbrain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\"\u8111\u76f8\u4f3c\u7a7a\u95f4\"\u7684\u6982\u5ff5\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u51e0\u4f55\u7a7a\u95f4\uff0c\u53ef\u4ee5\u5c06\u4e0d\u540c\u6a21\u6001\u7684AI\u6a21\u578b\u6620\u5c04\u5230\u4eba\u7c7b\u529f\u80fd\u6027\u8111\u7f51\u7edc\u4e0a\u8fdb\u884c\u6bd4\u8f83\uff0c\u63ed\u793a\u4e86AI\u6a21\u578b\u4e0e\u5927\u8111\u7ec4\u7ec7\u4e4b\u95f4\u7684\u8fde\u7eed\u51e0\u4f55\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8111-AI\u5bf9\u9f50\u7814\u7a76\u5c40\u9650\u4e8e\u7279\u5b9a\u8f93\u5165\u548c\u4efb\u52a1\uff0c\u7f3a\u4e4f\u6bd4\u8f83\u4e0d\u540c\u6a21\u6001AI\u6a21\u578b\u5185\u5728\u7ec4\u7ec7\u65b9\u5f0f\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5c06AI\u6a21\u578b\u7684\u5185\u5728\u7a7a\u95f4\u6ce8\u610f\u529b\u62d3\u6251\u7ec4\u7ec7\u6620\u5c04\u5230\u5178\u578b\u4eba\u7c7b\u529f\u80fd\u6027\u8111\u7f51\u7edc\u4e0a\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u8111\u76f8\u4f3c\u7a7a\u95f4\uff0c\u5206\u6790\u4e86151\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u4e2a\u8fde\u7eed\u7684\u5f27\u5f62\u51e0\u4f55\u7ed3\u6784\uff0c\u53cd\u6620\u4e86\u8111\u76f8\u4f3c\u5ea6\u7684\u9010\u6e10\u589e\u52a0\uff1b\u4e0d\u540c\u6a21\u578b\u5728\u8be5\u51e0\u4f55\u4e2d\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u5206\u5e03\u6a21\u5f0f\uff0c\u53d7\u9884\u8bad\u7ec3\u8303\u5f0f\u548c\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u5f71\u54cd\u3002", "conclusion": "\u8111\u76f8\u4f3c\u5ea6\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5e76\u975e\u5b8c\u5168\u4e00\u81f4\uff0c\u8111\u76f8\u4f3c\u7a7a\u95f4\u4e3a\u8de8\u9886\u57df\u667a\u80fd\u7684\u5b9a\u4f4d\u3001\u91cf\u5316\u548c\u6bd4\u8f83\u63d0\u4f9b\u4e86\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2510.24359", "categories": ["cs.AI", "cs.SY", "eess.SY", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24359", "abs": "https://arxiv.org/abs/2510.24359", "authors": ["Pedram Fard", "Alaleh Azhir", "Neguine Rezaii", "Jiazi Tian", "Hossein Estiri"], "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine", "comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535", "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u7528\u4e8eN-of-1\u51b3\u7b56\u652f\u6301\uff0c\u89e3\u51b3\u4f20\u7edf\u533b\u7597AI\u9762\u5411\u5e73\u5747\u60a3\u8005\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e2a\u6027\u5316\u3001\u900f\u660e\u7684\u533b\u7597\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u4f20\u7edf\u533b\u7597AI\u7cfb\u7edf\u4e3a\u5e73\u5747\u60a3\u8005\u8bbe\u8ba1\uff0c\u5728\u7f55\u89c1\u53d8\u5f02\u3001\u591a\u75c5\u5171\u5b58\u548c\u4ee3\u8868\u6027\u4e0d\u8db3\u4eba\u7fa4\u7b49\u8fb9\u7f18\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u516c\u5e73\u6027\u548c\u4fe1\u4efb\u95ee\u9898\u3002", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u6309\u5668\u5b98\u7cfb\u7edf\u3001\u60a3\u8005\u7fa4\u4f53\u548c\u5206\u6790\u6a21\u5f0f\u805a\u7c7b\u667a\u80fd\u4f53\uff0c\u5171\u4eab\u6a21\u578b\u548c\u8bc1\u636e\u5408\u6210\u5de5\u5177\uff0c\u901a\u8fc7\u534f\u8c03\u5c42\u6574\u5408\u7ed3\u679c\u5e76\u8003\u8651\u53ef\u9760\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u6570\u636e\u5bc6\u5ea6\u3002", "result": "\u63d0\u4f9b\u5305\u542b\u98ce\u9669\u4f30\u8ba1\u3001\u7f6e\u4fe1\u533a\u95f4\u3001\u5f02\u5e38\u6807\u5fd7\u548c\u5173\u8054\u8bc1\u636e\u7684\u51b3\u7b56\u652f\u6301\u5305\uff0c\u9a8c\u8bc1\u91cd\u70b9\u4ece\u7fa4\u4f53\u5e73\u5747\u8f6c\u5411\u4e2a\u4f53\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u534f\u8c03\u667a\u80fd\uff0c\u4f7f\u533b\u7597AI\u4e0e\u533b\u5b66\u9996\u8981\u539f\u5219\u4fdd\u6301\u4e00\u81f4\uff1a\u63d0\u4f9b\u900f\u660e\u3001\u516c\u5e73\u4e14\u4ee5\u4e2a\u4f53\u4e3a\u4e2d\u5fc3\u7684\u533b\u7597\u670d\u52a1\u3002"}}
{"id": "2510.24383", "categories": ["cs.AI", "cs.CY", "cs.MA", "I.2.11; I.2.1; I.2.4; K.4.1; K.4.3"], "pdf": "https://arxiv.org/pdf/2510.24383", "abs": "https://arxiv.org/abs/2510.24383", "authors": ["Juraj Mavra\u010di\u0107"], "title": "Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents", "comment": "First published on 19/10/2025. Canonical archived record and DOI:\n  10.5281/zenodo.17391796", "summary": "Policy Cards are introduced as a machine-readable, deployment-layer standard\nfor expressing operational, regulatory, and ethical constraints for AI agents.\nThe Policy Card sits with the agent and enables it to follow required\nconstraints at runtime. It tells the agent what it must and must not do. As\nsuch, it becomes an integral part of the deployed agent. Policy Cards extend\nexisting transparency artifacts such as Model, Data, and System Cards by\ndefining a normative layer that encodes allow/deny rules, obligations,\nevidentiary requirements, and crosswalk mappings to assurance frameworks\nincluding NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can\nbe validated automatically, version-controlled, and linked to runtime\nenforcement or continuous-audit pipelines. The framework enables verifiable\ncompliance for autonomous agents, forming a foundation for distributed\nassurance in multi-agent ecosystems. Policy Cards provide a practical mechanism\nfor integrating high-level governance with hands-on engineering practice and\nenabling accountable autonomy at scale.", "AI": {"tldr": "Policy Cards\u662f\u4e00\u79cd\u673a\u5668\u53ef\u8bfb\u7684\u6807\u51c6\uff0c\u7528\u4e8e\u4e3aAI\u4ee3\u7406\u8868\u8fbe\u64cd\u4f5c\u3001\u76d1\u7ba1\u548c\u4f26\u7406\u7ea6\u675f\uff0c\u4f7f\u5176\u5728\u8fd0\u884c\u65f6\u9075\u5faa\u8981\u6c42\uff0c\u652f\u6301\u81ea\u52a8\u9a8c\u8bc1\u548c\u5408\u89c4\u6027\u8bc1\u660e\u3002", "motivation": "\u73b0\u6709\u900f\u660e\u5ea6\u5de5\u5177\u5982Model Cards\u7b49\u7f3a\u4e4f\u89c4\u8303\u5c42\u6765\u7f16\u7801AI\u4ee3\u7406\u7684\u7ea6\u675f\u89c4\u5219\uff0c\u9700\u8981\u4e00\u79cd\u673a\u5236\u5c06\u9ad8\u5c42\u6cbb\u7406\u4e0e\u5de5\u7a0b\u5b9e\u8df5\u7ed3\u5408\uff0c\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u5408\u89c4\u6027\u3002", "method": "\u63d0\u51faPolicy Cards\u6846\u67b6\uff0c\u5b9a\u4e49\u89c4\u8303\u5c42\u6765\u7f16\u7801\u5141\u8bb8/\u62d2\u7edd\u89c4\u5219\u3001\u4e49\u52a1\u3001\u8bc1\u636e\u8981\u6c42\u548c\u4e0eNIST AI RMF\u3001ISO/IEC 42001\u3001\u6b27\u76dfAI\u6cd5\u6848\u7b49\u4fdd\u8bc1\u6846\u67b6\u7684\u6620\u5c04\u5173\u7cfb\u3002", "result": "Policy Cards\u53ef\u81ea\u52a8\u9a8c\u8bc1\u3001\u7248\u672c\u63a7\u5236\uff0c\u5e76\u4e0e\u8fd0\u884c\u65f6\u6267\u884c\u6216\u6301\u7eed\u5ba1\u8ba1\u7ba1\u9053\u94fe\u63a5\uff0c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u5408\u89c4\u6027\u57fa\u7840\u3002", "conclusion": "Policy Cards\u4e3a\u591a\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5206\u5e03\u5f0f\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u5b9e\u7528\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u53ef\u95ee\u8d23\u7684\u81ea\u4e3b\u6027\u3002"}}
{"id": "2510.24390", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24390", "abs": "https://arxiv.org/abs/2510.24390", "authors": ["Xianjun Gao", "Jianchun Liu", "Hongli Xu", "Liusheng Huang"], "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion", "comment": null, "summary": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies.", "AI": {"tldr": "Orion\u662f\u4e00\u4e2a\u9ad8\u6548\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4f9d\u8d56\u611f\u77e5\u7684\u67e5\u8be2\u5206\u89e3\u548c\u903b\u8f91\u5e76\u884c\u5185\u5bb9\u6269\u5c55\uff0c\u89e3\u51b3\u4e86LLM\u5728\u5b9e\u65f6Web\u5e94\u7528\u4e2d\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u987a\u5e8f\u751f\u6210\u548c\u50f5\u5316\u63a8\u7406\u7b56\u7565\u7684\u9650\u5236\u4e0b\uff0c\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3Web\u670d\u52a1\u5bf9\u9ad8\u8d28\u91cf\u590d\u6742\u63a8\u7406\u548c\u4f4e\u5ef6\u8fdf\u9ad8\u541e\u5410\u91cf\u7684\u53cc\u91cd\u9700\u6c42\u3002", "method": "Orion\u5c06\u67e5\u8be2\u63a8\u7406\u5206\u89e3\u4e3a\u4e24\u4e2a\u534f\u540c\u9636\u6bb5\uff1a\u5173\u952e\u70b9\u751f\u6210\uff08\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u5c11\u6837\u672c\u63d0\u793a\u63d0\u70bc\u903b\u8f91\u7ed3\u6784\u5316\u7684\u5173\u952e\u70b9\uff09\u548c\u5185\u5bb9\u5e76\u884c\u6269\u5c55\uff08\u57fa\u4e8e\u4f9d\u8d56\u56fe\u5e76\u884c\u6269\u5c55\u8fd9\u4e9b\u70b9\u4ee5\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\uff09\uff0c\u5e76\u5f15\u5165\u6d41\u6c34\u7ebf\u8c03\u5ea6\u673a\u5236\u5b9e\u73b0\u8de8\u67e5\u8be2\u5e76\u884c\u3002", "result": "\u5b9e\u9a8c\u663e\u793aOrion\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e864.33\u500d\u7684token\u751f\u6210\u901f\u5ea6\u63d0\u5347\u30013.42\u500d\u7684\u7b54\u6848\u5ef6\u8fdf\u964d\u4f4e\uff0c\u5e76\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u70b9\u95f4\u4f9d\u8d56\u5173\u7cfb\u4f7f\u63a8\u7406\u8d28\u91cf\u63d0\u5347\u9ad8\u8fbe18.75%\u3002", "conclusion": "Orion\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86LLM\u5728Web\u5e94\u7528\u4e2d\u7684\u6548\u7387-\u8d28\u91cf\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5206\u89e3\u548c\u5e76\u884c\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u548c\u6548\u679c\u3002"}}
{"id": "2510.24397", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24397", "abs": "https://arxiv.org/abs/2510.24397", "authors": ["Jiarui Qin", "Yunjia Xi", "Junjie Huang", "Renting Rui", "Di Yin", "Weiwen Liu", "Yong Yu", "Weinan Zhang", "Xing Sun"], "title": "APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training", "comment": "46 pages", "summary": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training.", "AI": {"tldr": "\u63d0\u51fa\u4e86APTBench\u6846\u67b6\uff0c\u5c06\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u4efb\u52a1\u8f6c\u5316\u4e3a\u9002\u5408\u57fa\u7840\u6a21\u578b\u7684\u591a\u9009\u9898\u6216\u6587\u672c\u8865\u5168\u95ee\u9898\uff0c\u7528\u4e8e\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u8bc4\u4f30\u667a\u80fd\u4f53\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u9884\u8bad\u7ec3\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u9759\u6001\u6280\u80fd\uff0c\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u7684\u667a\u80fd\u4f53\u80fd\u529b\uff1b\u800c\u667a\u80fd\u4f53\u57fa\u51c6\u901a\u5e38\u4e3a\u540e\u8bad\u7ec3\u6a21\u578b\u8bbe\u8ba1\uff0c\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u652f\u6301\u591a\u8f6e\u4efb\u52a1\u6267\u884c\u3002\u9700\u8981\u80fd\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u8bc4\u4f30\u667a\u80fd\u4f53\u6f5c\u529b\u7684\u57fa\u51c6\u3002", "method": "\u5c06\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u4efb\u52a1\u548c\u6210\u529f\u8f68\u8ff9\u8f6c\u5316\u4e3a\u591a\u9009\u9898\u6216\u6587\u672c\u8865\u5168\u95ee\u9898\uff0c\u805a\u7126\u89c4\u5212\u548c\u884c\u52a8\u7b49\u6838\u5fc3\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u8986\u76d6\u8f6f\u4ef6\u5de5\u7a0b\u548c\u6df1\u5ea6\u7814\u7a76\u7b49\u5173\u952e\u573a\u666f\u3002", "result": "\u76f8\u6bd4\u901a\u7528\u57fa\u51c6\uff0cAPTBench\u80fd\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4f53\u7684\u4e0b\u6e38\u6027\u80fd\uff0c\u540c\u65f6\u6bd4\u540e\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u66f4\u8f7b\u91cf\u3001\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "APTBench\u586b\u8865\u4e86\u9884\u8bad\u7ec3\u9636\u6bb5\u667a\u80fd\u4f53\u80fd\u529b\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6307\u5bfc\u3002"}}
{"id": "2510.24411", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24411", "abs": "https://arxiv.org/abs/2510.24411", "authors": ["Qiushi Sun", "Mukai Li", "Zhoumianze Liu", "Zhihui Xie", "Fangzhi Xu", "Zhangyue Yin", "Kanzhi Cheng", "Zehao Li", "Zichen Ding", "Qi Liu", "Zhiyong Wu", "Zhuosheng Zhang", "Ben Kao", "Lingpeng Kong"], "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows", "comment": "work in progress", "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86MobileRisk-Live\u52a8\u6001\u6c99\u76d2\u73af\u5883\u548cOS-Sentinel\u6df7\u5408\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u79fb\u52a8AI\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534710%-30%\u3002", "motivation": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u673a\u4ee3\u7406\u5728\u79fb\u52a8\u5e73\u53f0\u4e0a\u5c55\u73b0\u51fa\u7c7b\u4eba\u64cd\u4f5c\u80fd\u529b\uff0c\u4f46\u5176\u6f5c\u5728\u7684\u5b89\u5168\u98ce\u9669\uff08\u5982\u7cfb\u7edf\u7834\u574f\u548c\u9690\u79c1\u6cc4\u9732\uff09\u5f15\u53d1\u4e25\u91cd\u62c5\u5fe7\uff0c\u4e9f\u9700\u5efa\u7acb\u79fb\u52a8\u4ee3\u7406\u5b89\u5168\u7814\u7a76\u57fa\u7840\u3002", "method": "\u6784\u5efaMobileRisk-Live\u52a8\u6001\u6c99\u76d2\u73af\u5883\u548c\u5b89\u5168\u68c0\u6d4b\u57fa\u51c6\uff0c\u63d0\u51faOS-Sentinel\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u5f62\u5f0f\u9a8c\u8bc1\u5668\u68c0\u6d4b\u7cfb\u7edf\u7ea7\u8fdd\u89c4\u548c\u57fa\u4e8eVLM\u7684\u4e0a\u4e0b\u6587\u5224\u65ad\u5668\u8bc4\u4f30\u4e0a\u4e0b\u6587\u98ce\u9669\u3002", "result": "OS-Sentinel\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534710%-30%\uff0c\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684\u81ea\u4e3b\u79fb\u52a8\u4ee3\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u79fb\u52a8\u4ee3\u7406\u5b89\u5168\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u51fa\u7684\u6df7\u5408\u68c0\u6d4b\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u5b89\u5168\u98ce\u9669\uff0c\u4fc3\u8fdb\u66f4\u5b89\u5168\u53ef\u9760\u7684\u81ea\u4e3b\u79fb\u52a8\u4ee3\u7406\u53d1\u5c55\u3002"}}
{"id": "2510.24435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24435", "abs": "https://arxiv.org/abs/2510.24435", "authors": ["Benjamin Grando Moreira"], "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning", "comment": "12 pages", "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86LLMs\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u7684\u56f0\u96be\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u63a8\u8fdb\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u8d85\u8d8a\u4e86\u5355\u7eaf\u7684\u8bed\u8a00\u4efb\u52a1\u8868\u73b0\uff0c\u6d89\u53ca\u7406\u89e3\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4fe1\u606f\u3001\u8fdb\u884c\u63a8\u7406\u5e76\u4ee5\u903b\u8f91\u6709\u6548\u7684\u65b9\u5f0f\u5f97\u51fa\u7ed3\u8bba\u3002", "method": "\u4f7f\u7528\u516b\u4e2a\u81ea\u5b9a\u4e49\u8bbe\u8ba1\u7684\u63a8\u7406\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86GPT\u3001Claude\u3001DeepSeek\u3001Gemini\u3001Grok\u3001Llama\u3001Mistral\u3001Perplexity\u548cSabi'a\u7b49\u591a\u4e2aLLMs\u7684\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u6280\u80fd\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5728\u76f8\u540c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660eLLMs\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u5f85\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u6f14\u7ece\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u660e\u663e\u77ed\u677f\u3002"}}
{"id": "2510.24442", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24442", "abs": "https://arxiv.org/abs/2510.24442", "authors": ["Yiding Wang", "Yuxuan Chen", "Fanxu Meng", "Xifan Chen", "Xiaolei Yang", "Muhan Zhang"], "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents", "comment": null, "summary": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.", "AI": {"tldr": "Law in Silico\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u6cd5\u5f8b\u573a\u666f\u4e2d\u7684\u4e2a\u4f53\u51b3\u7b56\u548c\u5236\u5ea6\u673a\u5236\uff0c\u80fd\u591f\u518d\u73b0\u5b8f\u89c2\u72af\u7f6a\u8d8b\u52bf\u5e76\u4e3a\u5f31\u52bf\u7fa4\u4f53\u6743\u5229\u4fdd\u62a4\u63d0\u4f9b\u89c1\u89e3\u3002", "motivation": "\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u6cd5\u5f8b\u5b9e\u9a8c\u6210\u672c\u9ad8\u6602\u6216\u4e0d\u53ef\u884c\uff0c\u5229\u7528AI\u7cfb\u7edf\u6a21\u62df\u6cd5\u5f8b\u793e\u4f1a\u6210\u4e3a\u9a8c\u8bc1\u548c\u53d1\u5c55\u6cd5\u5f8b\u7406\u8bba\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002LLMs\u51ed\u501f\u5176\u4e16\u754c\u77e5\u8bc6\u548c\u89d2\u8272\u626e\u6f14\u80fd\u529b\uff0c\u662f\u6784\u5efa\u6cd5\u5f8b\u793e\u4f1a\u6a21\u62df\u7684\u5f3a\u6709\u529b\u5019\u9009\u8005\u3002", "method": "\u5f15\u5165Law in Silico\u6846\u67b6\uff0c\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6a21\u62df\u6cd5\u5f8b\u573a\u666f\uff0c\u5305\u62ec\u4e2a\u4f53\u51b3\u7b56\u548c\u7acb\u6cd5\u3001\u88c1\u51b3\u3001\u6267\u6cd5\u7b49\u5236\u5ea6\u673a\u5236\u3002\u901a\u8fc7\u6bd4\u8f83\u6a21\u62df\u72af\u7f6a\u7387\u4e0e\u73b0\u5b9e\u6570\u636e\u6765\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u80fd\u591f\u5f88\u5927\u7a0b\u5ea6\u4e0a\u518d\u73b0\u5b8f\u89c2\u5c42\u9762\u7684\u72af\u7f6a\u8d8b\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e0e\u73b0\u5b9e\u89c2\u5bdf\u4e00\u81f4\u7684\u89c1\u89e3\u3002\u5fae\u89c2\u5c42\u9762\u6a21\u62df\u663e\u793a\uff0c\u529f\u80fd\u826f\u597d\u3001\u900f\u660e\u4e14\u9002\u5e94\u6027\u7684\u6cd5\u5f8b\u7cfb\u7edf\u80fd\u66f4\u597d\u5730\u4fdd\u62a4\u5f31\u52bf\u4e2a\u4f53\u7684\u6743\u5229\u3002", "conclusion": "LLM-based agents can effectively simulate legal systems, reproducing macro-level crime trends and offering insights that align with real-world observations, while highlighting the importance of well-functioning legal systems for protecting vulnerable individuals."}}
{"id": "2510.24461", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24461", "abs": "https://arxiv.org/abs/2510.24461", "authors": ["Korneel Van den Berghe", "Stein Stroobants", "Vijay Janapa Reddi", "G. C. H. E. de Croon"], "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks", "comment": null, "summary": "Neuromorphic computing systems are set to revolutionize energy-constrained\nrobotics by achieving orders-of-magnitude efficiency gains, while enabling\nnative temporal processing. Spiking Neural Networks (SNNs) represent a\npromising algorithmic approach for these systems, yet their application to\ncomplex control tasks faces two critical challenges: (1) the non-differentiable\nnature of spiking neurons necessitates surrogate gradients with unclear\noptimization properties, and (2) the stateful dynamics of SNNs require training\non sequences, which in reinforcement learning (RL) is hindered by limited\nsequence lengths during early training, preventing the network from bridging\nits warm-up period.\n  We address these challenges by systematically analyzing surrogate gradient\nslope settings, showing that shallower slopes increase gradient magnitude in\ndeeper layers but reduce alignment with true gradients. In supervised learning,\nwe find no clear preference for fixed or scheduled slopes. The effect is much\nmore pronounced in RL settings, where shallower slopes or scheduled slopes lead\nto a 2.1x improvement in both training and final deployed performance. Next, we\npropose a novel training approach that leverages a privileged guiding policy to\nbootstrap the learning process, while still exploiting online environment\ninteractions with the spiking policy. Combining our method with an adaptive\nslope schedule for a real-world drone position control task, we achieve an\naverage return of 400 points, substantially outperforming prior techniques,\nincluding Behavioral Cloning and TD3BC, which achieve at most --200 points\nunder the same conditions. This work advances both the theoretical\nunderstanding of surrogate gradient learning in SNNs and practical training\nmethodologies for neuromorphic controllers demonstrated in real-world robotic\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\u548c\u5f15\u5165\u5f15\u5bfc\u7b56\u7565\u6765\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\uff0c\u5728\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u80fd\u6548\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u8109\u51b2\u795e\u7ecf\u5143\u7684\u4e0d\u53ef\u5fae\u7279\u6027\u9700\u8981\u4f7f\u7528\u66ff\u4ee3\u68af\u5ea6\uff0c\u4ee5\u53ca\u72b6\u6001\u52a8\u6001\u9700\u8981\u5e8f\u5217\u8bad\u7ec3\uff0c\u8fd9\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u53d7\u5230\u65e9\u671f\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u9650\u5236\u7684\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\uff0c\u53d1\u73b0\u8f83\u6d45\u659c\u7387\u80fd\u589e\u52a0\u6df1\u5c42\u68af\u5ea6\u5e45\u5ea6\u4f46\u51cf\u5c11\u4e0e\u771f\u5b9e\u68af\u5ea6\u7684\u5bf9\u9f50\uff1b\u63d0\u51fa\u5229\u7528\u7279\u6743\u5f15\u5bfc\u7b56\u7565\u6765\u5f15\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73af\u5883\u7684\u5728\u7ebf\u4ea4\u4e92\uff1b\u7ed3\u5408\u81ea\u9002\u5e94\u659c\u7387\u8c03\u5ea6\u65b9\u6cd5\u3002", "result": "\u5728\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u8f83\u6d45\u659c\u7387\u6216\u8c03\u5ea6\u659c\u7387\u4f7f\u8bad\u7ec3\u548c\u6700\u7ec8\u90e8\u7f72\u6027\u80fd\u63d0\u53472.1\u500d\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u56de\u62a5\u8fbe\u5230400\u70b9\uff0c\u663e\u8457\u4f18\u4e8e\u884c\u4e3a\u514b\u9686\u548cTD3BC\u7b49\u65b9\u6cd5\uff08\u6700\u591a-200\u70b9\uff09\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u5bf9SNN\u4e2d\u66ff\u4ee3\u68af\u5ea6\u5b66\u4e60\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5e76\u5f00\u53d1\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u6f14\u793a\u7684\u5b9e\u7528\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u5668\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2510.24528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24528", "abs": "https://arxiv.org/abs/2510.24528", "authors": ["Zihan Chen", "Song Wang", "Xingbo Fu", "Chengshuai Shi", "Zhenyu Lei", "Cong Shen", "Jundong Li"], "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning", "comment": null, "summary": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u901a\u8fc7\u4ea4\u53c9\u4efb\u52a1\u793a\u4f8b\u548c\u57fa\u4e8e\u56fe\u7684\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9LLM\u6570\u636e\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u6784\u5efa\u9ad8\u8d28\u91cfICL\u6f14\u793a\u3002", "motivation": "\u4e3a\u65b0\u9896\u6216\u56f0\u96be\u4efb\u52a1\u6536\u96c6\u9ad8\u8d28\u91cf\u793a\u4f8b\u6210\u672c\u9ad8\u6602\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u9700\u8981\u51cf\u5c11\u5bf9LLM\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "method": "\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u5229\u7528\u4ea4\u53c9\u4efb\u52a1\u793a\u4f8b\u63d0\u793aLLM\u4f2a\u6807\u6ce8\u5c11\u91cf\u76ee\u6807\u5b9e\u4f8b\uff1b2) \u5f15\u5165\u57fa\u4e8e\u56fe\u7684\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916LLM\u67e5\u8be2\u5373\u53ef\u5c06\u6807\u7b7e\u4fe1\u606f\u4f20\u64ad\u5230\u5176\u4f59\u76ee\u6807\u793a\u4f8b\u3002", "result": "\u5728\u4e94\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "\u8be5\u7ba1\u9053\u7ed3\u5408\u4e86\u4ea4\u53c9\u4efb\u52a1\u76d1\u7763\u7684\u7075\u6d3b\u6027\u548c\u65e0\u9700LLM\u4f20\u64ad\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3aICL\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24551", "abs": "https://arxiv.org/abs/2510.24551", "authors": ["Gang Chen", "Changshuo Liu", "Gene Anne Ooi", "Marcus Tan", "Zhongle Xie", "Jianwei Yin", "James Wei Luen Yip", "Wenqiao Zhang", "Jiaqi Zhu", "Beng Chin Ooi"], "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u6765\u8bbe\u8ba1\u548c\u90e8\u7f72\u533b\u7597\u9886\u57df\u7684\u751f\u6210\u5f0fAI\u7cfb\u7edf\uff0c\u5c06\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u4f5c\u4e3a\u57fa\u7840\uff0c\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u7684\u96c6\u6210\u3001\u8868\u793a\u548c\u68c0\u7d22\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u533b\u7597\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u6df1\u5165\u7406\u89e3\u533b\u7597\u4efb\u52a1\u548c\u53ef\u5b9e\u73b0\u7684\u8303\u56f4\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u6570\u636e\u7ba1\u7406\u6765\u652f\u6301AI\u7cfb\u7edf\u7684\u6709\u6548\u90e8\u7f72\u3002", "method": "\u91cd\u65b0\u5b9a\u4f4d\u6570\u636e\u751f\u547d\u5468\u671f\uff0c\u6784\u5efa\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u4f5c\u4e3a\u751f\u6210\u5f0f\u533b\u7597\u7cfb\u7edf\u7684\u57fa\u7840\u3002\u901a\u8fc7\u8bed\u4e49\u5411\u91cf\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u67e5\u8be2\u7b49\u9ad8\u6548\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u652f\u6301\u4e0a\u6e38\u6a21\u578b\u7ec4\u4ef6\u548c\u4e0b\u6e38\u4e34\u5e8a\u5e94\u7528\u3002", "result": "\u8be5\u751f\u6001\u7cfb\u7edf\u4e0d\u4ec5\u4e3a\u57fa\u5ea7\u6a21\u578b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u7528\u4e8e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0c\u8fd8\u4f5c\u4e3a\u77e5\u8bc6\u68c0\u7d22\u540e\u7aef\u901a\u8fc7\u4ee3\u7406\u5c42\u652f\u6301\u4efb\u52a1\u7279\u5b9a\u63a8\u7406\u3002", "conclusion": "\u8be5\u6570\u636e\u4e2d\u5fc3\u7684\u8303\u5f0f\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u6709\u6548\u7684\u533b\u7597\u751f\u6210\u5f0fAI\u90e8\u7f72\uff0c\u6539\u5584\u533b\u7597\u670d\u52a1\u4ea4\u4ed8\u3002"}}
{"id": "2510.24645", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24645", "abs": "https://arxiv.org/abs/2510.24645", "authors": ["Zengzhuang Xu", "Bingguang Hao", "Zechuan Wang", "Yuntao Wen", "Maolin Wang", "Yang Liu", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Chenyi Zhuang", "Jinjie Gu", "Leilei Gan", "Xiangyu Zhao", "Shi Gu"], "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling", "comment": null, "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.", "AI": {"tldr": "FunReason-MT\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u6570\u636e\u5408\u6210\u7684\u521b\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u73af\u5883-API\u56fe\u4ea4\u4e92\u3001\u9ad8\u7ea7\u5de5\u5177\u67e5\u8be2\u5408\u6210\u548c\u5f15\u5bfc\u8fed\u4ee3\u94fe\u6765\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u7684\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff08\u5982\u968f\u673a\u73af\u5883\u91c7\u6837\u6216\u591a\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\uff09\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u5b58\u5728\u76ee\u6807\u6a21\u578b\u8bad\u7ec3\u3001\u5de5\u5177\u67b6\u6784\u9694\u79bb\u548c\u591a\u8f6e\u903b\u8f91\u4f9d\u8d56\u4e09\u5927\u6311\u6218\u3002", "method": "\u91c7\u7528\u73af\u5883-API\u56fe\u4ea4\u4e92\u6536\u96c6\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u9ad8\u7ea7\u5de5\u5177\u67e5\u8be2\u5408\u6210\u7b80\u5316\u56f0\u96be\u67e5\u8be2\u6784\u5efa\uff0c\u4ee5\u53ca\u5f15\u5bfc\u8fed\u4ee3\u94fe\u751f\u6210\u590d\u6742\u601d\u7ef4\u94fe\u3002", "result": "\u5728Berkeley\u51fd\u6570\u8c03\u7528\u6392\u884c\u699c\u4e0a\uff0c\u57fa\u4e8eFunReason-MT\u751f\u6210\u6570\u636e\u8bad\u7ec3\u76844B\u6a21\u578b\u5728\u540c\u7b49\u89c4\u6a21\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u5927\u591a\u6570\u95ed\u6e90\u6a21\u578b\uff0c\u5e76\u5728BFCLv4\u4e0a\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "FunReason-MT\u4e3a\u667a\u80fd\u4f53\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u5f3a\u5927\u7684\u6570\u636e\u6e90\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u6570\u636e\u5408\u6210\u7684\u7ed3\u6784\u6027\u95ee\u9898\u3002"}}
{"id": "2510.24650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24650", "abs": "https://arxiv.org/abs/2510.24650", "authors": ["Nitin Rai", "Daeun", "Choi", "Nathan S. Boyd", "Arnold W. Schumann"], "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning", "comment": "26 pages, 8 figures, and 2 tables", "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u7684\u4f5c\u7269\u7cbe\u51c6\u75c5\u5bb3\u7ba1\u7406\uff08SSDM\uff09\u7814\u7a76\u8fdb\u5c55\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\u4e0e\u53d1\u5c55\u8d8b\u52bf\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f5c\u7269\u75c5\u5bb3\u7ba1\u7406\u4ece\u624b\u5de5\u7279\u5f81\u63d0\u53d6\u53d1\u5c55\u5230\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u7279\u5f81\u5b66\u4e60\u3002\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\uff0c\u80fd\u591f\u89e3\u91ca\u75c7\u72b6\u6587\u672c\u3001\u63a8\u7406\u75c7\u72b6\u4e0e\u6cbb\u7406\u5173\u7cfb\uff0c\u5e76\u4e3a\u79cd\u690d\u8005\u548c\u6559\u80b2\u8005\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u95ee\u7b54\u652f\u6301\u3002", "method": "\u901a\u8fc7\u7b5b\u9009\u7ea640\u7bc7\u5173\u4e8e\u57fa\u7840\u6a21\u578b\u5728\u7cbe\u51c6\u75c5\u5bb3\u7ba1\u7406\u4e2d\u5e94\u7528\u7684\u6587\u732e\uff0c\u91cd\u70b9\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u8ba8\u8bba\u5b83\u4eec\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\uff1a(a) \u57fa\u7840\u6a21\u578b\u57282023-24\u5e74\u6587\u732e\u6fc0\u589e\uff1b(b) \u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u5feb\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u51fa\u7248\u7269\u6570\u91cf\u589e\u52a05-10\u500d\uff1b(c) \u5f3a\u5316\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u5728\u667a\u80fd\u55b7\u6d12\u4e2d\u4ecd\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5\uff1b(d) \u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u5b57\u5b6a\u751f\u53ef\u865a\u62df\u6a21\u62df\u7cbe\u51c6\u55b7\u6d12\uff1b(e) \u89e3\u51b3\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u5bf9\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff1b(f) \u4eba\u673a\u534f\u4f5c\u4ecd\u7136\u6709\u9650\uff1b(g) \u5177\u6709\u5b9e\u65f6\u53cd\u9988\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5c06\u63a8\u52a8\u4e0b\u4e00\u4ee3\u7cbe\u51c6\u75c5\u5bb3\u7ba1\u7406\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u6b63\u5728\u6539\u53d8\u4f5c\u7269\u75c5\u5bb3\u6570\u636e\u5904\u7406\u65b9\u5f0f\uff0c\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0e\u5b9e\u65f6\u53cd\u9988\u5c06\u63a8\u52a8\u4e0b\u4e00\u4ee3\u7cbe\u51c6\u75c5\u5bb3\u7ba1\u7406\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u548c\u4eba\u673a\u534f\u4f5c\u7b49\u6311\u6218\u3002"}}
{"id": "2510.24663", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24663", "abs": "https://arxiv.org/abs/2510.24663", "authors": ["Yifu Lu", "Shengjie Liu", "Li Dong"], "title": "OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs", "comment": "9 pages, 4 figures", "summary": "Agentic tool use has gained traction with the rise of agentic tool calling,\nyet most existing work overlooks the complexity of multi-turn tool\ninteractions. We introduce OrchDAG, a synthetic data generation pipeline that\nmodels tool execution as directed acyclic graphs (DAGs) with controllable\ncomplexity. Using this dataset, we benchmark model performance and propose a\ngraph-based reward to enhance RLVR training. Experiments show that the dataset\npresents a challenging but solvable benchmark, and the proposed reward is\neffective when combined with GRPO-style algorithms, highlighting the importance\nof leveraging topological structure and data complexity in multi-turn tool use.", "AI": {"tldr": "\u63d0\u51fa\u4e86OrchDAG\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u5177\u6709\u53ef\u63a7\u590d\u6742\u5ea6\u7684\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u6765\u589e\u5f3aRLVR\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u667a\u80fd\u4f53\u5de5\u5177\u4f7f\u7528\u7814\u7a76\u5927\u591a\u5ffd\u89c6\u4e86\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u79cd\u590d\u6742\u573a\u666f\u3002", "method": "\u5f15\u5165OrchDAG\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff1b\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u673a\u5236\uff0c\u7ed3\u5408GRPO\u98ce\u683c\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u4f46\u53ef\u89e3\u51b3\u7684\u57fa\u51c6\uff0c\u6240\u63d0\u51fa\u7684\u5956\u52b1\u673a\u5236\u5728\u7ed3\u5408GRPO\u7b97\u6cd5\u65f6\u8868\u73b0\u6709\u6548\u3002", "conclusion": "\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4e2d\uff0c\u5229\u7528\u62d3\u6251\u7ed3\u6784\u548c\u6570\u636e\u590d\u6742\u6027\u81f3\u5173\u91cd\u8981\uff0c\u56fe\u7ed3\u6784\u5efa\u6a21\u80fd\u591f\u6709\u6548\u63d0\u5347\u667a\u80fd\u4f53\u5de5\u5177\u4ea4\u4e92\u6027\u80fd\u3002"}}
{"id": "2510.24690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24690", "abs": "https://arxiv.org/abs/2510.24690", "authors": ["Shengjie Liu", "Li Dong", "Zhenyu Zhang"], "title": "Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning", "comment": "4 pages, 2 figures, short paper, NeurIPS 2025 workshop on Bridging\n  Language, Agent, and World Models for Reasoning and Planning", "summary": "We present a framework for uncovering and exploiting dependencies among tools\nand documents to enhance exemplar artifact generation. Our method begins by\nconstructing a tool knowledge graph from tool schemas,including descriptions,\narguments, and output payloads, using a DeepResearch-inspired analysis. In\nparallel, we derive a complementary knowledge graph from internal documents and\nSOPs, which is then fused with the tool graph. To generate exemplar plans, we\nadopt a deep-sparse integration strategy that aligns structural tool\ndependencies with procedural knowledge. Experiments demonstrate that this\nunified framework effectively models tool interactions and improves plan\ngeneration, underscoring the benefits of linking tool graphs with domain\nknowledge graphs for tool-augmented reasoning and planning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\u548c\u6587\u6863\u77e5\u8bc6\u56fe\u8c31\u7684\u878d\u5408\uff0c\u6765\u589e\u5f3a\u8303\u4f8b\u5de5\u4ef6\u7684\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u5de5\u5177\u4f7f\u7528\u548c\u6587\u6863\u77e5\u8bc6\u5f80\u5f80\u662f\u5206\u79bb\u7684\uff0c\u7f3a\u4e4f\u5bf9\u5de5\u5177\u95f4\u4f9d\u8d56\u5173\u7cfb\u548c\u9886\u57df\u77e5\u8bc6\u7684\u7edf\u4e00\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u8303\u4f8b\u8ba1\u5212\u7684\u751f\u6210\u8d28\u91cf\u3002", "method": "\u4ece\u5de5\u5177\u6a21\u5f0f\u6784\u5efa\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ece\u5185\u90e8\u6587\u6863\u548cSOP\u6784\u5efa\u6587\u6863\u77e5\u8bc6\u56fe\u8c31\uff0c\u7136\u540e\u5c06\u4e24\u8005\u878d\u5408\uff0c\u91c7\u7528\u6df1\u5ea6\u7a00\u758f\u96c6\u6210\u7b56\u7565\u5bf9\u9f50\u5de5\u5177\u7ed3\u6784\u4f9d\u8d56\u548c\u7a0b\u5e8f\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u7edf\u4e00\u6846\u67b6\u80fd\u6709\u6548\u5efa\u6a21\u5de5\u5177\u4ea4\u4e92\u5e76\u6539\u8fdb\u8ba1\u5212\u751f\u6210\u3002", "conclusion": "\u5c06\u5de5\u5177\u56fe\u8c31\u4e0e\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u5bf9\u4e8e\u5de5\u5177\u589e\u5f3a\u7684\u63a8\u7406\u548c\u89c4\u5212\u5177\u6709\u663e\u8457\u76ca\u5904\u3002"}}
