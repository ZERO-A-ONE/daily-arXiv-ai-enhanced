<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [ExplainableGuard: Interpretable Adversarial Defense for Large Language Models Using Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.13771)
*Shaowei Guan,Yu Zhai,Zhengyu Zhang,Yanze Wang,Hin Chi Kwok*

Main category: cs.CR

TL;DR: 本文提出了ExplainableGuard框架，利用DeepSeek-Reasoner的思维链推理能力来检测和中和文本中的对抗性扰动，并提供逐步解释。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到对抗性攻击，现有防御机制多为黑盒，缺乏决策透明度。

Method: 使用定制化的思维链提示引导LLM进行多层面分析（字符、词汇、结构和语义），生成净化输出和人类可读的正当化解释。

Result: 在GLUE基准和IMDB电影评论数据集上显示出有前景的防御效果，人类评估显示解释在清晰度、特异性和可操作性方面优于消融变体，部署可信度评分为72.5%。

Conclusion: ExplainableGuard框架具有构建更可信LLM部署的潜力。

Abstract: Large Language Models (LLMs) are increasingly vulnerable to adversarial attacks that can subtly manipulate their outputs. While various defense mechanisms have been proposed, many operate as black boxes, lacking transparency in their decision-making. This paper introduces ExplainableGuard, an interpretable adversarial defense framework leveraging the chain-of-thought (CoT) reasoning capabilities of DeepSeek-Reasoner. Our approach not only detects and neutralizes adversarial perturbations in text but also provides step-by-step explanations for each defense action. We demonstrate how tailored CoT prompts guide the LLM to perform a multi-faceted analysis (character, word, structural, and semantic) and generate a purified output along with a human-readable justification. Preliminary results on the GLUE Benchmark and IMDB Movie Reviews dataset show promising defense efficacy. Additionally, a human evaluation study reveals that ExplainableGuard's explanations outperform ablated variants in clarity, specificity, and actionability, with a 72.5% deployability-trust rating, underscoring its potential for more trustworthy LLM deployments.

</details>


### [2] [Hashpower allocation in Pay-per-Share blockchain mining pools](https://arxiv.org/abs/2511.13777)
*Pierre-Olivier Goffard,Hansjoerg Albrecher,Jean-Pierre Fouque*

Main category: cs.CR

TL;DR: 本文研究比特币挖矿中的风险管理和矿池选择策略，分析PPS奖励系统中矿工如何在风险转移和管理费之间权衡，以最优分配计算资源。


<details>
  <summary>Details</summary>
Motivation: 比特币挖矿采用工作量证明协议，矿工面临持续的运营成本和不确定的收益风险。加入矿池是常见的风险缓解策略，但需要研究矿工如何在不同的矿池间分配计算资源。

Method: 使用简化的矿工财富模型，研究PPS奖励系统，分析矿池管理者如何调整份额难度和管理费，以及矿工如何在不同矿池间分配计算资源。

Result: 研究发现矿工需要在风险转移给矿池管理者和支付管理费之间进行权衡，以优化计算资源的分配策略。

Conclusion: 矿工应综合考虑风险转移和管理费成本，在不同矿池间合理分配计算资源，以实现收益最大化和风险最小化。

Abstract: Mining blocks in a blockchain using the \textit{Proof-of-Work} consensus protocol involves significant risk, as network participants face continuous operational costs while earning infrequent capital gains upon successfully mining a block. A common risk mitigation strategy is to join a mining pool, which combines the computing resources of multiple miners to provide a more stable income. This article examines a Pay-per-Share (PPS) reward system, where the pool manager can adjust both the share difficulty and the management fee. Using a simplified wealth model for miners, we explore how miners should allocate their computing resources among different mining pools, considering the trade-off between risk transfer to the manager and management fees.

</details>


### [3] [Human-Centered Threat Modeling in Practice: Lessons, Challenges, and Paths Forward](https://arxiv.org/abs/2511.13781)
*Warda Usman,Yixin Zou,Daniel Zappala*

Main category: cs.CR

TL;DR: 本文通过23个半结构化访谈研究了以人为中心的威胁建模(HCTM)实践现状，发现HCTM不是规定性过程而是一套受多种因素影响的演化实践，研究人员面临情感压力、伦理困境和结构障碍等挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然研究人员越来越多地从以人为中心的视角进行威胁建模，但对其在实践中如何准备和参与HCTM知之甚少，需要了解研究人员如何设计研究、识别威胁并处理价值观、约束和长期目标。

Method: 对23名研究人员进行半结构化访谈，考察HCTM的现状，包括研究设计、威胁识别以及价值观、约束和长期目标的处理方式。

Result: 发现HCTM是一套受参与者关系、学科背景和制度结构影响的演化实践，研究人员通过持续的基础工作和以参与者为中心的调查进行威胁建模，面临情感压力、伦理困境和结构障碍等挑战。

Conclusion: 提出了通过共享基础设施、更广泛认可多样化贡献以及更强有力的将研究成果转化为政策、设计和社会变革的机制来推进HCTM的机会。

Abstract: Human-centered threat modeling (HCTM) is an emerging area within security and privacy research that focuses on how people define and navigate threats in various social, cultural, and technological contexts. While researchers increasingly approach threat modeling from a human-centered perspective, little is known about how they prepare for and engage with HCTM in practice. In this work, we conduct 23 semi-structured interviews with researchers to examine the state of HCTM, including how researchers design studies, elicit threats, and navigate values, constraints, and long-term goals. We find that HCTM is not a prescriptive process but a set of evolving practices shaped by relationships with participants, disciplinary backgrounds, and institutional structures. Researchers approach threat modeling through sustained groundwork and participant-centered inquiry, guided by values such as care, justice, and autonomy. They also face challenges including emotional strain, ethical dilemmas, and structural barriers that complicate efforts to translate findings into real-world impact. We conclude by identifying opportunities to advance HCTM through shared infrastructure, broader recognition of diverse contributions, and stronger mechanisms for translating findings into policy, design, and societal change.

</details>


### [4] [Uncovering and Aligning Anomalous Attention Heads to Defend Against NLP Backdoor Attacks](https://arxiv.org/abs/2511.13789)
*Haotian Jin,Yang Li,Haihui Fan,Lin Shen,Xiangfang Li,Bo Li*

Main category: cs.CR

TL;DR: 提出了一种基于注意力相似性的后门检测方法，无需先验知识即可检测大语言模型中的后门攻击，并通过注意力安全对齐和逐头微调来缓解攻击影响。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对LLMs安全构成严重威胁，现有防御方法局限于特定触发器类型或依赖额外干净模型支持，需要一种无需触发器先验知识的通用检测方法。

Method: 基于注意力相似性检测后门攻击，发现受攻击模型在触发条件下注意力头间相似度异常高，采用注意力安全对齐和逐头微调来修正受污染的注意力头。

Result: 实验结果表明该方法显著降低了后门攻击成功率，同时保持了模型在下游任务上的性能。

Conclusion: 提出的基于注意力相似性的方法能够有效检测和缓解LLMs中的后门攻击，无需触发器先验知识，具有实用价值。

Abstract: Backdoor attacks pose a serious threat to the security of large language models (LLMs), causing them to exhibit anomalous behavior under specific trigger conditions. The design of backdoor triggers has evolved from fixed triggers to dynamic or implicit triggers. This increased flexibility in trigger design makes it challenging for defenders to identify their specific forms accurately. Most existing backdoor defense methods are limited to specific types of triggers or rely on an additional clean model for support. To address this issue, we propose a backdoor detection method based on attention similarity, enabling backdoor detection without prior knowledge of the trigger. Our study reveals that models subjected to backdoor attacks exhibit unusually high similarity among attention heads when exposed to triggers. Based on this observation, we propose an attention safety alignment approach combined with head-wise fine-tuning to rectify potentially contaminated attention heads, thereby effectively mitigating the impact of backdoor attacks. Extensive experimental results demonstrate that our method significantly reduces the success rate of backdoor attacks while preserving the model's performance on downstream tasks.

</details>


### [5] [Zipf-Gramming: Scaling Byte N-Grams Up to Production Sized Malware Corpora](https://arxiv.org/abs/2511.13808)
*Edward Raff,Ryan R. Curtin,Derek Everett,Robert J. Joyce,James Holt*

Main category: cs.CR

TL;DR: 提出了一种名为Zipf-Gramming的新算法，用于高效提取字节n-gram特征，比现有最佳方法快35倍，使得在恶意软件检测中能够部署定期更新的模型，AUC提升达30%。


<details>
  <summary>Details</summary>
Motivation: 现有的字节n-gram分类器在恶意软件检测中具有快速、低延迟的优势，但由于计算成本过高，无法定期更新模型。需要一种高效的方法来从TB级可执行程序中提取最频繁的n-gram。

Method: 利用Zipf分布的性质开发新的top-k n-gram提取器Zipf-Gramming算法，通过理论和工程结合实现高效的特征提取。

Result: 新算法比之前最佳方法快35倍，能够扩展生产训练集，在检测新恶意软件时AUC提升达30%。

Conclusion: Zipf-Gramming算法通过利用Zipf分布特性，成功解决了大规模n-gram提取的效率问题，为恶意软件检测系统的实时更新提供了可行方案。

Abstract: A classifier using byte n-grams as features is the only approach we have found fast enough to meet requirements in size (sub 2 MB), speed (multiple GB/s), and latency (sub 10 ms) for deployment in numerous malware detection scenarios. However, we've consistently found that 6-8 grams achieve the best accuracy on our production deployments but have been unable to deploy regularly updated models due to the high cost of finding the top-k most frequent n-grams over terabytes of executable programs. Because the Zipfian distribution well models the distribution of n-grams, we exploit its properties to develop a new top-k n-gram extractor that is up to $35\times$ faster than the previous best alternative. Using our new Zipf-Gramming algorithm, we are able to scale up our production training set and obtain up to 30\% improvement in AUC at detecting new malware. We show theoretically and empirically that our approach will select the top-k items with little error and the interplay between theory and engineering required to achieve these results.

</details>


### [6] [The Battle of Metasurfaces: Understanding Security in Smart Radio Environments](https://arxiv.org/abs/2511.13939)
*Paul Staat,Christof Paar,Swarun Kumar*

Main category: cs.CR

TL;DR: 本文首次系统研究了对称场景下竞争性超表面的相互作用及其对无线安全的影响，通过理论和实验分析发现对抗性超表面可以相互抵消效果，揭示了超表面"战斗"结果取决于时机、位置、算法策略和硬件规模等因素。


<details>
  <summary>Details</summary>
Motivation: 超表面技术将传统被动无线电环境转变为智能可编程介质，为通信和感知带来进步，但也创造了新的安全前沿。先前研究主要探索单边超表面应用，而本研究关注双方都拥有相当超表面能力的对称场景。

Method: 采用理论建模和真实世界实验相结合的方法，分析竞争性超表面在不同目标（包括信号功率和感知感知）下的相互作用，并在Wi-Fi环境中进行多个案例研究。

Result: 研究结果表明，对抗性超表面可以显著或完全抵消彼此的效果，通过破坏先前提出的安全和隐私方案，揭示了超表面"战斗"结果取决于时机、位置、算法策略和硬件规模的相互作用。

Conclusion: 这些发现为在智能无线电环境中设计弹性和高保证的物理层系统开辟了新机会，强调了在对称超表面场景下重新思考无线安全方案的必要性。

Abstract: Metasurfaces, or Reconfigurable Intelligent Surfaces (RISs), have emerged as a transformative technology for next-generation wireless systems, enabling digitally controlled manipulation of electromagnetic wave propagation. By turning the traditionally passive radio environment into a smart, programmable medium, metasurfaces promise advances in communication and sensing. However, metasurfaces also present a new security frontier: both attackers and defenders can exploit them to alter wireless propagation for their own advantage. While prior security research has primarily explored unilateral metasurface applications - empowering either attackers or defenders - this work investigates symmetric scenarios, where both sides possess comparable metasurface capabilities. Using both theoretical modeling and real-world experiments, we analyze how competing metasurfaces interact for diverse objectives, including signal power and sensing perception. Thereby, we present the first systematic study of context-agnostic metasurface-to-metasurface interactions and their implications for wireless security. Our results reveal that the outcome of metasurface "battles" depends on an interplay of timing, placement, algorithmic strategy, and hardware scale. Across multiple case studies in Wi-Fi environments, including wireless jamming, channel obfuscation for sensing and communication, and sensing spoofing, we demonstrate that opposing metasurfaces can substantially or fully negate each other's effects. By undermining previously proposed security and privacy schemes, our findings open new opportunities for designing resilient and high-assurance physical-layer systems in smart radio environments.

</details>


### [7] [Privis: Towards Content-Aware Secure Volumetric Video Delivery](https://arxiv.org/abs/2511.14005)
*Kaiyuan Hu,Hong Kang,Yili Jin,Junhua Liu,Chengming Hu,Haolun Wu,Xue Liu*

Main category: cs.CR

TL;DR: Privis是一个基于显著性的安全体视频传输框架，通过分区、轻量级加密和选择性流量整形来平衡机密性和低延迟需求。


<details>
  <summary>Details</summary>
Motivation: 现有体视频流媒体管道继承了2D视频的统一加密方案，忽略了不同几何体的异构隐私敏感性和实时XR的严格运动到光子延迟约束。

Method: 将体视频资产划分为独立单元，应用具有自适应密钥轮换的轻量级认证加密，并采用选择性流量整形。

Result: 提出了通用的体视频传输层安全架构，定义了核心抽象和自适应保护机制，并通过原型实现和初始延迟测量验证了可行性。

Conclusion: 为实时、基于显著性的安全体视频传输提供了早期实证指导，展示了设计权衡和未来研究方向。

Abstract: Volumetric video has emerged as a key paradigm in eXtended Reality (XR) and immersive multimedia because it enables highly interactive, spatially consistent 3D experiences. However, the transport-layer security for such 3D content remains largely unaddressed. Existing volumetric streaming pipelines inherit uniform encryption schemes from 2D video, overlooking the heterogeneous privacy sensitivity of different geometry and the strict motion-to-photon latency constraints of real-time XR.
  We take an initial step toward content-aware secure volumetric video delivery by introducing Privis, a saliency-guided transport framework that (i) partitions volumetric assets into independent units, (ii) applies lightweight authenticated encryption with adaptive key rotation, and (iii) employs selective traffic shaping to balance confidentiality and low latency. Privis specifies a generalized transport-layer security architecture for volumetric media, defining core abstractions and adaptive protection mechanisms. We further explore a prototype implementation and present initial latency measurements to illustrate feasibility and design tradeoffs, providing early empirical guidance toward future work on real-time, saliency-conditioned secure delivery.

</details>


### [8] [Location-Dependent Cryptosystem](https://arxiv.org/abs/2511.14032)
*Kunal Mukherjee*

Main category: cs.CR

TL;DR: 提出了一种基于超宽带(UWB)数据传输包精确飞行时间差异的位置依赖加密系统，解密密钥不是直接传输，而是通过时间差隐式编码，只有特定空间区域内的接收器才能正确解密。


<details>
  <summary>Details</summary>
Motivation: 解决传统加密方案在解密密钥泄露后无法限制解密位置的问题，防止知识产权被盗和未经授权的再分发。

Method: 利用精确计时硬件和自定义JMTK协议，将SHA-256哈希的AES密钥映射到预定的传输时间戳上，通过UWB数据包的飞行时间差异隐式编码解密密钥。

Result: 实现了完整的原型系统，能够加密和传输音频数据，只有在授权区域内的接收器才能正确解密，系统无需电子或物理共享解密密码，且窃听者无法恢复解密密钥。

Conclusion: 该位置依赖加密系统通过UWB时间差隐式编码密钥，有效限制了解密操作的地理位置，为数字内容分发提供了更强的安全保护。

Abstract: Digital content distribution and proprietary research-driven industries face persistent risks from intellectual property theft and unauthorized redistribution. Conventional encryption schemes such as AES, TDES, ECC, and ElGamal provide strong cryptographic guarantees, but they remain fundamentally agnostic to where decryption takes place.In practice, this means that once a decryption key is leaked or intercepted, any adversary can misuse the key to decrypt the protected content from any location. We present a location-dependent cryptosystem in which the decryption key is not transmitted as human- or machine-readable data, but implicitly encoded in precise time-of-flight differences of ultra-wideband (UWB) data transmission packets. The system leverages precise timing hardware and a custom JMTK protocol to map a SHA-256 hashed AES key onto scheduled transmission timestamps. Only receivers located within a predefined spatial region can observe the packet timings that align with the intended "time slot" pattern, enabling them to reconstruct the key and decrypt the secret. Receivers outside the authorized region observe incorrect keys. We implement a complete prototype that encrypts and transmits audio data using our cryptosystem, and only when the receiver is within the authorized data, they are able to decrypt the data. Our evaluation demonstrates that the system (i) removes the need to share decryption passwords electronically or physically, (ii) ensures the decryption key cannot be recovered by the eavesdropper, and (iii) provides a non-trivial spatial tolerance for legitimate users.

</details>


### [9] [GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards](https://arxiv.org/abs/2511.14045)
*Yule Liu,Heyi Zhang,Jinyi Zheng,Zhen Sun,Zifan Peng,Tianshuo Cong,Yilong Yang,Xinlei He,Zhuo Ma*

Main category: cs.CR

TL;DR: 本文提出了DIBA攻击方法，专门针对强化学习与可验证奖励（RLVR）训练的大语言模型进行成员推理攻击，通过检测模型行为变化而非记忆来推断训练数据，在多个场景下显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: RLVR训练范式在LLM中引入新的隐私泄露模式：由于训练依赖自生成响应且无固定真实输出，成员推理需要仅基于提示判断是否用于微调，这创造了不依赖答案记忆的新型隐私威胁。

Method: 提出DIBA攻击框架，将焦点从记忆转向行为变化，利用模型在两个轴上的可测量行为偏移：优势侧改进（如正确性增益）和对数侧分歧（如策略漂移）。

Result: DIBA显著优于现有基线，达到约0.8的AUC和一个数量级更高的TPR@0.1%FPR，在多种设置下验证了其优越性，包括同分布、跨数据集、跨算法、黑盒场景以及扩展到视觉语言模型。

Conclusion: 这是首个系统分析RLVR隐私漏洞的工作，揭示了即使在缺乏显式监督的情况下，训练数据暴露仍可通过行为痕迹可靠推断。

Abstract: Membership inference attacks (MIAs) on large language models (LLMs) pose significant privacy risks across various stages of model training. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have brought a profound paradigm shift in LLM training, particularly for complex reasoning tasks. However, the on-policy nature of RLVR introduces a unique privacy leakage pattern: since training relies on self-generated responses without fixed ground-truth outputs, membership inference must now determine whether a given prompt (independent of any specific response) is used during fine-tuning. This creates a threat where leakage arises not from answer memorization.
  To audit this novel privacy risk, we propose Divergence-in-Behavior Attack (DIBA), the first membership inference framework specifically designed for RLVR. DIBA shifts the focus from memorization to behavioral change, leveraging measurable shifts in model behavior across two axes: advantage-side improvement (e.g., correctness gain) and logit-side divergence (e.g., policy drift). Through comprehensive evaluations, we demonstrate that DIBA significantly outperforms existing baselines, achieving around 0.8 AUC and an order-of-magnitude higher TPR@0.1%FPR. We validate DIBA's superiority across multiple settings--including in-distribution, cross-dataset, cross-algorithm, black-box scenarios, and extensions to vision-language models. Furthermore, our attack remains robust under moderate defensive measures.
  To the best of our knowledge, this is the first work to systematically analyze privacy vulnerabilities in RLVR, revealing that even in the absence of explicit supervision, training data exposure can be reliably inferred through behavioral traces.

</details>


### [10] [Dynamic Black-box Backdoor Attacks on IoT Sensory Data](https://arxiv.org/abs/2511.14074)
*Ajesh Koyatan Chathoth,Stephen Lee*

Main category: cs.CR

TL;DR: 本文提出了一种针对传感器数据物联网系统的黑盒对抗攻击方法，通过动态触发器生成技术，在最小扰动下成功攻击多种数据集和分类器模型。


<details>
  <summary>Details</summary>
Motivation: 虽然基于深度学习的模型在人类活动和手势分类方面取得了成功，但它们存在各种安全风险。本文旨在探讨传感器数据物联网系统的安全漏洞。

Method: 采用新颖的动态触发器生成技术，对基于传感器数据的物联网系统执行黑盒对抗攻击。

Result: 实证分析表明，该攻击方法在多种数据集和分类器模型上都取得了成功，且对输入数据的扰动最小。与后门攻击中的其他中毒技术相比，在性能和隐蔽性方面具有优势。

Conclusion: 本文讨论了对抗性防御机制及其对触发器生成技术有效性的影响，为传感器数据物联网系统的安全性提供了重要见解。

Abstract: Sensor data-based recognition systems are widely used in various applications, such as gait-based authentication and human activity recognition (HAR). Modern wearable and smart devices feature various built-in Inertial Measurement Unit (IMU) sensors, and such sensor-based measurements can be fed to a machine learning-based model to train and classify human activities. While deep learning-based models have proven successful in classifying human activity and gestures, they pose various security risks. In our paper, we discuss a novel dynamic trigger-generation technique for performing black-box adversarial attacks on sensor data-based IoT systems. Our empirical analysis shows that the attack is successful on various datasets and classifier models with minimal perturbation on the input data. We also provide a detailed comparative analysis of performance and stealthiness to various other poisoning techniques found in backdoor attacks. We also discuss some adversarial defense mechanisms and their impact on the effectiveness of our trigger-generation technique.

</details>


### [11] [Resolving Availability and Run-time Integrity Conflicts in Real-Time Embedded Systems](https://arxiv.org/abs/2511.14088)
*Adam Caulfield,Muhammad Wasif Kamran,N. Asokan*

Main category: cs.CR

TL;DR: PAIR提出了一种在实时系统中平衡安全性和可用性的方法，通过监控运行时完整性违规，维护安全任务可用区域，仅终止违规任务而继续执行非违规任务。


<details>
  <summary>Details</summary>
Motivation: 现有实时系统在检测到完整性违规时面临两难选择：要么优先考虑可用性允许受感染系统继续运行，要么优先考虑安全性中止所有执行。需要在这两个极端之间找到平衡点。

Method: PAIR监控实时任务的运行时完整性违规，维护安全任务可用区域(AR)。当任务引发违规时，触发不可屏蔽中断终止该任务，同时继续执行AR内的非违规任务。

Result: PAIR通过硬件方法实现，对执行任务不产生运行时开销，可与实时操作系统集成，在内存和硬件使用上仅增加2.3%的开销，适用于低端微控制器单元。

Conclusion: PAIR在实时系统的安全性和可用性之间提供了中间解决方案，能够仅阻止违规任务的执行，同时为剩余任务保持可用性，且硬件开销极小。

Abstract: Run-time integrity enforcement in real-time systems presents a fundamental conflict with availability. Existing approaches in real- time systems primarily focus on minimizing the execution-time overhead of monitoring. After a violation is detected, prior works face a trade-off: (1) prioritize availability and allow a compromised system to continue to ensure applications meet their deadlines, or (2) prioritize security by generating a fault to abort all execution. In this work, we propose PAIR, an approach that offers a middle ground between the stark extremes of this trade-off. PAIR monitors real-time tasks for run-time integrity violations and maintains an Availability Region (AR) of all tasks that are safe to continue. When a task causes a violation, PAIR triggers a non-maskable interrupt to kill the task and continue executing a non-violating task within AR. Thus, PAIR ensures only violating tasks are prevented from execution, while granting availability to remaining tasks. With its hardware approach, PAIR does not cause any run-time overhead to the executing tasks, integrates with real-time operating systems (RTOSs), and is affordable to low-end microcontroller units (MCUs) by incurring +2.3% overhead in memory and hardware usage.

</details>


### [12] [Beyond Fixed and Dynamic Prompts: Embedded Jailbreak Templates for Advancing LLM Security](https://arxiv.org/abs/2511.14140)
*Hajun Kim,Hyunsik Na,Daeseon Choi*

Main category: cs.CR

TL;DR: 本文提出了嵌入式越狱模板，通过将有害查询自然嵌入现有模板结构中，解决了当前越狱攻击模板生成方法在意图清晰度和可复现性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，确保其安全性和鲁棒性变得至关重要。当前越狱攻击模板生成方法主要依赖两种有限策略：将有害查询替换到固定模板中，或让LLM生成整个模板，但这往往损害意图清晰度和可复现性。

Method: 引入嵌入式越狱模板，保留现有模板结构的同时自然嵌入有害查询；提出渐进式提示工程方法确保模板质量和一致性；制定标准化的生成和评估协议。

Result: 提供了一个更准确反映真实世界使用场景和有害意图的基准，支持红队测试和政策回归测试。

Conclusion: 嵌入式越狱模板和渐进式提示工程方法有效解决了当前越狱攻击模板生成方法的局限性，为加强LLM安全防御提供了更可靠的基准工具。

Abstract: As the use of large language models (LLMs) continues to expand, ensuring their safety and robustness has become a critical challenge. In particular, jailbreak attacks that bypass built-in safety mechanisms are increasingly recognized as a tangible threat across industries, driving the need for diverse templates to support red-teaming efforts and strengthen defensive techniques. However, current approaches predominantly rely on two limited strategies: (i) substituting harmful queries into fixed templates, and (ii) having the LLM generate entire templates, which often compromises intent clarity and reproductibility. To address this gap, this paper introduces the Embedded Jailbreak Template, which preserves the structure of existing templates while naturally embedding harmful queries within their context. We further propose a progressive prompt-engineering methodology to ensure template quality and consistency, alongside standardized protocols for generation and evaluation. Together, these contributions provide a benchmark that more accurately reflects real-world usage scenarios and harmful intent, facilitating its application in red-teaming and policy regression testing.

</details>


### [13] [Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion](https://arxiv.org/abs/2511.14301)
*Eric Xue,Ruiyi Zhang,Zijun Zhang,Pengtao Xie*

Main category: cs.CR

TL;DR: SteganoBackdoor是一种新型的后门攻击方法，利用自然语言隐写术技术将语义触发器转化为隐写载体，在极低数据投毒率下实现高攻击成功率，并能有效规避现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 当前研究过于关注基于风格化伪影或令牌级扰动的后门攻击，而忽略了更现实和危险的语义触发器攻击，这种攻击可能在实际部署系统中操纵与真实人物或事件相关的输出。

Method: 利用自然语言隐写术的无害特性，应用梯度引导的数据优化过程，将语义触发器种子转化为隐写载体，这些载体嵌入高后门负载、保持流畅性，且与触发器没有表征相似性。

Result: 在多样化实验设置中，SteganoBackdoor以比先前方法低一个数量级的数据投毒率实现了超过99%的攻击成功率，同时在面对全面数据级防御套件时保持无与伦比的规避能力。

Conclusion: SteganoBackdoor揭示了当前防御机制中的一个紧急盲点，迫切需要关注对抗性数据防御和现实世界威胁建模。

Abstract: Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.

</details>


### [14] [Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection](https://arxiv.org/abs/2511.14422)
*Zhengchunmin Dai,Jiaxiong Tang,Peng Sun,Honglong Chen,Liantao Wu*

Main category: cs.CR

TL;DR: Sigil是一个专为能力受限服务器设计的强制水印框架，通过梯度注入在客户端模型中嵌入水印，无需数据知识即可保护服务器知识产权。


<details>
  <summary>Details</summary>
Motivation: 在Split Federated Learning等去中心化机器学习范式中，服务器能力受限虽然增强了客户端隐私，但也使服务器容易受到恶意客户端的模型窃取攻击，现有水印方案在对抗环境中不可靠或技术上不可行。

Method: Sigil将水印定义为服务器可见激活空间的统计约束，通过梯度注入将水印嵌入客户端模型，设计了自适应梯度裁剪机制确保水印过程的强制性和隐蔽性。

Result: 在多个数据集和模型上的广泛实验表明，Sigil具有良好的保真度、鲁棒性和隐蔽性，能够有效对抗现有的梯度异常检测方法和专门设计的自适应子空间移除攻击。

Conclusion: Sigil为能力受限服务器提供了一种有效的知识产权保护方案，通过统计约束和梯度注入实现了强制水印嵌入，解决了传统水印方案在对抗环境中的不足。

Abstract: In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.
  To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.

</details>


### [15] [SecureSign: Bridging Security and UX in Mobile Web3 through Emulated EIP-6963 Sandboxing](https://arxiv.org/abs/2511.14611)
*Charles Cheng Ji,Brandon Kong*

Main category: cs.CR

TL;DR: SecureSign是一个PWA架构，通过EIP-6963提供程序沙箱化将桌面浏览器扩展安全性适配到移动端，解决了移动Web3用户留存率低和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 移动Web3面临灾难性的用户留存率（<5%），导致每个留存用户的有效获取成本高达500-1000美元。现有解决方案存在两难选择：嵌入式钱包具有中等可用性但存在固有的点击劫持漏洞；应用钱包保持安全性但下载摩擦和上下文切换导致留存率仅为2-3%。

Method: SecureSign采用PWA架构，通过EIP-6963提供程序沙箱化将桌面浏览器扩展安全性适配到移动端。它在可信父应用程序中使用iframe隔离dApp执行，实现点击劫持免疫和交易完整性，同时支持原生移动功能（推送通知、主屏幕安装、零上下文切换）。

Result: 威胁模型分析证明SecureSign对点击劫持、覆盖和窃取攻击具有免疫力，同时保持跨dApp的钱包互操作性。该即插即用SDK无需对现有Web3应用程序进行代码库更改。

Conclusion: SecureSign成功解决了移动Web3在安全性和用户体验之间的权衡问题，提供了一种既安全又用户友好的解决方案，有望显著提高移动Web3的用户留存率。

Abstract: Mobile Web3 faces catastrophic retention (< 5%) yielding effective acquisition costs of \$500 - \$1,000 per retained user. Existing solutions force an impossible tradeoff: embedded wallets achieve moderate usability but suffer inherent click-jacking vulnerabilities; app wallets maintain security at the cost of 2 - 3% retention due to download friction and context-switching penalties. We present SecureSign, a PWA-based architecture that adapts desktop browser extension security to mobile via EIP-6963 provider sandboxing. SecureSign isolates dApp execution in iframes within a trusted parent application, achieving click-jacking immunity and transaction integrity while enabling native mobile capabilities (push notifications, home screen installation, zero context-switching). Our drop-in SDK requires no codebase changes for existing Web3 applications. Threat model analysis demonstrates immunity to click-jacking, overlay, and skimming attacks while maintaining wallet interoperability across dApps.

</details>


### [16] [A Unified Compositional View of Attack Tree Metrics](https://arxiv.org/abs/2511.14717)
*Benedikt Peterseim,Milan Lopuhaä-Zwakenberg*

Main category: cs.CR

TL;DR: 本文通过基于gs-幺半范畴的组合理论，为攻击树及其度量提供了函子语义学，将攻击树视为字符串图，证明其组件形成通道范畴，从而统一了各种攻击树度量。


<details>
  <summary>Details</summary>
Motivation: 现有的攻击树度量方法要么无法包含重要度量，要么过于抽象无法提供定义具体度量的系统方法，缺乏对攻击树度量的系统化处理。

Method: 将攻击树视为字符串图，证明其组件形成通道范畴（一种特殊的gs-幺半范畴），攻击树度量对应于通道范畴的函子。

Result: 该特征化既足够通用以包含所有常见的攻击树度量，又足够具体以通过逻辑结构定义攻击树度量。

Conclusion: 基于gs-幺半范畴的组合理论为攻击树度量提供了统一且实用的系统化框架。

Abstract: Attack trees (ATs) are popular graphical models for reasoning about the security of complex systems, allowing for the quantification of risk through so-called AT metrics. A large variety of different such AT metrics have been proposed, and despite their wide-spread practical use, no systematic treatment of attack tree metrics so far is fully satisfactory. Existing approaches either fail to include important metrics, or they are too general to provide a useful systematic way for defining concrete AT metrics, giving only an abstract characterisation of their behaviour. We solve this problem by developing a compositional theory of ATs and their functorial semantics based on gs-monoidal categories. Viewing attack trees as string diagrams, we show that components of ATs form a channel category, a particular type of gs-monoidal category. AT metrics then correspond to functors of channel categories. This characterisation is both general enough to include all common AT metrics, and concrete enough to define AT metrics by their logical structure.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models](https://arxiv.org/abs/2511.13782)
*Xiaoxing Lian,Aidong Yang,Jun Zhu,Peng Wang,Yue Zhang*

Main category: cs.AI

TL;DR: 本文介绍了SpatiaLite基准测试，用于评估视觉语言模型的空间推理能力，发现现有模型主要依赖语言表征，在视觉中心任务上表现不足且效率低下，并提出了Imagery Driven Framework来改进。


<details>
  <summary>Details</summary>
Motivation: 当前先进的视觉语言模型在逻辑推理、问题解决等方面表现出色，但在空间推理这一人类认知的基本能力上仍面临重大挑战。作者假设想象力是空间世界模型中的主导推理机制。

Method: 引入SpatiaLite合成基准测试，联合测量空间推理准确性和效率；提出Imagery Driven Framework用于数据合成和训练，隐式构建内部世界模型。

Result: 发现三个关键结果：1）先进VLMs主要依赖语言表征，在需要感知空间关系和3D几何变换的视觉中心任务上表现显著不足；2）当前空间推理机制效率严重低下，token使用随变换复杂度快速增加；3）IDF框架能有效构建内部世界模型。

Conclusion: 本研究明确了先进VLMs的空间推理局限和模式，识别了关键缺陷，为未来改进提供了指导方向。

Abstract: Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances

</details>


### [18] [Causal computations in Semi Markovian Structural Causal Models using divide and conquer](https://arxiv.org/abs/2511.13852)
*Anna Rodum Bjøru,Rafael Cabañas,Helge Langseth,Antonio Salmerón*

Main category: cs.AI

TL;DR: 本文研究了将Bjøru等人提出的反事实概率边界计算方法从马尔可夫模型扩展到半马尔可夫结构因果模型的方法，解决了外生变量影响多个内生变量的挑战。


<details>
  <summary>Details</summary>
Motivation: Bjøru等人的方法仅适用于马尔可夫模型，而半马尔可夫模型能够表示更复杂的混淆关系，因此需要扩展该方法以处理更一般的因果模型。

Method: 通过最小示例说明扩展挑战，提出替代解决方案策略，并进行理论和计算评估。

Result: 开发了针对半马尔可夫结构因果模型的反事实概率边界计算方法，能够处理外生变量影响多个内生变量的情况。

Conclusion: 成功将反事实概率边界计算方法扩展到半马尔可夫模型，为处理更复杂的因果推断问题提供了有效工具。

Abstract: Recently, Bjøru et al. proposed a novel divide-and-conquer algorithm for bounding counterfactual probabilities in structural causal models (SCMs). They assumed that the SCMs were learned from purely observational data, leading to an imprecise characterization of the marginal distributions of exogenous variables. Their method leveraged the canonical representation of structural equations to decompose a general SCM with high-cardinality exogenous variables into a set of sub-models with low-cardinality exogenous variables. These sub-models had precise marginals over the exogenous variables and therefore admitted efficient exact inference. The aggregated results were used to bound counterfactual probabilities in the original model. The approach was developed for Markovian models, where each exogenous variable affects only a single endogenous variable. In this paper, we investigate extending the methodology to \textit{semi-Markovian} SCMs, where exogenous variables may influence multiple endogenous variables. Such models are capable of representing confounding relationships that Markovian models cannot. We illustrate the challenges of this extension using a minimal example, which motivates a set of alternative solution strategies. These strategies are evaluated both theoretically and through a computational study.

</details>


### [19] [Jailbreaking Large Vision Language Models in Intelligent Transportation Systems](https://arxiv.org/abs/2511.13892)
*Badhan Chandra Das,Md Tasnim Jawad,Md Jueal Mia,M. Hadi Amini,Yanzhao Wu*

Main category: cs.AI

TL;DR: 该论文系统分析了智能交通系统中大型视觉语言模型在精心设计的越狱攻击下的脆弱性，提出了基于图像排版操纵和多轮提示的新型越狱攻击方法，并开发了多层响应过滤防御技术。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态推理和实际应用中表现出强大能力，但在智能交通系统等关键应用中存在严重的安全漏洞，易受越狱攻击影响，需要系统性的安全分析。

Method: 首先构建了与交通相关的有害查询数据集；然后提出通过图像排版操纵和多轮提示的新型越狱攻击方法；最后设计了多层响应过滤防御技术来防止模型生成不当响应。

Result: 在开源和闭源的最新大型视觉语言模型上进行了广泛实验，使用GPT-4判断和人工验证评估攻击方法和防御技术的毒性得分，发现图像排版操纵和多轮提示在智能交通系统中存在严重安全风险。

Conclusion: 研究揭示了智能交通系统中大型视觉语言模型的安全脆弱性，提出的攻击方法比现有技术更有效，同时防御技术能够有效防止不当响应生成，强调了在关键应用中加强模型安全性的重要性。

Abstract: Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.

</details>


### [20] [CORGI: Efficient Pattern Matching With Quadratic Guarantees](https://arxiv.org/abs/2511.13942)
*Daniel Weitekamp*

Main category: cs.AI

TL;DR: CORGI是一种新的模式匹配算法，针对规则系统中指数级时间和空间复杂度问题，提供二次时间空间保证，通过两步法避免传统RETE算法的内存溢出问题。


<details>
  <summary>Details</summary>
Motivation: 解决实时AI系统和数据库查询中规则匹配的指数级复杂度问题，特别是当AI系统自动生成规则时容易产生最坏情况匹配模式，导致程序执行缓慢或内存溢出。

Method: 采用两步法：正向构建/维护基础关系图，反向通过迭代器按需生成匹配，避免传统RETE算法中收集部分匹配的β内存机制。

Result: 在性能评估中，CORGI在简单组合匹配任务上显著优于SOAR和OPS5的RETE实现。

Conclusion: CORGI算法通过消除填充完整冲突集带来的高延迟和内存溢出，使学习型认知系统在实际应用中更加可行。

Abstract: Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $β$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.

</details>


### [21] [Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios](https://arxiv.org/abs/2511.13970)
*Sanjay Acharjee,Abir Khan Ratul,Diego Patino,Md Nazmus Sakib*

Main category: cs.AI

TL;DR: 提出基于场景图引导的生成AI框架，通过分析OSHA事故报告生成逼真的工作场所危险场景图像，并引入VQA框架评估生成数据的真实性和语义保真度。


<details>
  <summary>Details</summary>
Motivation: 获取真实的工作场所危险场景图像数据集困难，因为捕捉实际发生的事故触发场景几乎不可能。

Method: 使用GPT-4o分析OSHA事故报告提取结构化危险推理，转换为对象级场景图，指导文本到图像扩散模型生成构图准确的危险场景。

Result: 提出的VQA图评分在四个最先进的生成模型中，基于熵验证优于CLIP和BLIP指标，确认其更高的判别敏感性。

Conclusion: 该框架能够有效合成逼真的危险场景图像，为工作场所安全培训提供有价值的视觉数据。

Abstract: Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.

</details>


### [22] [ALEX:A Light Editing-knowledge Extractor](https://arxiv.org/abs/2511.14018)
*Minghu Wang,Shuliang Zhao,Yuanyuan Zhao,Hongxia Xu*

Main category: cs.AI

TL;DR: ALEX是一个轻量级知识编辑框架，通过分层内存架构将知识更新组织为语义簇，将检索复杂度从O(N)降低到O(K+N/C)，显著提高了多跳问题的准确性和推理路径可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的知识是静态的，难以适应不断发展的信息，现有方法在处理需要多步推理的复杂多跳问题时面临可扩展性和检索效率的挑战。

Method: ALEX采用分层内存架构组织知识更新为语义簇，包含推理查询合成模块来弥合查询与事实之间的语义差距，以及动态证据裁决引擎执行高效的两阶段检索过程。

Result: 在MQUAKE基准测试中，ALEX显著提高了多跳答案的准确性和推理路径的可靠性，同时将所需搜索空间减少了80%以上。

Conclusion: ALEX为构建可扩展、高效和准确的知识编辑系统提供了一条有前景的路径。

Abstract: The static nature of knowledge within Large Language Models (LLMs) makes it difficult for them to adapt to evolving information, rendering knowledge editing a critical task. However, existing methods struggle with challenges of scalability and retrieval efficiency, particularly when handling complex, multi-hop questions that require multi-step reasoning. To address these challenges, this paper introduces ALEX (A Light Editing-knowledge Extractor), a lightweight knowledge editing framework. The core innovation of ALEX is its hierarchical memory architecture, which organizes knowledge updates (edits) into semantic clusters. This design fundamentally reduces retrieval complexity from a linear O(N) to a highly scalable O(K+N/C). Furthermore, the framework integrates an Inferential Query Synthesis (IQS) module to bridge the semantic gap between queries and facts , and a Dynamic Evidence Adjudication (DEA) engine that executes an efficient two-stage retrieval process. Experiments on the MQUAKE benchmark demonstrate that ALEX significantly improves both the accuracy of multi-hop answers (MultiHop-ACC) and the reliability of reasoning paths (HopWise-ACC). It also reduces the required search space by over 80% , presenting a promising path toward building scalable, efficient, and accurate knowledge editing systems.

</details>


### [23] [Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation](https://arxiv.org/abs/2511.14023)
*Chiharu Hagiwara,Naoki Nonaka,Yuhta Hashimoto,Ryu Uchimido,Jun Seita*

Main category: cs.AI

TL;DR: Syn-STARTS框架使用LLMs生成大规模分类案例，解决了大规模伤亡事件中真实数据难以获取的问题，生成的合成数据在质量上与手动整理的真实数据集难以区分。


<details>
  <summary>Details</summary>
Motivation: 大规模伤亡事件中分类决策至关重要，但真实数据难以积累，阻碍了AI模型的开发与评估。

Method: 开发Syn-STARTS框架，利用LLMs生成分类案例，并与手动整理的TRIAGE开放数据集进行质量对比验证。

Result: Syn-STARTS生成的分类案例在质量上与真实数据集难以区分，且在标准START分类法的四个类别（绿、黄、红、黑）中表现出高度稳定的准确性。

Conclusion: 合成数据在开发高性能AI模型用于严重和危急医疗情况方面具有巨大潜力。

Abstract: Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation require benchmark datasets of sufficient quantity and quality. However, MCIs occur infrequently, and sufficient records are difficult to accumulate at the scene, making it challenging to collect large-scale realworld data for research use. Therefore, we developed Syn-STARTS, a framework that uses LLMs to generate triage cases, and verified its effectiveness. The results showed that the triage cases generated by Syn-STARTS were qualitatively indistinguishable from the TRIAGE open dataset generated by manual curation from training materials. Furthermore, when evaluating the LLM accuracy using hundreds of cases each from the green, yellow, red, and black categories defined by the standard triage method START, the results were found to be highly stable. This strongly indicates the possibility of synthetic data in developing high-performance AI models for severe and critical medical situations.

</details>


### [24] [Making Evidence Actionable in Adaptive Learning](https://arxiv.org/abs/2511.14052)
*Amirreza Mehrabi,Jason W. Morphew,Breejha Quezada,N. Sanjay Rebello*

Main category: cs.AI

TL;DR: 本文提出了一种由教师主导的反馈循环系统，将概念级评估证据转化为经过验证的微干预措施，通过三个保障机制（充分性、注意力预算、多样性）实现自适应学习。


<details>
  <summary>Details</summary>
Motivation: 传统自适应学习系统诊断精确但干预薄弱，导致帮助时机不当或内容不匹配。需要建立诊断与教学之间的闭环，提供公平、负载感知的个性化学习。

Method: 将干预分配形式化为带约束的二元整数规划，包括覆盖范围、时间、难度窗口、概念矩阵编码的先决条件以及通过多样性强制执行的抗冗余性。使用贪婪选择、基于梯度的松弛和混合方法。

Result: 在模拟和1204名学生的物理课程部署中，两种求解器都能在有限观看时间内为几乎所有学习者实现完整的技能覆盖。基于梯度的方法比贪婪方法减少约12%的冗余覆盖，并在资源稀缺时以较低计算成本提供相当的充分性。

Conclusion: 该系统提供了一个可追踪和可审计的控制器，能够闭合诊断-教学循环，在课堂规模上实现公平、负载感知的个性化学习。

Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.

</details>


### [25] [APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design](https://arxiv.org/abs/2511.14101)
*Xinpeng Chen,Xiaofeng Han,Kaihao Zhang,Guochao Ren,Yujie Wang,Wenhao Cao,Yang Zhou,Jianfeng Lu,Zhenbo Song*

Main category: cs.AI

TL;DR: APD-agents是一个基于大语言模型的多智能体框架，用于自动化移动应用页面设计，通过多个智能体协作将用户描述转换为结构化布局设计。


<details>
  <summary>Details</summary>
Motivation: 移动应用页面布局设计耗时且需要专业技能，现有设计软件使用复杂且跨页面协作效率低，需要自动化解决方案来提升设计效率。

Method: 提出多智能体框架，包含编排智能体、语义解析智能体、主布局智能体、模板检索智能体和递归组件智能体，通过智能体协作实现从用户描述到页面布局的自动化生成。

Result: 在RICO数据集上的实验结果表明，APD-agents达到了最先进的性能水平。

Conclusion: 该工作充分利用了大模型驱动的多智能体系统的自动协作能力，为移动应用页面设计提供了有效的自动化解决方案。

Abstract: Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.

</details>


### [26] [Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation](https://arxiv.org/abs/2511.14131)
*Yu Zhong,Zihao Zhang,Rui Zhang,Lingdong Huang,Haihan Gao,Shuo Wang,Da Li,Ruijian Han,Jiaming Guo,Shaohui Peng,Di Huang,Yunji Chen*

Main category: cs.AI

TL;DR: R3是一个用于视觉语言导航的双过程思考框架，通过整合大型语言模型的泛化能力和VLN特定专业知识，在零样本设置下显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的VLN方法在任务完成性能上与领域专家存在显著差距，且LLM难以精确理解真实世界空间关系，同时计算成本和推理延迟较高。

Method: 提出R3框架，包含三个核心模块：轻量级专家模型Runner负责常规导航，多模态LLM Ruminator进行结构化推理，Regulator根据三个标准监控导航进度并控制思维模式。

Result: 在REVERIE基准测试中，SPL和RGSPL分别超过最先进方法3.28%和3.30%，显著提升了挑战性VLN任务的处理效果。

Conclusion: R3框架通过整合LLM的泛化能力和VLN专业知识，在零样本设置下实现了显著的性能提升，证明了该方法在处理复杂导航任务中的有效性。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.

</details>


### [27] [Do Large Language Models (LLMs) Understand Chronology?](https://arxiv.org/abs/2511.14214)
*Pattaraphon Kenny Wongchamcharoen,Paul Glasserman*

Main category: cs.AI

TL;DR: 测试大型语言模型在金融应用中理解时间顺序的能力，发现模型在复杂时间排序任务中存在困难，但增加推理预算能显著提升GPT-5和Claude-3.7 Sonnet的表现。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在金融和经济学中广泛应用，需要验证其是否真正理解时间顺序，以避免前瞻性偏差。

Method: 设计了一系列时间顺序任务，包括时间排序、条件排序（先过滤后排序）和时代错误检测，评估GPT-4.1、Claude-3.7 Sonnet和GPT-5在不同推理强度下的表现。

Result: 模型在长序列中的精确匹配率显著下降，但排名相关性保持较高；条件排序中失败主要来自过滤步骤；时代错误检测是最简单的任务。GPT-5在中等/高推理强度下在所有长度上都能实现完美排序。

Conclusion: 当前LLMs在时间任务上存在局限性，明确分配推理预算有助于提升时间排序能力，这对LLMs在金融领域的实时应用具有重要意义。

Abstract: Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.

</details>


### [28] [Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation](https://arxiv.org/abs/2511.14219)
*Kumud Tripathi,Aditya Srinivas Menon,Aman Gaurav,Raj Prakash Gohil,Pankaj Wasnik*

Main category: cs.AI

TL;DR: 提出两阶段架构来减少Whisper模型在噪声条件下的幻觉错误：第一阶段通过自适应层注意力增强编码器鲁棒性，第二阶段使用多目标知识蒸馏框架抑制幻觉


<details>
  <summary>Details</summary>
Motivation: Whisper模型在噪声声学条件下经常出现幻觉错误，而现有方法主要关注音频预处理或转录后处理，对模型本身的修改探索不足

Method: 两阶段方法：1) 自适应层注意力将编码器层分组并通过多头注意力融合块表示；2) 多目标知识蒸馏框架训练学生模型在噪声音频上对齐教师模型的语义和注意力分布

Result: 在噪声语音基准测试中显著减少了幻觉和词错误率，同时保持了在干净语音上的性能

Conclusion: ALA和KD为在真实世界噪声条件下提高Whisper的可靠性提供了原则性策略

Abstract: The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.

</details>


### [29] [DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home](https://arxiv.org/abs/2511.14227)
*Yuxiang Wang,Siwen Wang,Haowei Han,Ao Wang,Boya Liu,Yong Zhao,Chengbo Wu,Bin Zhu,Bin Qin,Xiaokai Zhou,Xiao Yan,Jiawei Jiang,Bo Du*

Main category: cs.AI

TL;DR: DevPiolt是一个基于大语言模型的物联网设备操作推荐系统，通过持续预训练、多任务微调、直接偏好优化和置信度控制机制，显著提升了操作推荐的准确性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有推荐模型在处理物联网设备操作时面临复杂操作逻辑、多样化用户偏好和对次优建议敏感等问题，限制了其在实际应用中的效果。

Method: 1) 通过持续预训练和多任务微调为LLM注入物联网领域知识；2) 使用直接偏好优化使模型与用户偏好对齐；3) 设计基于置信度的曝光控制机制避免低质量推荐。

Result: 在多个数据集上显著优于基线方法，所有指标平均提升69.5%。在小米家庭应用中实际部署一个季度，服务25.5万用户，在线实验显示独特访客设备覆盖率提升21.6%，页面浏览接受率提升29.1%。

Conclusion: DevPiolt通过结合领域知识注入、偏好对齐和置信度控制，有效解决了物联网设备操作推荐的关键挑战，在实际部署中取得了显著效果。

Abstract: Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.

</details>


### [30] [Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility](https://arxiv.org/abs/2511.14248)
*Hongju Lee,Youngjun Park,Jisun An,Dongman Lee*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的时间序列预测框架，用于预测区域层面的Airbnb关键指标（收入、预订天数、预订数量），通过结合房源特征与外部环境因素构建区域表示，使用LLM生成区域嵌入，并采用先进的时间序列模型进行预测，在首尔数据集上相比传统基线方法将RMSE和MAE降低了约48%。


<details>
  <summary>Details</summary>
Motivation: 短期租赁平台（如Airbnb）的扩张严重扰乱了当地住房市场，导致租金上涨和住房负担能力问题。准确预测区域Airbnb市场趋势可为政策制定者和城市规划者提供关键见解，以减轻这些影响。

Method: 采用滑动窗口方法预测1-3个月的趋势，通过整合房源特征与城市可达性、人口流动等外部环境因素构建区域表示，将结构化表格数据转换为基于提示的LLM输入以生成综合区域嵌入，然后将这些嵌入输入到先进的时间序列模型（RNN、LSTM、Transformer）中捕捉复杂的时空动态。

Result: 在首尔Airbnb数据集上的实验表明，该方法相比传统统计和机器学习基线，平均RMSE和MAE降低了约48%。

Conclusion: 该框架不仅提高了预测准确性，还为检测供应过剩区域和支持数据驱动的城市政策决策提供了实用见解。

Abstract: The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.

</details>


### [31] [PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2511.14256)
*Yu Liu,Xixun Lin,Yanmin Shang,Yangxi Li,Shi Wang,Yanan Cao*

Main category: cs.AI

TL;DR: PathMind是一个基于大语言模型的知识图谱推理框架，通过"检索-优先化-推理"范式，选择性引导LLM使用重要推理路径，解决现有方法中路径重要性评估不足和频繁调用LLM的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的知识图谱推理方法存在两个关键局限：一是无差别提取推理路径可能引入无关噪声误导LLM；二是动态探索推理路径需要高检索需求和频繁LLM调用。

Method: PathMind采用"检索-优先化-推理"范式：首先通过检索模块从KG中获取查询子图；然后引入路径优先化机制，使用语义感知路径优先级函数识别重要推理路径；最后通过双阶段训练策略生成准确响应。

Result: 在基准数据集上的广泛实验表明，PathMind始终优于竞争基线，特别是在复杂推理任务上，通过识别关键推理路径实现了更少的输入token和更好的性能。

Conclusion: PathMind通过选择性引导LLM使用重要推理路径，增强了忠实性和可解释性推理，在复杂推理任务中表现出色，解决了现有LLM-based KGR方法的关键局限。

Abstract: Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.

</details>


### [32] [When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling](https://arxiv.org/abs/2511.14334)
*Alessio Pellegrino,Jacopo Mauro*

Main category: cs.AI

TL;DR: 本研究通过系统性地重述和扰动CSPLib问题，测试LLMs在上下文和语言变化下的模型生成能力，发现其表现显著下降，揭示其浅层理解和对措辞的敏感性。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs自动生成优化和约束编程模型的能力是否源于数据污染而非真正的推理能力，因为许多标准CP问题可能已包含在训练数据中。

Method: 系统性地重述和扰动一组知名CSPLib问题，保持其结构但修改上下文并引入误导元素，然后比较三个代表性LLM在原始和修改描述下生成的模型。

Result: LLMs能够生成语法有效且语义合理的模型，但在上下文和语言变化下性能急剧下降，显示出浅层理解和对措辞的敏感性。

Conclusion: LLMs在自动生成优化模型方面的成功可能更多源于数据污染而非真正的推理能力，其理解深度有限且对语言表述敏感。

Abstract: One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.

</details>


### [33] [Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior](https://arxiv.org/abs/2511.14476)
*Dalia Ali,Dora Zhao,Allison Koenecke,Orestis Papakyriakopoulos*

Main category: cs.AI

TL;DR: 本研究探讨了在LLM对齐过程中考虑多元社会价值观的影响，通过收集来自美国和德国参与者的27,375个评分数据，分析了人口统计差异和技术设计参数对模型行为的影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的对齐决策往往忽视人类社会的多样性，本研究旨在探索如何将多元价值观纳入对齐流程，以平衡安全性和公平代表性。

Method: 收集来自美国和德国1,095名参与者的评分数据，涵盖毒性、情感意识、敏感性、刻板偏见和帮助性五个维度。使用不同社会群体的偏好微调多个大型语言模型和推理模型，同时改变评分尺度、分歧处理方法和优化技术。

Result: 发现系统性人口统计效应：男性参与者对毒性的评分比女性低18%；保守派和黑人参与者对情感意识的评分分别比自由派和白人参与者高27.9%和44%。技术设计选择显示强烈影响：保留评分者分歧比多数投票减少约53%的毒性；5点量表比二元格式减少约22%的毒性；DPO在多值优化中持续优于GRPO。

Conclusion: 这些发现为回答关键问题提供了初步步骤：对齐应如何平衡专家驱动和用户驱动的信号，以确保安全性和公平代表性？

Abstract: Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?

</details>


### [34] [Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2511.14595)
*Yuan An,Ruhma Hashmi,Michelle Rogers,Jane Greenberg,Brian K. Smith*

Main category: cs.AI

TL;DR: 提出基于率失真理论和最优传输几何的知识图谱构建与优化框架，通过FGW耦合量化语义失真，使用细化操作最小化率失真拉格朗日函数，生成紧凑且信息保留的知识图谱，显著提升多选题生成质量。


<details>
  <summary>Details</summary>
Motivation: 将非结构化教育材料（如讲义和幻灯片）转换为捕捉关键教学内容的知识图谱仍然困难，需要建立理论基础的KG优化方法以支持AI辅助教育。

Method: 将讲座内容建模为度量-测度空间，使用Fused Gromov-Wasserstein耦合量化语义失真，通过添加、合并、拆分、移除和重连等细化操作最小化率失真拉格朗日函数。

Result: 在数据科学讲座上的原型应用显示，从优化KG生成的多选题在15个质量标准上持续优于从原始笔记生成的问题，并产生可解释的率失真曲线。

Conclusion: 本研究为个性化AI辅助教育中的信息论知识图谱优化建立了理论基础，证明优化后的KG能显著提升自动生成问题的质量。

Abstract: Task-oriented knowledge graphs (KGs) enable AI-powered learning assistant systems to automatically generate high-quality multiple-choice questions (MCQs). Yet converting unstructured educational materials, such as lecture notes and slides, into KGs that capture key pedagogical content remains difficult. We propose a framework for knowledge graph construction and refinement grounded in rate-distortion (RD) theory and optimal transport geometry. In the framework, lecture content is modeled as a metric-measure space, capturing semantic and relational structure, while candidate KGs are aligned using Fused Gromov-Wasserstein (FGW) couplings to quantify semantic distortion. The rate term, expressed via the size of KG, reflects complexity and compactness. Refinement operators (add, merge, split, remove, rewire) minimize the rate-distortion Lagrangian, yielding compact, information-preserving KGs. Our prototype applied to data science lectures yields interpretable RD curves and shows that MCQs generated from refined KGs consistently surpass those from raw notes on fifteen quality criteria. This study establishes a principled foundation for information-theoretic KG optimization in personalized and AI-assisted education.

</details>


### [35] [SkillGen: Learning Domain Skills for In-Context Sequential Decision Making](https://arxiv.org/abs/2511.14670)
*Ruomeng Ding,Wei Cheng,Minglai Shao,Chen Zhao*

Main category: cs.AI

TL;DR: SkillGen是一个基于技能的上下文学习框架，通过构建动作中心图、识别高效用动作和检索步骤技能，为序列决策生成细粒度的上下文感知提示，在多个基准测试中显著提升LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在序列决策中难以同时满足三个关键原则：关注决策关键信息、提供步骤级粒度、最小化专家标注依赖。这些挑战促使开发SkillGen框架。

Method: SkillGen构建动作中心的领域级图，通过时间差分信用分配识别高效用动作，并检索步骤级技能来生成细粒度的上下文感知提示。

Result: 在ALFWorld、BabyAI和ScienceWorld基准测试中，使用开源和专有LLM，SkillGen平均提高进度率5.9%-16.5%。

Conclusion: 关注高效用片段支持任务可识别性，并为更有效的ICL提示设计提供信息，SkillGen在序列推理任务中实现了持续的性能提升。

Abstract: Large language models (LLMs) are increasingly applied to sequential decision-making through in-context learning (ICL), yet their effectiveness is highly sensitive to prompt quality. Effective prompts should meet three principles: focus on decision-critical information, provide step-level granularity, and minimize reliance on expert annotations through label efficiency. However, existing ICL methods often fail to satisfy all three criteria simultaneously. Motivated by these challenges, we introduce SkillGen, a skill-based ICL framework for structured sequential reasoning. It constructs an action-centric, domain-level graph from sampled trajectories, identifies high-utility actions via temporal-difference credit assignment, and retrieves step-wise skills to generate fine-grained, context-aware prompts. We further present a theoretical analysis showing that focusing on high-utility segments supports task identifiability and informs more effective ICL prompt design. Experiments on ALFWorld, BabyAI, and ScienceWorld, using both open-source and proprietary LLMs, show that SkillGen achieves consistent gains, improving progress rate by 5.9%-16.5% on average across models.

</details>


### [36] [Heterogeneous Multi-Agent Proximal Policy Optimization for Power Distribution System Restoration](https://arxiv.org/abs/2511.14730)
*Parya Dolatyabi,Mahdi Khodayar*

Main category: cs.AI

TL;DR: 本文提出了一种异构智能体强化学习框架（HARL），通过异构智能体近端策略优化（HAPPO）实现互联微电网的协调恢复，解决了传统优化方法在非线性约束下的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 大规模停电后恢复配电系统需要顺序切换操作，在非线性约束下重新配置馈线拓扑和协调分布式能源资源，传统优化方法和基于价值的强化学习方法计算效率低且难以扩展。

Method: 采用异构智能体强化学习框架，每个智能体控制具有不同负载、DER容量和开关数量的微电网，使用分散的参与者策略和集中评论家进行训练，通过物理信息化的OpenDSS环境提供完整潮流反馈。

Result: 在IEEE 123总线和IEEE 8500节点系统上的实验表明，HAPPO相比DQN、PPO、MAES等方法具有更快的收敛速度、更高的恢复功率和更平滑的多种子训练效果。

Conclusion: 在HARL框架中引入微电网级异构性，为复杂配电系统恢复提供了可扩展、稳定且约束感知的解决方案。

Abstract: Restoring power distribution systems (PDS) after large-scale outages requires sequential switching operations that reconfigure feeder topology and coordinate distributed energy resources (DERs) under nonlinear constraints such as power balance, voltage limits, and thermal ratings. These challenges make conventional optimization and value-based RL approaches computationally inefficient and difficult to scale. This paper applies a Heterogeneous-Agent Reinforcement Learning (HARL) framework, instantiated through Heterogeneous-Agent Proximal Policy Optimization (HAPPO), to enable coordinated restoration across interconnected microgrids. Each agent controls a distinct microgrid with different loads, DER capacities, and switch counts, introducing practical structural heterogeneity. Decentralized actor policies are trained with a centralized critic to compute advantage values for stable on-policy updates. A physics-informed OpenDSS environment provides full power flow feedback and enforces operational limits via differentiable penalty signals rather than invalid action masking. The total DER generation is capped at 2400 kW, and each microgrid must satisfy local supply-demand feasibility. Experiments on the IEEE 123-bus and IEEE 8500-node systems show that HAPPO achieves faster convergence, higher restored power, and smoother multi-seed training than DQN, PPO, MAES, MAGDPG, MADQN, Mean-Field RL, and QMIX. Results demonstrate that incorporating microgrid-level heterogeneity within the HARL framework yields a scalable, stable, and constraint-aware solution for complex PDS restoration.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [37] [Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation](https://arxiv.org/abs/2511.13972)
*Jeremiah Bohr*

Main category: cs.SE

TL;DR: 该研究比较了基于指令、基于示例和组合提示在代码生成中的风格控制效果，发现组合提示在初始压缩和扩展约束方面表现最佳，指令提示有较大初始效果但扩展约束中等，示例提示初始效果有限且无扩展约束。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成的代码往往过于冗长，与人类基准存在风格差异。需要研究不同提示机制在保持功能准确性的同时，能否持续控制代码风格。

Method: 采用四组系统提示条件（指令、示例、组合和无提示），在双轮协议中让模型首先生成Python任务解决方案，然后在通用改进指令下修订代码，保持用户任务不变（N=160对程序）。

Result: 组合提示产生最强的初始压缩和最大的扩展约束；指令提示显示较大的初始效果和中等扩展约束；示例提示显示适度的初始效果且无扩展约束。

Conclusion: 初始提示有效性和扩展约束是提示设计的两个独立方面，组合方法在两轮工作流程中提供最稳定的风格控制。

Abstract: Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.

</details>


### [38] [Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education](https://arxiv.org/abs/2511.13996)
*Daihan Xu,Diana Martin*

Main category: cs.SE

TL;DR: 本研究通过定性访谈探讨计算机科学学生如何在软件开发项目中策略性和伦理性地使用ChatGPT，发现学生的学习模式从传统独立编程转向AI辅助协作，学生有意识地限制ChatGPT贡献度约30%，但缺乏对AI生成代码的深入分析，需要明确的指导方针。


<details>
  <summary>Details</summary>
Motivation: ChatGPT在计算机科学教育中的使用日益增多，虽然能帮助学生应对编程挑战，但也引发了学术诚信和过度依赖的担忧。现有研究多依赖调查问卷，缺乏对学生策略和伦理意识的深入分析。

Method: 采用半结构化访谈的定性研究方法，探讨英国一所院校计算机科学学生在软件开发项目中如何策略性和伦理性地使用ChatGPT。

Result: 发现学生学习模式从"独立思考-手动编码-迭代调试"转变为"AI辅助构思-交互编程-协作优化"。学生通过对话方式使用ChatGPT加深理解，但保留创意和高级决策任务，通常限制ChatGPT贡献度约30%。然而只有少数学生深入分析AI生成代码，存在批判性参与减少的风险。

Conclusion: 学生拒绝未经授权的使用，强调隐私泄露和技能退化等风险，呼吁教师制定明确的使用指南。研究揭示了学习者与AI动态关系的演变，需要明确指导来支持负责任和教学合理的使用。

Abstract: ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional "independent thinking-manual coding-iterative debugging" to "AI-assisted ideation-interactive programming-collaborative optimization." Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.

</details>


### [39] [LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering](https://arxiv.org/abs/2511.13998)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Roshan Ram,Akshara Prabhakar,Tulika Awalgaonkar,Zixiang Chen,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench-Agent是一个专门评估LLM智能体在真实长上下文软件工程工作流程中的综合框架，扩展了LoCoBench的8000个场景为交互式环境，评估多轮对话、工具使用效率、错误恢复和架构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基准如LoCoBench主要评估单轮长上下文代码理解，无法捕捉真实编码智能体所需的多轮交互特性、工具使用模式和自适应推理能力。

Method: 将LoCoBench的8000个场景扩展为交互式智能体环境，提供8个专业工具（文件操作、搜索、代码分析），在10K到1M token的上下文长度范围内评估，使用9个理解度和效率维度的指标。

Result: 评估发现：(1)智能体表现出显著的长上下文鲁棒性；(2)理解度与效率存在负相关的权衡关系；(3)不同模型间的对话效率差异巨大，战略性的工具使用模式区分了高性能智能体。

Conclusion: 作为首个软件工程领域的长上下文LLM智能体基准，LoCoBench-Agent为测量智能体能力、识别性能差距和推进大规模自主软件开发建立了严谨基础。

Abstract: As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.

</details>


### [40] [FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale](https://arxiv.org/abs/2511.14002)
*Chengpeng Li,Farnaz Behrang,August Shi,Peng Liu*

Main category: cs.SE

TL;DR: FlakyGuard通过将代码视为图结构并使用选择性图探索来找到最相关的上下文，解决了现有方法在工业环境中修复不稳定测试时上下文过多或过少的问题。


<details>
  <summary>Details</summary>
Motivation: 不稳定的测试会浪费开发者时间并减缓发布周期，现有基于LLM的方法在工业环境中由于上下文问题（提供太少或太多上下文）而失败。

Method: 将代码视为图结构，使用选择性图探索来找到最相关的上下文，从而为LLM提供适当的修复信息。

Result: 在工业仓库的真实不稳定测试中，FlakyGuard修复了47.6%的可重现不稳定测试，其中51.8%的修复被开发者接受，比现有方法至少提高22%的修复成功率。

Conclusion: FlakyGuard通过选择性图探索有效解决了不稳定测试修复中的上下文问题，在工业环境中表现出色，开发者对其根因解释给予100%的正面评价。

Abstract: Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.

</details>


### [41] [Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning](https://arxiv.org/abs/2511.14022)
*Pradeep Kumar Sharma,Ishaan Puri,Mantinder Jit Singh,Swapnil Shivaprasad,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 该论文研究了在代码库持续演化的背景下，如何保持代码搜索模型的新鲜度而不丢失对旧代码的记忆。比较了三种更新策略：完全刷新、上下文学习和增量微调，发现增量微调在平衡新旧代码性能方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现代代码库持续演化，导致训练好的代码搜索模型性能随时间退化。需要研究如何在保持对旧代码记忆的同时，让模型适应代码变化。

Method: 将代码新鲜度视为领域漂移问题，比较三种更新策略：(A)完全刷新模型；(B)上下文学习，在推理时注入最近的代码变更；(C)增量微调，使用变更生成的训练集进行微调，控制新旧数据混合比例以避免灾难性遗忘。

Result: 在Flask、SQLAlchemy、Pandas和Poetry等项目中，增量微调配合旧代码感知的混合策略在混合数据集上表现最佳；上下文学习在无法训练时能最快提升新代码性能；完全刷新在追求最大新代码准确率时仍是上限。

Conclusion: 增量微调是保持代码搜索模型新鲜度的最佳平衡策略，上下文学习在训练不可行时是有效的替代方案，完全刷新在需要最高新代码准确率时仍是黄金标准。

Abstract: Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.

</details>


### [42] [LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering](https://arxiv.org/abs/2511.14062)
*Shenglin Zhang,Ziang Chen,Zijing Que,Yilun Liu,Yongqian Sun,Sicheng Wei,Dan Pei,Hailin Li*

Main category: cs.SE

TL;DR: 提出LogPurge框架，通过两阶段过滤算法自动从受污染的日志序列中选择足够的正常子集来训练异常检测模型，显著提高异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法需要干净无异常的日志数据进行训练，但获取这样的数据需要昂贵的人工标注，现有自动清洗方法未能充分整合日志的特定特征和实际语义。

Method: 采用两阶段过滤算法：第一阶段使用大语言模型去除聚类异常模式并增强系统规则；第二阶段使用分治策略将剩余污染区域分解为更小的子问题，通过第一阶段程序有效净化。

Result: 在两个公共数据集和一个工业数据集上的实验表明，该方法平均移除98.74%的异常，同时保留82.39%的正常样本。与最新无监督日志样本选择算法相比，在公共数据集上F-1分数分别提高35.7%和84.11%，在私有数据集上F-1分数提高149.72%。

Conclusion: LogPurge框架有效解决了日志数据自动净化问题，显著提升了日志异常检测模型的训练效果。

Abstract: Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.

</details>


### [43] [A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints](https://arxiv.org/abs/2511.14215)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: 本研究提出了一个经过实证验证的基于Scrum的敏捷框架，专门针对符合DO-178C标准的安全关键航空航天软件。该框架通过定制化角色、工件和事件来满足认证要求，相比传统瀑布模型显著提升了效率和缺陷管理能力。


<details>
  <summary>Details</summary>
Motivation: 航空航天系统日益复杂，需要在敏捷开发与严格的安全认证要求之间找到平衡。传统开发方法难以同时满足快速迭代和DO-178C等严格认证标准的要求。

Method: 开发了一个定制的Scrum敏捷框架，包括多学科产品所有权模型、双重验收标准、独立测试和文档团队、专门的认证联络人等关键增强功能。通过两个可比航空航天项目（一个使用定制敏捷流程，另一个使用传统瀑布模型）进行评估。

Result: 结果显示显著改进：每个需求的总工作量减少76%，缺陷检测速度提高75%，缺陷解决速度提高78%，缺陷密度降低50%以上，同时完全符合DO-178C设计保证等级A的要求。

Conclusion: 敏捷实践和监管合规可以共存，但需要严格的定制化和与认证机构的主动合作。未来可通过工作流自动化、CI/CD实践以及自动化文档、验证和配置管理获得进一步收益。

Abstract: The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.

</details>


### [44] [KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation](https://arxiv.org/abs/2511.14224)
*Anji Li,Mingwei Liu,Zhenxi Chen,Zheng Pei,Zike Li,Dekun Dai,Yanlin Wang,Zibin Zheng*

Main category: cs.SE

TL;DR: KTester是一个集成项目特定知识和测试领域知识的LLM测试生成框架，通过静态分析提取项目结构和使用知识，采用测试领域知识引导的测试用例设计与测试方法生成分离策略，结合多视角提示技术，显著提升了测试生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动化单元测试生成方法在真实项目中难以生成既正确又可维护的测试用例，需要结合项目特定知识和测试领域知识来提升测试生成质量。

Method: KTester框架首先通过静态分析提取项目结构和使用知识，然后采用测试领域知识引导的测试用例设计与测试方法生成分离策略，结合多视角提示技术指导LLM考虑多样化的测试启发式方法，生成的测试遵循结构化模板以提高清晰度和可维护性。

Result: 在多个开源项目上的评估显示，KTester在六个关键指标上显著优于现有最先进的LLM基线方法，执行通过率提高5.69%，行覆盖率提高8.83%，同时需要更少时间并生成更少的测试用例。人工评估也确认KTester生成的测试在正确性、可读性和可维护性方面评分更高。

Conclusion: KTester证明了知识驱动框架在LLM测试生成中的实际优势，通过集成项目特定知识和测试领域知识，能够生成更高质量、更可维护的单元测试。

Abstract: Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.

</details>


### [45] [Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems](https://arxiv.org/abs/2511.14435)
*Angelo Ferrando*

Main category: cs.SE

TL;DR: 本文提出将运行时验证（RV）与大语言模型（LLMs）进行共生集成，RV为LLM驱动的自主系统提供安全保障，而LLMs则通过辅助规范捕获、支持预期推理和处理不确定性来扩展RV能力。


<details>
  <summary>Details</summary>
Motivation: 在涉及学习组件和开放环境的自主系统中，确保安全性和可信度具有挑战性。形式化方法需要完整模型和静态假设，而LLMs虽然擅长模式识别但缺乏形式化保证，因此需要将两者结合以实现更可靠的自主系统。

Method: 提出RV与LLMs的共生集成方法：RV作为LLM驱动自主系统的防护栏，LLMs通过自然语言翻译为形式化构件、辅助规范捕获、支持预期推理和处理不确定性来增强RV能力。

Result: 这种相互强化的方法不同于现有的调查和路线图，能够为自主系统提供更全面的安全保障，同时处理形式化方法和LLMs各自的局限性。

Conclusion: RV与LLMs的共生集成为实现可信赖的自主系统提供了有前景的研究方向，需要进一步探索挑战、认证影响和未来研究方向。

Abstract: Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.

</details>


### [46] [LLM-Assisted Thematic Analysis: Opportunities, Limitations, and Recommendations](https://arxiv.org/abs/2511.14528)
*Tatiane Ornelas,Allysson Allex Araújo,Júlia Araújo,Marina Araújo,Bianca Trinkenreich,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本研究探讨了在软件工程定性研究中整合大语言模型到主题分析方法中的机会、风险和方法论影响，通过反思性工作坊收集了25位研究者的观点。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程定性研究中的使用日益增多，其在解释性过程中的整合对严谨性、透明度和研究者能动性产生了根本性问题，需要深入探讨。

Method: 组织反思性工作坊，引导25位ISERN研究人员通过结构化讨论，使用彩色编码画布记录对LLM辅助开放编码、主题生成和主题审查的看法。

Result: 参与者认识到潜在的效率和可扩展性收益，但强调了与偏见、上下文丢失、可重复性以及LLM快速演化相关的风险，同时强调需要提示素养和持续的人工监督。

Conclusion: 研究结果表明，大语言模型可以作为支持但不能替代解释性分析的工具，为社区关于如何负责任地增强软件工程定性研究的持续反思做出贡献。

Abstract: [Context] Large Language Models (LLMs) are increasingly used to assist qualitative research in Software Engineering (SE), yet the methodological implications of this usage remain underexplored. Their integration into interpretive processes such as thematic analysis raises fundamental questions about rigor, transparency, and researcher agency. [Objective] This study investigates how experienced SE researchers conceptualize the opportunities, risks, and methodological implications of integrating LLMs into thematic analysis. [Method] A reflective workshop with 25 ISERN researchers guided participants through structured discussions of LLM-assisted open coding, theme generation, and theme reviewing, using color-coded canvases to document perceived opportunities, limitations, and recommendations. [Results] Participants recognized potential efficiency and scalability gains, but highlighted risks related to bias, contextual loss, reproducibility, and the rapid evolution of LLMs. They also emphasized the need for prompting literacy and continuous human oversight. [Conclusion] Findings portray LLMs as tools that can support, but not substitute, interpretive analysis. The study contributes to ongoing community reflections on how LLMs can responsibly enhance qualitative research in SE.

</details>


### [47] [FHIRconnect: Towards a seamless integration of openEHR and FHIR](https://arxiv.org/abs/2511.14618)
*Severin Kohler,Jordi Piera Jiménez,Michael Anywar,Lars Fuhrmann,Heather Leslie,Maximilian Meixner,Julian Saß,Florian Kärcher,Diego Boscá,Birger Haarbrandt,Michael Marschollek,Roland Eils*

Main category: cs.SE

TL;DR: FHIRconnect是一个创新的领域特定语言和开源转换引擎，用于实现openEHR和HL7 FHIR之间的标准化双向数据交换，通过三层架构实现65%的映射重用率。


<details>
  <summary>Details</summary>
Motivation: 解决openEHR和HL7 FHIR之间由于数据建模方法根本差异和缺乏标准化转换机制而导致的医疗互操作性挑战。

Method: 开发FHIRconnect领域特定语言和开源转换引擎，采用三层架构，利用国际原型基础支持本地定制，实现24个国际原型到15个FHIR配置文件的映射。

Result: 成功映射了7个临床领域的24个国际原型到15个FHIR配置文件，建立了技术基础，减少对自定义ETL解决方案的依赖。

Conclusion: FHIRconnect为社区驱动的映射标准化建立了技术基础，推进了基于开放标准的医疗IT系统中的语法和语义互操作性。

Abstract: Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in their data modeling approaches and the absence of standardized transformation mechanisms. This paper presents FHIRconnect, a novel domain-specific language and open-source transformation engine that enables standardized, bidirectional data exchange between openEHR and FHIR. Our approach addresses critical interoperability gaps through a triple-layered architecture that achieves 65% mapping reuse across projects by leveraging international archetype-based foundations while supporting local customizations. Using this framework, FHIRconnect successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains. Key contributions include the first comprehensive DSL for openEHR-FHIR transformation with a formal specification, an open-source execution engine (openFHIR), and an accessible mapping library covering high-impact clinical archetypes. Together, these components establish the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems built on open standards.

</details>


### [48] [Why Do We Code? A Theory on Motivations and Challenges in Software Engineering from Education to Practice](https://arxiv.org/abs/2511.14711)
*Aaliyah Chang,Mariam Guizani,Brittany Johnson*

Main category: cs.SE

TL;DR: 本文研究了软件工程从业者从教育到职业转型过程中动机与挑战的相互作用，提出了暴露-追求-评估(EPE)过程模型，揭示了不同暴露类型对内在/外在动机的影响，以及持续存在的归属感障碍等问题。


<details>
  <summary>Details</summary>
Motivation: 探索软件工程从业者在从教育到职业实践转型过程中，动机与挑战如何共同塑造其进入、坚持和演变的过程，这一相互作用尚未得到充分研究。

Method: 采用15个半结构化访谈，运用来自组织行为学的Gioia方法论（一种改进的扎根理论方法），归纳推导动机和挑战的分类法，并构建暴露-追求-评估(EPE)过程模型。

Result: 研究发现：有影响力的早期暴露触发内在动机，而无影响力的暴露需要外在推动；好奇心与避免替代选择是独特的教育驱动因素；归属感障碍是唯一贯穿教育和职业生涯的挑战；职业发展挑战制约外在满足，而技术培训挑战、归属感障碍和动机威胁制约内在满足。

Conclusion: 未满足的动机和反复出现的挑战影响从业者的坚持、职业转变或离开该领域。该理论为设计干预措施提供了基础模型，以增强软件工程教育和实践中的内在满足感并减少系统性障碍。

Abstract: Motivations and challenges jointly shape how individuals enter, persist, and evolve within software engineering (SE), yet their interplay remains underexplored across the transition from education to professional practice. We conducted 15 semi-structured interviews and employed the Gioia Methodology, an adapted grounded theory methodology from organizational behavior, to inductively derive taxonomies of motivations and challenges, and build the Exposure-Pursuit-Evaluation (EPE) Process Model. Our findings reveal that impactful early exposure triggers intrinsic motivations, while non-impactful exposure requires an extrinsic push (e.g., career/ personal goals, external validation). We identify curiosity and avoiding alternatives as a distinct educational drivers, and barriers to belonging as the only challenge persisting across education and career. Our findings show that career progression challenges (e.g., navigating the corporate world) constrain extrinsic fulfillment while technical training challenges, barriers to belonging and threats to motivation constrain intrinsic fulfillment. The theory shows how unmet motivations and recurring challenges influence persistence, career shifts, or departure from the field. Our results provide a grounded model for designing interventions that strengthen intrinsic fulfillment and reduce systemic barriers in SE education and practice.

</details>
