<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.CR](#cs.CR) [Total: 26]
- [cs.AI](#cs.AI) [Total: 36]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AgentArcEval: An Architecture Evaluation Method for Foundation Model based Agents](https://arxiv.org/abs/2510.21031)
*Qinghua Lu,Dehai Zhao,Yue Liu,Hao Zhang,Liming Zhu,Xiwei Xu,Angela Shi,Tristan Tan,Rick Kazman*

Main category: cs.SE

TL;DR: 提出了AgentArcEval方法，专门用于评估基于基础模型的智能体架构，并提供了一个智能体特定通用场景目录来指导架构设计和评估。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以评估智能体架构，因为智能体具有复合架构、自主非确定性行为和持续演化等独特特性，需要专门的评估方法。

Method: 开发了AgentArcEval评估方法和智能体特定通用场景目录，通过真实世界税务助手Luna的案例研究来验证其有效性。

Result: 成功演示了AgentArcEval方法和场景目录在评估真实世界智能体架构方面的实用性。

Conclusion: AgentArcEval为基于基础模型的智能体架构评估提供了有效的解决方案，能够应对智能体架构的复杂性。

Abstract: The emergence of foundation models (FMs) has enabled the development of
highly capable and autonomous agents, unlocking new application opportunities
across a wide range of domains. Evaluating the architecture of agents is
particularly important as the architectural decisions significantly impact the
quality attributes of agents given their unique characteristics, including
compound architecture, autonomous and non-deterministic behaviour, and
continuous evolution. However, these traditional methods fall short in
addressing the evaluation needs of agent architecture due to the unique
characteristics of these agents. Therefore, in this paper, we present
AgentArcEval, a novel agent architecture evaluation method designed specially
to address the complexities of FM-based agent architecture and its evaluation.
Moreover, we present a catalogue of agent-specific general scenarios, which
serves as a guide for generating concrete scenarios to design and evaluate the
agent architecture. We demonstrate the usefulness of AgentArcEval and the
catalogue through a case study on the architecture evaluation of a real-world
tax copilot, named Luna.

</details>


### [2] [BDiff: Block-aware and Accurate Text-based Code Differencing](https://arxiv.org/abs/2510.21094)
*Yao Lu,Wanwei Liu,Tanghaoran Zhang,Kang Yang,Yang Zhang,Wenyu Xu,Longfei Sun,Xinjun Mao,Shuzheng Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: BDiff是一种基于文本的代码差异分析算法，能够识别块级和行级编辑操作，相比现有工具能生成更高质量的差异结果。


<details>
  <summary>Details</summary>
Motivation: 现有代码差异分析工具在处理跨多行的块级编辑操作时存在局限，只能将其表示为离散的行级操作序列，这降低了变更理解的效率。

Method: 基于传统差异算法构建候选映射集，使用Kuhn-Munkres算法计算最优映射，最小化编辑脚本大小并贴近开发者意图。

Result: 实验表明BDiff在差异结果质量上优于现有工具（包括LLMs），同时保持竞争力的运行时间性能。

Conclusion: BDiff能有效识别块级编辑操作，提升代码差异分析质量，而LLMs在代码差异任务中结果质量不可靠且运行效率不可行。

Abstract: Code differencing is a fundamental technique in software engineering practice
and research. While researchers have proposed text-based differencing
techniques capable of identifying line changes over the past decade, existing
methods exhibit a notable limitation in identifying edit actions (EAs) that
operate on text blocks spanning multiple lines. Such EAs are common in
developers' practice, such as moving a code block for conditional branching or
duplicating a method definition block for overloading. Existing tools represent
such block-level operations as discrete sequences of line-level EAs, compelling
developers to manually correlate them and thereby substantially impeding the
efficiency of change comprehension. To address this issue, we propose BDiff, a
text-based differencing algorithm capable of identifying two types of
block-level EAs and five types of line-level EAs. Building on traditional
differencing algorithms, we first construct a candidate set containing all
possible line mappings and block mappings. Leveraging the Kuhn-Munkres
algorithm, we then compute the optimal mapping set that can minimize the size
of the edit script (ES) while closely aligning with the original developer's
intent. To validate the effectiveness of BDiff, we selected five
state-of-the-art tools, including large language models (LLMs), as baselines
and adopted a combined qualitative and quantitative approach to evaluate their
performance in terms of ES size, result quality, and running time. Experimental
results show that BDiff produces higher-quality differencing results than
baseline tools while maintaining competitive runtime performance. Our
experiments also show the unreliability of LLMs in code differencing tasks
regarding result quality and their infeasibility in terms of runtime
efficiency. We have implemented a web-based visual differencing tool.

</details>


### [3] [R2ComSync: Improving Code-Comment Synchronization with In-Context Learning and Reranking](https://arxiv.org/abs/2510.21106)
*Zhen Yang,Hongyi Lin,Xiao Yu,Jacky Wai Keung,Shuo Liu,Pak Yuen Patrick Chan,Yicheng Sun,Fengji Zhang*

Main category: cs.SE

TL;DR: R2ComSync是一个基于检索增强和重排序的代码-注释同步方法，通过集成混合检索和多轮重排序策略，显著提升了大型语言模型在代码注释同步任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的代码-注释同步方法存在泛化能力不足或需要大量特定任务学习资源的问题，而大型语言模型虽然潜力巨大，但在该任务中表现不佳，主要原因是缺乏有效的上下文学习示例和正确候选的优先级排序。

Method: 提出R2ComSync方法，包含两个创新点：(1) 集成混合检索：同时考虑代码-注释语义和变更模式的相似性，创建有效的上下文学习提示；(2) 多轮重排序策略：通过大规模样本分析得出三个重要规则，逐步利用这些规则对LLM推理结果进行重排序，优先选择相对正确的候选。

Result: 在三个涵盖Java和Python的CCS数据集上使用五个最新LLM进行评估，与五个SOTA方法比较，实验证明R2ComSync具有优越性能，定量和定性分析都表明其同步的注释质量显著更高。

Conclusion: R2ComSync通过检索增强和重排序策略有效解决了LLM在代码-注释同步任务中的局限性，显著提升了同步质量和性能，为自动化软件维护提供了有力工具。

Abstract: Code-Comment Synchronization (CCS) aims to synchronize the comments with code
changes in an automated fashion, thereby significantly reducing the workload of
developers during software maintenance and evolution. While previous studies
have proposed various solutions that have shown success, they often exhibit
limitations, such as a lack of generalization ability or the need for extensive
task-specific learning resources. This motivates us to investigate the
potential of Large Language Models (LLMs) in this area. However, a pilot
analysis proves that LLMs fall short of State-Of-The-Art (SOTA) CCS approaches
because (1) they lack instructive demonstrations for In-Context Learning (ICL)
and (2) many correct-prone candidates are not prioritized.To tackle the above
challenges, we propose R2ComSync, an ICL-based code-Comment Synchronization
approach enhanced with Retrieval and Re-ranking. Specifically, R2ComSync
carries corresponding two novelties: (1) Ensemble hybrid retrieval. It equally
considers the similarity in both code-comment semantics and change patterns
when retrieval, thereby creating ICL prompts with effective examples. (2)
Multi-turn re-ranking strategy. We derived three significant rules through
large-scale CCS sample analysis. Given the inference results of LLMs, it
progressively exploits three re-ranking rules to prioritize relatively
correct-prone candidates. We evaluate R2ComSync using five recent LLMs on three
CCS datasets covering both Java and Python programming languages, and make
comparisons with five SOTA approaches. Extensive experiments demonstrate the
superior performance of R2ComSync against other approaches. Moreover, both
quantitative and qualitative analyses provide compelling evidence that the
comments synchronized by our proposal exhibit significantly higher quality.}

</details>


### [4] [GreenMalloc: Allocator Optimisation for Industrial Workloads](https://arxiv.org/abs/2510.21405)
*Aidan Dakhama,W. B. Langdon,Hector D. Menendez,Karine Even-Mendoza*

Main category: cs.SE

TL;DR: GreenMalloc是一个基于多目标搜索的框架，用于自动配置内存分配器，使用NSGA-II算法和rand_malloc作为轻量级代理基准测试工具，在保持运行效率的同时显著减少堆内存使用。


<details>
  <summary>Details</summary>
Motivation: 传统内存分配器配置通常依赖手动调优，难以在多个目标（如内存使用和运行效率）之间找到最优平衡。需要自动化方法来高效探索配置空间并优化内存分配器性能。

Method: 使用NSGA-II多目标遗传算法，结合rand_malloc作为轻量级代理基准测试工具，从执行轨迹中高效探索分配器参数，并将最佳配置转移到gem5系统模拟器进行验证。

Result: 在GNU glibc malloc和Google TCMalloc两种分配器上的实验表明，该方法能在不同工作负载下实现高达4.1%的平均堆使用减少，同时运行效率没有损失，甚至还有0.25%的提升。

Conclusion: GreenMalloc框架能够有效自动化内存分配器配置过程，在保持运行效率的同时显著优化内存使用，为内存分配器性能调优提供了实用的自动化解决方案。

Abstract: We present GreenMalloc, a multi objective search-based framework for
automatically configuring memory allocators. Our approach uses NSGA II and
rand_malloc as a lightweight proxy benchmarking tool. We efficiently explore
allocator parameters from execution traces and transfer the best configurations
to gem5, a large system simulator, in a case study on two allocators: the GNU
C/CPP compiler's glibc malloc and Google's TCMalloc. Across diverse workloads,
our empirical results show up to 4.1 percantage reduction in average heap usage
without loss of runtime efficiency; indeed, we get a 0.25 percantage reduction.

</details>


### [5] [Context Engineering for AI Agents in Open-Source Software](https://arxiv.org/abs/2510.21413)
*Seyedmoein Mohsenimofidi,Matthias Galster,Christoph Treude,Sebastian Baltes*

Main category: cs.SE

TL;DR: 研究了466个开源项目中AI配置文件（如AGENTS.md）的采用情况，重点关注开发者如何提供上下文信息、文件结构变化以及这些文件的演化过程。


<details>
  <summary>Details</summary>
Motivation: 随着基于代理的AI编码助手的发展，需要为AI代理提供足够的项目上下文信息。AGENTS.md作为潜在标准格式出现，但缺乏对其实际采用情况的了解。

Method: 对466个开源软件项目进行初步研究，分析AI配置文件的采用情况、内容信息、呈现方式及时序演化。

Result: 目前尚未建立统一的结构标准，上下文提供方式存在很大差异（描述性、规范性、禁止性、解释性、条件性）。AGENTS.md文件的提交修改分析提供了项目持续维护这些文件的初步见解。

Conclusion: AI配置文件的采用为研究真实世界的提示和上下文工程提供了独特机会，研究文件结构或呈现方式的修改如何影响生成内容质量具有巨大潜力。

Abstract: GenAI-based coding assistants have disrupted software development. Their next
generation is agent-based, operating with more autonomy and potentially without
human oversight. One challenge is to provide AI agents with sufficient context
about the software projects they operate in. Like humans, AI agents require
contextual information to develop solutions that are in line with the target
architecture, interface specifications, coding guidelines, standard workflows,
and other project-specific policies. Popular AI agents for software development
(e.g., Claude Code) advocate for maintaining tool-specific version-controlled
Markdown files that cover aspects such as the project structure, building and
testing, or code style. The content of these files is automatically added to
each prompt. AGENTS.md has emerged as a potential standard that consolidates
tool-specific formats. However, little is known about whether and how
developers adopt this format. Therefore, in this paper, we present the results
of a preliminary study investigating the adoption of AI configuration files in
466 open-source software projects, what information developers provide in these
files, how they present that information, and how they evolve over time. Our
findings indicate that there is no established structure yet, and that there is
a lot of variation in terms of how context is provided (descriptive,
prescriptive, prohibitive, explanatory, conditional). We see great potential in
studying which modifications in structure or presentation can positively affect
the quality of the generated content. Finally, our analysis of commits that
have modified AGENTS.md files provides first insights into how projects
continuously extend and maintain these files. We conclude the paper by
outlining how the adoption of AI configuration files in provides a unique
opportunity to study real-world prompt and context engineering.

</details>


### [6] [Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification](https://arxiv.org/abs/2510.21443)
*Mohammad Amin Zadenoori,Vincenzo De Martino,Jacek Dabrowski,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: 小型语言模型(SLMs)在需求工程分类任务中几乎达到大型语言模型(LLMs)的性能，尽管模型大小相差300倍，且具有隐私、成本和本地部署优势。


<details>
  <summary>Details</summary>
Motivation: LLMs在需求工程任务中表现良好，但存在计算成本高、数据共享风险和依赖外部服务的问题。SLMs提供了轻量级、本地可部署的替代方案。

Method: 比较了8个模型（3个LLMs和5个SLMs）在PROMISE、PROMISE Reclass和SecReq数据集上的需求分类任务表现。

Result: LLMs平均F1分数比SLMs高2%，但差异无统计学意义。SLMs在所有数据集上几乎达到LLMs性能，在PROMISE Reclass数据集上召回率甚至超过LLMs。数据集特征对性能的影响比模型大小更重要。

Conclusion: SLMs是需求分类任务中LLMs的有效替代方案，在隐私、成本和本地部署方面具有优势。

Abstract: [Context and motivation] Large language models (LLMs) show notable results in
natural language processing (NLP) tasks for requirements engineering (RE).
However, their use is compromised by high computational cost, data sharing
risks, and dependence on external services. In contrast, small language models
(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]
It remains unclear how well SLMs perform compared to LLMs in RE tasks in terms
of accuracy. [Results] Our preliminary study compares eight models, including
three LLMs and five SLMs, on requirements classification tasks using the
PROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although
LLMs achieve an average F1 score of 2% higher than SLMs, this difference is not
statistically significant. SLMs almost reach LLMs performance across all
datasets and even outperform them in recall on the PROMISE Reclass dataset,
despite being up to 300 times smaller. We also found that dataset
characteristics play a more significant role in performance than model size.
[Contribution] Our study contributes with evidence that SLMs are a valid
alternative to LLMs for requirements classification, offering advantages in
privacy, cost, and local deployability.

</details>


### [7] [Scalpel: Automotive Deep Learning Framework Testing via Assembling Model Components](https://arxiv.org/abs/2510.21451)
*Yinglong Zou,Juan Zhai,Chunrong Fang,An Guo,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: Scalpel是一种针对自动驾驶深度学习框架的测试方法，通过组件级模型生成来解决现有测试方法无法检测框架质量问题的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有DL框架测试方法无法检测自动驾驶系统中框架特有的质量问题，如内存崩溃和错误分配，因为生成的测试模型缺乏多输入/输出张量处理、多模态数据处理和多级数据特征提取等关键能力。

Method: Scalpel在模型组件级别生成测试输入模型，通过维护和更新模型组件库（包括头部、颈部和骨干网络），选择、变异和组装这些组件来生成支持自动驾驶系统所需能力的模型。

Result: 成功生成的模型被添加回组件库以丰富资源库，新生成的模型通过差分测试在自动驾驶系统中部署来测试汽车DL框架。

Conclusion: Scalpel填补了现有测试方法的空白，能够有效检测自动驾驶DL框架特有的质量问题，提高了框架的可靠性和安全性。

Abstract: Deep learning (DL) plays a key role in autonomous driving systems. DL models
support perception modules, equipped with tasks such as object detection and
sensor fusion. These DL models enable vehicles to process multi-sensor inputs
to understand complex surroundings. Deploying DL models in autonomous driving
systems faces stringent challenges, including real-time processing, limited
computational resources, and strict power constraints. To address these
challenges, automotive DL frameworks (e.g., PaddleInference) have emerged to
optimize inference efficiency. However, these frameworks encounter unique
quality issues due to their more complex deployment environments, such as
crashes stemming from limited scheduled memory and incorrect memory allocation.
Unfortunately, existing DL framework testing methods fail to detect these
quality issues due to the failure in deploying generated test input models, as
these models lack three essential capabilities: (1) multi-input/output tensor
processing, (2) multi-modal data processing, and (3) multi-level data feature
extraction. These capabilities necessitate specialized model components, which
existing testing methods neglect during model generation. To bridge this gap,
we propose Scalpel, an automotive DL frameworks testing method that generates
test input models at the model component level. Scalpel generates models by
assembling model components (heads, necks, backbones) to support capabilities
required by autonomous driving systems. Specifically, Scalpel maintains and
updates a repository of model components, generating test inputs by selecting,
mutating, and assembling them. Successfully generated models are added back to
enrich the repository. Newly generated models are then deployed within the
autonomous driving system to test automotive DL frameworks via differential
testing.

</details>


### [8] [Towards Socio-Technical Topology-Aware Adaptive Threat Detection in Software Supply Chains](https://arxiv.org/abs/2510.21452)
*Thomas Welsh,Kristófer Finnsson,Brynjólfur Stefánsson,Helmut Neukirchen*

Main category: cs.SE

TL;DR: 提出使用社会技术模型来支持软件供应链的自适应威胁检测，通过分析XZ Utils攻击案例说明监控技术和社交数据可以识别可疑行为趋势


<details>
  <summary>Details</summary>
Motivation: 软件供应链攻击日益增多，但由于其复杂性，普遍漏洞分析具有挑战性。现有工作主要关注技术方法，缺乏通过理解社会技术动态来指导威胁检测的方法

Method: 提出研究愿景：开发和研究使用社会技术模型来支持软件供应链的自适应威胁检测。通过分析XZ Utils攻击案例，展示监控技术和社交数据如何识别可疑行为

Result: 监控技术和社交数据可以识别指示可疑行为的趋势，从而为有针对性和深入的漏洞评估提供信息

Conclusion: 需要进一步研究开发者分析、软件分析、去中心化适应等技术，以及建立软件供应链安全研究的测试平台

Abstract: Software supply chains (SSCs) are complex systems composed of dynamic,
heterogeneous technical and social components which collectively achieve the
production and maintenance of software artefacts. Attacks on SSCs are
increasing, yet pervasive vulnerability analysis is challenging due to their
complexity. Therefore, threat detection must be targeted, to account for the
large and dynamic structure, and adaptive, to account for its change and
diversity. While current work focuses on technical approaches for monitoring
supply chain dependencies and establishing component controls, approaches which
inform threat detection through understanding the socio-technical dynamics are
lacking. We outline a position and research vision to develop and investigate
the use of socio-technical models to support adaptive threat detection of SSCs.
We motivate this approach through an analysis of the XZ Utils attack whereby
malicious actors undermined the maintainers' trust via the project's GitHub and
mailing lists. We highlight that monitoring technical and social data can
identify trends which indicate suspicious behaviour to then inform targeted and
intensive vulnerability assessment. We identify challenges and research
directions to achieve this vision considering techniques for developer and
software analysis, decentralised adaptation and the need for a test bed for
software supply chain security research.

</details>


### [9] [Risk Management for Mitigating Benchmark Failure Modes: BenchRisk](https://arxiv.org/abs/2510.21460)
*Sean McGregor,Victor Lu,Vassil Tashev,Armstrong Foundjem,Aishwarya Ramasethu,Sadegh AlMahdi Kazemi Zarkouei,Chris Knotz,Kongtao Chen,Alicia Parrish,Anka Reuel,Heather Frase*

Main category: cs.SE

TL;DR: 本文提出了BenchRisk框架，用于评估大型语言模型基准测试的风险，识别了57种潜在失效模式和196种缓解策略，帮助用户更可靠地使用基准测试结果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试可能存在各种失效模式，影响基准测试的偏差、方差、覆盖范围和用户理解能力，导致部署决策不可靠。

Method: 基于NIST风险管理流程，迭代分析26个流行基准测试，识别失效模式和缓解策略，开发BenchRisk评分系统评估基准测试风险。

Result: 所有26个基准测试在五个维度（全面性、可理解性、一致性、正确性、持久性）中至少一个存在显著风险，BenchRisk能够比较不同基准测试的风险水平。

Conclusion: LLM基准测试领域存在重要研究空白，BenchRisk作为开源工具有助于识别和分享风险及其缓解措施，提高基准测试的可靠性。

Abstract: Large language model (LLM) benchmarks inform LLM use decisions (e.g., "is
this LLM safe to deploy for my use case and context?"). However, benchmarks may
be rendered unreliable by various failure modes that impact benchmark bias,
variance, coverage, or people's capacity to understand benchmark evidence.
Using the National Institute of Standards and Technology's risk management
process as a foundation, this research iteratively analyzed 26 popular
benchmarks, identifying 57 potential failure modes and 196 corresponding
mitigation strategies. The mitigations reduce failure likelihood and/or
severity, providing a frame for evaluating "benchmark risk," which is scored to
provide a metaevaluation benchmark: BenchRisk. Higher scores indicate that
benchmark users are less likely to reach an incorrect or unsupported conclusion
about an LLM. All 26 scored benchmarks present significant risk within one or
more of the five scored dimensions (comprehensiveness, intelligibility,
consistency, correctness, and longevity), which points to important open
research directions for the field of LLM benchmarking. The BenchRisk workflow
allows for comparison between benchmarks; as an open-source tool, it also
facilitates the identification and sharing of risks and their mitigations.

</details>


### [10] [Wisdom and Delusion of LLM Ensembles for Code Generation and Repair](https://arxiv.org/abs/2510.21513)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 本文研究发现，在软件工程任务中，通过多样性策略组合多个LLM可以显著超越单个最佳模型，实现高达83%的性能提升，而基于共识的策略容易陷入"流行度陷阱"。


<details>
  <summary>Details</summary>
Motivation: 当前追求单一大型语言模型处理所有软件工程任务既资源密集又忽视了模型间的互补性潜力，但如何最大化集成模型的潜力尚不明确。

Method: 实证比较了来自5个家族的10个LLM个体模型和3个集成模型，在代码生成和程序修复等3个软件工程基准上进行评估，分析了模型间的互补性和性能差距，并评估了多种从集成候选池中选择正确解决方案的启发式方法。

Result: 集成模型的理论上限性能比最佳单模型高出83%，基于共识的策略会放大常见但不正确的输出，而基于多样性的策略可实现高达95%的理论潜力，即使在小型两模型集成中也有效。

Conclusion: 通过多样性策略组合多个LLM提供了一种成本效益高的性能提升方式，能够充分利用不同模型的独特优势。

Abstract: Today's pursuit of a single Large Language Model (LMM) for all software
engineering tasks is resource-intensive and overlooks the potential benefits of
complementarity, where different models contribute unique strengths. However,
the degree to which coding LLMs complement each other and the best strategy for
maximizing an ensemble's potential are unclear, leaving practitioners without a
clear path to move beyond single-model systems.
  To address this gap, we empirically compare ten individual LLMs from five
families, and three ensembles of these LLMs across three software engineering
benchmarks covering code generation and program repair. We assess the
complementarity between models and the performance gap between the best
individual model and the ensembles. Next, we evaluate various selection
heuristics to identify correct solutions from an ensemble's candidate pool.
  We find that the theoretical upperbound for an ensemble's performance can be
83% above the best single model. Our results show that consensus-based
strategies for selecting solutions fall into a "popularity trap," amplifying
common but incorrect outputs. In contrast, a diversity-based strategy realizes
up to 95% of this theoretical potential, and proves effective even in small
two-model ensembles, enabling a cost-efficient way to enhance performance by
leveraging multiple LLMs.

</details>


### [11] [Lights-Out: An Automated Ground Segment for unstaffed Satellite Operations](https://arxiv.org/abs/2510.21516)
*Marvin Böcker,Ralph Biggins,Michael Schmeing*

Main category: cs.SE

TL;DR: 提出了首个无人值守、全自动地面段运行概念，应用于德国Heinrich Hertz卫星通信任务，实现了卫星平台操作的完全自动化，包括自动跟踪、遥测和指令控制。


<details>
  <summary>Details</summary>
Motivation: 为德国Heinrich Hertz卫星通信任务开发自动化地面段系统，实现24/7无人值守运行，提高操作效率并降低人力成本。

Method: 采用预规划和自动执行的时间表进行指令控制，用户任务与主任务计划分开规划并自动协调，建立自动监控系统监测卫星和地面段资产，配置可定制的应急响应机制。

Result: 成功实现了卫星平台在非工作时间段的完全自动化操作，建立了支持24/7访问的自助服务用户门户，用户可灵活调度实验并实时监控执行情况。

Conclusion: 该自动化概念在Heinrich Hertz任务中成功应用，证明了全自动地面段运行的可行性，为未来卫星任务提供了可复用的自动化操作模式。

Abstract: We present our approach for a periodically unstaffed, fully automated ground
segment. The concept is in use for the first time on the German satellite
communications mission Heinrich Hertz on behalf of the German Space Agency at
DLR. Heinrich Hertz was launched in July 2023 and offers access to scientific
and technical experiments to its users. The mission utilizes major automation
concepts for the satellite platform operations, allowing fully automated
operations outside of office hours. The concept includes tracking, telemetry
and commanding (TTC) of the satellite. Pre-planned and automatically executed
schedules enable commanding without human interaction. The user mission
schedule is planned separately from the main mission schedule and is
automatically de-conflicted. The automatic monitoring concept monitors the
systems of the satellite and all assets in the ground segment and triggers
reactions in operator-configurable ways depending on the mission needs, for
example emergency notifications or automated execution of flight operation
procedures. Additionally, the concept also puts special emphasis on a
self-service user portal that provides flexible access 24/7, even when the
control center is not staffed. The portal allows external users of the payload
to schedule pre-defined experiments, monitor the live execution of the
experiment with browser-based displays and access ground station telemetry and
dedicated RF test equipment during the time of their scheduled experiment.
Tasks can be planned long in advance as well as with a short reaction time
(less than 1 minute), which allows, for example, the reconfiguration of the
payload during a running experiment.

</details>


### [12] [Privacy by Design: Aligning GDPR and Software Engineering Specifications with a Requirements Engineering Approach](https://arxiv.org/abs/2510.21591)
*Oleksandr Kosenkov,Ehsan Zabardast,Davide Fucci,Daniel Mendez,Michael Unterkalmsteiner*

Main category: cs.SE

TL;DR: 本研究探讨了GDPR合规性中需求与系统规范的联合规范方法，提出了基于法律概念建模的方法来支持隐私设计。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分处理GDPR中问题空间与解决方案空间的复杂交叉，缺乏对从业者视角的理解，需要开发联合规范方法来确保隐私设计。

Method: 通过回顾二手研究和相关一手研究，进行从业者访谈，调查实践现状并理解规范目标，开发并评估了基于GDPR法律概念建模的规范方法。

Result: GDPR中问题空间与解决方案空间的关系对支持隐私设计至关重要，该方法能够有效捕获法律知识、支持规范透明度和可追溯性。

Conclusion: GDPR需求需要在工程生命周期不同抽象层次中解决，法律知识应被捕获在规范中以满足不同利益相关者需求并确保合规性，同时需要进一步操作化该方法。

Abstract: Context: Consistent requirements and system specifications are essential for
the compliance of software systems towards the General Data Protection
Regulation (GDPR). Both artefacts need to be grounded in the original text and
conjointly assure the achievement of privacy by design (PbD). Objectives: There
is little understanding of the perspectives of practitioners on specification
objectives and goals to address PbD. Existing approaches do not account for the
complex intersection between problem and solution space expressed in GDPR. In
this study we explore the demand for conjoint requirements and system
specification for PbD and suggest an approach to address this demand. Methods:
We reviewed secondary and related primary studies and conducted interviews with
practitioners to (1) investigate the state-of-practice and (2) understand the
underlying specification objectives and goals (e.g., traceability). We
developed and evaluated an approach for requirements and systems specification
for PbD, and evaluated it against the specification objectives. Results: The
relationship between problem and solution space, as expressed in GDPR, is
instrumental in supporting PbD. We demonstrate how our approach, based on the
modeling GDPR content with original legal concepts, contributes to
specification objectives of capturing legal knowledge, supporting specification
transparency, and traceability. Conclusion: GDPR demands need to be addressed
throughout different levels of abstraction in the engineering lifecycle to
achieve PbD. Legal knowledge specified in the GDPR text should be captured in
specifications to address the demands of different stakeholders and ensure
compliance. While our results confirm the suitability of our approach to
address practical needs, we also revealed specific needs for the future
effective operationalization of the approach.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [13] [FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics](https://arxiv.org/abs/2510.20852)
*Safa Ben Atitallah,Maha Driss,Henda Ben Ghezela*

Main category: cs.CR

TL;DR: 提出了一种基于微服务架构和联邦学习的物联网数据分析解决方案，通过将云计算能力推向边缘来降低延迟、保护数据隐私，并在恶意软件检测用例中实现了99.24%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 物联网数据分析和处理面临隐私安全、延迟和带宽拥堵等挑战，需要一种既能保护数据隐私又能提供高效数据分析的分布式解决方案。

Method: 采用微服务架构将物联网应用构建为细粒度、松耦合的可重用实体，结合联邦学习技术提供智能微服务，在边缘/雾计算环境中实现分布式数据分析。

Result: 在恶意软件检测和分类用例中，使用MaleVis数据集（包含14,000多张RGB转换图像，涵盖25个恶意软件类别和1个良性类别），提出的方法在检测和分类性能上优于现有最先进方法，准确率达到99.24%。

Conclusion: 基于微服务和联邦学习的架构能够有效解决物联网数据分析中的隐私保护、延迟和带宽问题，为边缘计算环境提供了高效、灵活和可扩展的数据分析解决方案。

Abstract: The Internet of Things (IoT) has recently proliferated in both size and
complexity. Using multi-source and heterogeneous IoT data aids in providing
efficient data analytics for a variety of prevalent and crucial applications.
To address the privacy and security concerns raised by analyzing IoT data
locally or in the cloud, distributed data analytics techniques were proposed to
collect and analyze data in edge or fog devices. In this context, federated
learning has been recommended as an ideal distributed machine/deep
learning-based technique for edge/fog computing environments. Additionally, the
data analytics results are time-sensitive; they should be generated with
minimal latency and high reliability. As a result, reusing efficient
architectures validated through a high number of challenging test cases would
be advantageous. The work proposed here presents a solution using a
microservices-based architecture that allows an IoT application to be
structured as a collection of fine-grained, loosely coupled, and reusable
entities. The proposed solution uses the promising capabilities of federated
learning to provide intelligent microservices that ensure efficient, flexible,
and extensible data analytics. This solution aims to deliver cloud calculations
to the edge to reduce latency and bandwidth congestion while protecting the
privacy of exchanged data. The proposed approach was validated through an
IoT-malware detection and classification use case. MaleVis, a publicly
available dataset, was used in the experiments to analyze and validate the
proposed approach. This dataset included more than 14,000 RGB-converted images,
comprising 25 malware classes and one benign class. The results showed that our
proposed approach outperformed existing state-of-the-art methods in terms of
detection and classification performance, with a 99.24%.

</details>


### [14] [FPT-Noise: Dynamic Scene-Aware Counterattack for Test-Time Adversarial Defense in Vision-Language Models](https://arxiv.org/abs/2510.20856)
*Jia Deng,Jin Li,Zhenhua Zhao,Shaowei Wang*

Main category: cs.CR

TL;DR: 提出FPT-Noise测试时防御方法，在不进行昂贵微调的情况下增强CLIP模型的对抗鲁棒性，通过动态特征调制器和特征感知阈值来区分干净图像和对抗样本。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）在零样本泛化方面表现出色，但对视觉模态的对抗攻击高度脆弱。传统对抗训练方法计算成本高昂，需要寻找无需重新训练的有效防御方案。

Method: 1. 动态特征调制器生成图像特定和攻击自适应的噪声强度参数；2. 分析CLIP图像特征变化率，建立特征感知阈值区分干净和对抗图像；3. 集成场景感知调节和测试时变换集成来减轻残留噪声影响。

Result: FPT-Noise显著优于现有测试时防御方法，在AutoAttack下将平均鲁棒准确率从0.07%提升至56.86%，同时在干净图像上性能损失很小（-1.1%）。

Conclusion: FPT-Noise是一种高效且有效的测试时防御方法，能够在保持干净图像性能的同时显著提升CLIP模型的对抗鲁棒性，无需昂贵的重新训练过程。

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot generalizability across diverse downstream tasks. However, recent
studies have revealed that VLMs, including CLIP, are highly vulnerable to
adversarial attacks, particularly on their visual modality. Traditional methods
for improving adversarial robustness, such as adversarial training, involve
extensive retraining and can be computationally expensive. In this paper, we
propose a new Test-Time defense: Feature Perception Threshold Counterattack
Noise (FPT-Noise), which enhances the adversarial robustness of CLIP without
costly fine-tuning. Our core contributions are threefold: First, we introduce a
Dynamic Feature Modulator that dynamically generate an image-specific and
attack-adaptive noise intensity parameter. Second, We reanalyzed the image
features of CLIP. When images are exposed to different levels of noise, clean
images and adversarial images exhibit distinct rates of feature change. We
established a feature perception threshold to distinguish clean images from
attacked ones. Finally, we integrate a Scene-Aware Regulation guided by a
stability threshold and leverage Test-Time Transformation Ensembling (TTE) to
further mitigate the impact of residual noise and enhance robustness.Extensive
experimentation has demonstrated that FPT-Noise significantly outperforms
existing Test-Time defense methods, boosting average robust accuracy from 0.07%
to 56.86% under AutoAttack while maintaining high performance on clean images
(-1.1%). The code will be made public following the publication of the study.
The code will be made public following the publication of the study.

</details>


### [15] [Everyone Needs AIR: An Agnostic Incident Reporting Framework for Cybersecurity in Operational Technology](https://arxiv.org/abs/2510.20858)
*Nubio Vidal,Naghmeh Moradpoor,Leandros Maglaras*

Main category: cs.CR

TL;DR: 提出了Agnostic Incident Reporting (AIR)框架，包含25个元素和7个分组，用于标准化实时OT事件报告，支持技术协调和监管一致性。


<details>
  <summary>Details</summary>
Motivation: OT网络与IT日益融合扩大了攻击面，但现有标准未明确事件期间应捕获哪些数据，阻碍了利益相关者间的协调。

Method: 开发AIR框架，将其映射到主要OT标准，定义集成触发点，并回顾性应用于2015年乌克兰电网事件进行评估。

Result: 评估表明AIR能将高层次需求转化为具体字段，无供应商依赖地覆盖现有框架，支持响应期间的情境感知和通信。

Conclusion: AIR为标准化实时OT事件报告提供了基础，同时支持技术协调和监管对齐。

Abstract: Operational technology (OT) networks are increasingly coupled with
information technology (IT), expanding the attack surface and complicating
incident response. Although OT standards emphasise incident reporting and
evidence preservation, they do not specify what data to capture during an
incident, which hinders coordination across stakeholders. In contrast, IT
guidance defines reporting content but does not address OT constraints. This
paper presents the Agnostic Incident Reporting (AIR) framework for live OT
incident reporting. AIR comprises 25 elements organised into seven groups to
capture incident context, chronology, impacts, and actions, tailored to
technical, managerial, and regulatory needs. We evaluate AIR by mapping it to
major OT standards, defining activation points for integration and triggering
established OT frameworks, and then retrospectively applying it to the 2015
Ukrainian distribution grid incident. The evaluation indicates that AIR
translates high-level requirements into concrete fields, overlays existing
frameworks without vendor dependence, and can support situational awareness and
communication during response. AIR offers a basis for standardising live OT
incident reporting while supporting technical coordination and regulatory
alignment.

</details>


### [16] [A new measure for dynamic leakage based on quantitative information flow](https://arxiv.org/abs/2510.20922)
*Luigi D. C. Soares,Mário S. Alvim,Natasha Fernandes*

Main category: cs.CR

TL;DR: 本文提出了一个新的动态信息泄漏定义，将攻击者对秘密值的信念与衡量攻击成功率的基线分布解耦，并验证了该定义满足信息论公理，同时与静态视角兼容。


<details>
  <summary>Details</summary>
Motivation: 定量信息流(QIF)中，静态视角已有成熟理论，但动态视角仍缺乏同等理论深度。动态视角关注具体系统运行中的信息泄漏，对系统监控和跟踪器很重要。

Method: 提出新的动态泄漏定义，将攻击者信念与基线分布分离；验证该定义满足非干扰性、单调性和数据处理不等式等公理；分析强版本公理不成立的条件；展示与静态视角的兼容性；在隐私保护数据发布攻击中应用验证。

Result: 新定义满足相关信息论公理；识别了强版本单调性和数据处理不等式不成立的条件；证明了与静态视角的兼容性；在隐私保护数据发布攻击中成功应用。

Conclusion: 该研究填补了动态信息泄漏理论的重要空白，提出的定义既满足基本公理要求，又与实际应用场景兼容，为系统监控和动态决策提供了理论基础。

Abstract: Quantitative information flow (QIF) is concerned with assessing the leakage
of information in computational systems. In QIF there are two main perspectives
for the quantification of leakage. On one hand, the static perspective
considers all possible runs of the system in the computation of information
flow, and is usually employed when preemptively deciding whether or not to run
the system. On the other hand, the dynamic perspective considers only a
specific, concrete run of the system that has been realised, while ignoring all
other runs. The dynamic perspective is relevant for, e.g., system monitors and
trackers, especially when deciding whether to continue or to abort a particular
run based on how much leakage has occurred up to a certain point. Although the
static perspective of leakage is well-developed in the literature, the dynamic
perspective still lacks the same level of theoretical maturity. In this paper
we take steps towards bridging this gap with the following key contributions:
(i) we provide a novel definition of dynamic leakage that decouples the
adversary's belief about the secret value from a baseline distribution on
secrets against which the success of the attack is measured; (ii) we
demonstrate that our formalisation satisfies relevant information-theoretic
axioms, including non-interference and relaxed versions of monotonicity and the
data-processing inequality (DPI); (iii) we identify under what kind of analysis
strong versions of the axioms of monotonicity and the DPI might not hold, and
explain the implications of this (perhaps counter-intuitive) outcome; (iv) we
show that our definition of dynamic leakage is compatible with the
well-established static perspective; and (v) we exemplify the use of our
definition on the formalisation of attacks against privacy-preserving data
releases.

</details>


### [17] [Security Logs to ATT&CK Insights: Leveraging LLMs for High-Level Threat Understanding and Cognitive Trait Inference](https://arxiv.org/abs/2510.20930)
*Soham Hans,Stacy Marsella,Sophia Hirschmann,Nikolos Gurney*

Main category: cs.CR

TL;DR: 提出了一种利用大语言模型分析Suricata IDS日志的新框架，能够从网络层事件推断攻击者的MITRE ATT&CK技术和认知策略，为认知自适应网络防御铺平道路。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全依赖于高层情报报告和手动分析攻击链，而实时防御需要直接从低级系统遥测数据推断攻击者意图和认知策略。

Method: 开发了策略驱动的提示系统，将大量网络日志数据高效分段为不同行为阶段，使LLM能够将每个阶段与可能的技术和认知动机相关联。

Result: 结果表明LLM能够弥合数据包级日志与战略意图之间的语义鸿沟，揭示了工具切换、协议转换等行为信号与心理决策点的对应关系。

Conclusion: 该方法为行为自适应网络防御和认知特征推断奠定了基础，提供了通向认知自适应网络防御的途径。

Abstract: Understanding adversarial behavior in cybersecurity has traditionally relied
on high-level intelligence reports and manual interpretation of attack chains.
However, real-time defense requires the ability to infer attacker intent and
cognitive strategy directly from low-level system telemetry such as intrusion
detection system (IDS) logs. In this paper, we propose a novel framework that
leverages large language models (LLMs) to analyze Suricata IDS logs and infer
attacker actions in terms of MITRE ATT&CK techniques. Our approach is grounded
in the hypothesis that attacker behavior reflects underlying cognitive biases
such as loss aversion, risk tolerance, or goal persistence that can be
extracted and modeled through careful observation of log sequences. This lays
the groundwork for future work on behaviorally adaptive cyber defense and
cognitive trait inference. We develop a strategy-driven prompt system to
segment large amounts of network logs data into distinct behavioral phases in a
highly efficient manner, enabling the LLM to associate each phase with likely
techniques and underlying cognitive motives. By mapping network-layer events to
high-level attacker strategies, our method reveals how behavioral signals such
as tool switching, protocol transitions, or pivot patterns correspond to
psychologically meaningful decision points. The results demonstrate that LLMs
can bridge the semantic gap between packet-level logs and strategic intent,
offering a pathway toward cognitive-adaptive cyber defense.
  Keywords: Cognitive Cybersecurity, Large Language Models (LLMs),
Cyberpsychology, Intrusion Detection Systems (IDS), MITRE ATT&CK, Cognitive
Biases

</details>


### [18] [An Experimental Study of Trojan Vulnerabilities in UAV Autonomous Landing](https://arxiv.org/abs/2510.20932)
*Reza Ahmari,Ahmad Mohammadi,Vahid Hemmati,Mohammed Mynuddin,Mahmoud Nabil Mahmoud,Parham Kebria,Abdollah Homaifar,Mehrdad Saif*

Main category: cs.CR

TL;DR: 该研究调查了城市空中交通车辆自主导航和着陆系统对特洛伊木马攻击的脆弱性，发现攻击导致模型准确率从96.4%大幅下降至73.3%。


<details>
  <summary>Details</summary>
Motivation: 研究城市空中交通车辆自主系统的安全风险，特别是针对深度学习模型的特洛伊木马攻击，这些攻击可能在实际应用中造成严重后果。

Method: 使用DroNet框架评估城市自主飞行器的脆弱性，收集自定义数据集并训练模型模拟真实条件，开发评估框架识别特洛伊木马感染模型。

Result: 实验显示特洛伊木马攻击导致模型准确率显著下降，从96.4%降至73.3%，证明此类攻击对自主系统构成严重安全威胁。

Conclusion: 特洛伊木马攻击对城市空中交通系统构成重大安全风险，为未来增强UAM系统韧性的研究奠定了基础。

Abstract: This study investigates the vulnerabilities of autonomous navigation and
landing systems in Urban Air Mobility (UAM) vehicles. Specifically, it focuses
on Trojan attacks that target deep learning models, such as Convolutional
Neural Networks (CNNs). Trojan attacks work by embedding covert triggers within
a model's training data. These triggers cause specific failures under certain
conditions, while the model continues to perform normally in other situations.
We assessed the vulnerability of Urban Autonomous Aerial Vehicles (UAAVs) using
the DroNet framework. Our experiments showed a significant drop in accuracy,
from 96.4% on clean data to 73.3% on data triggered by Trojan attacks. To
conduct this study, we collected a custom dataset and trained models to
simulate real-world conditions. We also developed an evaluation framework
designed to identify Trojan-infected models. This work demonstrates the
potential security risks posed by Trojan attacks and lays the groundwork for
future research on enhancing the resilience of UAM systems.

</details>


### [19] [Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training](https://arxiv.org/abs/2510.20956)
*Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CR

TL;DR: 本文发现推理语言模型存在自越狱现象，即在良性推理训练后，模型会使用多种策略绕过自身的安全防护机制，包括引入良性假设来合理化有害请求。


<details>
  <summary>Details</summary>
Motivation: 研究推理语言模型在数学或代码领域进行良性推理训练后出现的意外安全对齐失败现象，即自越狱行为。

Method: 通过分析多个开源推理语言模型（如DeepSeek-R1-distilled、s1.1等）的行为，研究自越狱的机制，并探索通过在训练中加入少量安全推理数据来缓解此问题。

Result: 发现多个推理语言模型都存在自越狱现象，模型在自越狱后会认为恶意请求的危害性降低，从而更愿意执行这些请求。加入少量安全推理数据可以有效保持模型的安全对齐。

Conclusion: 自越狱是推理语言模型的一个重要安全问题，但可以通过在训练中加入安全推理数据来有效缓解，为保持未来更强大推理语言模型的安全性提供了实用路径。

Abstract: We discover a novel and surprising phenomenon of unintentional misalignment
in reasoning language models (RLMs), which we call self-jailbreaking.
Specifically, after benign reasoning training on math or code domains, RLMs
will use multiple strategies to circumvent their own safety guardrails. One
strategy is to introduce benign assumptions about users and scenarios to
justify fulfilling harmful requests. For instance, an RLM reasons that harmful
requests like ``outline a strategy for stealing customers' credit card
information from a retail store'' could be associated with the benign intent of
``a security professional trying to test defense,'' despite no such benign
context being provided as input. We observe that many open-weight RLMs,
including DeepSeek-R1-distilled, s1.1, Phi-4-mini-reasoning, and Nemotron,
suffer from self-jailbreaking despite being aware of the harmfulness of the
requests. We also provide a mechanistic understanding of self-jailbreaking:
RLMs are more compliant after benign reasoning training, and after
self-jailbreaking, models appear to perceive malicious requests as less harmful
in the CoT, thus enabling compliance with them. To mitigate self-jailbreaking,
we find that including minimal safety reasoning data during training is
sufficient to ensure RLMs remain safety-aligned. Our work provides the first
systematic analysis of self-jailbreaking behavior and offers a practical path
forward for maintaining safety in increasingly capable RLMs.

</details>


### [20] [REx86: A Local Large Language Model for Assisting in x86 Assembly Reverse Engineering](https://arxiv.org/abs/2510.20975)
*Darrin Lea,James Ghawaly,Golden Richard III,Aisha Ali-Gombe,Andrew Case*

Main category: cs.CR

TL;DR: 该论文提出了REx86，一个通过参数高效微调本地开源大语言模型来辅助x86二进制逆向工程的系统，在隐私敏感环境中显著提升了代码理解和注释质量。


<details>
  <summary>Details</summary>
Motivation: 解决在恶意软件和固件分析中，由于缺乏元数据和对抗性混淆导致的x86二进制逆向工程效率低下问题，同时避免使用云托管闭源模型带来的隐私和安全风险。

Method: 在5,981个x86汇编示例的自定义数据集上，对CodeLlama、Qwen2.5-Coder和CodeGemma系列的8个开源模型进行参数高效微调，评估其在逆向工程任务中的表现。

Result: 微调的Qwen2.5-Coder-7B模型（REx86）表现最佳，测试集交叉熵损失降低64.2%，语义余弦相似度提升20.3%。用户研究表明代码理解显著改善，正确解决率从31%提升至53%。

Conclusion: REx86在本地开源LLM中提供了最先进的x86逆向工程辅助能力，证明了领域特定微调的价值，并强调了需要更多注释反汇编数据来进一步提升LLM在逆向工程中的性能。

Abstract: Reverse engineering (RE) of x86 binaries is indispensable for malware and
firmware analysis, but remains slow due to stripped metadata and adversarial
obfuscation. Large Language Models (LLMs) offer potential for improving RE
efficiency through automated comprehension and commenting, but cloud-hosted,
closed-weight models pose privacy and security risks and cannot be used in
closed-network facilities. We evaluate parameter-efficient fine-tuned local
LLMs for assisting with x86 RE tasks in these settings. Eight open-weight
models across the CodeLlama, Qwen2.5-Coder, and CodeGemma series are fine-tuned
on a custom curated dataset of 5,981 x86 assembly examples. We evaluate them
quantitatively and identify the fine-tuned Qwen2.5-Coder-7B as the top
performer, which we name REx86.
  REx86 reduces test-set cross-entropy loss by 64.2% and improves semantic
cosine similarity against ground truth by 20.3\% over its base model. In a
limited user case study (n=43), REx86 significantly enhanced line-level code
understanding (p = 0.031) and increased the correct-solve rate from 31% to 53%
(p = 0.189), though the latter did not reach statistical significance.
Qualitative analysis shows more accurate, concise comments with fewer
hallucinations.
  REx86 delivers state-of-the-art assistance in x86 RE among local, open-weight
LLMs. Our findings demonstrate the value of domain-specific fine-tuning, and
highlight the need for more commented disassembly data to further enhance LLM
performance in RE. REx86, its dataset, and LoRA adapters are publicly available
at https://github.com/dlea8/REx86 and https://zenodo.org/records/15420461.

</details>


### [21] [Can Current Detectors Catch Face-to-Voice Deepfake Attacks?](https://arxiv.org/abs/2510.21004)
*Nguyen Linh Bao Nguyen,Alsharif Abuadbba,Kristen Moore,Tingming Wu*

Main category: cs.CR

TL;DR: 本文评估了FOICE音频深度伪造技术的检测效果，发现现有检测器在标准及噪声条件下均表现不佳，提出针对性微调策略能显著提升检测准确率，但存在与未知合成器鲁棒性的权衡。


<details>
  <summary>Details</summary>
Motivation: FOICE技术仅凭单张面部图像就能生成逼真的合成语音，可绕过行业标准认证系统，且面部图像比语音样本更易获取，这显著降低了大规模攻击的门槛，引发严重安全担忧。

Method: 研究两个核心问题：现有音频深度伪造检测器能否可靠检测FOICE生成的语音；对这些检测器进行FOICE数据微调是否能提升检测效果而不产生过拟合。采用系统评估和针对性微调策略。

Result: 领先的检测器在标准和噪声条件下均持续失败；针对性微调策略能显著提高检测准确率；微调后在FOICE专业化和对未知合成器鲁棒性之间存在权衡。

Conclusion: 当前防御系统存在根本性弱点，需要为下一代音频深度伪造检测开发新架构和训练协议。

Abstract: The rapid advancement of generative models has enabled the creation of
increasingly stealthy synthetic voices, commonly referred to as audio
deepfakes. A recent technique, FOICE [USENIX'24], demonstrates a particularly
alarming capability: generating a victim's voice from a single facial image,
without requiring any voice sample. By exploiting correlations between facial
and vocal features, FOICE produces synthetic voices realistic enough to bypass
industry-standard authentication systems, including WeChat Voiceprint and
Microsoft Azure. This raises serious security concerns, as facial images are
far easier for adversaries to obtain than voice samples, dramatically lowering
the barrier to large-scale attacks. In this work, we investigate two core
research questions: (RQ1) can state-of-the-art audio deepfake detectors
reliably detect FOICE-generated speech under clean and noisy conditions, and
(RQ2) whether fine-tuning these detectors on FOICE data improves detection
without overfitting, thereby preserving robustness to unseen voice generators
such as SpeechT5.
  Our study makes three contributions. First, we present the first systematic
evaluation of FOICE detection, showing that leading detectors consistently fail
under both standard and noisy conditions. Second, we introduce targeted
fine-tuning strategies that capture FOICE-specific artifacts, yielding
significant accuracy improvements. Third, we assess generalization after
fine-tuning, revealing trade-offs between specialization to FOICE and
robustness to unseen synthesis pipelines. These findings expose fundamental
weaknesses in today's defenses and motivate new architectures and training
protocols for next-generation audio deepfake detection.

</details>


### [22] [JSTprove: Pioneering Verifiable AI for a Trustless Future](https://arxiv.org/abs/2510.21024)
*Jonathan Gold,Tristan Freiberg,Haruna Isah,Shirin Shahabi*

Main category: cs.CR

TL;DR: JSTprove是一个基于Polyhedra Network Expander后端的zkML工具包，旨在让AI开发者和ML工程师能够生成和验证AI推理证明，而无需深厚的密码学知识。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在医疗、金融等关键行业的广泛应用，确保AI决策的透明性和正确性变得至关重要。传统zkML系统需要密码学专业知识，限制了ML工程师的使用。

Method: JSTprove提供了一个端到端的可验证AI推理管道，通过简单的命令行界面隐藏密码学复杂性，同时暴露可审计的工件以确保可重现性。

Result: JSTprove既可作为满足当前工程需求的可用zkML产品，也可作为未来可验证AI研究和生产部署的可重现基础。

Conclusion: JSTprove通过降低zkML的使用门槛，为解决AI系统在关键应用中的信任、安全和问责挑战提供了实用解决方案。

Abstract: The integration of machine learning (ML) systems into critical industries
such as healthcare, finance, and cybersecurity has transformed decision-making
processes, but it also brings new challenges around trust, security, and
accountability. As AI systems become more ubiquitous, ensuring the transparency
and correctness of AI-driven decisions is crucial, especially when they have
direct consequences on privacy, security, or fairness. Verifiable AI, powered
by Zero-Knowledge Machine Learning (zkML), offers a robust solution to these
challenges. zkML enables the verification of AI model inferences without
exposing sensitive data, providing an essential layer of trust and privacy.
However, traditional zkML systems typically require deep cryptographic
expertise, placing them beyond the reach of most ML engineers. In this paper,
we introduce JSTprove, a specialized zkML toolkit, built on Polyhedra Network's
Expander backend, to enable AI developers and ML engineers to generate and
verify proofs of AI inference. JSTprove provides an end-to-end verifiable AI
inference pipeline that hides cryptographic complexity behind a simple
command-line interface while exposing auditable artifacts for reproducibility.
We present the design, innovations, and real-world use cases of JSTprove as
well as our blueprints and tooling to encourage community review and extension.
JSTprove therefore serves both as a usable zkML product for current engineering
needs and as a reproducible foundation for future research and production
deployments of verifiable AI.

</details>


### [23] [A Reinforcement Learning Framework for Robust and Secure LLM Watermarking](https://arxiv.org/abs/2510.21053)
*Li An,Yujian Liu,Yepeng Liu,Yuheng Bu,Yang Zhang,Shiyu Chang*

Main category: cs.CR

TL;DR: 提出了一种端到端的强化学习框架，用于优化大语言模型水印技术中的绿/红令牌列表设计，解决了多标准优化时的训练不稳定和奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 现有水印算法大多依赖启发式的绿/红令牌列表设计，直接使用强化学习优化面临多标准冲突导致训练不稳定，以及巨大动作空间易受奖励黑客攻击的问题。

Method: 采用端到端强化学习框架，引入锚定机制确保训练稳定，并添加正则化项防止奖励黑客。

Result: 在两个骨干大语言模型的标准基准测试中，该方法在所有标准上实现了最先进的权衡，特别是在抵抗欺骗攻击方面有显著改进，且不降低其他标准。

Conclusion: 提出的强化学习框架能够有效优化水印令牌列表设计，在检测性、文本质量、抗移除攻击和抗欺骗攻击等多个标准上取得平衡。

Abstract: Watermarking has emerged as a promising solution for tracing and
authenticating text generated by large language models (LLMs). A common
approach to LLM watermarking is to construct a green/red token list and assign
higher or lower generation probabilities to the corresponding tokens,
respectively. However, most existing watermarking algorithms rely on heuristic
green/red token list designs, as directly optimizing the list design with
techniques such as reinforcement learning (RL) comes with several challenges.
First, desirable watermarking involves multiple criteria, i.e., detectability,
text quality, robustness against removal attacks, and security against spoofing
attacks. Directly optimizing for these criteria introduces many partially
conflicting reward terms, leading to an unstable convergence process. Second,
the vast action space of green/red token list choices is susceptible to reward
hacking. In this paper, we propose an end-to-end RL framework for robust and
secure LLM watermarking. Our approach adopts an anchoring mechanism for reward
terms to ensure stable training and introduces additional regularization terms
to prevent reward hacking. Experiments on standard benchmarks with two backbone
LLMs show that our method achieves a state-of-the-art trade-off across all
criteria, with notable improvements in resistance to spoofing attacks without
degrading other criteria. Our code is available at
https://github.com/UCSB-NLP-Chang/RL-watermark.

</details>


### [24] [Soft Instruction De-escalation Defense](https://arxiv.org/abs/2510.21057)
*Nils Philipp Walter,Chawin Sitawarin,Jamie Hayes,David Stutz,Ilia Shumailov*

Main category: cs.CR

TL;DR: 提出SIC方法，通过迭代式提示净化循环来保护工具增强的LLM代理免受提示注入攻击，在多次重写中检测和修正恶意指令。


<details>
  <summary>Details</summary>
Motivation: LLM代理在处理不可信数据时容易受到提示注入攻击，需要有效的防护机制来确保系统安全。

Method: SIC方法采用迭代式提示净化循环，反复检查输入数据中的恶意指令，通过重写、掩码或删除来处理，直到输入干净或达到最大迭代次数。

Result: 方法有效提高了安全性，但最坏情况下仍有15%的攻击成功率，特别是当攻击者嵌入非命令式工作流时。

Conclusion: SIC方法虽然不能完全防止提示注入攻击，但显著提高了攻击门槛，为LLM代理安全提供了实用的防护方案。

Abstract: Large Language Models (LLMs) are increasingly deployed in agentic systems
that interact with an external environment; this makes them susceptible to
prompt injections when dealing with untrusted data. To overcome this
limitation, we propose SIC (Soft Instruction Control)-a simple yet effective
iterative prompt sanitization loop designed for tool-augmented LLM agents. Our
method repeatedly inspects incoming data for instructions that could compromise
agent behavior. If such content is found, the malicious content is rewritten,
masked, or removed, and the result is re-evaluated. The process continues until
the input is clean or a maximum iteration limit is reached; if imperative
instruction-like content remains, the agent halts to ensure security. By
allowing multiple passes, our approach acknowledges that individual rewrites
may fail but enables the system to catch and correct missed injections in later
steps. Although immediately useful, worst-case analysis shows that SIC is not
infallible; strong adversary can still get a 15% ASR by embedding
non-imperative workflows. This nonetheless raises the bar.

</details>


### [25] [QAE-BAC: Achieving Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with Attribute](https://arxiv.org/abs/2510.21124)
*Jie Zhang,Xiaohong Li,Mengke Zhang,Ruitao Feng,Shanshan Xu,Zhe Hou,Guangdong Bai*

Main category: cs.CR

TL;DR: 提出QAE-BAC方案解决区块链属性访问控制中的隐私和性能双重挑战，通过量化匿名性模型和熵加权路径树优化，在Hyperledger Fabric上实现隐私保护和性能提升。


<details>
  <summary>Details</summary>
Motivation: 区块链属性访问控制面临两个固有挑战：区块链账本透明度威胁用户隐私（通过属性分析实现重识别攻击），策略匹配的计算复杂度与区块链性能限制冲突。现有解决方案存在高开销和缺乏可量化匿名保证的问题。

Method: 提出QAE-BAC方案，包含：(r,t)-匿名性模型动态量化用户重识别风险，以及熵加权路径树(EWPT)基于实时匿名性指标优化策略结构，大幅降低策略匹配复杂度。

Result: 在Hyperledger Fabric上实现和评估，实验结果显示：有效缓解重识别风险，性能优于现有基线方法，吞吐量提升高达11倍，延迟降低87%。

Conclusion: QAE-BAC在隐私敏感的去中心化应用中实现了隐私保护和性能之间的优越平衡，证明了其实际可行性。

Abstract: Blockchain-based Attribute-Based Access Control (BC-ABAC) offers a
decentralized paradigm for secure data governance but faces two inherent
challenges: the transparency of blockchain ledgers threatens user privacy by
enabling reidentification attacks through attribute analysis, while the
computational complexity of policy matching clashes with blockchain's
performance constraints. Existing solutions, such as those employing
Zero-Knowledge Proofs (ZKPs), often incur high overhead and lack measurable
anonymity guarantees, while efficiency optimizations frequently ignore privacy
implications. To address these dual challenges, this paper proposes QAEBAC
(Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with
Attribute). QAE-BAC introduces a formal (r, t)-anonymity model to dynamically
quantify the re-identification risk of users based on their access attributes
and history. Furthermore, it features an Entropy-Weighted Path Tree (EWPT) that
optimizes policy structure based on realtime anonymity metrics, drastically
reducing policy matching complexity. Implemented and evaluated on Hyperledger
Fabric, QAE-BAC demonstrates a superior balance between privacy and
performance. Experimental results show that it effectively mitigates
re-identification risks and outperforms state-of-the-art baselines, achieving
up to an 11x improvement in throughput and an 87% reduction in latency, proving
its practicality for privacy-sensitive decentralized applications.

</details>


### [26] [Quantifying CBRN Risk in Frontier Models](https://arxiv.org/abs/2510.21133)
*Divyanshu Kumar,Nitin Aravind Birur,Tanay Baswa,Sahil Agarwal,Prashanth Harshangi*

Main category: cs.CR

TL;DR: 对10个主流商业大语言模型在CBRN武器知识安全性的首次全面评估，发现深度诱导攻击成功率高达86%，暴露了当前安全对齐机制的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型存在被滥用于化学、生物、放射性和核武器知识传播的双重使用风险，需要评估其安全防护能力。

Method: 使用包含200个提示的CBRN数据集和180个提示的FORTRESS基准，采用三层攻击方法学进行测试。

Result: 深度诱导攻击成功率86%远高于直接请求的33.8%；模型安全性能差异巨大（2%-96%）；8个模型在增强危险材料属性方面超过70%的脆弱性。

Conclusion: 当前安全对齐机制存在根本性脆弱性，需要标准化评估框架、透明安全指标和更强大的对齐技术来减轻灾难性滥用风险。

Abstract: Frontier Large Language Models (LLMs) pose unprecedented dual-use risks
through the potential proliferation of chemical, biological, radiological, and
nuclear (CBRN) weapons knowledge. We present the first comprehensive evaluation
of 10 leading commercial LLMs against both a novel 200-prompt CBRN dataset and
a 180-prompt subset of the FORTRESS benchmark, using a rigorous three-tier
attack methodology. Our findings expose critical safety vulnerabilities: Deep
Inception attacks achieve 86.0\% success versus 33.8\% for direct requests,
demonstrating superficial filtering mechanisms; Model safety performance varies
dramatically from 2\% (claude-opus-4) to 96\% (mistral-small-latest) attack
success rates; and eight models exceed 70\% vulnerability when asked to enhance
dangerous material properties. We identify fundamental brittleness in current
safety alignment, where simple prompt engineering techniques bypass safeguards
for dangerous CBRN information. These results challenge industry safety claims
and highlight urgent needs for standardized evaluation frameworks, transparent
safety metrics, and more robust alignment techniques to mitigate catastrophic
misuse risks while preserving beneficial capabilities.

</details>


### [27] [Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency](https://arxiv.org/abs/2510.21189)
*Yukun Jiang,Mingjie Li,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 提出了一种基于任务并发的LLM越狱攻击方法JAIL-CON，通过在相邻单词中编码不同意图来实现并发任务，显著降低了有害内容被防护系统过滤的概率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击主要遵循顺序逻辑，而并发作为顺序场景的自然扩展被忽视。研究发现将有害任务与良性任务结合能显著降低被防护系统检测的概率。

Method: 提出单词级方法实现LLM中的任务并发，开发了JAIL-CON迭代攻击框架，通过在相邻单词中编码不同意图来同时执行多个任务。

Result: 在广泛使用的LLM上实验表明，JAIL-CON相比现有攻击具有更强的越狱能力，且并发答案比顺序答案更具隐蔽性，更难被防护系统检测。

Conclusion: 任务并发在LLM越狱中具有独特优势，JAIL-CON框架展示了并发攻击的有效性和隐蔽性，凸显了LLM并发使用的潜在风险。

Abstract: Despite their superior performance on a wide range of domains, large language
models (LLMs) remain vulnerable to misuse for generating harmful content, a
risk that has been further amplified by various jailbreak attacks. Existing
jailbreak attacks mainly follow sequential logic, where LLMs understand and
answer each given task one by one. However, concurrency, a natural extension of
the sequential scenario, has been largely overlooked. In this work, we first
propose a word-level method to enable task concurrency in LLMs, where adjacent
words encode divergent intents. Although LLMs maintain strong utility in
answering concurrent tasks, which is demonstrated by our evaluations on
mathematical and general question-answering benchmarks, we notably observe that
combining a harmful task with a benign one significantly reduces the
probability of it being filtered by the guardrail, showing the potential risks
associated with concurrency in LLMs. Based on these findings, we introduce
$\texttt{JAIL-CON}$, an iterative attack framework that
$\underline{\text{JAIL}}$breaks LLMs via task $\underline{\text{CON}}$currency.
Experiments on widely-used LLMs demonstrate the strong jailbreak capabilities
of $\texttt{JAIL-CON}$ compared to existing attacks. Furthermore, when the
guardrail is applied as a defense, compared to the sequential answers generated
by previous attacks, the concurrent answers in our $\texttt{JAIL-CON}$ exhibit
greater stealthiness and are less detectable by the guardrail, highlighting the
unique feature of task concurrency in jailbreaking LLMs.

</details>


### [28] [The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning](https://arxiv.org/abs/2510.21190)
*Mingrui Liu,Sixiao Zhang,Cheng Long,Kwok Yan Lam*

Main category: cs.CR

TL;DR: TrojFill是一种黑盒越狱方法，通过将有害指令嵌入模板填充任务来绕过LLM的安全防护，在多个主流模型上达到高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱技术存在局限性：白盒方法需要模型内部信息不适用于闭源API，黑盒方法生成的提示缺乏可解释性和可迁移性。

Method: 将不安全指令通过占位符替换或编码方式嵌入多部分模板，让模型进行不安全推理并生成详细示例，利用示例组件作为特洛伊木马。

Result: 在多个主流LLM上表现优异：Gemini-flash-2.5和DeepSeek-3.1达到100%攻击成功率，GPT-4o达到97%。

Conclusion: TrojFill在保持高攻击成功率的同时，相比现有黑盒优化方法具有更好的可解释性和可迁移性。

Abstract: Large Language Models (LLMs) have advanced rapidly and now encode extensive
world knowledge. Despite safety fine-tuning, however, they remain susceptible
to adversarial prompts that elicit harmful content. Existing jailbreak
techniques fall into two categories: white-box methods (e.g., gradient-based
approaches such as GCG), which require model internals and are infeasible for
closed-source APIs, and black-box methods that rely on attacker LLMs to search
or mutate prompts but often produce templates that lack explainability and
transferability. We introduce TrojFill, a black-box jailbreak that reframes
unsafe instruction as a template-filling task. TrojFill embeds obfuscated
harmful instructions (e.g., via placeholder substitution or Caesar/Base64
encoding) inside a multi-part template that asks the model to (1) reason why
the original instruction is unsafe (unsafety reasoning) and (2) generate a
detailed example of the requested text, followed by a sentence-by-sentence
analysis. The crucial "example" component acts as a Trojan Horse that contains
the target jailbreak content while the surrounding task framing reduces refusal
rates. We evaluate TrojFill on standard jailbreak benchmarks across leading
LLMs (e.g., ChatGPT, Gemini, DeepSeek, Qwen), showing strong empirical
performance (e.g., 100% attack success on Gemini-flash-2.5 and DeepSeek-3.1,
and 97% on GPT-4o). Moreover, the generated prompts exhibit improved
interpretability and transferability compared with prior black-box optimization
approaches. We release our code, sample prompts, and generated outputs to
support future red-teaming research.

</details>


### [29] [Securing AI Agent Execution](https://arxiv.org/abs/2510.21236)
*Christoph Bühler,Matteo Biagiola,Luca Di Grazia,Guido Salvaneschi*

Main category: cs.CR

TL;DR: AgentBound是首个针对MCP服务器的访问控制框架，结合声明式策略机制和策略执行引擎，无需修改MCP服务器即可遏制恶意行为。


<details>
  <summary>Details</summary>
Motivation: MCP已成为连接AI代理与外部工具的事实标准，但安全滞后，数千个MCP服务器在主机系统上拥有无限制访问权限，形成了广泛的攻击面。

Method: 结合受Android权限模型启发的声明式策略机制和策略执行引擎，自动从源代码生成访问控制策略，准确率达80.9%。

Result: AgentBound在多个恶意MCP服务器中成功阻止了大部分安全威胁，策略执行引擎引入的开销可忽略不计。

Conclusion: 为开发者和项目经理提供了实用的MCP服务器安全基础，同时保持生产力，使研究人员和工具构建者能够探索声明式访问控制和MCP安全的新方向。

Abstract: Large Language Models (LLMs) have evolved into AI agents that interact with
external tools and environments to perform complex tasks. The Model Context
Protocol (MCP) has become the de facto standard for connecting agents with such
resources, but security has lagged behind: thousands of MCP servers execute
with unrestricted access to host systems, creating a broad attack surface. In
this paper, we introduce AgentBound, the first access control framework for MCP
servers. AgentBound combines a declarative policy mechanism, inspired by the
Android permission model, with a policy enforcement engine that contains
malicious behavior without requiring MCP server modifications. We build a
dataset containing the 296 most popular MCP servers, and show that access
control policies can be generated automatically from source code with 80.9%
accuracy. We also show that AgentBound blocks the majority of security threats
in several malicious MCP servers, and that policy enforcement engine introduces
negligible overhead. Our contributions provide developers and project managers
with a practical foundation for securing MCP servers while maintaining
productivity, enabling researchers and tool builders to explore new directions
for declarative access control and MCP security.

</details>


### [30] [Enhanced MLLM Black-Box Jailbreaking Attacks and Defenses](https://arxiv.org/abs/2510.21214)
*Xingwei Zhong,Kar Wai Fok,Vrizlynn L. L. Thing*

Main category: cs.CR

TL;DR: 提出了一种针对多模态大语言模型的黑盒越狱方法，通过文本和图像提示来评估模型安全性，并设计了重新攻击策略和防御方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理视觉语言任务时存在安全漏洞，特别是越狱攻击可能导致未经授权或有害的响应，需要评估和改进其安全性。

Method: 设计了包含挑衅指令的文本提示和具有突变、多图像能力的图像提示，采用黑盒越狱方法评估模型，并实施重新攻击策略。

Result: 实验结果显示，该方法能够有效评估开源和闭源多模态大语言模型的安全性，并识别出现有防御方法的不足。

Conclusion: 重新设计的防御方法在训练时和推理时都能提高对越狱攻击的防护能力，为多模态大语言模型的安全提供了改进策略。

Abstract: Multimodal large language models (MLLMs) comprise of both visual and textual
modalities to process vision language tasks. However, MLLMs are vulnerable to
security-related issues, such as jailbreak attacks that alter the model's input
to induce unauthorized or harmful responses. The incorporation of the
additional visual modality introduces new dimensions to security threats. In
this paper, we proposed a black-box jailbreak method via both text and image
prompts to evaluate MLLMs. In particular, we designed text prompts with
provocative instructions, along with image prompts that introduced mutation and
multi-image capabilities. To strengthen the evaluation, we also designed a
Re-attack strategy. Empirical results show that our proposed work can improve
capabilities to assess the security of both open-source and closed-source
MLLMs. With that, we identified gaps in existing defense methods to propose new
strategies for both training-time and inference-time defense methods, and
evaluated them across the new jailbreak methods. The experiment results showed
that the re-designed defense methods improved protections against the jailbreak
attacks.

</details>


### [31] [LLM-Powered Detection of Price Manipulation in DeFi](https://arxiv.org/abs/2510.21272)
*Lu Liu,Wuqi Zhang,Lili Wei,Hao Guan,Yongqiang Tian,Yepang Liu*

Main category: cs.CR

TL;DR: PMDetector是一个结合静态分析和LLM推理的混合框架，用于主动检测DeFi智能合约中的价格操纵漏洞，在真实数据集上达到88%精确率和90%召回率。


<details>
  <summary>Details</summary>
Motivation: DeFi智能合约管理数十亿美元资金，价格操纵漏洞（通常通过闪电贷）造成重大财务损失。现有检测方法有限：反应性方法仅在攻击发生后分析，而主动静态分析工具依赖僵化的预定义启发式规则，无法识别新型变体或理解复杂经济逻辑。

Method: 提出PMDetector混合框架，采用三阶段流水线：1）静态污点分析识别潜在漏洞代码路径；2）两阶段LLM过程分析防御措施并模拟攻击评估可利用性；3）静态分析检查器验证LLM结果，保留高风险路径并生成详细漏洞报告。

Result: 在包含73个真实漏洞和288个良性DeFi协议的数据集上，使用Gemini 2.5-flash达到88%精确率和90%召回率，显著优于最先进的静态分析和基于LLM的方法。使用GPT-4.1审计一个漏洞仅需0.03美元和4.0秒。

Conclusion: PMDetector提供了一种高效且经济实惠的替代方案，能够主动检测价格操纵漏洞，优于现有方法，为DeFi安全审计提供了实用的解决方案。

Abstract: Decentralized Finance (DeFi) smart contracts manage billions of dollars,
making them a prime target for exploits. Price manipulation vulnerabilities,
often via flash loans, are a devastating class of attacks causing significant
financial losses. Existing detection methods are limited. Reactive approaches
analyze attacks only after they occur, while proactive static analysis tools
rely on rigid, predefined heuristics, limiting adaptability. Both depend on
known attack patterns, failing to identify novel variants or comprehend complex
economic logic. We propose PMDetector, a hybrid framework combining static
analysis with Large Language Model (LLM)-based reasoning to proactively detect
price manipulation vulnerabilities. Our approach uses a formal attack model and
a three-stage pipeline. First, static taint analysis identifies potentially
vulnerable code paths. Second, a two-stage LLM process filters paths by
analyzing defenses and then simulates attacks to evaluate exploitability.
Finally, a static analysis checker validates LLM results, retaining only
high-risk paths and generating comprehensive vulnerability reports. To evaluate
its effectiveness, we built a dataset of 73 real-world vulnerable and 288
benign DeFi protocols. Results show PMDetector achieves 88% precision and 90%
recall with Gemini 2.5-flash, significantly outperforming state-of-the-art
static analysis and LLM-based approaches. Auditing a vulnerability with
PMDetector costs just $0.03 and takes 4.0 seconds with GPT-4.1, offering an
efficient and cost-effective alternative to manual audits.

</details>


### [32] [FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract Security](https://arxiv.org/abs/2510.21401)
*Mojtaba Eshghie,Gabriele Morello,Matteo Lauretano,Alexandre Bartel,Martin Monperrus*

Main category: cs.CR

TL;DR: FLAMES是一个自动化方法，使用领域适应的大型语言模型生成可执行的运行时防护，作为Solidity的"require"语句来强化智能合约安全。


<details>
  <summary>Details</summary>
Motivation: 智能合约漏洞每年造成数十亿美元损失，现有自动化分析工具无法生成可部署的防御措施。

Method: 使用在514,506个已验证合约中提取的真实世界不变式进行填空式监督微调，训练领域适应的大型语言模型。

Result: 编译成功率96.7%；在5000个挑战性不变式测试集上，44.5%产生精确或语义等价匹配；阻止了108个真实攻击中的22个(20.4%)；成功阻止了APEMAGA真实世界攻击。

Conclusion: 领域适应的LLM能够自动生成生产就绪的智能合约安全防御，无需漏洞检测、形式规范或人工干预。

Abstract: Smart contract vulnerabilities cost billions of dollars annually, yet
existing automated analysis tools fail to generate deployable defenses. We
present FLAMES, a novel automated approach that synthesizes executable runtime
guards as Solidity "require" statements to harden smart contracts against
exploits. Unlike prior work that relies on vulnerability labels, symbolic
analysis, or natural language specifications, FLAMES employs domain-adapted
large language models trained through fill-in-the-middle supervised fine-tuning
on real-world invariants extracted from 514,506 verified contracts. Our
extensive evaluation across three dimensions demonstrates FLAMES's
effectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for
synthesized invariant (2) Semantic Quality: on a curated test set of 5,000
challenging invariants, FLAMES produces exact or semantically equivalent
matches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES
prevents 22 out of 108 real exploits (20.4%) while preserving contract
functionality, and (4) FLAMES successfully blocks the real-world APEMAGA
incident by synthesizing a pre-condition that mitigates the attack. FLAMES
establishes that domain-adapted LLMs can automatically generate
production-ready security defenses for smart contracts without requiring
vulnerability detection, formal specifications, or human intervention. We
release our code, model weights, datasets, and evaluation infrastructure to
enable reproducible research in this critical domain.

</details>


### [33] [What's Next, Cloud? A Forensic Framework for Analyzing Self-Hosted Cloud Storage Solutions](https://arxiv.org/abs/2510.21246)
*Michael Külper,Jan-Niclas Hilgert,Frank Breitinger,Martin Lambertz*

Main category: cs.CR

TL;DR: 提出针对自托管云存储系统（如Nextcloud）的扩展取证框架，通过设备监控和云API实现结构化、可重复的证据获取。


<details>
  <summary>Details</summary>
Motivation: 自托管云存储平台的普及给数字取证带来新挑战，现有取证框架存在局限性，Nextcloud作为流行平台在取证研究中关注不足。

Method: 扩展取证框架，整合设备监控和云API，开发开源获取工具，以Nextcloud为案例研究展示其原生API在取证中的应用。

Result: 成功演示了如何利用Nextcloud原生API可靠访问取证工件，提供了更灵活的自托管云存储系统分析方法。

Conclusion: 该框架为自托管云存储系统的数字取证提供了有效方法，并为该领域进一步发展奠定了基础。

Abstract: Self-hosted cloud storage platforms like Nextcloud are gaining popularity
among individuals and organizations seeking greater control over their data.
However, this shift introduces new challenges for digital forensic
investigations, particularly in systematically analyzing both client and server
components. Despite Nextcloud's widespread use, it has received limited
attention in forensic research. In this work, we critically examine existing
cloud storage forensic frameworks and highlight their limitations. To address
the gaps, we propose an extended forensic framework that incorporates device
monitoring and leverages cloud APIs for structured, repeatable evidence
acquisition. Using Nextcloud as a case study, we demonstrate how its native
APIs can be used to reliably access forensic artifacts, and we introduce an
open-source acquisition tool that implements this approach. Our framework
equips investigators with a more flexible method for analyzing self-hosted
cloud storage systems, and offers a foundation for further development in this
evolving area of digital forensics.

</details>


### [34] [The Qey: Implementation and performance study of post quantum cryptography in FIDO2](https://arxiv.org/abs/2510.21353)
*Aditya Mitra,Sibi Chakkaravarthy Sethuraman*

Main category: cs.CR

TL;DR: 该研究探索将基于模块格的后量子密码签名算法ML-DSA用于FIDO2认证标准，以应对量子计算机的威胁。


<details>
  <summary>Details</summary>
Motivation: 当前FIDO2标准使用ECDSA、RSA等经典密码算法，这些算法在面对大规模量子计算机时存在安全风险。

Method: 研究基于Crystals Dilithium的模块格数字签名算法(ML-DSA)，分析其作为FIDO2后量子密码签名标准的适用性。

Result: 论文比较了ML-DSA与经典算法在性能和安全性方面的表现。

Conclusion: ML-DSA有望成为FIDO2的后量子密码签名标准，提升认证系统在量子计算时代的长期安全性。

Abstract: Authentication systems have evolved a lot since the 1960s when Fernando
Corbato first proposed the password-based authentication. In 2013, the FIDO
Alliance proposed using secure hardware for authentication, thus marking a
milestone in the passwordless authentication era [1]. Passwordless
authentication with a possession-based factor often relied on hardware-backed
cryptographic methods. FIDO2 being one an amalgamation of the W3C Web
Authentication and FIDO Alliance Client to Authenticator Protocol is an
industry standard for secure passwordless authentication with rising adoption
for the same [2]. However, the current FIDO2 standards use ECDSA with SHA-256
(ES256), RSA with SHA-256 (RS256) and similar classical cryptographic signature
algorithms. This makes it insecure against attacks involving large-scale
quantum computers [3]. This study aims at exploring the usability of Module
Lattice based Digital Signature Algorithm (ML-DSA), based on Crystals Dilithium
as a post quantum cryptographic signature standard for FIDO2. The paper
highlights the performance and security in comparison to keys with classical
algorithms.

</details>


### [35] [SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots](https://arxiv.org/abs/2510.21459)
*Adetayo Adebimpe,Helmut Neukirchen,Thomas Welsh*

Main category: cs.CR

TL;DR: 提出SBASH框架，使用轻量级本地LLM解决蜜罐的数据保护问题，研究RAG和非RAG LLM在Linux shell命令中的表现，发现RAG提高未调优模型的准确性，而通过系统提示调优的模型无需RAG也能达到类似准确性且延迟更低。


<details>
  <summary>Details</summary>
Motivation: 蜜罐需要最大化攻击者参与度，上下文感知能力至关重要。现有LLM方法存在准确性、响应时间、运营成本高和云部署数据保护问题。

Method: 提出SBASH框架，使用轻量级本地LLM管理数据保护问题。研究RAG支持和非RAG的LLM在Linux shell命令中的应用，评估响应时间差异、人类测试者评估的真实性，以及使用Levenshtein距离、SBert和BertScore计算的与真实系统相似度。

Result: RAG提高了未调优模型的准确性，而通过系统提示调优的模型无需RAG也能达到类似准确性，且延迟略低。

Conclusion: SBASH框架通过轻量级本地LLM有效解决了数据保护问题，RAG和非RAG方法在不同场景下各有优势，调优后的非RAG模型在准确性和延迟方面表现良好。

Abstract: Honeypots are decoy systems used for gathering valuable threat intelligence
or diverting attackers away from production systems. Maximising attacker
engagement is essential to their utility. However research has highlighted that
context-awareness, such as the ability to respond to new attack types, systems
and attacker agents, is necessary to increase engagement. Large Language Models
(LLMs) have been shown as one approach to increase context awareness but suffer
from several challenges including accuracy and timeliness of response time,
high operational costs and data-protection issues due to cloud deployment. We
propose the System-Based Attention Shell Honeypot (SBASH) framework which
manages data-protection issues through the use of lightweight local LLMs. We
investigate the use of Retrieval Augmented Generation (RAG) supported LLMs and
non-RAG LLMs for Linux shell commands and evaluate them using several different
metrics such as response time differences, realism from human testers, and
similarity to a real system calculated with Levenshtein distance, SBert, and
BertScore. We show that RAG improves accuracy for untuned models while models
that have been tuned via a system prompt that tells the LLM to respond like a
Linux system achieve without RAG a similar accuracy as untuned with RAG, while
having a slightly lower latency.

</details>


### [36] [Introducing GRAFHEN: Group-based Fully Homomorphic Encryption without Noise](https://arxiv.org/abs/2510.21483)
*Pierre Guillot,Auguste Hoang Duc,Michel Koskas,Florian Méhats*

Main category: cs.CR

TL;DR: GRAFHEN是一种无需自举的无噪声全同态加密方案，基于群编码实现，通过重写系统表示群，使得攻击者需要解决的子群成员问题变得极其困难，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有全同态加密方案通常需要自举操作来处理噪声，这限制了性能和实用性。GRAFHEN旨在消除这一限制，提供无需自举的无噪声全同态加密。

Method: 基于Nuida等人的工作，使用群编码实现全同态加密。通过重写系统表示群，使子群成员问题变得困难，同时保持计算效率。

Result: 实现比现有标准快几个数量级的性能，并分析了多种可能的攻击方式，提供了相应的防护措施。

Conclusion: GRAFHEN提供了一种无需自举的高效全同态加密方案，通过群编码和重写系统在安全性和性能之间取得了良好平衡。

Abstract: We present GRAFHEN, a new cryptographic scheme which offers Fully Homomorphic
Encryption without the need for bootstrapping (or in other words, without
noise). Building on the work of Nuida and others, we achieve this using
encodings in groups.
  The groups are represented on a machine using rewriting systems. In this way
the subgroup membership problem, which an attacker would have to solve in order
to break the scheme, becomes maximally hard, while performance is preserved. In
fact we include a simple benchmark demonstrating that our implementation runs
several orders of magnitude faster than existing standards.
  We review many possible attacks against our protocol and explain how to
protect the scheme in each case.

</details>


### [37] [PTMF: A Privacy Threat Modeling Framework for IoT with Expert-Driven Threat Propagation Analysis](https://arxiv.org/abs/2510.21601)
*Emmanuel Dare Alalade,Ashraf Matrawy*

Main category: cs.CR

TL;DR: 提出了一种新的隐私威胁模型框架PTMF，通过分析威胁行为者的行动和意图来深入理解隐私威胁，并在IoT系统中进行了用户研究验证。


<details>
  <summary>Details</summary>
Motivation: 现有PTA研究主要关注隐私威胁的发生区域和可能性，但缺乏对威胁行为者、其行动和意图的深入理解。需要开发一个以隐私为中心的框架来更好地分析隐私威胁。

Method: 基于MITRE ATT&CK框架和LINDDUN隐私威胁模型开发PTMF框架，通过问卷调查对12个IoT隐私威胁进行用户研究，收集行业和学术界专家的意见。

Result: 识别了IoT用户识别隐私威胁中的前三大威胁行为者及其关键路径，以及其余11个隐私威胁的威胁行为者。

Conclusion: PTMF为理解如何在IoT系统中主动有效部署隐私措施提供了坚实基础，能够基于威胁行为者的活动和意图来缓解隐私威胁。

Abstract: Previous studies on PTA have focused on analyzing privacy threats based on
the potential areas of occurrence and their likelihood of occurrence. However,
an in-depth understanding of the threat actors involved, their actions, and the
intentions that result in privacy threats is essential. In this paper, we
present a novel Privacy Threat Model Framework (PTMF) that analyzes privacy
threats through different phases.
  The PTMF development is motivated through the selected tactics from the MITRE
ATT\&CK framework and techniques from the LINDDUN privacy threat model, making
PTMF a privacy-centered framework. The proposed PTMF can be employed in various
ways, including analyzing the activities of threat actors during privacy
threats and assessing privacy risks in IoT systems, among others. In this
paper, we conducted a user study on 12 privacy threats associated with IoT by
developing a questionnaire based on PTMF and recruited experts from both
industry and academia in the fields of security and privacy to gather their
opinions. The collected data were analyzed and mapped to identify the threat
actors involved in the identification of IoT users (IU) and the remaining 11
privacy threats. Our observation revealed the top three threat actors and the
critical paths they used during the IU privacy threat, as well as the remaining
11 privacy threats. This study could provide a solid foundation for
understanding how and where privacy measures can be proactively and effectively
deployed in IoT systems to mitigate privacy threats based on the activities and
intentions of threat actors within these systems.

</details>


### [38] [Toward provably private analytics and insights into GenAI use](https://arxiv.org/abs/2510.21684)
*Albert Cheu,Artem Lagzdin,Brett McLarnon,Daniel Ramage,Katharine Daly,Marco Gruteser,Peter Kairouz,Rakshita Tandon,Stanislav Chiknavaryan,Timon Van Overveldt,Zoe Gong*

Main category: cs.CR

TL;DR: 提出基于可信执行环境(TEE)的新一代联邦分析系统，为设备数据分析提供可验证的隐私保证，支持包括LLM处理在内的灵活工作负载，并已成功部署到生产环境。


<details>
  <summary>Details</summary>
Motivation: 大规模设备数据分析系统需要在保证高隐私安全标准的同时，满足数据质量、可用性和资源效率要求。

Method: 使用基于AMD SEV-SNP和Intel TDX的TEE技术，设备加密上传数据并标记允许的服务端处理步骤，通过开源TEE托管密钥管理服务确保数据仅能被授权处理步骤访问。

Result: 系统已成功部署到生产环境，为真实世界的生成式AI体验提供了有价值的洞察。

Conclusion: 该系统通过TEE技术实现了可验证的隐私保证，支持灵活的工作负载处理，包括LLM结构化和差分隐私聚合，为设备数据分析提供了安全可靠的解决方案。

Abstract: Large-scale systems that compute analytics over a fleet of devices must
achieve high privacy and security standards while also meeting data quality,
usability, and resource efficiency expectations. We present a next-generation
federated analytics system that uses Trusted Execution Environments (TEEs)
based on technologies like AMD SEV-SNP and Intel TDX to provide verifiable
privacy guarantees for all server-side processing. In our system, devices
encrypt and upload data, tagging it with a limited set of allowable server-side
processing steps. An open source, TEE-hosted key management service guarantees
that the data is accessible only to those steps, which are themselves protected
by TEE confidentiality and integrity assurance guarantees. The system is
designed for flexible workloads, including processing unstructured data with
LLMs (for structured summarization) before aggregation into differentially
private insights (with automatic parameter tuning). The transparency properties
of our system allow any external party to verify that all raw and derived data
is processed in TEEs, protecting it from inspection by the system operator, and
that differential privacy is applied to all released results. This system has
been successfully deployed in production, providing helpful insights into
real-world GenAI experiences.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [Sketch2BIM: A Multi-Agent Human-AI Collaborative Pipeline to Convert Hand-Drawn Floor Plans to 3D BIM](https://arxiv.org/abs/2510.20838)
*Abir Khan Ratul,Sanjay Acharjee,Somin Park,Md Nazmus Sakib*

Main category: cs.AI

TL;DR: 提出了一种人机协作流程，将手绘平面图转换为语义一致的3D BIM模型，使用多模态大语言模型和多智能体框架，通过迭代反馈实现高精度转换。


<details>
  <summary>Details</summary>
Motivation: 让BIM创建对专家和非专家都更加便捷，仅需使用手绘草图即可完成，降低BIM建模的技术门槛。

Method: 采用多模态大语言模型的多智能体框架，结合感知提取、人工反馈、模式验证和自动化BIM脚本生成，将草图迭代优化为结构化JSON布局，再转换为可执行脚本生成3D BIM模型。

Result: 在10个不同平面图上的实验显示：门窗开口在初次处理中即可高可靠性捕获，墙体检测初始准确率约83%，经过几次反馈迭代后达到近乎完美对齐。所有类别的精确率、召回率和F1分数均超过0.83，几何误差通过反馈修正逐步降至零。

Conclusion: MLLM驱动的多智能体推理能够使BIM创建对专家和非专家都变得可访问，仅需使用手绘草图即可完成高质量的3D BIM建模。

Abstract: This study introduces a human-in-the-loop pipeline that converts unscaled,
hand-drawn floor plan sketches into semantically consistent 3D BIM models. The
workflow leverages multimodal large language models (MLLMs) within a
multi-agent framework, combining perceptual extraction, human feedback, schema
validation, and automated BIM scripting. Initially, sketches are iteratively
refined into a structured JSON layout of walls, doors, and windows. Later,
these layouts are transformed into executable scripts that generate 3D BIM
models. Experiments on ten diverse floor plans demonstrate strong convergence:
openings (doors, windows) are captured with high reliability in the initial
pass, while wall detection begins around 83% and achieves near-perfect
alignment after a few feedback iterations. Across all categories, precision,
recall, and F1 scores remain above 0.83, and geometric errors (RMSE, MAE)
progressively decrease to zero through feedback corrections. This study
demonstrates how MLLM-driven multi-agent reasoning can make BIM creation
accessible to both experts and non-experts using only freehand sketches.

</details>


### [40] [Cultural Alien Sampler: Open-ended art generation balancing originality and coherence](https://arxiv.org/abs/2510.20849)
*Alejandro H. Artiles,Hiromu Yakura,Levin Brinkmann,Mar Canet Sola,Hassan Abu Alhaija,Ignacio Serna,Nasim Rahaman,Bernhard Schölkopf,Iyad Rahwan*

Main category: cs.AI

TL;DR: 提出Cultural Alien Sampler (CAS)方法，通过分离概念组合的连贯性和文化典型性，在保持内部一致性的同时产生偏离文化惯例的新颖创意。


<details>
  <summary>Details</summary>
Motivation: 解决当前大语言模型在开放领域创意生成中要么陷入熟悉文化模式，要么为追求新颖性而牺牲连贯性的问题。

Method: 使用两个在WikiArt概念上微调的GPT-2模型：概念连贯性模型评估概念组合的合理性，文化语境模型估计组合在艺术家作品中的典型程度。CAS选择高连贯性、低典型性的组合。

Result: 人类评估显示CAS在原创性和和谐度方面优于随机选择和GPT-4o基线，与艺术专业学生表现相当。定量研究显示该方法产生更多样化输出，探索更广泛的概念空间。

Conclusion: 人工文化异化能够释放自主代理的创意潜力，在保持连贯性的同时产生新颖创意。

Abstract: In open-ended domains like art, autonomous agents must generate ideas that
are both original and internally coherent, yet current Large Language Models
(LLMs) either default to familiar cultural patterns or sacrifice coherence when
pushed toward novelty. We address this by introducing the Cultural Alien
Sampler (CAS), a concept-selection method that explicitly separates
compositional fit from cultural typicality. CAS uses two GPT-2 models
fine-tuned on WikiArt concepts: a Concept Coherence Model that scores whether
concepts plausibly co-occur within artworks, and a Cultural Context Model that
estimates how typical those combinations are within individual artists' bodies
of work. CAS targets combinations that are high in coherence and low in
typicality, yielding ideas that maintain internal consistency while deviating
from learned conventions and embedded cultural context. In a human evaluation
(N = 100), our approach outperforms random selection and GPT-4o baselines and
achieves performance comparable to human art students in both perceived
originality and harmony. Additionally, a quantitative study shows that our
method produces more diverse outputs and explores a broader conceptual space
than its GPT-4o counterpart, demonstrating that artificial cultural alienness
can unlock creative potential in autonomous agents.

</details>


### [41] [Fuzzy numbers revisited: operations on extensional fuzzy numbers](https://arxiv.org/abs/2510.20861)
*Krzysztof Siminski*

Main category: cs.AI

TL;DR: 本文提出了一种新的模糊数表示方法——外延模糊数，以解决传统模糊数运算中的计算复杂度高、结果不保持形状特征和模糊扩散等问题。


<details>
  <summary>Details</summary>
Motivation: 传统模糊数使用模糊集表示，但运算复杂且结果可能不保持原有特征（如两个三角模糊数相乘结果不是三角模糊数），还存在模糊扩散问题，限制了模糊数的应用范围。

Method: 提出外延模糊数的概念，定义了外延模糊数的运算和关系运算符（=、>、>=、<、<=），并通过应用示例进行说明。

Result: 开发了C++实现并公开在GitHub仓库中，展示了外延模糊数方法在解决传统模糊数问题上的有效性。

Conclusion: 外延模糊数方法能够有效解决传统模糊数运算中的计算复杂度和特征保持问题，为模糊数的应用提供了新的可能性。

Abstract: Fuzzy numbers are commonly represented with fuzzy sets. Their objective is to
better represent imprecise data. However, operations on fuzzy numbers are not
as straightforward as maths on crisp numbers. Commonly, the Zadeh's extension
rule is applied to elaborate a result. This can produce two problems: (1) high
computational complexity and (2) for some fuzzy sets and some operations the
results is not a fuzzy set with the same features (eg. multiplication of two
triangular fuzzy sets does not produce a triangular fuzzy set). One more
problem is the fuzzy spread -- fuzziness of the result increases with the
number of operations. These facts can severely limit the application field of
fuzzy numbers. In this paper we would like to revisite this problem with a
different kind of fuzzy numbers -- extensional fuzzy numbers. The paper defines
operations on extensional fuzzy numbers and relational operators (=, >, >=, <,
<=) for them. The proposed approach is illustrated with several applicational
examples. The C++ implementation is available from a public GitHub repository.

</details>


### [42] [Customizing Open Source LLMs for Quantitative Medication Attribute Extraction across Heterogeneous EHR Systems](https://arxiv.org/abs/2510.21027)
*Zhe Fei,Mehmet Yigit Turali,Shreyas Rajesh,Xinyang Dai,Huyen Pham,Pavan Holur,Yuhui Zhu,Larissa Mooney,Yih-Ing Hser,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: 提出一个基于开源大语言模型的框架，用于从异构电子健康记录中提取阿片类药物使用障碍治疗处方信息，并计算标准化的药物覆盖天数指标。


<details>
  <summary>Details</summary>
Motivation: 解决不同电子健康记录系统中药物数据格式不统一的问题，特别是阿片类药物使用障碍治疗处方的标准化提取和监测。

Method: 使用定制化的开源大语言模型（Llama、Qwen、Gemma、MedGemma）从异构数据中提取处方属性，通过JSON模式处理记录，并进行轻量级归一化和跨字段一致性检查。

Result: 在25,605条记录上评估，较大模型表现最佳：Qwen2.5-32B达到93.4%覆盖率和93.0%精确匹配准确率，MedGemma-27B达到93.1%/92.2%。

Conclusion: 该方法消除了脆弱的站点特定ETL流程，支持本地隐私保护部署，实现了跨站点MOUD暴露、依从性和保留率的一致分析。

Abstract: Harmonizing medication data across Electronic Health Record (EHR) systems is
a persistent barrier to monitoring medications for opioid use disorder (MOUD).
In heterogeneous EHR systems, key prescription attributes are scattered across
differently formatted fields and freetext notes. We present a practical
framework that customizes open source large language models (LLMs), including
Llama, Qwen, Gemma, and MedGemma, to extract a unified set of MOUD prescription
attributes (prescription date, drug name, duration, total quantity, daily
quantity, and refills) from heterogeneous, site specific data and compute a
standardized metric of medication coverage, \emph{MOUD days}, per patient. Our
pipeline processes records directly in a fixed JSON schema, followed by
lightweight normalization and cross-field consistency checks. We evaluate the
system on prescription level EHR data from five clinics in a national OUD study
(25{,}605 records from 1{,}257 patients), using a previously annotated
benchmark of 10{,}369 records (776 patients) as the ground truth. Performance
is reported as coverage (share of records with a valid, matchable output) and
record-level exact-match accuracy. Larger models perform best overall:
Qwen2.5-32B achieves \textbf{93.4\%} coverage with \textbf{93.0\%} exact-match
accuracy across clinics, and MedGemma-27B attains
\textbf{93.1\%}/\textbf{92.2\%}. A brief error review highlights three common
issues and fixes: imputing missing dosage fields using within-drug norms,
handling monthly/weekly injectables (e.g., Vivitrol) by setting duration from
the documented schedule, and adding unit checks to prevent mass units (e.g.,
``250 g'') from being misread as daily counts. By removing brittle,
site-specific ETL and supporting local, privacy-preserving deployment, this
approach enables consistent cross-site analyses of MOUD exposure, adherence,
and retention in real-world settings.

</details>


### [43] [Epistemic Deference to AI](https://arxiv.org/abs/2510.21043)
*Benjamin Lange*

Main category: cs.AI

TL;DR: 该论文探讨何时应该优先采纳AI输出而非人类专家判断，提出AI可以作为人工认知权威，但反对直接替代人类独立认知的预占主义，主张采用全证据视角将AI输出作为贡献性理由而非完全替代。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在可靠性方面表现出认知优势，需要建立原则性框架来确定何时应该信任AI输出而非人类专家判断，特别是在高风险情境下。

Method: 基于社会认识论理论，分析AI作为人工认知权威的特性，批判预占主义观点，发展全证据视角的AI信任理论。

Result: 提出了全证据视角的AI信任方法，该方法具有三个优势：防止专业知识萎缩、为有意义的人类监督提供认知基础、解释AI不可靠时的合理不信任。

Conclusion: 全证据视角为确定何时应该信任AI提供了原则性方法，特别是在需要严格可靠性的高风险情境中，虽然实践上要求较高，但能更好地平衡AI与人类认知的关系。

Abstract: When should we defer to AI outputs over human expert judgment? Drawing on
recent work in social epistemology, I motivate the idea that some AI systems
qualify as Artificial Epistemic Authorities (AEAs) due to their demonstrated
reliability and epistemic superiority. I then introduce AI Preemptionism, the
view that AEA outputs should replace rather than supplement a user's
independent epistemic reasons. I show that classic objections to preemptionism
- such as uncritical deference, epistemic entrenchment, and unhinging epistemic
bases - apply in amplified form to AEAs, given their opacity, self-reinforcing
authority, and lack of epistemic failure markers. Against this, I develop a
more promising alternative: a total evidence view of AI deference. According to
this view, AEA outputs should function as contributory reasons rather than
outright replacements for a user's independent epistemic considerations. This
approach has three key advantages: (i) it mitigates expertise atrophy by
keeping human users engaged, (ii) it provides an epistemic case for meaningful
human oversight and control, and (iii) it explains the justified mistrust of AI
when reliability conditions are unmet. While demanding in practice, this
account offers a principled way to determine when AI deference is justified,
particularly in high-stakes contexts requiring rigorous reliability.

</details>


### [44] [From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL](https://arxiv.org/abs/2510.21045)
*Ali Khosravi Kazazi,Zhenlong Li,M. Naser Lessani,Guido Cervone*

Main category: cs.AI

TL;DR: 提出了一种多智能体框架，用于将自然语言问题准确翻译为空间SQL查询，通过知识库、语义增强和多智能体协作来解决空间查询的复杂性。


<details>
  <summary>Details</summary>
Motivation: SQL的复杂性和PostGIS等工具中地理空间函数的专业性为非专家分析空间数据设置了障碍，而单智能体方法在处理空间查询的语义和句法复杂性方面存在困难。

Method: 采用多智能体框架，包括知识库（程序化模式分析和语义增强）、上下文检索嵌入以及协作多智能体管道（实体提取、元数据检索、查询逻辑制定、SQL生成和审查代理进行程序化和语义验证）。

Result: 在非空间KaggleDBQA基准测试中达到81.2%准确率（272个问题中221个正确）；在空间查询中达到87.7%准确率（90个问题中79个正确），相比无审查代理的76.7%有显著提升。

Conclusion: 该系统使空间分析更加普及，为空间Text-to-SQL系统提供了稳健、可推广的基础，推动了自主GIS的发展。

Abstract: The complexity of Structured Query Language (SQL) and the specialized nature
of geospatial functions in tools like PostGIS present significant barriers to
non-experts seeking to analyze spatial data. While Large Language Models (LLMs)
offer promise for translating natural language into SQL (Text-to-SQL),
single-agent approaches often struggle with the semantic and syntactic
complexities of spatial queries. To address this, we propose a multi-agent
framework designed to accurately translate natural language questions into
spatial SQL queries. The framework integrates several innovative components,
including a knowledge base with programmatic schema profiling and semantic
enrichment, embeddings for context retrieval, and a collaborative multi-agent
pipeline as its core. This pipeline comprises specialized agents for entity
extraction, metadata retrieval, query logic formulation, SQL generation, and a
review agent that performs programmatic and semantic validation of the
generated SQL to ensure correctness (self-verification). We evaluate our system
using both the non-spatial KaggleDBQA benchmark and a new, comprehensive
SpatialQueryQA benchmark that includes diverse geometry types, predicates, and
three levels of query complexity. On KaggleDBQA, the system achieved an overall
accuracy of 81.2% (221 out of 272 questions) after the review agent's review
and corrections. For spatial queries, the system achieved an overall accuracy
of 87.7% (79 out of 90 questions), compared with 76.7% without the review
agent. Beyond accuracy, results also show that in some instances the system
generates queries that are more semantically aligned with user intent than
those in the benchmarks. This work makes spatial analysis more accessible, and
provides a robust, generalizable foundation for spatial Text-to-SQL systems,
advancing the development of autonomous GIS.

</details>


### [45] [MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning](https://arxiv.org/abs/2510.21093)
*Siyong Chen,Jinbo Wen,Jiawen Kang,Tenghui Huang,Xumin Huang,Yuanjia Su,Hudan Pan,Zishao Zhong,Dusit Niyato,Shengli Xie,Dong In Kim*

Main category: cs.AI

TL;DR: MedAlign是一个用于医学视觉问答的新框架，通过多模态直接偏好优化、检索感知专家混合架构和联邦治理机制，解决大型视觉语言模型在医疗部署中的幻觉、固定深度推理效率低和多机构协作困难等问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在医疗领域部署面临三个关键挑战：产生无视觉依据的幻觉答案、固定深度推理效率低下、多机构协作困难。

Method: 提出多模态直接偏好优化(mDPO)目标，设计检索感知专家混合(RA-MoE)架构，采用联邦治理机制进行自适应推理。

Result: 在三个代表性Med-VQA数据集上实现最先进性能，F1分数比强检索增强基线提升高达11.85%，平均推理长度比固定深度CoT方法减少51.60%。

Conclusion: MedAlign框架能有效确保大型视觉语言模型在医学视觉问答中的视觉准确性响应，同时提高推理效率和促进多机构协作。

Abstract: Recently, large models have shown significant potential for smart healthcare.
However, the deployment of Large Vision-Language Models (LVLMs) for clinical
services is currently hindered by three critical challenges: a tendency to
hallucinate answers not grounded in visual evidence, the inefficiency of
fixed-depth reasoning, and the difficulty of multi-institutional collaboration.
To address these challenges, in this paper, we develop MedAlign, a novel
framework to ensure visually accurate LVLM responses for Medical Visual
Question Answering (Med-VQA). Specifically, we first propose a multimodal
Direct Preference Optimization (mDPO) objective to explicitly align preference
learning with visual context. We then design a Retrieval-Aware
Mixture-of-Experts (RA-MoE) architecture that utilizes image and text
similarity to route queries to a specialized and context-augmented LVLM (i.e.,
an expert), thereby mitigating hallucinations in LVLMs. To achieve adaptive
reasoning and facilitate multi-institutional collaboration, we propose a
federated governance mechanism, where the selected expert, fine-tuned on
clinical datasets based on mDPO, locally performs iterative Chain-of-Thought
(CoT) reasoning via the local meta-cognitive uncertainty estimator. Extensive
experiments on three representative Med-VQA datasets demonstrate that MedAlign
achieves state-of-the-art performance, outperforming strong retrieval-augmented
baselines by up to $11.85\%$ in F1-score, and simultaneously reducing the
average reasoning length by $51.60\%$ compared with fixed-depth CoT approaches.

</details>


### [46] [Confounding Robust Deep Reinforcement Learning: A Causal Approach](https://arxiv.org/abs/2510.21110)
*Mingxuan Li,Junzhe Zhang,Elias Bareinboim*

Main category: cs.AI

TL;DR: 提出了一种针对观测数据中存在未观测混杂偏差的深度强化学习算法，在12个存在混杂的Atari游戏中表现优于标准DQN


<details>
  <summary>Details</summary>
Motivation: 在复杂高维领域中，当存在未观测混杂时，传统的离策略学习方法可能失效，需要开发对混杂偏差具有鲁棒性的算法

Method: 基于深度Q网络(DQN)，提出了一种新算法，寻找与观测数据兼容的最坏情况环境下的安全策略

Result: 在12个存在混杂的Atari游戏中，该算法在所有存在行为策略和目标策略输入不匹配以及未观测混杂的游戏中都优于标准DQN

Conclusion: 所提出的算法能够有效处理观测数据中的混杂偏差，在存在未观测混杂的复杂环境中具有更好的性能

Abstract: A key task in Artificial Intelligence is learning effective policies for
controlling agents in unknown environments to optimize performance measures.
Off-policy learning methods, like Q-learning, allow learners to make optimal
decisions based on past experiences. This paper studies off-policy learning
from biased data in complex and high-dimensional domains where \emph{unobserved
confounding} cannot be ruled out a priori. Building on the well-celebrated Deep
Q-Network (DQN), we propose a novel deep reinforcement learning algorithm
robust to confounding biases in observed data. Specifically, our algorithm
attempts to find a safe policy for the worst-case environment compatible with
the observations. We apply our method to twelve confounded Atari games, and
find that it consistently dominates the standard DQN in all games where the
observed input to the behavioral and target policies mismatch and unobserved
confounders exist.

</details>


### [47] [DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance](https://arxiv.org/abs/2510.21117)
*Chunghyun Han,Alfio Gliozzo,Junkyu Lee,Agostino Capponi*

Main category: cs.AI

TL;DR: 本文首次实证研究AI代理作为去中心化治理中的自主决策者，通过构建AI投票代理在真实区块链数据环境中评估其与人类决策的一致性。


<details>
  <summary>Details</summary>
Motivation: 探索AI代理在去中心化自治组织(DAO)治理中作为自主决策者的可行性，研究AI如何增强集体决策过程。

Method: 使用3000多个主要协议提案，构建AI投票代理，通过模块化可组合程序(MCP)工作流程解释提案背景、检索历史审议数据并独立确定投票立场。

Result: 发现AI代理的决策与人类和代币加权结果高度一致，通过精心设计的评估指标验证了强对齐性。

Conclusion: AI代理能够通过产生可解释、可审计且基于实证的信号来增强集体决策，为去中心化金融系统的可解释和经济严谨的AI代理设计做出贡献。

Abstract: This paper presents a first empirical study of agentic AI as autonomous
decision-makers in decentralized governance. Using more than 3K proposals from
major protocols, we build an agentic AI voter that interprets proposal
contexts, retrieves historical deliberation data, and independently determines
its voting position. The agent operates within a realistic financial simulation
environment grounded in verifiable blockchain data, implemented through a
modular composable program (MCP) workflow that defines data flow and tool usage
via Agentics framework. We evaluate how closely the agent's decisions align
with the human and token-weighted outcomes, uncovering strong alignments
measured by carefully designed evaluation metrics. Our findings demonstrate
that agentic AI can augment collective decision-making by producing
interpretable, auditable, and empirically grounded signals in realistic DAO
governance settings. The study contributes to the design of explainable and
economically rigorous AI agents for decentralized financial systems.

</details>


### [48] [PanicToCalm: A Proactive Counseling Agent for Panic Attacks](https://arxiv.org/abs/2510.21143)
*Jihyun Lee,Yejin Min,San Kim,Yejin Jeon,SungJun Yang,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.AI

TL;DR: 该论文提出了PACE数据集和PACER咨询模型，用于恐慌发作时的心理干预，通过多维度评估框架PanicEval验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 恐慌发作是急性的恐惧和痛苦发作，及时适当的干预能显著帮助个体恢复稳定。但由于伦理和后勤问题，训练此类模型的合适数据集仍然稀缺。

Method: 引入PACE数据集（包含基于第一人称叙事构建的高痛苦发作事件，围绕心理急救原则构建），训练PACER咨询模型（提供共情和指导性支持，通过监督学习和模拟偏好对齐优化），提出PanicEval多维度评估框架。

Result: 实验结果显示PACER在咨询师侧指标和客户情绪改善方面均优于强基线模型。人类评估进一步证实其实际价值，在恐慌场景中始终优于通用、CBT基础和GPT-4驱动的模型。

Conclusion: PACER模型在恐慌发作干预中表现出色，为心理健康危机干预提供了有效的AI解决方案。

Abstract: Panic attacks are acute episodes of fear and distress, in which timely,
appropriate intervention can significantly help individuals regain stability.
However, suitable datasets for training such models remain scarce due to
ethical and logistical issues. To address this, we introduce PACE, which is a
dataset that includes high-distress episodes constructed from first-person
narratives, and structured around the principles of Psychological First Aid
(PFA). Using this data, we train PACER, a counseling model designed to provide
both empathetic and directive support, which is optimized through supervised
learning and simulated preference alignment. To assess its effectiveness, we
propose PanicEval, a multi-dimensional framework covering general counseling
quality and crisis-specific strategies. Experimental results show that PACER
outperforms strong baselines in both counselor-side metrics and client affect
improvement. Human evaluations further confirm its practical value, with PACER
consistently preferred over general, CBT-based, and GPT-4-powered models in
panic scenarios (Code is available at https://github.com/JihyunLee1/PanicToCalm
).

</details>


### [49] [NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge](https://arxiv.org/abs/2510.21144)
*Hanyu Zhu,Lance Fiondella,Jiawei Yuan,Kai Zeng,Long Jiao*

Main category: cs.AI

TL;DR: 提出NeuroGenPoisoning攻击框架，通过LLM内部神经元归因和遗传优化生成对抗性外部知识，实现高效的RAG投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 现有RAG投毒攻击主要关注检索内容或提示结构，忽略了模型内部表示动态和神经元级敏感性，且未充分考虑与强参数化知识的知识冲突问题。

Method: 首先识别与上下文投毒知识强烈相关的毒物响应神经元，然后使用遗传算法进化对抗性段落以最大化激活这些神经元，并通过观察到的归因信号重用有潜力但最初不成功的外部知识变体。

Result: 在多个模型和数据集上的实验结果显示，该方法能持续实现超过90%的人口覆盖成功率，同时保持流畅性。

Conclusion: NeuroGenPoisoning能够有效解决知识冲突问题，实现大规模生成有效的RAG投毒知识。

Abstract: Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to
dynamically integrate external knowledge during inference, improving their
factual accuracy and adaptability. However, adversaries can inject poisoned
external knowledge to override the model's internal memory. While existing
attacks iteratively manipulate retrieval content or prompt structure of RAG,
they largely ignore the model's internal representation dynamics and
neuron-level sensitivities. The underlying mechanism of RAG poisoning has not
been fully studied and the effect of knowledge conflict with strong parametric
knowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning,
a novel attack framework that generates adversarial external knowledge in RAG
guided by LLM internal neuron attribution and genetic optimization. Our method
first identifies a set of Poison-Responsive Neurons whose activation strongly
correlates with contextual poisoning knowledge. We then employ a genetic
algorithm to evolve adversarial passages that maximally activate these neurons.
Crucially, our framework enables massive-scale generation of effective poisoned
RAG knowledge by identifying and reusing promising but initially unsuccessful
external knowledge variants via observed attribution signals. At the same time,
Poison-Responsive Neurons guided poisoning can effectively resolves knowledge
conflict. Experimental results across models and datasets demonstrate
consistently achieving high Population Overwrite Success Rate (POSR) of over
90% while preserving fluency. Empirical evidence shows that our method
effectively resolves knowledge conflict.

</details>


### [50] [How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation](https://arxiv.org/abs/2510.21148)
*Yang Zhao,Pu Wang,Hao Frank Yang*

Main category: cs.AI

TL;DR: 提出了EGO-Prompt框架，通过进化图优化自动设计更好的提示和推理过程，在领域特定任务中显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，为LLMs设计领域特定任务的最优提示和推理过程既必要又具挑战性，需要解决如何整合领域知识、提升推理效率等问题。

Method: EGO-Prompt从专家构建的初始语义因果图开始，通过因果引导的文本梯度过程生成推理指导，并迭代优化语义因果图和推理机制。

Result: 在公共卫生、交通和人类行为任务中，EGO-Prompt比前沿方法F1分数提高7.32%-12.61%，让小模型以不到20%的成本达到大模型性能。

Conclusion: EGO-Prompt能自动优化提示和推理过程，显著提升LLM性能，同时输出精炼的领域特定语义因果图提高可解释性。

Abstract: Designing optimal prompts and reasoning processes for large language models
(LLMs) on domain-specific tasks is both necessary and challenging in real-world
applications. Determining how to integrate domain knowledge, enhance reasoning
efficiency, and even provide domain experts with refined knowledge integration
hints are particularly crucial yet unresolved tasks. In this research, we
propose Evolutionary Graph Optimization for Prompting (EGO-Prompt), an
automated framework to designing better prompts, efficient reasoning processes
and providing enhanced causal-informed process. EGO-Prompt begins with a
general prompt and fault-tolerant initial Semantic Causal Graph (SCG)
descriptions, constructed by human experts, which is then automatically refined
and optimized to guide LLM reasoning. Recognizing that expert-defined SCGs may
be partial or imperfect and that their optimal integration varies across LLMs,
EGO-Prompt integrates a novel causal-guided textual gradient process in two
steps: first, generating nearly deterministic reasoning guidance from the SCG
for each instance, and second, adapting the LLM to effectively utilize the
guidance alongside the original input. The iterative optimization algorithm
further refines both the SCG and the reasoning mechanism using textual
gradients with ground-truth. We tested the framework on real-world public
health, transportation and human behavior tasks. EGO-Prompt achieves
7.32%-12.61% higher F1 than cutting-edge methods, and allows small models to
reach the performence of larger models at under 20% of the original cost. It
also outputs a refined, domain-specific SCG that improves interpretability.

</details>


### [51] [String Seed of Thought: Prompting LLMs for Distribution-Faithful and Diverse Generation](https://arxiv.org/abs/2510.21150)
*Kou Misaki,Takuya Akiba*

Main category: cs.AI

TL;DR: 提出String Seed of Thought (SSoT)提示方法，通过让LLM先生成随机字符串来增加熵，然后从中提取随机性选择答案，显著改善了概率指令跟随(PIF)性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在需要单一确定性答案的任务上表现出色，但在概率指令跟随(PIF)任务中表现不佳，存在偏见问题，影响需要非确定性行为的应用如人类行为模拟、内容多样化和多人游戏。

Method: SSoT提示方法：首先让LLM输出随机字符串生成足够熵，然后通过操作该字符串提取随机性来推导最终答案，在保持多样性的同时遵守特定约束。

Result: SSoT显著提高了LLMs的PIF性能，接近伪随机数生成器的理想性能。在NoveltyBench上的实验表明SSoT还能增强开放任务的响应多样性。

Conclusion: SSoT是一种简单有效的提示方法，能够解决LLMs在概率指令跟随任务中的偏见问题，提升响应多样性，适用于需要非确定性行为的各种应用场景。

Abstract: We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs
that improves Probabilistic Instruction Following (PIF). We define PIF as a
task requiring an LLM to select its answer from a predefined set of options,
each associated with a specific probability, such that the empirical
distribution of the generated answers aligns with the target distribution when
prompted multiple times. While LLMs excel at tasks with single, deterministic
answers, they often fail at PIF, exhibiting biases problematic for applications
requiring non-deterministic behaviors, such as human-behavior simulation,
content diversification, and multiplayer games. It also harms the diversity of
generated responses, a crucial factor in test-time scaling, by causing the
outputs to collapse into a limited set of answers. To address this, we propose
SSoT, a simple prompting method that instructs an LLM to first output a random
string to generate sufficient entropy. SSoT also instructs the LLM to extract
randomness by manipulating this string to derive a final answer, thereby
preserving diversity while adhering to specific constraints. We demonstrate
that SSoT significantly improves the PIF performance of LLMs, approaching the
ideal performance of a pseudo-random number generator. Furthermore, our
experiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks
to open-ended tasks by enhancing response diversity.

</details>


### [52] [Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models](https://arxiv.org/abs/2510.21175)
*Yujin Jo,Taesup Kim*

Main category: cs.AI

TL;DR: NuSA-CL是一个轻量级无内存的持续学习框架，通过低秩适应和约束权重更新在参数近似零空间内，保护预训练视觉语言模型的零样本能力，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型在现实部署中面临分布漂移和新任务挑战，静态零样本能力不足，需要持续学习方法来适应环境变化同时避免遗忘已有知识。

Method: 采用低秩适应技术，将任务特定的权重更新约束在当前模型参数的近似零空间内，最小化对已学知识的干扰，无需重放缓冲区或蒸馏。

Result: 实验表明该框架有效保护零样本迁移能力，在持续学习基准上取得有竞争力的性能，计算和内存开销最小。

Conclusion: NuSA-CL为现实应用中持续演化的零样本视觉语言模型提供了一个实用且可扩展的解决方案。

Abstract: Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated
remarkable zero-shot generalization, enabling deployment in a wide range of
real-world tasks without additional task-specific training. However, in real
deployment scenarios with evolving environments or emerging classes, these
models inevitably face distributional shifts and novel tasks. In such contexts,
static zero-shot capabilities are insufficient, and there is a growing need for
continual learning methods that allow models to adapt over time while avoiding
catastrophic forgetting. We introduce NuSA-CL (Null Space Adaptation for
Continual Learning), a lightweight memory-free continual learning framework
designed to address this challenge. NuSA-CL employs low-rank adaptation and
constrains task-specific weight updates to lie within an approximate null space
of the model's current parameters. This strategy minimizes interference with
previously acquired knowledge, effectively preserving the zero-shot
capabilities of the original model. Unlike methods relying on replay buffers or
costly distillation, NuSA-CL imposes minimal computational and memory overhead,
making it practical for deployment in resource-constrained, real-world
continual learning environments. Experiments show that our framework not only
effectively preserves zero-shot transfer capabilities but also achieves highly
competitive performance on continual learning benchmarks. These results
position NuSA-CL as a practical and scalable solution for continually evolving
zero-shot VLMs in real-world applications.

</details>


### [53] [Shylock: Causal Discovery in Multivariate Time Series based on Hybrid Constraints](https://arxiv.org/abs/2510.21181)
*Shuo Li,Keqin Xu,Jie Liu,Dan Ye*

Main category: cs.AI

TL;DR: 提出Shylock方法，用于在少样本和正常多变量时间序列中有效发现因果关系，通过组扩张卷积和共享核减少参数数量，结合全局和局部约束提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有因果关系发现方法依赖人工经验、统计方法或图准则方法，容易出错、依赖理想化假设和大量数据，且在少样本多变量时间序列上容易过拟合。

Method: 使用组扩张卷积和共享核来指数级减少参数数量，同时学习带时间延迟的变量表示；结合全局和局部约束实现网络间信息共享；设计数据生成方法生成带时间延迟的多变量时间序列。

Result: 在常用基准测试和生成数据集上的广泛实验表明，Shylock在少样本和正常多变量时间序列上都优于两种现有最先进方法。

Conclusion: Shylock方法有效解决了多变量时间序列因果关系发现中的少样本问题，并开发了Tcausal库便于使用，已部署在EarthDataMiner平台上。

Abstract: Causal relationship discovery has been drawing increasing attention due to
its prevalent application. Existing methods rely on human experience,
statistical methods, or graphical criteria methods which are error-prone, stuck
at the idealized assumption, and rely on a huge amount of data. And there is
also a serious data gap in accessing Multivariate time series(MTS) in many
areas, adding difficulty in finding their causal relationship. Existing methods
are easy to be over-fitting on them. To fill the gap we mentioned above, in
this paper, we propose Shylock, a novel method that can work well in both
few-shot and normal MTS to find the causal relationship. Shylock can reduce the
number of parameters exponentially by using group dilated convolution and a
sharing kernel, but still learn a better representation of variables with time
delay. By combing the global constraint and the local constraint, Shylock
achieves information sharing among networks to help improve the accuracy. To
evaluate the performance of Shylock, we also design a data generation method to
generate MTS with time delay. We evaluate it on commonly used benchmarks and
generated datasets. Extensive experiments show that Shylock outperforms two
existing state-of-art methods on both few-shot and normal MTS. We also
developed Tcausal, a library for easy use and deployed it on the EarthDataMiner
platform

</details>


### [54] [OutboundEval: A Dual-Dimensional Benchmark for Expert-Level Intelligent Outbound Evaluation of Xbench's Professional-Aligned Series](https://arxiv.org/abs/2510.21244)
*Pengyu Xu,Shijia Li,Ao Sun,Feng Zhang,Yahan Li,Bo Wu,Zhanyu Ma,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Rui Wang,Yang Liu,Xiaobo Hu,Fan Yang,Jia Zheng,Guanghua Yao*

Main category: cs.AI

TL;DR: 提出了OutboundEval基准，用于评估大语言模型在专业级智能外呼场景中的表现，解决了数据集多样性不足、用户模拟不真实和评估指标不准确三大问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在智能外呼场景评估中存在三个关键限制：数据集多样性和类别覆盖不足、用户模拟不真实、评估指标不准确，需要建立一个更全面、真实的评估框架。

Method: 1) 设计涵盖6大业务领域和30个子场景的基准，包含场景特定流程分解、加权评分和领域自适应指标；2) 开发大模型驱动的用户模拟器，生成多样化、角色丰富的虚拟用户；3) 引入动态评估方法，结合自动化和人工评估。

Result: 对12个最先进的大语言模型进行实验，揭示了专家级任务完成度与交互流畅性之间的权衡关系，为构建可靠、拟人化的外呼AI系统提供了实用见解。

Conclusion: OutboundEval为专业应用中的大语言模型基准测试建立了一个实用、可扩展且面向领域的新标准。

Abstract: We propose OutboundEval, a comprehensive benchmark for evaluating large
language models (LLMs) in expert-level intelligent outbound calling scenarios.
Unlike existing methods that suffer from three key limitations - insufficient
dataset diversity and category coverage, unrealistic user simulation, and
inaccurate evaluation metrics - OutboundEval addresses these issues through a
structured framework. First, we design a benchmark spanning six major business
domains and 30 representative sub-scenarios, each with scenario-specific
process decomposition, weighted scoring, and domain-adaptive metrics. Second,
we develop a large-model-driven User Simulator that generates diverse,
persona-rich virtual users with realistic behaviors, emotional variability, and
communication styles, providing a controlled yet authentic testing environment.
Third, we introduce a dynamic evaluation method that adapts to task variations,
integrating automated and human-in-the-loop assessment to measure task
execution accuracy, professional knowledge application, adaptability, and user
experience quality. Experiments on 12 state-of-the-art LLMs reveal distinct
trade-offs between expert-level task completion and interaction fluency,
offering practical insights for building reliable, human-like outbound AI
systems. OutboundEval establishes a practical, extensible, and domain-oriented
standard for benchmarking LLMs in professional applications.

</details>


### [55] [Out-of-Distribution Detection for Safety Assurance of AI and Autonomous Systems](https://arxiv.org/abs/2510.21254)
*Victoria J. Hodge,Colin Paterson,Ibrahim Habli*

Main category: cs.AI

TL;DR: 本文综述了自主系统中OOD检测技术，分析了其在安全保证中的作用，探讨了挑战和未来研究方向


<details>
  <summary>Details</summary>
Motivation: 随着AI自主系统能力的扩展，如何严格证明其安全性成为关键挑战，特别是处理分布外数据的能力对安全至关重要

Method: 通过全面文献综述，分析OOD检测技术在自主系统安全保证中的应用，包括概念定义、原因分析和生命周期整合方法

Result: 识别了ML开发生命周期中可用的OOD检测技术，提出了在生命周期中支持安全保证论证的应用建议

Conclusion: 需要解决OOD检测在系统生命周期整合中的注意事项，并为跨领域自主系统的安全开发和运营提供未来研究方向

Abstract: The operational capabilities and application domains of AI-enabled autonomous
systems have expanded significantly in recent years due to advances in robotics
and machine learning (ML). Demonstrating the safety of autonomous systems
rigorously is critical for their responsible adoption but it is challenging as
it requires robust methodologies that can handle novel and uncertain situations
throughout the system lifecycle, including detecting out-of-distribution (OoD)
data. Thus, OOD detection is receiving increased attention from the research,
development and safety engineering communities. This comprehensive review
analyses OOD detection techniques within the context of safety assurance for
autonomous systems, in particular in safety-critical domains. We begin by
defining the relevant concepts, investigating what causes OOD and exploring the
factors which make the safety assurance of autonomous systems and OOD detection
challenging. Our review identifies a range of techniques which can be used
throughout the ML development lifecycle and we suggest areas within the
lifecycle in which they may be used to support safety assurance arguments. We
discuss a number of caveats that system and safety engineers must be aware of
when integrating OOD detection into system lifecycles. We conclude by outlining
the challenges and future work necessary for the safe development and operation
of autonomous systems across a range of domains and applications.

</details>


### [56] [Investigating Scale Independent UCT Exploration Factor Strategies](https://arxiv.org/abs/2510.21275)
*Robin Schmöcker,Christoph Schnell,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 本文提出了一种自适应选择UCT探索常数λ的策略，使其对游戏奖励尺度具有不变性，并推荐使用2σ作为λ值，其中σ是搜索树中所有状态-动作对Q值的经验标准差。


<details>
  <summary>Details</summary>
Motivation: UCT算法对游戏奖励尺度敏感，在具有密集奖励且奖励尺度人为设定的游戏中，不同游戏的Q值可能跨越不同数量级，这会影响算法性能。

Method: 评估了文献中提出的各种λ选择策略以及五种新策略，包括选择λ为2σ的方法，其中σ是搜索树中所有状态-动作对Q值的经验标准差。

Result: 实验结果表明，使用2σ作为λ值的新策略在广泛任务中优于现有λ策略，无论是在单一参数值还是优化所有可用参数后的峰值性能方面。

Conclusion: 推荐使用λ=2σ的自适应策略，该方法对游戏奖励尺度具有不变性，并在各种任务中表现出色。

Abstract: The Upper Confidence Bounds For Trees (UCT) algorithm is not agnostic to the
reward scale of the game it is applied to. For zero-sum games with the sparse
rewards of $\{-1,0,1\}$ at the end of the game, this is not a problem, but many
games often feature dense rewards with hand-picked reward scales, causing a
node's Q-value to span different magnitudes across different games. In this
paper, we evaluate various strategies for adaptively choosing the UCT
exploration constant $\lambda$, called $\lambda$-strategies, that are agnostic
to the game's reward scale. These $\lambda$-strategies include those proposed
in the literature as well as five new strategies. Given our experimental
results, we recommend using one of our newly suggested $\lambda$-strategies,
which is to choose $\lambda$ as $2 \cdot \sigma$ where $\sigma$ is the
empirical standard deviation of all state-action pairs' Q-values of the search
tree. This method outperforms existing $\lambda$-strategies across a wide range
of tasks both in terms of a single parameter value and the peak performances
obtained by optimizing all available parameters.

</details>


### [57] [When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails](https://arxiv.org/abs/2510.21285)
*Yingzhi Mao,Chunkang Zhang,Junxiang Wang,Xinyan Guan,Boxi Cao,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: 该论文提出Chain-of-Guardrail(CoG)训练框架，通过重组或回溯不安全的推理步骤，在保持有效推理链的同时引导模型回到安全轨迹，解决了大型推理模型在安全性和推理能力之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务上表现出色，但存在严重的安全风险，包括有害内容生成和越狱攻击。现有缓解策略依赖注入启发式安全信号，往往会抑制推理能力，无法解决安全-推理权衡问题。

Method: 提出Chain-of-Guardrail(CoG)训练框架，分析模型的推理轨迹，发现Self-Jailbreak现象，然后通过重组或回溯不安全的推理步骤，引导模型回到安全轨迹。

Result: 在多个推理和安全基准测试上的广泛实验表明，CoG显著提高了当前大型推理模型的安全性，同时保持了相当的推理能力，明显优于先前存在严重安全-推理权衡的方法。

Conclusion: CoG框架有效解决了大型推理模型的安全-推理权衡问题，通过系统性地处理推理轨迹中的不安全步骤，在提升安全性的同时保持推理能力。

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex
reasoning tasks but remain vulnerable to severe safety risks, including harmful
content generation and jailbreak attacks. Existing mitigation strategies rely
on injecting heuristic safety signals during training, which often suppress
reasoning ability and fail to resolve the safety-reasoning trade-off. To
systematically investigate this issue, we analyze the reasoning trajectories of
diverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models
override their own risk assessments and justify responding to unsafe prompts.
This finding reveals that LRMs inherently possess the ability to reject unsafe
queries, but this ability is compromised, resulting in harmful outputs.
Building on these insights, we propose the Chain-of-Guardrail (CoG), a training
framework that recomposes or backtracks unsafe reasoning steps, steering the
model back onto safe trajectories while preserving valid reasoning chains.
Extensive experiments across multiple reasoning and safety benchmarks
demonstrate that CoG substantially improves the safety of current LRMs while
preserving comparable reasoning ability, significantly outperforming prior
methods that suffer from severe safety-reasoning trade-offs.

</details>


### [58] [Understanding AI Trustworthiness: A Scoping Review of AIES & FAccT Articles](https://arxiv.org/abs/2510.21293)
*Siddharth Mehrotra,Jin Huang,Xuelong Fu,Roel Dobbe,Clara I. Sánchez,Maarten de Rijke*

Main category: cs.AI

TL;DR: 这篇论文对AIES和FAccT两个AI伦理会议的研究进行了范围综述，发现当前可信AI研究过于技术中心化，忽视了社会技术维度，提出了需要结合技术严谨性与社会文化制度考量的跨学科方法。


<details>
  <summary>Details</summary>
Motivation: 当前可信AI研究主要采用技术中心方法，过度关注可靠性、鲁棒性和公平性等技术属性，而忽视了理解真实世界环境中AI可信度所需的社会技术维度。

Method: 对AIES和FAccT会议论文集进行范围综述，系统分析可信度在不同研究领域中的定义、操作化和应用方式，重点关注概念化方法、测量方法、验证技术、应用领域和基础价值观。

Result: 研究发现虽然在定义透明度、问责制和鲁棒性等技术属性方面取得显著进展，但存在关键差距。当前研究往往过度强调技术精确性而牺牲社会伦理考量，AI系统的社会技术性质较少被探索，可信度成为由有权定义者塑造的争议概念。

Conclusion: 需要采用结合技术严谨性与社会、文化和制度考量的跨学科方法推进可信AI。为AI伦理社区提出了可操作措施，采用真正解决AI系统与社会复杂互动的整体框架，最终促进惠及所有利益相关者的负责任技术发展。

Abstract: Background: Trustworthy AI serves as a foundational pillar for two major AI
ethics conferences: AIES and FAccT. However, current research often adopts
techno-centric approaches, focusing primarily on technical attributes such as
reliability, robustness, and fairness, while overlooking the sociotechnical
dimensions critical to understanding AI trustworthiness in real-world contexts.
  Objectives: This scoping review aims to examine how the AIES and FAccT
communities conceptualize, measure, and validate AI trustworthiness,
identifying major gaps and opportunities for advancing a holistic understanding
of trustworthy AI systems.
  Methods: We conduct a scoping review of AIES and FAccT conference proceedings
to date, systematically analyzing how trustworthiness is defined,
operationalized, and applied across different research domains. Our analysis
focuses on conceptualization approaches, measurement methods, verification and
validation techniques, application areas, and underlying values.
  Results: While significant progress has been made in defining technical
attributes such as transparency, accountability, and robustness, our findings
reveal critical gaps. Current research often predominantly emphasizes technical
precision at the expense of social and ethical considerations. The
sociotechnical nature of AI systems remains less explored and trustworthiness
emerges as a contested concept shaped by those with the power to define it.
  Conclusions: An interdisciplinary approach combining technical rigor with
social, cultural, and institutional considerations is essential for advancing
trustworthy AI. We propose actionable measures for the AI ethics community to
adopt holistic frameworks that genuinely address the complex interplay between
AI systems and society, ultimately promoting responsible technological
development that benefits all stakeholders.

</details>


### [59] [Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning](https://arxiv.org/abs/2510.21302)
*Sanghyun Ahn,Wonje Choi,Junyong Lee,Jinwoo Park,Honguk Woo*

Main category: cs.AI

TL;DR: 提出了一种结合符号验证和交互验证的神经符号具身任务规划框架，通过生成探索性代码主动与环境交互获取缺失观察，在动态和部分可观测环境中显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码即策略方法在动态或部分可观测环境中存在环境基础不足的问题，导致代码生成不准确或不完整，影响任务成功率。

Method: 采用神经符号方法，在代码生成过程中加入显式符号验证和交互验证过程，生成探索性代码主动与环境交互以获取缺失观察，同时保持任务相关状态。

Result: 在RLBench和真实世界环境中评估，任务成功率比Code-as-Policies基线提高了46.2%，任务相关动作的可执行性达到86.8%以上。

Conclusion: 该框架通过增强生成代码的环境基础，显著提高了动态环境中任务规划的可靠性。

Abstract: Recent advances in large language models (LLMs) have enabled the automatic
generation of executable code for task planning and control in embodied agents
such as robots, demonstrating the potential of LLM-based embodied intelligence.
However, these LLM-based code-as-policies approaches often suffer from limited
environmental grounding, particularly in dynamic or partially observable
settings, leading to suboptimal task success rates due to incorrect or
incomplete code generation. In this work, we propose a neuro-symbolic embodied
task planning framework that incorporates explicit symbolic verification and
interactive validation processes during code generation. In the validation
phase, the framework generates exploratory code that actively interacts with
the environment to acquire missing observations while preserving task-relevant
states. This integrated process enhances the grounding of generated code,
resulting in improved task reliability and success rates in complex
environments. We evaluate our framework on RLBench and in real-world settings
across dynamic, partially observable scenarios. Experimental results
demonstrate that our framework improves task success rates by 46.2% over
Code-as-Policies baselines and attains over 86.8% executability of
task-relevant actions, thereby enhancing the reliability of task planning in
dynamic environments.

</details>


### [60] [CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation](https://arxiv.org/abs/2510.21324)
*Jinhui Lou,Yan Yang,Zhou Yu,Zhenqi Fu,Weidong Han,Qingming Huang,Jun Yu*

Main category: cs.AI

TL;DR: CXRAgent是一个基于LLM的导演编排多阶段智能体，用于胸部X光片分析，通过工具协调、多阶段推理和团队协作，提高诊断的适应性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的CXR分析模型难以适应新诊断任务和复杂推理场景，现有智能体依赖单一诊断流程且缺乏工具可靠性评估机制。

Method: 采用导演编排的三阶段方法：工具调用（包含证据驱动验证器）、诊断规划（组建专家团队）、协作决策（整合上下文记忆）。

Result: 实验显示CXRAgent在各种CXR解释任务中表现优异，能提供视觉证据并良好泛化到不同复杂度的临床任务。

Conclusion: CXRAgent通过多阶段协作框架显著提升了CXR诊断的适应性、可靠性和推理能力。

Abstract: Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety
of task-specific and foundation models have been developed for automatic CXR
interpretation. However, these models often struggle to adapt to new diagnostic
tasks and complex reasoning scenarios. Recently, LLM-based agent models have
emerged as a promising paradigm for CXR analysis, enhancing model's capability
through tool coordination, multi-step reasoning, and team collaboration, etc.
However, existing agents often rely on a single diagnostic pipeline and lack
mechanisms for assessing tools' reliability, limiting their adaptability and
credibility. To this end, we propose CXRAgent, a director-orchestrated,
multi-stage agent for CXR interpretation, where a central director coordinates
the following stages: (1) Tool Invocation: The agent strategically orchestrates
a set of CXR-analysis tools, with outputs normalized and verified by the
Evidence-driven Validator (EDV), which grounds diagnostic outputs with visual
evidence to support reliable downstream diagnosis; (2) Diagnostic Planning:
Guided by task requirements and intermediate findings, the agent formulates a
targeted diagnostic plan. It then assembles an expert team accordingly,
defining member roles and coordinating their interactions to enable adaptive
and collaborative reasoning; (3) Collaborative Decision-making: The agent
integrates insights from the expert team with accumulated contextual memories,
synthesizing them into an evidence-backed diagnostic conclusion. Experiments on
various CXR interpretation tasks show that CXRAgent delivers strong
performance, providing visual evidence and generalizes well to clinical tasks
of different complexity. Code and data are valuable at this
\href{https://github.com/laojiahuo2003/CXRAgent/}{link}.

</details>


### [61] [Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation](https://arxiv.org/abs/2510.21341)
*Lufan Chang*

Main category: cs.AI

TL;DR: Magellan框架通过蒙特卡洛树搜索和分层引导系统，解决了LLMs在生成创新想法时过度依赖训练数据中常见概念的问题，显著提升了科学想法的可信度和创新性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成真正创新想法时往往陷入训练数据的"引力井"，而现有的搜索方法如思维树依赖不可靠的自评估启发式方法，存在根本性限制。

Method: 使用蒙特卡洛树搜索，配备分层引导系统：语义罗盘向量进行长程方向引导，景观感知价值函数进行局部决策，平衡内在连贯性、外在新颖性和叙事进展。

Result: 在广泛实验中，Magellan显著优于ReAct和思维树等强基线方法，在生成科学想法方面展现出更高的可信度和创新性。

Conclusion: 对于创造性发现，有原则的引导搜索比无约束的自主性更有效，为LLMs成为创新合作伙伴铺平了道路。

Abstract: Large Language Models (LLMs) often struggle with generating truly innovative
ideas, typically defaulting to high-probability, familiar concepts within their
training data's "gravity wells." While advanced search-based methods like Tree
of Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by
their reliance on unprincipled, inconsistent self-evaluation heuristics to
guide exploration. To address this gap, we introduce \textbf{Magellan}, a novel
framework that reframes creative generation as a principled, guided exploration
of an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo
Tree Search (MCTS) governed by a hierarchical guidance system. For long-range
direction, a "semantic compass" vector, formulated via orthogonal projection,
steers the search towards relevant novelty. For local, step-by-step decisions,
a landscape-aware value function replaces flawed self-evaluation with an
explicit reward structure that balances intrinsic coherence, extrinsic novelty,
and narrative progress. Extensive experiments demonstrate that Magellan
significantly outperforms strong baselines, including ReAct and ToT, in
generating scientific ideas with superior plausibility and innovation. Our work
shows that for creative discovery, a principled, guided search is more
effective than unconstrained agency, paving the way for LLMs to become more
capable partners in innovation.

</details>


### [62] [Boosting Accuracy and Efficiency of Budget Forcing in LLMs via Reinforcement Learning for Mathematical Reasoning](https://arxiv.org/abs/2510.21398)
*Ravindra Aribowo Tarunokusumo,Rafael Fernandes Cunha*

Main category: cs.AI

TL;DR: 提出一个结合强化学习(RL)的框架，用于提高1.5B模型在数学推理中的性能，通过减少40%以上的token使用量来提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有的预算强制方法依赖长上下文推理轨迹的监督微调(SFT)，导致小模型因冗长响应而性能下降，需要提高token效率。

Method: 集成强化学习(RL)到框架中，仅使用1.5K训练样本，结合SFT和RL训练来优化token使用效率。

Result: SFT+RL模型在GSM8K数据集上表现更好，总体准确率更高，token使用量比SFT模型减少40%以上。

Conclusion: RL可以恢复长上下文训练带来的损失，显著提高数学推理性能，同时大幅减少计算资源消耗。

Abstract: Test-time scaling methods have seen a rapid increase in popularity for its
computational efficiency and parameter-independent training to improve
reasoning performance on Large Language Models. One such method is called
budget forcing, a decoding intervention strategy which allocates extra compute
budget for thinking and elicits the inherent self-correcting behavior of the
model. However, this relies on supervised fine-tuning (SFT) on long-context
reasoning traces which causes performance degradation on smaller models due to
verbose responses. For this reason, we offer a framework integrating
reinforcement learning (RL) to improve token efficiency and boost the
performance of a 1.5B model for mathematical reasoning. We demonstrate this
using only 1.5K training samples and found that our SFT+RL model performed
better on the GSM8K dataset with varying compute budgets. Our main findings
showed an overall higher accuracy while significantly reducing its token usage
by over 40% compared to the SFT model, revealing how RL can recover the losses
due to long-context training and altogether improving performance in
mathematical reasoning.

</details>


### [63] [Advancing Symbolic Integration in Large Language Models: Beyond Conventional Neurosymbolic AI](https://arxiv.org/abs/2510.21425)
*Maneeha Rani,Bhupesh Kumar Mishra,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: 本文综述了神经符号AI在LLMs中的应用，提出了符号集成的新分类法，并制定了将符号技术与LLMs融合的路线图。


<details>
  <summary>Details</summary>
Motivation: LLMs在关键领域表现出色但缺乏透明度，现有神经符号AI方法主要针对传统神经网络，不适用于LLMs的独特特性，需要系统理解如何有效将符号AI集成到LLMs中。

Method: 首先回顾已建立的神经符号AI方法，然后提出LLMs中符号集成的新分类法，制定将符号技术与LLMs融合的路线图，包括四个维度的分类框架。

Result: 提出了涵盖LLM各个阶段、耦合机制、架构范式以及算法和应用层面视角的符号集成分类框架，识别了当前基准、前沿进展和关键差距。

Conclusion: 通过突出文献中的最新发展和显著差距，为在LLMs中实现符号集成框架以增强透明度提供了实用见解，并提出了未来研究的路线图。

Abstract: LLMs have demonstrated highly effective learning, human-like response
generation,and decision-making capabilities in high-risk sectors. However,
these models remain black boxes because they struggle to ensure transparency in
responses. The literature has explored numerous approaches to address
transparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI
approaches were primarily developed for conventional neural networks and are
not well-suited to the unique features of LLMs. Consequently, there is a
limited systematic understanding of how symbolic AI can be effectively
integrated into LLMs. This paper aims to address this gap by first reviewing
established NeSy AI methods and then proposing a novel taxonomy of symbolic
integration in LLMs, along with a roadmap to merge symbolic techniques with
LLMs. The roadmap introduces a new categorisation framework across four
dimensions by organising existing literature within these categories. These
include symbolic integration across various stages of LLM, coupling mechanisms,
architectural paradigms, as well as algorithmic and application-level
perspectives. The paper thoroughly identifies current benchmarks, cutting-edge
advancements, and critical gaps within the field to propose a roadmap for
future research. By highlighting the latest developments and notable gaps in
the literature, it offers practical insights for implementing frameworks for
symbolic integration into LLMs to enhance transparency.

</details>


### [64] [AutoOpt: A Dataset and a Unified Framework for Automating Optimization Problem Solving](https://arxiv.org/abs/2510.21436)
*Ankur Sinha,Shobhit Arora,Dhaval Pujara*

Main category: cs.AI

TL;DR: AutoOpt-11k是一个包含11,000多个手写和打印数学优化模型图像的数据集，配合AutoOpt框架实现从图像到优化问题求解的端到端自动化流程。


<details>
  <summary>Details</summary>
Motivation: 解决传统优化问题求解需要人工建模和编程的繁琐过程，实现仅通过提供优化问题图像就能自动求解的目标。

Method: 开发三模块框架：M1使用深度学习模型将优化问题图像转换为LaTeX代码；M2使用微调的小型LLM将LaTeX代码转换为PYOMO脚本；M3使用双层优化分解方法求解PYOMO描述的优化问题。

Result: MER模型在BLEU评分上优于ChatGPT、Gemini和Nougat；BOBD方法在复杂测试问题上比内点算法和遗传算法表现更好。

Conclusion: AutoOpt框架能够有效自动化优化问题的求解过程，在数学表达式识别和复杂问题求解方面展现出优越性能。

Abstract: This study presents AutoOpt-11k, a unique image dataset of over 11,000
handwritten and printed mathematical optimization models corresponding to
single-objective, multi-objective, multi-level, and stochastic optimization
problems exhibiting various types of complexities such as non-linearity,
non-convexity, non-differentiability, discontinuity, and high-dimensionality.
The labels consist of the LaTeX representation for all the images and modeling
language representation for a subset of images. The dataset is created by 25
experts following ethical data creation guidelines and verified in two-phases
to avoid errors. Further, we develop AutoOpt framework, a machine learning
based automated approach for solving optimization problems, where the user just
needs to provide an image of the formulation and AutoOpt solves it efficiently
without any further human intervention. AutoOpt framework consists of three
Modules: (i) M1 (Image_to_Text)- a deep learning model performs the
Mathematical Expression Recognition (MER) task to generate the LaTeX code
corresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)-
a small-scale fine-tuned LLM generates the PYOMO script (optimization modeling
language) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization
based Decomposition (BOBD) method solves the optimization formulation described
in the PYOMO script. We use AutoOpt-11k dataset for training and testing of
deep learning models employed in AutoOpt. The deep learning model for MER task
(M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method
(M3), which is a hybrid approach, yields better results on complex test
problems compared to common approaches, like interior-point algorithm and
genetic algorithm.

</details>


### [65] [Multi-Task Vehicle Routing Solver via Mixture of Specialized Experts under State-Decomposable MDP](https://arxiv.org/abs/2510.21453)
*Yuxin Pan,Zhiguang Cao,Chengyang Gu,Liu Liu,Peilin Zhao,Yize Chen,Fangzhen Lin*

Main category: cs.AI

TL;DR: 提出了MoSES框架，通过状态可分解MDP和专家混合机制，让统一求解器能够感知VRP变体的共享组件特性，重用基础求解器，避免神经网络求解器数量指数增长。


<details>
  <summary>Details</summary>
Motivation: 现有多任务VRP神经方法通常学习统一求解器同时处理多个约束，但未能充分利用VRP变体的组合结构，每个变体都可由一组基础VRP变体推导而来。

Method: 引入状态可分解MDP将VRP状态空间表示为基础状态空间的笛卡尔积，开发基于潜在空间的扩展，结合最优基础策略和可学习混合函数，实现策略在潜在空间的重用。

Result: 在多个VRP变体上的广泛实验表明，MoSES优于先前方法。

Conclusion: 所提框架能够有效利用VRP变体的组合结构，通过重用基础求解器提升统一求解器性能，同时控制训练神经求解器的数量增长。

Abstract: Existing neural methods for multi-task vehicle routing problems (VRPs)
typically learn unified solvers to handle multiple constraints simultaneously.
However, they often underutilize the compositional structure of VRP variants,
each derivable from a common set of basis VRP variants. This critical oversight
causes unified solvers to miss out the potential benefits of basis solvers,
each specialized for a basis VRP variant. To overcome this limitation, we
propose a framework that enables unified solvers to perceive the
shared-component nature across VRP variants by proactively reusing basis
solvers, while mitigating the exponential growth of trained neural solvers.
Specifically, we introduce a State-Decomposable MDP (SDMDP) that reformulates
VRPs by expressing the state space as the Cartesian product of basis state
spaces associated with basis VRP variants. More crucially, this formulation
inherently yields the optimal basis policy for each basis VRP variant.
Furthermore, a Latent Space-based SDMDP extension is developed by incorporating
both the optimal basis policies and a learnable mixture function to enable the
policy reuse in the latent space. Under mild assumptions, this extension
provably recovers the optimal unified policy of SDMDP through the mixture
function that computes the state embedding as a mapping from the basis state
embeddings generated by optimal basis policies. For practical implementation,
we introduce the Mixture-of-Specialized-Experts Solver (MoSES), which realizes
basis policies through specialized Low-Rank Adaptation (LoRA) experts, and
implements the mixture function via an adaptive gating mechanism. Extensive
experiments conducted across VRP variants showcase the superiority of MoSES
over prior methods.

</details>


### [66] [EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law](https://arxiv.org/abs/2510.21524)
*Ilija Lichkovski,Alexander Müller,Mariam Ibrahim,Tiwai Mhundwa*

Main category: cs.AI

TL;DR: EU-Agent-Bench：一个评估LLM代理在欧盟法律框架下合规性的基准测试，通过对比模型函数调用与法律条文来测量代理的非法行为倾向。


<details>
  <summary>Details</summary>
Motivation: LLM代理在各种环境中部署时可能表现出不可预测的行为，包括采取不良和/或不安全的行动，需要评估其在欧盟立法背景下的非法行为倾向。

Method: 创建可验证的人工策划基准，涵盖数据保护、偏见/歧视和科学诚信等多个类别，将模型的函数调用与详尽引用相关立法的评分标准进行对比。

Result: 评估了前沿LLM的法律合规性，并研究了在代理系统提示中提供相关立法摘录对合规性的影响。

Conclusion: 鼓励未来工作将代理安全基准扩展到不同法律管辖区以及多轮和多语言交互。

Abstract: Large language models (LLMs) are increasingly deployed as agents in various
contexts by providing tools at their disposal. However, LLM agents can exhibit
unpredictable behaviors, including taking undesirable and/or unsafe actions. In
order to measure the latent propensity of LLM agents for taking illegal actions
under an EU legislative context, we introduce EU-Agent-Bench, a verifiable
human-curated benchmark that evaluates an agent's alignment with EU legal norms
in situations where benign user inputs could lead to unlawful actions. Our
benchmark spans scenarios across several categories, including data protection,
bias/discrimination, and scientific integrity, with each user request allowing
for both compliant and non-compliant execution of the requested actions.
Comparing the model's function calls against a rubric exhaustively supported by
citations of the relevant legislature, we evaluate the legal compliance of
frontier LLMs, and furthermore investigate the compliance effect of providing
the relevant legislative excerpts in the agent's system prompt along with
explicit instructions to comply. We release a public preview set for the
research community, while holding out a private test set to prevent data
contamination in evaluating upcoming models. We encourage future work extending
agentic safety benchmarks to different legal jurisdictions and to multi-turn
and multilingual interactions. We release our code on
\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.

</details>


### [67] [Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware Meta-Verification and Trustworthy Reasoning with Structured Facts](https://arxiv.org/abs/2510.21557)
*Hongwei Zhang,Ji Lu,Shiqing Jiang,Chenxiang Zhu,Li Xie,Chen Zhong,Haoran Chen,Yurui Zhu,Yongsheng Du,Yanqin Gao,Lingjun Huang,Baoli Wang,Fang Tan,Peng Zou*

Main category: cs.AI

TL;DR: Co-Sight通过冲突感知元验证和可信推理结构化事实机制，将推理转化为可证伪和可审计的过程，解决了LLM智能体长程推理中验证不足的问题。


<details>
  <summary>Details</summary>
Motivation: LLM智能体长程推理失败往往不是生成能力不足，而是中间推理验证不足。需要将推理转化为可证伪和可审计的过程。

Method: 使用冲突感知元验证(CAMV)将验证重构为冲突识别和针对性证伪，只在专家智能体间的分歧热点分配计算；可信推理结构化事实(TRSF)通过结构化事实模块持续组织、验证和同步证据。

Result: 在GAIA上达到84.4%准确率，Humanity's Last Exam上35.5%，Chinese-SimpleQA上93.8%，均达到最先进水平。

Conclusion: Co-Sight为LLM智能体可靠长程推理提供了可扩展的范式，结构化事实基础和冲突感知验证的协同作用驱动了性能提升。

Abstract: Long-horizon reasoning in LLM-based agents often fails not from generative
weakness but from insufficient verification of intermediate reasoning. Co-Sight
addresses this challenge by turning reasoning into a falsifiable and auditable
process through two complementary mechanisms: Conflict-Aware Meta-Verification
(CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV
reformulates verification as conflict identification and targeted
falsification, allocating computation only to disagreement hotspots among
expert agents rather than to full reasoning chains. This bounds verification
cost to the number of inconsistencies and improves efficiency and reliability.
TRSF continuously organizes, validates, and synchronizes evidence across agents
through a structured facts module. By maintaining verified, traceable, and
auditable knowledge, it ensures that all reasoning is grounded in consistent,
source-verified information and supports transparent verification throughout
the reasoning process. Together, TRSF and CAMV form a closed verification loop,
where TRSF supplies structured facts and CAMV selectively falsifies or
reinforces them, yielding transparent and trustworthy reasoning. Empirically,
Co-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last
Exam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies
confirm that the synergy between structured factual grounding and
conflict-aware verification drives these improvements. Co-Sight thus offers a
scalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code
is available at
https://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks.

</details>


### [68] [Learning Neural Control Barrier Functions from Expert Demonstrations using Inverse Constraint Learning](https://arxiv.org/abs/2510.21560)
*Yuxuan Yang,Hussein Sibai*

Main category: cs.AI

TL;DR: 提出了一种使用模仿学习来训练神经控制屏障函数的方法，无需显式指定故障状态集，而是通过专家演示来学习安全约束分类器，然后用于标注数据训练神经CBF。


<details>
  <summary>Details</summary>
Motivation: 在关键领域运行的自主系统中，安全是基本要求。传统的控制屏障函数需要显式指定故障状态集，但在许多实际场景中（如自动驾驶中的跟车问题），故障状态集难以正式定义，而专家演示数据更容易获取。

Method: 使用模仿学习训练约束函数来分类系统状态为安全或不安全，然后用该函数标注新的模拟轨迹数据来训练神经控制屏障函数。

Result: 在四个不同环境中进行实证评估，该方法优于现有基线方法，并且与使用真实安全标签训练的神经CBF达到相当的性能。

Conclusion: 该方法提供了一种数据驱动的替代方案，能够在不显式指定故障状态集的情况下学习安全约束，为自主系统的安全控制提供了有效解决方案。

Abstract: Safety is a fundamental requirement for autonomous systems operating in
critical domains. Control barrier functions (CBFs) have been used to design
safety filters that minimally alter nominal controls for such systems to
maintain their safety. Learning neural CBFs has been proposed as a data-driven
alternative for their computationally expensive optimization-based synthesis.
However, it is often the case that the failure set of states that should be
avoided is non-obvious or hard to specify formally, e.g., tailgating in
autonomous driving, while a set of expert demonstrations that achieve the task
and avoid the failure set is easier to generate. We use ICL to train a
constraint function that classifies the states of the system under
consideration to safe, i.e., belong to a controlled forward invariant set that
is disjoint from the unspecified failure set, and unsafe ones, i.e., belong to
the complement of that set. We then use that function to label a new set of
simulated trajectories to train our neural CBF. We empirically evaluate our
approach in four different environments, demonstrating that it outperforms
existing baselines and achieves comparable performance to a neural CBF trained
with the same data but annotated with ground-truth safety labels.

</details>


### [69] [Huxley-Gödel Machine: Human-Level Coding Agent Development by an Approximation of the Optimal Self-Improving Machine](https://arxiv.org/abs/2510.21614)
*Wenyi Wang,Piotr Piękos,Li Nanbo,Firas Laakom,Yimeng Chen,Mateusz Ostaszewski,Mingchen Zhuge,Jürgen Schmidhuber*

Main category: cs.AI

TL;DR: 该论文提出了Huxley-Gödel Machine (HGM)方法，通过引入衡量自我改进潜力的CMP指标来指导代码智能体的自我改进过程，解决了传统方法中性能与自我改进潜力不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码智能体自我改进方法主要依赖软件工程基准性能来选择改进方向，但作者发现这种性能指标与智能体的自我改进潜力存在不匹配问题，需要更有效的指导指标。

Method: 提出CMP指标来评估智能体的自我改进潜力，并基于此构建Huxley-Gödel Machine (HGM)框架，通过估计CMP来指导自我修改树的搜索过程。

Result: 在SWE-bench Verified和Polyglot基准测试中，HGM超越了现有方法且使用更少的计算时间。优化后的智能体在SWE-bench Lite上达到了人类水平的性能。

Conclusion: CMP指标能有效指导代码智能体的自我改进过程，HGM框架在多个数据集和大型语言模型上都表现出良好的迁移性和性能优势。

Abstract: Recent studies operationalize self-improvement through coding agents that
edit their own codebases. They grow a tree of self-modifications through
expansion strategies that favor higher software engineering benchmark
performance, assuming that this implies more promising subsequent
self-modifications. However, we identify a mismatch between the agent's
self-improvement potential (metaproductivity) and its coding benchmark
performance, namely the Metaproductivity-Performance Mismatch. Inspired by
Huxley's concept of clade, we propose a metric ($\mathrm{CMP}$) that aggregates
the benchmark performances of the descendants of an agent as an indicator of
its potential for self-improvement. We show that, in our self-improving coding
agent development setting, access to the true $\mathrm{CMP}$ is sufficient to
simulate how the G\"odel Machine would behave under certain assumptions. We
introduce the Huxley-G\"odel Machine (HGM), which, by estimating $\mathrm{CMP}$
and using it as guidance, searches the tree of self-modifications. On SWE-bench
Verified and Polyglot, HGM outperforms prior self-improving coding agent
development methods while using less wall-clock time. Last but not least, HGM
demonstrates strong transfer to other coding datasets and large language
models. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and
evaluated on SWE-bench Lite with GPT-5 achieves human-level performance,
matching the best officially checked results of human-engineered coding agents.
Our code is available at https://github.com/metauto-ai/HGM.

</details>


### [70] [DeepAgent: A General Reasoning Agent with Scalable Toolsets](https://arxiv.org/abs/2510.21618)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Guanting Dong,Jiajie Jin,Yinuo Wang,Hao Wang,Yutao Zhu,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: DeepAgent是一个端到端的深度推理代理，通过自主思维、工具发现和动作执行在单一连贯推理过程中完成任务，解决了长时交互中的上下文爆炸和历史累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有代理框架通常遵循预定义工作流程，限制了自主性和全局任务完成能力。现实世界任务需要外部工具和长时交互，现有方法无法有效处理这些问题。

Method: 引入自主记忆折叠机制压缩过往交互为结构化记忆，开发ToolPO端到端强化学习策略，利用LLM模拟API并通过工具调用优势归因分配细粒度信用。

Result: 在8个基准测试中，包括通用工具使用任务和下游应用，DeepAgent在标记工具和开放集工具检索场景中均优于基线方法。

Conclusion: 这项工作朝着为现实世界应用构建更通用和有能力代理迈出了一步。

Abstract: Large reasoning models have demonstrated strong problem-solving abilities,
yet real-world tasks often require external tools and long-horizon
interactions. Existing agent frameworks typically follow predefined workflows,
which limit autonomous and global task completion. In this paper, we introduce
DeepAgent, an end-to-end deep reasoning agent that performs autonomous
thinking, tool discovery, and action execution within a single, coherent
reasoning process. To address the challenges of long-horizon interactions,
particularly the context length explosion from multiple tool calls and the
accumulation of interaction history, we introduce an autonomous memory folding
mechanism that compresses past interactions into structured episodic, working,
and tool memories, reducing error accumulation while preserving critical
information. To teach general-purpose tool use efficiently and stably, we
develop an end-to-end reinforcement learning strategy, namely ToolPO, that
leverages LLM-simulated APIs and applies tool-call advantage attribution to
assign fine-grained credit to the tool invocation tokens. Extensive experiments
on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,
TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,
HLE), demonstrate that DeepAgent consistently outperforms baselines across both
labeled-tool and open-set tool retrieval scenarios. This work takes a step
toward more general and capable agents for real-world applications. The code
and demo are available at https://github.com/RUC-NLPIR/DeepAgent.

</details>


### [71] [AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite](https://arxiv.org/abs/2510.21652)
*Jonathan Bragg,Mike D'Arcy,Nishant Balepur,Dan Bareket,Bhavana Dalvi,Sergey Feldman,Dany Haddad,Jena D. Hwang,Peter Jansen,Varsha Kishore,Bodhisattwa Prasad Majumder,Aakanksha Naik,Sigal Rahamimov,Kyle Richardson,Amanpreet Singh,Harshit Surana,Aryeh Tiktinsky,Rosni Vasu,Guy Wiener,Chloe Anastasiades,Stefan Candra,Jason Dunkelberger,Dan Emery,Rob Evans,Malachi Hamada,Regan Huff,Rodney Kinney,Matt Latzke,Jaron Lochner,Ruben Lozano-Aguilera,Cecile Nguyen,Smita Rao,Amber Tanaka,Brooke Vlahos,Peter Clark,Doug Downey,Yoav Goldberg,Ashish Sabharwal,Daniel S. Weld*

Main category: cs.AI

TL;DR: 本文提出了AstaBench基准套件，用于更严格地评估AI科学代理的能力，包含2400多个科学问题，提供可复现的研究环境和基线代理，评估发现AI在科学研究辅助方面仍有很大差距。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理评估基准存在多方面不足：无法提供真实科学研究的整体衡量、缺乏可复现的代理工具、未考虑模型成本和工具访问等混杂变量、缺乏标准化接口、缺少全面的基线代理。

Method: 定义了更严格的代理基准原则和工具，开发了AstaBench套件，包含2400+涵盖整个科学发现过程的问题，提供生产级搜索工具的研究环境，以及9类科学优化代理和众多基线。

Result: 对57个代理在22个代理类别上的广泛评估显示，尽管在某些具体方面有进展，但AI在解决科学研辅助挑战方面仍有很大差距。

Conclusion: 需要更严格的基准来评估AI科学代理，AstaBench提供了这样的框架，但当前AI在科学研辅助方面的能力仍然有限。

Abstract: AI agents hold the potential to revolutionize scientific productivity by
automating literature reviews, replicating experiments, analyzing data, and
even proposing new directions of inquiry; indeed, there are now many such
agents, ranging from general-purpose "deep research" systems to specialized
science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of
these agents is critical for progress. Yet existing benchmarks fall short on
several fronts: they (1) fail to provide holistic, product-informed measures of
real-world use cases such as science research; (2) lack reproducible agent
tools necessary for a controlled comparison of core agentic capabilities; (3)
do not account for confounding variables such as model cost and tool access;
(4) do not provide standardized interfaces for quick agent prototyping and
evaluation; and (5) lack comprehensive baseline agents necessary to identify
true advances. In response, we define principles and tooling for more
rigorously benchmarking agents. Using these, we present AstaBench, a suite that
provides the first holistic measure of agentic ability to perform scientific
research, comprising 2400+ problems spanning the entire scientific discovery
process and multiple scientific domains, and including many problems inspired
by actual user requests to deployed Asta agents. Our suite comes with the first
scientific research environment with production-grade search tools that enable
controlled, reproducible evaluation, better accounting for confounders.
Alongside, we provide a comprehensive suite of nine science-optimized classes
of Asta agents and numerous baselines. Our extensive evaluation of 57 agents
across 22 agent classes reveals several interesting findings, most importantly
that despite meaningful progress on certain individual aspects, AI remains far
from solving the challenge of science research assistance.

</details>


### [72] [CMOMgen: Complex Multi-Ontology Alignment via Pattern-Guided In-Context Learning](https://arxiv.org/abs/2510.21656)
*Marta Contreiras Silva,Daniel Faria,Catia Pesquita*

Main category: cs.AI

TL;DR: CMOMgen是首个端到端的复杂多本体匹配策略，能够生成完整且语义合理的映射，无需限制目标本体或实体的数量。


<details>
  <summary>Details</summary>
Motivation: 构建全面的知识图谱需要使用多个本体来将数据完全情境化到特定领域。简单的成对等价映射无法提供相关但不相交本体的完整语义集成。

Method: 采用检索增强生成方法选择相关类来组成映射，并过滤匹配参考映射作为示例，增强上下文学习。

Result: 在三个生物医学任务中评估，CMOMgen在类选择方面优于基线方法，F1分数最低达到63%，在两个任务中优于所有基线和消融版本，在第三个任务中排名第二。

Conclusion: CMOMgen能够构建语义合理的映射，46%的非参考映射获得最高评分，证明了其有效性。

Abstract: Constructing comprehensive knowledge graphs requires the use of multiple
ontologies in order to fully contextualize data into a domain. Ontology
matching finds equivalences between concepts interconnecting ontologies and
creating a cohesive semantic layer. While the simple pairwise state of the art
is well established, simple equivalence mappings cannot provide full semantic
integration of related but disjoint ontologies. Complex multi-ontology matching
(CMOM) aligns one source entity to composite logical expressions of multiple
target entities, establishing more nuanced equivalences and provenance along
the ontological hierarchy.
  We present CMOMgen, the first end-to-end CMOM strategy that generates
complete and semantically sound mappings, without establishing any restrictions
on the number of target ontologies or entities. Retrieval-Augmented Generation
selects relevant classes to compose the mapping and filters matching reference
mappings to serve as examples, enhancing In-Context Learning. The strategy was
evaluated in three biomedical tasks with partial reference alignments. CMOMgen
outperforms baselines in class selection, demonstrating the impact of having a
dedicated strategy. Our strategy also achieves a minimum of 63% in F1-score,
outperforming all baselines and ablated versions in two out of three tasks and
placing second in the third. Furthermore, a manual evaluation of non-reference
mappings showed that 46% of the mappings achieve the maximum score, further
substantiating its ability to construct semantically sound mappings.

</details>


### [73] [A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection](https://arxiv.org/abs/2510.21679)
*Gaku Morio,Harri Rowlands,Dominik Stammbach,Christopher D. Manning,Peter Henderson*

Main category: cs.AI

TL;DR: 提出了一个用于评估视觉语言模型的多模态框架分析基准数据集，包含专家标注的视频广告，涵盖13种框架类型、50多家公司和20个国家。


<details>
  <summary>Details</summary>
Motivation: 企业公关活动存在言行不一的问题（如石油公司的"漂绿"行为），需要大规模理解框架及其变化来识别公关活动的真实目标和性质。

Method: 从Facebook和YouTube收集视频广告，由专家标注13种框架类型，专门设计用于评估视觉语言模型的多模态分析能力。

Result: 基线实验显示GPT-4.1在检测环境信息方面达到79% F1分数，但最佳模型在识别绿色创新框架方面仅达到46% F1分数。

Conclusion: 该数据集为能源领域战略沟通的多模态分析研究做出贡献，同时揭示了视觉语言模型在处理隐性框架、不同长度视频和隐性文化背景方面的挑战。

Abstract: Companies spend large amounts of money on public relations campaigns to
project a positive brand image. However, sometimes there is a mismatch between
what they say and what they do. Oil & gas companies, for example, are accused
of "greenwashing" with imagery of climate-friendly initiatives. Understanding
the framing, and changes in framing, at scale can help better understand the
goals and nature of public relations campaigns. To address this, we introduce a
benchmark dataset of expert-annotated video ads obtained from Facebook and
YouTube. The dataset provides annotations for 13 framing types for more than 50
companies or advocacy groups across 20 countries. Our dataset is especially
designed for the evaluation of vision-language models (VLMs), distinguishing it
from past text-only framing datasets. Baseline experiments show some promising
results, while leaving room for improvement for future work: GPT-4.1 can detect
environmental messages with 79% F1 score, while our best model only achieves
46% F1 score on identifying framing around green innovation. We also identify
challenges that VLMs must address, such as implicit framing, handling videos of
various lengths, or implicit cultural backgrounds. Our dataset contributes to
research in multimodal analysis of strategic communication in the energy
sector.

</details>


### [74] [A Knowledge-Graph Translation Layer for Mission-Aware Multi-Agent Path Planning in Spatiotemporal Dynamics](https://arxiv.org/abs/2510.21695)
*Edward Holmberg,Elias Ioup,Mahdi Abdelguerfi*

Main category: cs.AI

TL;DR: 提出了一个基于知识图谱的框架，作为智能翻译层来弥合高层任务目标与低层规划器输入之间的语义鸿沟，通过双平面架构将声明性事实编译为任务感知的"世界观"和物理感知的遍历规则。


<details>
  <summary>Details</summary>
Motivation: 解决自主智能体在动态环境中协调时，高层任务目标与低层规划器输入之间的语义鸿沟问题。

Method: 使用知识图谱作为智能翻译层，采用双平面架构将声明性事实编译为每个智能体的任务感知"世界观"和物理感知的遍历规则，使任务语义与领域无关的规划器解耦。

Result: 在墨西哥湾的自主水下航行器案例研究中，可视化展示了端到端过程，并定量证明不同的声明性策略能产生不同的高性能结果。

Conclusion: 知识图谱不仅是数据存储库，更是创建自适应和可解释自主系统的强大、有状态编排器。

Abstract: The coordination of autonomous agents in dynamic environments is hampered by
the semantic gap between high-level mission objectives and low-level planner
inputs. To address this, we introduce a framework centered on a Knowledge Graph
(KG) that functions as an intelligent translation layer. The KG's two-plane
architecture compiles declarative facts into per-agent, mission-aware
``worldviews" and physics-aware traversal rules, decoupling mission semantics
from a domain-agnostic planner. This allows complex, coordinated paths to be
modified simply by changing facts in the KG. A case study involving Autonomous
Underwater Vehicles (AUVs) in the Gulf of Mexico visually demonstrates the
end-to-end process and quantitatively proves that different declarative
policies produce distinct, high-performing outcomes. This work establishes the
KG not merely as a data repository, but as a powerful, stateful orchestrator
for creating adaptive and explainable autonomous systems.

</details>
