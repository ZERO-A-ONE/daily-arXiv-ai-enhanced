<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 3]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.CR](#cs.CR) [Total: 9]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Coverage Isn't Enough: SBFL-Driven Insights into Manually Created vs. Automatically Generated Tests](https://arxiv.org/abs/2512.11223)
*Sasara Shimizu,Yoshiki Higo*

Main category: cs.SE

TL;DR: 该研究比较了自动生成测试与手动创建测试在故障定位能力（SBFL分数）和代码覆盖率方面的表现，发现自动测试覆盖率更高但故障定位能力较差，特别是在深层嵌套代码中。


<details>
  <summary>Details</summary>
Motivation: 软件测试中手动创建测试用例耗时，现有自动化测试生成工具评估大多关注覆盖率指标，很少研究其在故障定位方面的有效性，特别是使用变异测试引入的人工故障。

Method: 使用SBFL（基于频谱的故障定位）分数和代码覆盖率作为评估指标，比较自动生成测试与手动创建测试的性能。SBFL分数反映了使用SBFL技术准确定位故障的能力。

Result: 实验结果显示：自动生成测试比手动创建测试获得更高的分支覆盖率，但SBFL分数较低，特别是在具有深层嵌套结构的代码中。

Conclusion: 研究结果为如何有效结合自动生成测试和手动创建测试方法提供了指导，强调了在评估测试质量时需要考虑故障定位能力而不仅仅是覆盖率。

Abstract: The testing phase is an essential part of software development, but manually creating test cases can be time-consuming. Consequently, there is a growing need for more efficient testing methods. To reduce the burden on developers, various automated test generation tools have been developed, and several studies have been conducted to evaluate the effectiveness of the tests they produce. However, most of these studies focus primarily on coverage metrics, and only a few examine how well the tests support fault localization-particularly using artificial faults introduced through mutation testing. In this study, we compare the SBFL (Spectrum-Based Fault Localization) score and code coverage of automatically generated tests with those of manually created tests. The SBFL score indicates how accurately faults can be localized using SBFL techniques. By employing SBFL score as an evaluation metric-an approach rarely used in prior studies on test generation-we aim to provide new insights into the respective strengths and weaknesses of manually created and automatically generated tests. Our experimental results show that automatically generated tests achieve higher branch coverage than manually created tests, but their SBFL score is lower, especially for code with deeply nested structures. These findings offer guidance on how to effectively combine automatically generated and manually created testing approaches.

</details>


### [2] [REMODEL-LLM: Transforming C code to Java using LLMs](https://arxiv.org/abs/2512.11402)
*Aryan Gupta,Y. Raghu Reddy*

Main category: cs.SE

TL;DR: 本研究评估了19个小型量化LLM在C到Java代码翻译任务中的表现，发现只有少数模型能有效处理这一复杂转换，大多数模型完全失败。


<details>
  <summary>Details</summary>
Motivation: C到Java的自动翻译面临巨大挑战，包括编程范式差异（过程式vs面向对象）、内存模型差异（手动指针vs垃圾回收）以及数据类型不兼容。本研究旨在探索小型量化LLM在此任务上的实际效果。

Method: 采用混合管道方法：利用抽象语法树（AST）进行语义分解，并采用高度约束的基于规则的提示策略。评估了19个参数少于200亿的小型量化LLM。

Result: 结果呈现明显的三级性能划分：第三级模型（如llama3.1、gemma3、starcoder2）100%测试失败；第二级模型（如mistral-nemo、mistral）能生成可运行代码但存在危险的语义错误；只有三个第一级模型（phi4、deepseek-coder-v2、codeqwen）表现可行，通过率超过50%，但仍无法处理函数指针、sizeof和枚举逻辑等复杂C概念。

Conclusion: 当前量化模型在C到Java翻译任务上存在明显的能力天花板，只有极少数模型能有效处理基本转换，但面对复杂C概念时仍会失败，揭示了当前小型量化LLM推理能力的局限性。

Abstract: The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.

</details>


### [3] [Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models](https://arxiv.org/abs/2512.11482)
*Melih Catal,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: 本文首次系统评估差分隐私在代码大语言模型中的应用，发现DP能显著降低模型记忆训练数据片段的风险，同时保持甚至提升代码生成能力，且不影响训练效率。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型在生成代码时可能无意中记忆并复现训练数据片段，这带来了隐私泄露和知识产权侵权的风险，限制了模型在敏感领域的部署和训练数据来源。

Method: 首先识别和理解代码大语言模型在微调过程中的记忆行为原因，然后通过实验评估差分隐私在缓解记忆问题同时保持代码生成能力的效果，分析DP对训练效率和能耗的影响。

Result: DP显著降低了代码大语言模型对所有测试片段类型的记忆，最易被记忆的片段类型也是DP最有效缓解的类型；DP略微增加困惑度但保持甚至提升了代码生成能力；DP不影响训练时间和能耗。

Conclusion: 差分隐私是保护代码大语言模型隐私的实用选择，能有效降低记忆风险而不显著影响模型效用、训练效率或能耗，为在敏感领域部署CodeLLMs提供了可行方案。

Abstract: Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound](https://arxiv.org/abs/2512.11169)
*Akhil S Anand,Elias Aarekol,Martin Mziray Dalseg,Magnus Stalhane,Sebastien Gros*

Main category: cs.AI

TL;DR: 本文提出CORL框架，使用强化学习端到端微调混合整数线性规划方案，以最大化实际运营性能，将MILP求解过程转化为可微分的随机策略。


<details>
  <summary>Details</summary>
Motivation: 传统的组合顺序决策问题通常建模为混合整数线性规划，但准确建模现实世界随机问题很困难，导致实际性能不佳。现有机器学习方法依赖监督学习，需要真实最优决策，且使用MILP梯度的替代方法。

Method: 提出CORL概念验证框架，使用强化学习在真实世界数据上端到端微调MILP方案。通过将分支定界算法求解的MILP转化为与强化学习兼容的可微分随机策略来实现。

Result: 在简单的组合顺序决策示例中验证了CORL方法的有效性。

Conclusion: CORL框架能够通过强化学习直接优化MILP在实际运营中的性能，避免了传统方法对准确建模和真实最优决策的依赖。

Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.

</details>


### [5] [FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration](https://arxiv.org/abs/2512.11213)
*Dongwon Jung,Peng Shi,Yi Zhang*

Main category: cs.AI

TL;DR: FutureWeaver框架通过模块化协作和双级规划，在固定预算下优化多智能体系统中的测试时计算分配，显著提升协作性能


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展技术（如重复采样、自我验证）在单智能体场景中有效，但缺乏在多智能体系统中分配计算以促进协作的原则性机制，特别是在预算约束下

Method: 1. 模块化协作：将可重用的多智能体工作流封装为可调用函数，通过自我游戏反思从历史轨迹中抽象出重复交互模式；2. 双级规划架构：在当前任务状态推理基础上，同时推测未来步骤，优化计算分配

Result: 在复杂智能体基准测试中，FutureWeaver在不同预算设置下始终优于基线方法，验证了其在推理时优化中促进多智能体协作的有效性

Conclusion: FutureWeaver为多智能体系统提供了原则性的测试时计算分配框架，通过模块化协作和前瞻性规划，在预算约束下显著提升协作性能，扩展了测试时计算优化的应用范围

Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.

</details>


### [6] [CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving](https://arxiv.org/abs/2512.11323)
*Jianyi Zhang,Ziyin Zhou,Xu Ji,Shizhao Liu,Zhangchi Zhao*

Main category: cs.AI

TL;DR: 本文提出了首个专门针对大型视觉语言模型（LVLMs）的CAPTCHA基准测试CAPTURE，涵盖4种主要类型和25种子类型，用于全面评估LVLMs在解决验证码方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉验证码的基准测试存在局限性，无法全面覆盖所有验证码类型，且缺乏专门针对LVLMs的基准测试。现有研究在设计基准和数据集时往往根据特定研究目标定制，导致评估不够全面。

Method: 提出了CAPTURE（CAPTCHA for Testing Under Real-world Experiments）基准测试，包含来自31个供应商的4种主要CAPTCHA类型和25种子类型。该基准具有广泛的类别多样性、大规模数据以及专门为LVLMs定制的标签。

Result: 使用该基准测试评估当前LVLMs时，发现它们在解决验证码方面表现不佳，表明现有模型在实际验证码识别任务中存在局限性。

Conclusion: CAPTURE基准填补了先前研究在数据全面性和标签针对性方面的空白，为LVLMs在验证码识别任务的多维度和全面评估提供了有效工具，揭示了当前LVLMs在该领域的不足。

Abstract: Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.

</details>


### [7] [AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints](https://arxiv.org/abs/2512.11426)
*Shuowei Cai,Yansong Ning,Hao Liu*

Main category: cs.AI

TL;DR: AgentBalance是一个在明确token成本和延迟预算下构建成本效益多智能体系统的框架，采用"先骨干后拓扑"设计，相比现有方法在相同预算下性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体系统在web规模应用中成本效益日益重要，但现有方法很少在明确的token成本和延迟预算下建模和优化，导致预算约束时设计次优。

Method: 采用"先骨干后拓扑"设计：1)骨干导向的智能体生成（LLM池构建、池选择、角色-骨干匹配）；2)自适应MAS拓扑生成（智能体表示学习、门控、延迟感知拓扑合成）。

Result: 在14个候选LLM骨干的基准测试中，AgentBalance在匹配的token成本预算下实现高达10%性能提升，在延迟预算下实现22%性能提升，在性能-预算曲线上表现出强AUC。

Conclusion: AgentBalance框架能够在明确预算约束下构建成本效益多智能体系统，可作为现有MAS的插件提升性能，并能泛化到未见过的LLM，适用于实际预算感知部署。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance

</details>


### [8] [Back to the Baseline: Examining Baseline Effects on Explainability Metrics](https://arxiv.org/abs/2512.11433)
*Agustin Martin Picard,Thibaut Boissin,Varshini Subhash,Rémi Cadène,Thomas Fel*

Main category: cs.AI

TL;DR: 该论文指出当前XAI中基于插入和删除的保真度评估指标存在严重问题：不同基线选择会偏向不同的归因方法，甚至导致线性模型得出矛盾的最优方法结论。作者提出基线应满足两个理想属性：移除信息且不过度产生分布外图像，并引入一种新的模型依赖基线来改进这一权衡。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能（XAI）中广泛使用的归因方法通常通过插入和删除等保真度指标进行评估。然而，这些指标严重依赖于基线函数的选择，而不同基线的选择会不可避免地偏向某些归因方法，甚至导致评估结果相互矛盾。这引发了一个关键问题：我们应该使用哪种基线？

Method: 作者首先分析了现有基线的局限性，提出基线应满足两个理想属性：1）能够移除信息；2）不会产生过度分布外（OOD）的图像。通过实验发现现有基线都无法同时满足这两个标准，存在明显的权衡关系。为解决这一问题，作者利用特征可视化的最新工作，提出了一种新的模型依赖基线，能够移除信息同时不过度产生OOD图像。

Result: 研究发现，现有基线都存在信息移除与OOD生成之间的权衡：要么能有效移除信息但产生OOD图像，要么不产生OOD图像但信息移除效果不佳。作者提出的新基线在权衡方面优于现有基线，能够更好地同时满足两个理想属性。

Conclusion: 当前XAI评估中使用的基线选择存在严重偏差问题，不同基线会偏向不同的归因方法。作者提出的模型依赖基线在信息移除和避免过度OOD生成之间取得了更好的平衡，为更公平的归因方法评估提供了改进方案。

Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline

</details>


### [9] [Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes](https://arxiv.org/abs/2512.11463)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Minsu Ha,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.AI

TL;DR: Motif-2-12.7B-Reasoning是一个12.7B参数的语言模型，旨在弥合开源模型与前沿专有模型在复杂推理和长上下文理解方面的差距，通过创新的训练方法在有限计算资源下实现接近更大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决开源模型在复杂推理和长上下文理解方面与前沿专有模型之间的性能差距，同时克服推理适应过程中常见的模型崩溃和训练不稳定问题。

Method: 采用全面的可复现训练方案：1）使用混合并行和内核级优化的内存高效基础设施支持64K-token上下文；2）两阶段监督微调课程，通过验证对齐的合成数据缓解分布不匹配；3）鲁棒的强化学习微调管道，通过难度感知数据过滤和混合策略轨迹重用来稳定训练。

Result: Motif-2-12.7B-Reasoning在数学、编程和智能体基准测试中实现了与参数数量显著更大的模型相当的性能，为社区提供了一个具有竞争力的开源模型。

Conclusion: 该研究提供了一个实用的蓝图，展示了如何在现实计算约束下扩展推理能力，弥合了开源模型与前沿专有模型在复杂推理任务上的性能差距。

Abstract: We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.

</details>


### [10] [Three methods, one problem: Classical and AI approaches to no-three-in-line](https://arxiv.org/abs/2512.11469)
*Pranav Ramanathan,Thomas Prellberg,Matthew Lewis,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.AI

TL;DR: 该论文首次系统比较了经典优化方法（整数线性规划ILP）与AI方法（PatternBoost变换器学习和PPO强化学习）在No-Three-In-Line问题上的表现，发现ILP在19×19网格内能获得最优解，而AI方法在较小网格上表现良好但无法保证最优性。


<details>
  <summary>Details</summary>
Motivation: No-Three-In-Line是组合几何中的经典问题，传统ILP方法虽然能保证最优解但面临指数级扩展问题，而机器学习方法为模式近似提供了有前景的替代方案。论文旨在首次系统比较经典优化与AI方法在该问题上的性能。

Method: 应用三种方法：1）整数线性规划（ILP）作为经典优化基准；2）首次应用PatternBoost变换器学习；3）首次应用PPO强化学习。将这些方法与传统算法进行性能比较。

Result: ILP在19×19网格内获得可证明的最优解；PatternBoost在14×14网格内匹配最优性能，测试损失减少96%；PPO在10×10网格上获得完美解，但在11×11网格上因约束违反而失败。

Conclusion: 经典优化对于精确解仍然至关重要，而AI方法在较小实例上提供有竞争力的性能。混合方法为扩展到更大问题规模提供了最有前景的方向。

Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.

</details>


### [11] [BAID: A Benchmark for Bias Assessment of AI Detectors](https://arxiv.org/abs/2512.11505)
*Priyam Basu,Yunfeng Zhang,Vipul Raheja*

Main category: cs.AI

TL;DR: BAID是一个评估AI文本检测器偏见的综合框架，包含超过20万个样本，涵盖7个主要社会语言学类别，发现现有检测器对少数群体文本存在系统性偏见。


<details>
  <summary>Details</summary>
Motivation: AI生成的文本检测器已在教育和专业领域广泛使用，但先前研究仅发现孤立偏见案例（特别是针对英语学习者），缺乏对社会语言学因素的系统性评估。

Method: 提出BAID评估框架，引入超过20万个样本，涵盖7个主要类别：人口统计、年龄、教育年级、方言、正式程度、政治倾向和主题。为每个样本生成合成版本，通过精心设计的提示词保留原始内容同时反映特定子群体的写作风格。使用该框架评估4个开源最先进的AI文本检测器。

Result: 发现检测性能存在一致差异，特别是对来自代表性不足群体的文本召回率较低。检测器在不同社会语言学群体间表现出系统性偏见。

Conclusion: BAID提供了一个可扩展、透明的AI检测器审计方法，强调在工具部署前需要进行偏见感知评估。研究揭示了现有检测器需要改进以公平对待所有用户群体。

Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.

</details>


### [12] [AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives](https://arxiv.org/abs/2512.11544)
*Yuan Shen,Xiaojun Wu,Linghua Yu*

Main category: cs.AI

TL;DR: 研究通过模拟真实临床场景，评估主流大语言模型从嘈杂冗余的患者主诉中提取核心医疗信息的能力，发现所有模型均存在不同程度的功能缺陷，并提出"AI-MASLD"概念类比代谢功能障碍。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在真实临床环境中的实际表现，验证其是否存在类似代谢功能障碍的性能下降，为AI在医疗领域的应用提供安全警示。

Method: 采用横断面分析设计，基于标准化医疗探针，选取GPT-4o、Gemini 2.5、DeepSeek 3.1和Qwen3-Max四个主流LLMs作为研究对象，使用包含20个医疗探针的评估系统模拟真实临床沟通环境，由两位独立临床医生进行双盲逆向评分。

Result: 所有测试模型均表现出不同程度的功能缺陷，Qwen3-Max整体表现最佳，Gemini 2.5最差；极端噪音条件下多数模型出现功能崩溃；GPT-4o在深静脉血栓继发肺栓塞风险评估中做出严重误判。

Conclusion: 首次实证确认LLMs在处理临床信息时表现出类似代谢功能障碍的特征，提出"AI-MASLD"创新概念，强调当前LLMs必须在人类专家监督下作为辅助工具使用，其理论知识与实际临床应用仍存在显著差距。

Abstract: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.

</details>


### [13] [AI Benchmark Democratization and Carpentry](https://arxiv.org/abs/2512.11588)
*Gregor von Laszewski,Wesley Brewer,Jeyan Thiyagalingam,Juri Papay,Armstrong Foundjem,Piotr Luszczek,Murali Emani,Shirley V. Moore,Vijay Janapa Reddi,Matthew D. Sinclair,Sebastian Lobentanzer,Sujata Goswami,Benjamin Hawks,Marco Colombo,Nhan Tran,Christine R. Kirkpatrick,Abdulkareem Alsudais,Gregg Barrett,Tianhao Li,Kirsten Morehouse,Shivaram Venkataraman,Rutwik Jain,Kartik Mathur,Victor Lu,Tejinder Singh,Khojasteh Z. Mirza,Kongtao Chen,Sasidhar Kunapuli,Gavin Farrell,Renato Umeton,Geoffrey C. Fox*

Main category: cs.AI

TL;DR: 论文指出当前AI基准测试存在静态化、资源需求高、与现实应用脱节等问题，提出需要动态自适应基准测试框架和AI基准测试教育体系


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试无法适应AI快速发展，模型容易记忆静态测试集，导致基准测试结果与现实世界性能存在差距。同时，基准测试面临高资源需求、硬件访问限制、设计专业知识缺乏等障碍

Method: 提出动态自适应基准测试框架，结合持续演进的模型、更新数据和异构平台。倡导建立AI基准测试教育体系（AI Benchmark Carpentry），通过技术革新和系统性教育来提升基准测试设计和使用能力

Result: 识别了当前基准测试的主要障碍：高资源需求、专业硬件访问限制、设计专业知识缺乏、结果与应用领域关联不确定性。强调基准测试需要支持应用相关的比较，实现透明、可复现和可解释的评估

Conclusion: 动态包容的基准测试对于AI负责任、可复现和可访问的部署至关重要。社区努力可以为AI基准测试教育提供基础，确保评估与AI发展同步，支持基于情境的明智决策

Abstract: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.
  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.
  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.

</details>


### [14] [Causal Inference in Energy Demand Prediction](https://arxiv.org/abs/2512.11653)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 该论文提出了一种基于结构因果模型的能源需求预测方法，通过分析天气因素和日历信息的因果关系，构建贝叶斯模型实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 能源需求预测对电网运营商、工业能源消费者和服务提供商至关重要。能源需求受多种因素影响，包括天气条件和日历信息，这些因素之间存在因果依赖关系，使得简单基于相关性的学习方法难以充分解决这一复杂问题。

Method: 提出结构因果模型来解释变量间的因果关系，进行完整分析验证因果信念。然后构建贝叶斯模型，利用学到的因果洞察作为先验知识，在未见数据上进行训练和测试。

Result: 模型在测试集上获得3.84%的MAPE（平均绝对百分比误差），达到最先进性能。跨两年数据的交叉验证平均MAPE为3.88%，显示出强大的鲁棒性。因果分析揭示了能源需求对温度波动的响应具有季节依赖性敏感性，冬季能源需求方差较低。

Conclusion: 通过结构因果模型分析能源需求的影响因素，并利用这些因果洞察构建贝叶斯预测模型，能够实现高精度、鲁棒的能源需求预测，为相关决策提供可靠支持。

Abstract: Energy demand prediction is critical for grid operators, industrial energy
  consumers, and service providers. Energy demand is influenced by multiple
  factors, including weather conditions (e.g. temperature, humidity, wind
  speed, solar radiation), and calendar information (e.g. hour of day and
  month of year), which further affect daily work and life schedules. These
  factors are causally interdependent, making the problem more complex than
  simple correlation-based learning techniques satisfactorily allow for. We
  propose a structural causal model that explains the causal relationship
  between these variables. A full analysis is performed to validate our causal
  beliefs, also revealing important insights consistent with prior studies.
  For example, our causal model reveals that energy demand responds to
  temperature fluctuations with season-dependent sensitivity. Additionally, we
  find that energy demand exhibits lower variance in winter due to the
  decoupling effect between temperature changes and daily activity patterns.
  We then build a Bayesian model, which takes advantage of the causal insights
  we learned as prior knowledge. The model is trained and tested on unseen
  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on
  the test set. The model also demonstrates strong robustness, as the
  cross-validation across two years of data yields an average MAPE of 3.88 percent.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [15] [SCOUT: A Defense Against Data Poisoning Attacks in Fine-Tuned Language Models](https://arxiv.org/abs/2512.10998)
*Mohamed Afane,Abhishek Satyam,Ke Chen,Tao Li,Junaid Farooq,Juntao Chen*

Main category: cs.CR

TL;DR: 该论文提出了一种针对语言模型后门攻击的新型防御框架SCOUT，通过基于显著性的令牌分析来检测传统攻击和新型上下文感知攻击，解决了现有防御方法对语义合理触发器的检测盲点。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要针对明显的触发器（如上下文无关的触发词和安全对齐违规），但无法检测使用上下文适当、语义合理的触发器的高级后门攻击。这些攻击利用领域特定知识，在医疗健康等敏感领域构成严重安全威胁。

Method: 提出了三种新型上下文感知攻击场景（ViralApp、Fever、Referral），并开发了SCOUT防御框架。SCOUT通过构建显著性图来分析移除单个令牌对目标标签输出logits的影响，从而识别后门触发器。

Result: 在标准基准数据集（SST-2、IMDB、AG News）上评估SCOUT，结果显示它能够成功检测传统攻击（BadNet、AddSent、SynBkd、StyleBkd）和新型上下文感知攻击，同时在干净输入上保持准确性。

Conclusion: SCOUT提供了一种有效的后门攻击防御方案，能够检测传统和高级上下文感知攻击，解决了现有防御方法对语义合理触发器的检测盲点，为AI系统在敏感领域的安全部署提供了重要保障。

Abstract: Backdoor attacks create significant security threats to language models by embedding hidden triggers that manipulate model behavior during inference, presenting critical risks for AI systems deployed in healthcare and other sensitive domains. While existing defenses effectively counter obvious threats such as out-of-context trigger words and safety alignment violations, they fail against sophisticated attacks using contextually-appropriate triggers that blend seamlessly into natural language. This paper introduces three novel contextually-aware attack scenarios that exploit domain-specific knowledge and semantic plausibility: the ViralApp attack targeting social media addiction classification, the Fever attack manipulating medical diagnosis toward hypertension, and the Referral attack steering clinical recommendations. These attacks represent realistic threats where malicious actors exploit domain-specific vocabulary while maintaining semantic coherence, demonstrating how adversaries can weaponize contextual appropriateness to evade conventional detection methods. To counter both traditional and these sophisticated attacks, we present \textbf{SCOUT (Saliency-based Classification Of Untrusted Tokens)}, a novel defense framework that identifies backdoor triggers through token-level saliency analysis rather than traditional context-based detection methods. SCOUT constructs a saliency map by measuring how the removal of individual tokens affects the model's output logits for the target label, enabling detection of both conspicuous and subtle manipulation attempts. We evaluate SCOUT on established benchmark datasets (SST-2, IMDB, AG News) against conventional attacks (BadNet, AddSent, SynBkd, StyleBkd) and our novel attacks, demonstrating that SCOUT successfully detects these sophisticated threats while preserving accuracy on clean inputs.

</details>


### [16] [An LLVM-Based Optimization Pipeline for SPDZ](https://arxiv.org/abs/2512.11112)
*Tianye Dai,Hammurabi Mendes,Heuichan Lim*

Main category: cs.CR

TL;DR: 该论文提出了一个基于LLVM的优化框架，用于提升SPDZ协议在MPC中的性能，通过自动批处理、非阻塞调度和GPU加速来减少通信开销并提高并行性。


<details>
  <summary>Details</summary>
Motivation: 当前主动安全的算术MPC虽然实用，但仍受限于特定框架的编译栈、需要程序员显式表达并行性以及高通信开销。这些问题限制了MPC的性能和可用性。

Method: 设计并实现了一个基于LLVM的概念验证优化流水线：1）前端接受带有轻量隐私注释的C语言子集并转换为LLVM IR；2）重用成熟的LLVM分析和转换来自动批处理独立算术操作；3）后端对优化后的IR进行数据流和控制流分析，驱动非阻塞运行时调度器，重叠独立操作并积极重叠通信与计算；4）可选地将批处理操作映射到GPU内核。

Result: 在受控微基准测试中与MP-SPDZ对比：CPU后端在中等和重度代数工作负载下实现高达5.56倍加速，线程数扩展性良好；GPU后端随着输入规模增大扩展性更好。系统在保持低学习曲线的同时显著提升了性能。

Conclusion: 利用LLVM结合协议感知调度是提取并行性而不牺牲可用性的有效架构方向。该设计使用主流语言并隐藏优化和硬件特定机制，保持了低学习曲线。

Abstract: Actively secure arithmetic MPC is now practical for real applications, but performance and usability are still limited by framework-specific compilation stacks, the need for programmers to explicitly express parallelism, and high communication overhead. We design and implement a proof-of-concept LLVM-based optimization pipeline for the SPDZ protocol that addresses these bottlenecks. Our front end accepts a subset of C with lightweight privacy annotations and lowers it to LLVM IR, allowing us to reuse mature analyses and transformations to automatically batch independent arithmetic operations. Our back end performs data-flow and control-flow analysis on the optimized IR to drive a non-blocking runtime scheduler that overlaps independent operations and aggressively overlaps communication with computation; when enabled, it can map batched operations to GPU kernels. This design preserves a low learning curve by using a mainstream language and hiding optimization and hardware-specific mechanics from programmers. We evaluate the system on controlled microbenchmarks against MP-SPDZ, focusing on online phase performance. Our CPU back end achieves up to 5.56 times speedup under intermediate and heavy algebraic workloads, shows strong scaling with thread count, and our GPU back end scales better as the input size increases. Overall, these results indicate that leveraging LLVM with protocol-aware scheduling is an effective architectural direction for extracting parallelism without sacrificing usability.

</details>


### [17] [Visualisation for the CIS benchmark scanning results](https://arxiv.org/abs/2512.11316)
*Zhenshuo Zhao,Maria Spichkova,Duttkumari Champavat,Juilee N. Kulkarni,Sahil Singla,Muhammad A. Zulkefli,Pradhuman Khandelwal*

Main category: cs.CR

TL;DR: GraphSecure是一个用于分析和可视化AWS安全扫描结果的Web应用，支持CIS基准验证并提供统计图表和账户状态告警


<details>
  <summary>Details</summary>
Motivation: 为了解决AWS账户安全扫描结果分析复杂、可视化不足的问题，帮助用户更好地理解安全状态并采取相应措施

Method: 开发了一个Web应用程序，能够：1) 对AWS账户发起安全扫描；2) 根据CIS基准验证扫描结果；3) 通过统计图表展示结果；4) 提供账户状态告警功能

Result: 成功实现了GraphSecure系统，能够有效分析AWS安全扫描结果，提供直观的可视化展示，并帮助用户及时了解账户安全状态

Conclusion: GraphSecure为AWS安全扫描结果提供了有效的分析和可视化工具，帮助用户更好地理解和改善其云安全态势

Abstract: In this paper, we introduce GraphSecure, a web application that provides advanced analysis and visualisation of security scanning results. GraphSecure enables users to initiate scans for their AWS account, validate them against specific Center for Internet Security (CIS) Benchmarks and return results, showcase those returned results in the form of statistical charts and warn the users about their account status.

</details>


### [18] [A Scalable Multi-GPU Framework for Encrypted Large-Model Inference](https://arxiv.org/abs/2512.11269)
*Siddharth Jayashankar,Joshua Kim,Michael B. Sullivan,Wenting Zheng,Dimitrios Skarlatos*

Main category: cs.CR

TL;DR: Cerium是一个多GPU框架，用于全同态加密（FHE）的大型模型推理，通过自动生成高性能GPU内核、管理TB级内存和多GPU并行化，在GPU上实现接近ASIC的性能。


<details>
  <summary>Details</summary>
Motivation: FHE提供强隐私保证但性能缓慢，现有ASIC加速方案成本高且难以普及，GPU平台更易获取但难以达到ASIC级性能。此外，现有方法主要针对小型模型，支持LLM等大型模型面临计算复杂度剧增和TB级内存管理的挑战。

Method: Cerium集成了领域特定语言、优化编译器和运行时系统，引入新的IR构造、编译器传递、稀疏多项式表示、内存高效数据布局和通信感知并行化技术，自动生成高性能GPU内核，管理TB级内存，并在多GPU间并行化计算。

Result: 对于小型模型，Cerium比专家手写优化的GPU库快2.25倍；性能与最先进的FHE ASIC竞争，匹配之前的FHE ASIC CraterLake；首次在GPU上实现10毫秒以下的引导（7.5毫秒）；首次展示BERT-Base和Llama3-8B的加密推理，分别耗时8秒和134秒。

Conclusion: Cerium成功解决了FHE在GPU平台上部署大型模型的性能瓶颈，通过创新的编译器和运行时技术，在可访问的GPU平台上实现了接近ASIC的性能，为大型模型的加密推理提供了实用解决方案。

Abstract: Encrypted AI using fully homomorphic encryption (FHE) provides strong privacy guarantees; but its slow performance has limited practical deployment. Recent works proposed ASICs to accelerate FHE, but require expensive advanced manufacturing processes that constrain their accessibility. GPUs are a far more accessible platform, but achieving ASIC-level performance using GPUs has remained elusive. Furthermore, state-of-the-art approaches primarily focus on small models that fit comfortably within a single device. Supporting large models such as LLMs in FHE introduces a dramatic increase in computational complexity that requires optimized GPU kernels, along with managing terabyte-scale memory footprints that far exceed the capacity of a single GPU. This paper presents Cerium, a multi-GPU framework for FHE inference on large models. Cerium integrates a domain-specific language, an optimizing compiler, and a runtime system to automatically generate high-performance GPU kernels, manage terabyte-scale memory footprints, and parallelize computation across multiple GPUs. It introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization techniques that together enable encrypted inference for models ranging from small CNNs to Llama3-8B. We build Cerium on NVIDIA GPUs and demonstrate significant performance gains. For small models, Cerium outperforms expert-written hand-optimized GPU libraries by up to 2.25 times. Cerium achieves performance competitive with state-of-the-art FHE ASICs, outright matching prior FHE ASIC CraterLake. It is the first GPU system to execute bootstrapping in under 10 milliseconds, achieving 7.5 milliseconds, and is the first to demonstrate encrypted inference for BERT-Base and Llama3-8B in 8 seconds and 134 seconds, respectively.

</details>


### [19] [Proving DNSSEC Correctness: A Formal Approach to Secure Domain Name Resolution](https://arxiv.org/abs/2512.11431)
*Qifan Zhang,Zilin Shen,Imtiaz Karim,Elisa Bertino,Zhou Li*

Main category: cs.CR

TL;DR: DNSSECVerif是首个用于DNSSEC协议套件全面自动化形式化安全分析的框架，能够发现协议规范中的关键模糊性和漏洞，并通过大规模测量研究验证现实影响。


<details>
  <summary>Details</summary>
Motivation: DNSSEC对于防止DNS欺骗至关重要，但其规范存在模糊性和漏洞，传统"破坏-修复"方法无法解决，需要全面的基础安全分析。

Method: 基于SAPIC+符号验证器构建DNSSECVerif框架，建立高保真模型捕获协议级交互，包括加密操作和细粒度并发控制的状态缓存。

Result: 正式证明了DNSSEC的四个核心安全保证，发现了标准中的关键模糊性（特别是NSEC和NSEC3的不安全共存），自动重新发现了三类已知攻击，通过测试主流DNS软件和测量220多万个开放解析器验证了现实影响。

Conclusion: 该工作为强化DNSSEC规范和实现提供了基于证据的关键建议，填补了协议全面形式化安全分析的空白。

Abstract: The Domain Name System Security Extensions (DNSSEC) are critical for preventing DNS spoofing, yet its specifications contain ambiguities and vulnerabilities that elude traditional "break-and-fix" approaches. A holistic, foundational security analysis of the protocol has thus remained an open problem. This paper introduces DNSSECVerif, the first framework for comprehensive, automated formal security analysis of the DNSSEC protocol suite. Built on the SAPIC+ symbolic verifier, our high-fidelity model captures protocol-level interactions, including cryptographic operations and stateful caching with fine-grained concurrency control. Using DNSSECVerif, we formally prove four of DNSSEC's core security guarantees and uncover critical ambiguities in the standards--notably, the insecure coexistence of NSEC and NSEC3. Our model also automatically rediscovers three classes of known attacks, demonstrating fundamental weaknesses in the protocol design. To bridge the model-to-reality gap, we validate our findings through targeted testing of mainstream DNS software and a large-scale measurement study of over 2.2 million open resolvers, confirming the real-world impact of these flaws. Our work provides crucial, evidence-based recommendations for hardening DNSSEC specifications and implementations.

</details>


### [20] [Capacitive Touchscreens at Risk: Recovering Handwritten Trajectory on Smartphone via Electromagnetic Emanations](https://arxiv.org/abs/2512.11484)
*Yukun Cheng,Shiyu Zhu,Changhai Ou,Xingshuo Han,Yuan Li,Shihui Zheng*

Main category: cs.CR

TL;DR: TESLA攻击利用电容触摸屏的电磁侧信道漏洞，通过非接触方式捕获书写时的电磁信号，实时恢复出手写轨迹，在商用智能手机上达到77%的字符识别准确率。


<details>
  <summary>Details</summary>
Motivation: 揭示电容触摸屏存在严重的电磁侧信道安全漏洞，攻击者可以利用该漏洞从电磁信号中恢复出精细的连续手写轨迹，这对用户隐私构成重大威胁。

Method: 提出TESLA（触摸屏电磁侧信道泄漏攻击）框架，通过非接触方式捕获屏幕书写时产生的电磁信号，使用回归方法将这些信号实时转换为二维手写轨迹。

Result: 在多种商用智能手机上的广泛评估显示，TESLA达到77%的字符识别准确率和0.74的Jaccard指数，证明其能够恢复高度可识别且与原始手写非常相似的运动轨迹。

Conclusion: 电容触摸屏的电磁侧信道泄漏严重的安全漏洞，攻击者可以非接触地恢复用户手写内容，需要开发相应的防护措施来保护用户隐私。

Abstract: This paper reveals and exploits a critical security vulnerability: the electromagnetic (EM) side channel of capacitive touchscreens leaks sufficient information to recover fine-grained, continuous handwriting trajectories. We present Touchscreen Electromagnetic Side-channel Leakage Attack (TESLA), a non-contact attack framework that captures EM signals generated during on-screen writing and regresses them into two-dimensional (2D) handwriting trajectories in real time. Extensive evaluations across a variety of commercial off-the-shelf (COTS) smartphones show that TESLA achieves 77% character recognition accuracy and a Jaccard index of 0.74, demonstrating its capability to recover highly recognizable motion trajectories that closely resemble the original handwriting under realistic attack conditions.

</details>


### [21] [Leveraging FPGAs for Homomorphic Matrix-Vector Multiplication in Oblivious Message Retrieval](https://arxiv.org/abs/2512.11690)
*Grant Bosworth,Keewoo Lee,Sunwoong Kim*

Main category: cs.CR

TL;DR: 本文提出了一种硬件架构来加速Oblivious Message Retrieval（OMR）中的同态矩阵向量乘法算法，相比软件实现获得了13.86倍的加速比。


<details>
  <summary>Details</summary>
Motivation: 端到端加密保护消息内容但不保护元数据，OMR使用同态加密来保护元数据隐私，但其核心的同态矩阵向量乘法算法计算密集，限制了OMR的实用性。

Method: 提出硬件架构加速同态矩阵向量乘法算法，使用高层次综合实现同态算子，提供不同并行级别的设计参数，在FPGA平台上采用高效的设计空间探索策略进行部署。

Result: 相比软件实现，提出的硬件加速器实现了13.86倍的加速比。

Conclusion: 通过硬件加速同态矩阵向量乘法，显著提升了OMR系统的性能，使其更具实用性。

Abstract: While end-to-end encryption protects the content of messages, it does not secure metadata, which exposes sender and receiver information through traffic analysis. A plausible approach to protecting this metadata is to have senders post encrypted messages on a public bulletin board and receivers scan it for relevant messages. Oblivious message retrieval (OMR) leverages homomorphic encryption (HE) to improve user experience in this solution by delegating the scan to a resource-rich server while preserving privacy. A key process in OMR is the homomorphic detection of pertinent messages for the receiver from the bulletin board. It relies on a specialized matrix-vector multiplication algorithm, which involves extensive multiplications between ciphertext vectors and plaintext matrices, as well as homomorphic rotations. The computationally intensive nature of this process limits the practicality of OMR. To address this challenge, this paper proposes a hardware architecture to accelerate the matrix-vector multiplication algorithm. The building homomorphic operators in this algorithm are implemented using high-level synthesis, with design parameters for different parallelism levels. These operators are then deployed on a field-programmable gate array platform using an efficient design space exploration strategy to accelerate homomorphic matrix-vector multiplication. Compared to a software implementation, the proposed hardware accelerator achieves a 13.86x speedup.

</details>


### [22] [SoK: Demystifying the multiverse of MPC protocols](https://arxiv.org/abs/2512.11699)
*Roberta De Viti,Vaastav Anand,Pierfrancesco Ingo,Deepak Garg*

Main category: cs.CR

TL;DR: 本文系统化分析了多方计算(MPC)协议的性能，通过广泛实验研究为开发者提供实用指导，旨在缩小MPC理论与实践的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管MPC具有强大的隐私和正确性保证，但在实际应用中采用仍然有限，主要原因是恶意设置下的高成本和缺乏针对具体工作负载选择合适协议的指导。

Method: 识别影响MPC效率的理论和实际参数，对不同基准测试进行广泛的实验研究，分析协议之间的权衡，并突出显示哪些技术最适合不同的应用场景和需求。

Result: 通过实验分析揭示了MPC协议的性能特征和权衡关系，为不同应用场景提供了技术选择指导。

Conclusion: 本文通过系统化的性能分析和实用指导，旨在帮助开发者更好地选择MPC协议，并为研究人员指明开放挑战，从而缩小MPC理论与实践的差距。

Abstract: This paper systematizes knowledge on the performance of Multi-Party Computation (MPC) protocols. Despite strong privacy and correctness guarantees, MPC adoption in real-world applications remains limited by high costs (especially in the malicious setting) and lack of guidance on choosing suitable protocols for concrete workloads. We identify the theoretical and practical parameters that shape MPC efficiency and conduct an extensive experimental study across diverse benchmarks. Our analysis discusses the trade-offs between protocols, and highlights which techniques align best with different application scenarios and needs. By providing actionable guidance for developers and outlining open challenges for researchers, this work seeks to narrow the gap between MPC theory and practice.

</details>


### [23] [Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously](https://arxiv.org/abs/2512.11783)
*Andrew Adiletta,Kathryn Adiletta,Kemal Derya,Berk Sunar*

Main category: cs.CR

TL;DR: 本文提出Super Suffixes攻击方法，可绕过Llama Prompt Guard 2等防护模型，并开发DeltaGuard检测技术进行防御。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，处理不可信文本输入和执行代码生成时面临安全风险。现有防护模型存在被绕过的可能性，需要研究更强大的攻击方法和相应的防御措施。

Method: 1. 提出Super Suffixes攻击方法：通过联合优化技术生成能绕过多种对齐目标的后缀；2. 开发DeltaGuard检测技术：通过分析模型内部状态与特定概念方向余弦相似度的变化来识别攻击。

Result: 1. Super Suffixes成功绕过了Llama Prompt Guard 2对5个不同文本生成模型的保护；2. DeltaGuard将非良性分类率提升至近100%，显著提高了对Super Suffixes攻击的检测能力。

Conclusion: 本研究首次揭示了Llama Prompt Guard 2可通过联合优化被绕过，同时提出的DeltaGuard检测技术能有效防御此类攻击，为防护模型栈提供了有价值的增强方案。

Abstract: The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.
  Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.

</details>
