<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 48]
- [cs.CR](#cs.CR) [Total: 11]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bridging the Trust Gap: Clinician-Validated Hybrid Explainable AI for Maternal Health Risk Assessment in Bangladesh](https://arxiv.org/abs/2601.07866)
*Farjana Yesmin,Nusrat Shirmin,Suraiya Shabnam Bristy*

Main category: cs.AI

TL;DR: 本文提出了一种结合前向模糊逻辑和后向SHAP解释的混合可解释AI框架，用于孕产妇健康风险预测，在资源受限环境中提高临床信任度。


<details>
  <summary>Details</summary>
Motivation: 机器学习在孕产妇健康风险预测中虽有潜力，但在资源受限的临床环境中面临关键障碍：缺乏可解释性和信任度，阻碍了临床采用。

Method: 开发了模糊-XGBoost混合模型，结合前向模糊逻辑规则和后向SHAP解释，在1,014份孕产妇健康记录上进行训练，并通过14名孟加拉国医疗专业人员的系统性临床反馈进行验证。

Result: 模型达到88.67%准确率（ROC-AUC：0.9703）；临床验证显示71.4%的医疗专业人员偏好混合解释，54.8%表示对临床使用信任；SHAP分析确定医疗可及性为主要预测因子，工程化模糊风险评分排名第三。

Conclusion: 结合可解释的模糊规则与特征重要性解释能同时提升实用性和信任度，为孕产妇医疗保健中的XAI部署提供了实践见解，同时识别了产科病史、孕龄和连接障碍等关键信息缺口。

Abstract: While machine learning shows promise for maternal health risk prediction, clinical adoption in resource-constrained settings faces a critical barrier: lack of explainability and trust. This study presents a hybrid explainable AI (XAI) framework combining ante-hoc fuzzy logic with post-hoc SHAP explanations, validated through systematic clinician feedback. We developed a fuzzy-XGBoost model on 1,014 maternal health records, achieving 88.67% accuracy (ROC-AUC: 0.9703). A validation study with 14 healthcare professionals in Bangladesh revealed strong preference for hybrid explanations (71.4% across three clinical cases) with 54.8% expressing trust for clinical use. SHAP analysis identified healthcare access as the primary predictor, with the engineered fuzzy risk score ranking third, validating clinical knowledge integration (r=0.298). Clinicians valued integrated clinical parameters but identified critical gaps: obstetric history, gestational age, and connectivity barriers. This work demonstrates that combining interpretable fuzzy rules with feature importance explanations enhances both utility and trust, providing practical insights for XAI deployment in maternal healthcare.

</details>


### [2] [When Models Know When They Do Not Know: Calibration, Cascading, and Cleaning](https://arxiv.org/abs/2601.07965)
*Chenjie Hao,Weyl Lu,Yuko Ishiwaka,Zengyi Li,Weier Wan,Yubei Chen*

Main category: cs.AI

TL;DR: 提出一种无需训练的通用方法，通过模型校准、级联和数据清理，让模型更好地识别自身未知领域，提高AI效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 当模型能够识别自身未知时，会开启许多可能性。关键问题是如何让模型意识到自己不知道什么。现有研究表明校准可以提供可靠的置信度估计，但需要一种简单有效的通用方法。

Method: 提出无需训练的通用方法，适用于视觉和语言模型。基于两个关键观察：1) 单个模型内置信度越高准确率越高；2) 在验证集上校准的模型在测试集上保持校准。在此基础上开发两种应用：1) 基于校准优势路由的模型级联；2) 基于模型集成的数据清理方法。

Result: 通过校准置信度的可比性，级联大小模型可在几乎不影响准确率的情况下提高效率；级联两个规模相当的模型可获得超越任一单独模型的性能。数据清理方法在ImageNet和MMLU数据集上有效识别错误标注样本，平衡了精度和检测率。

Conclusion: 让模型识别自身未知是实现更高效、可靠、可信AI的实用步骤。校准置信度提供了可靠的路由信号，模型级联和数据清理展示了该方法在实际应用中的价值。

Abstract: When a model knows when it does not know, many possibilities emerge. The first question is how to enable a model to recognize that it does not know. A promising approach is to use confidence, computed from the model's internal signals, to reflect its ignorance. Prior work in specific domains has shown that calibration can provide reliable confidence estimates. In this work, we propose a simple, effective, and universal training-free method that applies to both vision and language models, performing model calibration, cascading, and data cleaning to better exploit a model's ability to recognize when it does not know. We first highlight two key empirical observations: higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set. These findings empirically establish the reliability and comparability of calibrated confidence. Building on this, we introduce two applications: (1) model cascading with calibrated advantage routing and (2) data cleaning based on model ensemble. Using the routing signal derived from the comparability of calibrated confidences, we cascade large and small models to improve efficiency with almost no compromise in accuracy, and we further cascade two models of comparable scale to achieve performance beyond either model alone. Leveraging multiple experts and their calibrated confidences, we design a simple yet effective data-cleaning method that balances precision and detection rate to identify mislabeled samples in ImageNet and Massive Multitask Language Understanding (MMLU) datasets. Our results demonstrate that enabling models to recognize when they do not know is a practical step toward more efficient, reliable, and trustworthy AI.

</details>


### [3] [Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety](https://arxiv.org/abs/2601.08000)
*Can Jin,Rui Wu,Tong Che,Qixin Zhang,Hongwu Peng,Jiahui Zhao,Zhenting Wang,Wenqi Wei,Ligong Han,Zhao Zhang,Yuan Cao,Ruixiang Tang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: 论文提出CADA方法，通过案例增强的审慎对齐来提升LLM安全性，避免过度拒绝良性请求，相比详细的代码式安全规则更有效。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐方法面临挑战：详细的安全规则可能导致过度拒绝良性请求，而开源LLM缺乏高级推理能力来有效处理这些规则。需要一种既能提升安全性又不损害实用性的方法。

Method: 提出CADA（案例增强的审慎对齐）方法：1）使用案例增强的简单代码而非详细规则；2）基于自生成的安全推理链进行强化学习；3）通过案例引导推理而非刚性规则遵守。

Result: CADA有效提升了无害性，增强了对抗攻击的鲁棒性，减少了过度拒绝，同时在多样化基准测试中保持了实用性，相比仅基于规则的方法表现更优。

Conclusion: 案例增强的审慎对齐为LLM安全对齐提供了实用替代方案，能够在提升安全性的同时保持模型的有用性，避免过度拒绝良性请求的问题。

Abstract: Ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests remains a significant challenge. While OpenAI introduces deliberative alignment (DA) to enhance the safety of its o-series models through reasoning over detailed ``code-like'' safety rules, the effectiveness of this approach in open-source LLMs, which typically lack advanced reasoning capabilities, is understudied. In this work, we systematically evaluate the impact of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases. We find that referencing explicit codes inconsistently improves harmlessness and systematically degrades helpfulness, whereas training on case-augmented simple codes yields more robust and generalized safety behaviors. By guiding LLMs with case-augmented reasoning instead of extensive code-like safety rules, we avoid rigid adherence to narrowly enumerated rules and enable broader adaptability. Building on these insights, we propose CADA, a case-augmented deliberative alignment method for LLMs utilizing reinforcement learning on self-generated safety reasoning chains. CADA effectively enhances harmlessness, improves robustness against attacks, and reduces over-refusal while preserving utility across diverse benchmarks, offering a practical alternative to rule-only DA for improving safety while maintaining helpfulness.

</details>


### [4] [Internal Deployment Gaps in AI Regulation](https://arxiv.org/abs/2601.08005)
*Joe Kwon,Stephen Casper*

Main category: cs.AI

TL;DR: 该论文分析了美国和欧盟2025年前沿AI法规在内部部署系统方面的监管漏洞，识别了三个可能导致内部部署系统逃避监管的缺口，并提出了解决方案。


<details>
  <summary>Details</summary>
Motivation: 前沿AI监管主要关注面向外部用户的系统部署，但企业内部部署的高风险AI系统（如研发自动化、关键业务流程加速、敏感数据处理）同样重要。当前法规在内部部署方面存在监管盲区，需要系统分析这些漏洞及其原因。

Method: 通过分析美国和欧盟2025年前沿AI法规，识别内部部署系统的监管缺口。从范围模糊性、合规评估的时间点限制、信息不对称三个维度分析漏洞，并探讨这些漏洞持续存在的原因（可测量性、激励机制、信息获取的张力）。最后提出可能的解决方案及其权衡。

Result: 识别出三个关键监管缺口：1) 范围模糊性允许内部系统逃避监管义务；2) 基于时间点的合规评估无法捕捉内部系统的持续演化；3) 信息不对称削弱监管意识和监督能力。这些漏洞源于可测量性、激励机制和信息获取之间的张力。

Conclusion: 前沿AI法规在内部部署系统方面存在系统性漏洞，可能导致高风险应用逃避监管。需要有针对性的政策选择来解决这些漏洞，而不是偶然性地处理。理解这些模式有助于政策制定者更审慎地设计针对内部部署AI系统的监管框架。

Abstract: Frontier AI regulations primarily focus on systems deployed to external users, where deployment is more visible and subject to outside scrutiny. However, high-stakes applications can occur internally when companies deploy highly capable systems within their own organizations, such as for automating R\&D, accelerating critical business processes, and handling sensitive proprietary data. This paper examines how frontier AI regulations in the United States and European Union in 2025 handle internal deployment. We identify three gaps that could cause internally-deployed systems to evade intended oversight: (1) scope ambiguity that allows internal systems to evade regulatory obligations, (2) point-in-time compliance assessments that fail to capture the continuous evolution of internal systems, and (3) information asymmetries that subvert regulatory awareness and oversight. We then analyze why these gaps persist, examining tensions around measurability, incentives, and information access. Finally, we map potential approaches to address them and their associated tradeoffs. By understanding these patterns, we hope that policy choices around internally deployed AI systems can be made deliberately rather than incidentally.

</details>


### [5] [Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms](https://arxiv.org/abs/2601.08049)
*Keith Ainebyona,Ann Move Oguti,Joseph Walusimbi,Ritah Kobusingye*

Main category: cs.AI

TL;DR: SCASED系统结合物联网技术，在智能教室中同时实现自动考勤和面部情绪识别，以监测学生课堂参与度，帮助教师实时调整教学策略。


<details>
  <summary>Details</summary>
Motivation: 当前高等教育中的智能教室技术主要关注自动化考勤，而忽视了学生在课堂中的情感和认知参与度，这限制了教师识别学生分心情况并实时调整教学策略的能力。

Method: 开发了基于物联网的SCASED系统，使用树莓派摄像头和OpenCV进行人脸检测，采用微调的MobileNetV2模型分类四种学习相关情绪状态（投入、无聊、困惑、沮丧），并实现基于会话的考勤管理机制。

Result: 在DAiSEE数据集上的实验评估显示，情绪分类准确率达到89.5%。系统能够将考勤数据与情绪分析相结合，为教师提供课堂动态的额外洞察。

Conclusion: 整合考勤数据与情绪分析可以为教师提供更全面的课堂动态信息，支持更具响应性的教学实践，提升智能教室技术的教育价值。

Abstract: The increasing adoption of smart classroom technologies in higher education has mainly focused on automating attendance, with limited attention given to students' emotional and cognitive engagement during lectures. This limits instructors' ability to identify disengagement and adapt teaching strategies in real time. This paper presents SCASED (Smart Classroom Attendance System with Emotion Detection), an IoT-based system that integrates automated attendance tracking with facial emotion recognition to support classroom engagement monitoring. The system uses a Raspberry Pi camera and OpenCV for face detection, and a finetuned MobileNetV2 model to classify four learning-related emotional states: engagement, boredom, confusion, and frustration. A session-based mechanism is implemented to manage attendance and emotion monitoring by recording attendance once per session and performing continuous emotion analysis thereafter. Attendance and emotion data are visualized through a cloud-based dashboard to provide instructors with insights into classroom dynamics. Experimental evaluation using the DAiSEE dataset achieved an emotion classification accuracy of 89.5%. The results show that integrating attendance data with emotion analytics can provide instructors with additional insight into classroom dynamics and support more responsive teaching practices.

</details>


### [6] [Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms](https://arxiv.org/abs/2601.08052)
*Nawazish Alia,Rachael Shawb,Karl Mason*

Main category: cs.AI

TL;DR: 提出一种基于深度强化学习的乳牛场能源管理框架，通过改进PPO算法实现电池储能和热水系统的智能调度，在动态电价和可再生能源波动环境下降低运营成本。


<details>
  <summary>Details</summary>
Motivation: 乳牛场是能源密集型行业，严重依赖电网供电。随着可再生能源比例增加，实时供需平衡成为挑战。现有强化学习方法通常假设完全知晓未来电价或发电量，这在动态环境中不现实，且标准PPO算法的固定阈值在可变电价下训练不稳定。

Method: 提出两种PPO变体：1) Forecast Aware PPO：结合短期需求与可再生能源预测，使用基于小时和月份的残差校准；2) PID KL PPO：采用比例-积分-微分控制器自适应调节KL散度，实现稳定的策略更新。框架针对电池储能和热水系统进行负载调度。

Result: 在真实乳牛场数据上训练，相比PPO降低1%电力成本，相比DQN降低4.8%，相比SAC降低1.5%。电池调度方面，PPO减少电网输入13.1%，证明该方法在现代乳牛场可持续能源管理中具有可扩展性和有效性。

Conclusion: 提出的深度强化学习框架能有效管理乳牛场能源系统，通过智能负载调度降低运营成本，减少电网依赖，支持联合国可持续发展目标7（经济适用的清洁能源），为农业能源管理提供实用解决方案。

Abstract: Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.

</details>


### [7] [MemoBrain: Executive Memory as an Agentic Brain for Reasoning](https://arxiv.org/abs/2601.08079)
*Hongjin Qian,Zhao Cao,Zheng Liu*

Main category: cs.AI

TL;DR: MemoBrain是一个面向工具增强型智能体的执行记忆模型，通过构建依赖感知的记忆系统来管理长时推理过程中的中间状态和逻辑关系，解决上下文累积问题。


<details>
  <summary>Details</summary>
Motivation: 工具增强型智能体框架中的复杂推理本质上是长时程的，导致推理轨迹和临时工具产物不断积累，超出大型语言模型的有限工作上下文容量。缺乏显式记忆机制会破坏逻辑连续性并削弱任务对齐性，因此记忆不是辅助效率问题，而是维持长时程连贯、目标导向推理的核心组件。

Method: 提出MemoBrain执行记忆模型，作为推理智能体的协同伙伴运行。它构建依赖感知的记忆系统，捕获关键的中间状态及其逻辑关系，在不阻塞执行的情况下组织推理进度，并主动管理工作上下文。具体机制包括：修剪无效步骤、折叠已完成的子轨迹、在固定上下文预算下保持紧凑的高显著性推理主干。

Result: 在具有挑战性的长时程基准测试（包括GAIA、WebWalker和BrowseComp-Plus）上评估MemoBrain，相比强基线模型展现出持续的性能改进。

Conclusion: MemoBrain通过显式认知控制机制管理推理轨迹，而非被动积累上下文，为工具增强型智能体提供了维持长时程连贯推理的关键记忆能力。

Abstract: Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.
  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.
  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.

</details>


### [8] [Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement](https://arxiv.org/abs/2601.08545)
*Zhenlong Dai,Zhuoluo Zhao,Hengning Wang,Xiu Tang,Sai Wu,Chang Yao,Zhipeng Gao,Jingyuan Chen*

Main category: cs.AI

TL;DR: 论文提出LPR任务和LSG框架，通过两阶段检索增强方法修复学习者代码错误并提供解释，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能编程辅导系统主要关注修复错误代码，但缺乏提供错误根本原因的分析，无法满足学习者深入理解的需求。

Method: 提出LSG框架：第一阶段使用修复方案检索框架构建数据库，采用编辑驱动代码检索获取有价值解决方案；第二阶段提出解决方案引导的程序修复方法，在检索方案指导下修复代码并提供解释；还提出迭代检索增强方法，利用生成代码评估结果优化检索方向。

Result: 实验结果表明，该方法在LPR任务上显著优于一系列基线方法，验证了框架的有效性。

Conclusion: 提出的LPR任务和LSG框架能够有效修复学习者代码错误并提供解释，为智能编程辅导系统提供了新的解决方案。

Abstract: With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \textbf{LPR} (\textbf{L}earner-Tailored \textbf{P}rogram \textbf{R}epair). We then propose a novel and effective framework, \textbf{\textsc{\MethodName{}}} (\textbf{L}earner-Tailored \textbf{S}olution \textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.

</details>


### [9] [How vehicles change lanes after encountering crashes: Empirical analysis and modeling](https://arxiv.org/abs/2601.08125)
*Kequan Chen,Yuxuan Wang,Pan Liu,Victor L. Knoop,David Z. W. Wang,Yu Han*

Main category: cs.AI

TL;DR: 研究分析了事故后车道变换行为的特征，发现其持续时间更长、插入速度更低、碰撞风险更高，并开发了基于图注意力模块的轨迹预测框架，性能优于现有基线10%以上。


<details>
  <summary>Details</summary>
Motivation: 事故发生后，后续车辆需要变换车道绕过障碍物，但目标车道车辆可能拒绝让行，增加了事故后车道变换的复杂性和碰撞风险，而目前对此类行为的特征和运动模式尚不清楚。

Method: 通过无人机视频提取事故后车辆轨迹构建数据集，开发了基于图注意力模块的轨迹预测框架，该模块将让行行为建模为辅助交互感知任务，指导条件变分自编码器和基于Transformer的解码器预测车道变换者轨迹。

Result: 实证分析显示，相比强制性和自由车道变换，事故后车道变换持续时间更长、插入速度更低、碰撞风险更高；79.4%的事故后车道变换涉及新跟随者至少一次不让行行为；提出的模型在不同预测时间范围内，平均位移误差和最终位移误差均优于现有基线10%以上，并能提供更可靠的碰撞风险分析。

Conclusion: 事故后车道变换具有独特的风险特征，提出的交互感知轨迹预测框架能有效预测此类行为并降低碰撞风险误判，模型在不同地点的数据集上表现出良好的可迁移性。

Abstract: When a traffic crash occurs, following vehicles need to change lanes to bypass the obstruction. We define these maneuvers as post crash lane changes. In such scenarios, vehicles in the target lane may refuse to yield even after the lane change has already begun, increasing the complexity and crash risk of post crash LCs. However, the behavioral characteristics and motion patterns of post crash LCs remain unknown. To address this gap, we construct a post crash LC dataset by extracting vehicle trajectories from drone videos captured after crashes. Our empirical analysis reveals that, compared to mandatory LCs (MLCs) and discretionary LCs (DLCs), post crash LCs exhibit longer durations, lower insertion speeds, and higher crash risks. Notably, 79.4% of post crash LCs involve at least one instance of non yielding behavior from the new follower, compared to 21.7% for DLCs and 28.6% for MLCs. Building on these findings, we develop a novel trajectory prediction framework for post crash LCs. At its core is a graph based attention module that explicitly models yielding behavior as an auxiliary interaction aware task. This module is designed to guide both a conditional variational autoencoder and a Transformer based decoder to predict the lane changer's trajectory. By incorporating the interaction aware module, our model outperforms existing baselines in trajectory prediction performance by more than 10% in both average displacement error and final displacement error across different prediction horizons. Moreover, our model provides more reliable crash risk analysis by reducing false crash rates and improving conflict prediction accuracy. Finally, we validate the model's transferability using additional post crash LC datasets collected from different sites.

</details>


### [10] [Embedded AI Companion System on Edge Devices](https://arxiv.org/abs/2601.08128)
*Rahul Gupta,Stephen D. H. Hsu*

Main category: cs.AI

TL;DR: 提出一种在边缘设备上运行的AI伴侣系统，通过活跃/非活跃阶段交替的内存范式，在资源受限环境下实现低延迟对话和长期个性化记忆


<details>
  <summary>Details</summary>
Motivation: 边缘设备计算资源有限，现有AI伴侣和记忆系统无法直接使用，需要解决计算资源和延迟问题

Method: 采用交替活跃/非活跃阶段的内存范式：用户活跃时进行低延迟实时对话和轻量检索；用户不活跃时进行计算密集的记忆提取、整合和维护

Result: 使用弱模型（Qwen2.5-7B-Instruct int4量化）的系统在大多数指标上优于无记忆的同等LLM，性能与GPT-3.5（16k上下文窗口）相当

Conclusion: 提出的内存范式能在嵌入式硬件严格约束下最小化延迟并保持长期个性化，同时设计了AI伴侣基准来全面评估对话质量和记忆能力

Abstract: Computational resource constraints on edge devices make it difficult to develop a fully embedded AI companion system with a satisfactory user experience. AI companion and memory systems detailed in existing literature cannot be directly used in such an environment due to lack of compute resources and latency concerns. In this paper, we propose a memory paradigm that alternates between active and inactive phases: during phases of user activity, the system performs low-latency, real-time dialog using lightweight retrieval over existing memories and context; whereas during phases of user inactivity, it conducts more computationally intensive extraction, consolidation, and maintenance of memories across full conversation sessions. This design minimizes latency while maintaining long-term personalization under the tight constraints of embedded hardware. We also introduce an AI Companion benchmark designed to holistically evaluate the AI Companion across both its conversational quality and memory capabilities. In our experiments, we found that our system (using a very weak model: Qwen2.5-7B-Instruct quantized int4) outperforms the equivalent raw LLM without memory across most metrics, and performs comparably to GPT-3.5 with 16k context window.

</details>


### [11] [Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions](https://arxiv.org/abs/2601.08156)
*Arin Gopalan Yadav,Varad Dherange,Kumar Shivam*

Main category: cs.AI

TL;DR: Project Synapse是一个用于自主解决最后一公里配送中断的新型智能体框架，采用分层多智能体架构，通过LangGraph管理复杂工作流，并在真实场景数据集上验证性能。


<details>
  <summary>Details</summary>
Motivation: 最后一公里配送中断是物流行业的重要挑战，传统解决方案往往缺乏自主性和适应性。该研究旨在开发一个能够自主解决复杂配送中断问题的智能体框架。

Method: 采用分层多智能体架构：中央Resolution Supervisor负责战略任务分解，将子任务委派给专门的worker智能体执行战术操作。使用LangGraph进行复杂循环工作流编排。通过分析6000多条真实用户评论构建包含30个复杂中断场景的基准数据集，采用LLM-as-a-Judge协议进行系统性能评估，并包含明确的偏见缓解机制。

Result: 开发了Project Synapse框架并构建了包含30个复杂中断场景的基准数据集，为最后一公里配送中断的自主解决提供了系统化的评估方法。

Conclusion: Project Synapse是一个有效的自主解决最后一公里配送中断的智能体框架，其分层架构和LangGraph编排能够处理复杂中断场景，为物流行业的自动化问题解决提供了新思路。

Abstract: This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery disruptions. Synapse employs a hierarchical multi-agent architecture in which a central Resolution Supervisor agent performs strategic task decomposition and delegates subtasks to specialized worker agents responsible for tactical execution. The system is orchestrated using LangGraph to manage complex and cyclical workflows. To validate the framework, a benchmark dataset of 30 complex disruption scenarios was curated from a qualitative analysis of over 6,000 real-world user reviews. System performance is evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.

</details>


### [12] [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)
*Mohammad Pivezhandi,Mahdi Banisharif,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.AI

TL;DR: 提出基于模型的分层多智能体强化学习框架，用于多核平台的温度和能量感知调度，结合LLM语义特征提取实现零样本部署，显著提升能效和调度性能。


<details>
  <summary>Details</summary>
Motivation: 现有DVFS和任务分配方法存在局限性：基于利用率的启发式方法忽略停顿时间，或需要大量离线分析生成表格而无法运行时适应。需要一种能实时适应、准确预测热动态和性能状态的调度方案。

Method: 1) 分层多智能体强化学习框架分解指数级动作空间；2) 准确环境模型使用回归技术预测热动态和性能状态；3) LLM语义特征提取从OpenMP程序中提取13个代码级特征；4) Dyna-Q启发框架结合直接强化学习和基于模型的规划；5) 零样本部署通过生成合成训练数据，无需特定工作负载分析样本。

Result: 1) 后续决策延迟358ms，首次决策3.5-8.0秒；2) 比Linux ondemand governor提升7.09倍能效和4.0倍makespan；3) 首次决策延迟比基于表格的分析快8,300倍；4) 比无模型方法收敛速度快20倍；5) 在NVIDIA Jetson TX2/Orin NX、RubikPi、Intel Core i7平台上验证。

Conclusion: 提出的框架通过模型驱动的MARL和LLM特征提取，实现了高效的热管理和能量感知调度，支持零样本部署到新工作负载，为动态嵌入式系统提供了实用的解决方案。

Abstract: Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.

</details>


### [13] [Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression](https://arxiv.org/abs/2601.08187)
*Zijun Di,Bin Lu,Huquan Kang,Luoyi Fu,Jiaxin Ding,Xiaoying Gan,Lei Zhou,Xinbing Wang,Chenghu Zhou*

Main category: cs.AI

TL;DR: HS2C框架利用图同质性进行结构和语义压缩，通过全局层次分区识别同质社区并压缩冗余背景信息，显著提升LLM在图理解任务中的推理性能和压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于上下文窗口，通常采用随机采样丢弃节点/边，这会引入噪声并导致推理不稳定。作者认为图本身包含丰富的结构和语义信息，有效利用这些信息可以释放LLM在图理解任务中的潜力。

Method: 提出HS2C框架：1）结构上基于结构熵最小化原则进行全局层次分区，识别自然凝聚的同质社区，去除随机连接噪声；2）语义上将检测到的结构同质性传递给LLM，使其能够基于预定义社区类型进行差异化的语义聚合，将冗余背景上下文压缩为简洁的社区级共识。

Result: 在10个节点级基准测试和不同规模、家族的LLM上验证了HS2C的有效性。通过向LLM输入经过结构和语义压缩的内容，HS2C同时提高了压缩率和下游推理准确率。在7个不同的图级基准测试上的扩展进一步巩固了其任务泛化能力。

Conclusion: HS2C框架通过利用图同质性进行结构和语义压缩，显著提升了LLM在图理解任务中的性能，证明了其优越性和可扩展性，为图属性文本理解提供了有效的解决方案。

Abstract: Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its neighborhood context into LLMs. However, constrained by the context window, existing methods mainly resort to random sampling, often implemented via dropping node/edge randomly, which inevitably introduces noise and cause reasoning instability. We argue that graphs inherently contain rich structural and semantic information, and that their effective exploitation can unlock potential gains in LLMs reasoning performance. To this end, we propose Homophily-aware Structural and Semantic Compression for LLMs (HS2C), a framework centered on exploiting graph homophily. Structurally, guided by the principle of Structural Entropy minimization, we perform a global hierarchical partition that decodes the graph's essential topology. This partition identifies naturally cohesive, homophilic communities, while discarding stochastic connectivity noise. Semantically, we deliver the detected structural homophily to the LLM, empowering it to perform differentiated semantic aggregation based on predefined community type. This process compresses redundant background contexts into concise community-level consensus, selectively preserving semantically homophilic information aligned with the target nodes. Extensive experiments on 10 node-level benchmarks across LLMs of varying sizes and families demonstrate that, by feeding LLMs with structurally and semantically compressed inputs, HS2C simultaneously enhances the compression rate and downstream inference accuracy, validating its superiority and scalability. Extensions to 7 diverse graph-level benchmarks further consolidate HS2C's task generalizability.

</details>


### [14] [Adapting Rules of Official International Mahjong for Online Players](https://arxiv.org/abs/2601.08211)
*Chucai Wang,Lingfeng Li,Yunlong Lu,Wenxin Li*

Main category: cs.AI

TL;DR: 该研究使用世界冠军AI进行自博弈分析，发现国际麻将在线单局游戏存在先手优势和子目标计分问题，提出了补偿分机制和计分规则调整方案，以提升在线游戏的公平性。


<details>
  <summary>Details</summary>
Motivation: 国际麻将作为全球流行的传统游戏，正从线下面对面游戏转向在线远程游戏。然而，在线玩家存在游戏时间碎片化、对手组合不固定的特点，与线下多轮固定对手的游戏模式不同，因此需要修改规则以确保在线单局游戏的公平性。

Method: 使用世界冠军AI进行自博弈竞赛，通过统计分析揭示游戏平衡性问题。具体包括分析先手优势和子目标计分设置的问题。

Result: 研究发现国际麻将存在明显的先手优势，同时子目标计分设置存在问题。相比传统通过多轮轮换位置来平衡先手优势的方法，研究者提出了每局补偿分机制，并针对不同牌型优化了子目标计分。

Conclusion: 该研究首次尝试利用AI系统数据评估国际麻将的游戏平衡性，提出了适应在线环境的规则调整方案，并实现了修订版的在线麻将游戏。这是将传统游戏通过数据驱动方法优化以适应在线环境的重要探索。

Abstract: As one of the worldwide spread traditional game, Official International Mahjong can be played and promoted online through remote devices instead of requiring face-to-face interaction. However, online players have fragmented playtime and unfixed combination of opponents in contrary to offline players who have fixed opponents for multiple rounds of play. Therefore, the rules designed for offline players need to be modified to ensure the fairness of online single-round play. Specifically, We employ a world champion AI to engage in self-play competitions and conduct statistical data analysis. Our study reveals the first-mover advantage and issues in the subgoal scoring settings. Based on our findings, we propose rule adaptations to make the game more suitable for the online environment, such as introducing compensatory points for the first-mover advantage and refining the scores of subgoals for different tile patterns. Compared with the traditional method of rotating positions over multiple rounds to balance first-mover advantage, our compensatory points mechanism in each round is more convenient for online players. Furthermore, we implement the revised Mahjong game online, which is open for online players. This work is an initial attempt to use data from AI systems to evaluate Official Internatinoal Mahjong's game balance and develop a revised version of the traditional game better adapted for online players.

</details>


### [15] [An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3](https://arxiv.org/abs/2601.08224)
*Daesuk Kwon,Won-gi Paeng*

Main category: cs.AI

TL;DR: SANC(E3)是一个理论框架，提出表征单元不是先验给定的，而是在有限激活容量下通过竞争选择、重建和压缩的稳定结果，由E3能量函数最小化驱动。


<details>
  <summary>Details</summary>
Motivation: 现有智能系统预设固定的原始单元（如token、像素等），绕过了表征单元如何自发涌现和稳定这一根本问题。需要建立理论框架解释表征单元如何从经验中自组织形成。

Method: 提出SANC(E3)公理化框架，包含五个核心公理：有限容量、共现关联、相似性竞争、置信度稳定、重建-压缩-更新权衡。引入伪内存映射I/O机制，使内部回放的格式塔与外部感官输入通过相同公理路径处理。

Result: 从公理推导出12个命题，表明类别形成、层次组织、无监督学习和高级认知活动都可以理解为E3最小化下的格式塔完成过程。统一了感知、想象、预测、规划和行动。

Conclusion: SANC(E3)为理解智能如何从有限资源经验中自组织表征提供了理论基础，将表征单元视为动态涌现而非静态给定的实体，为统一认知过程提供了能量最小化框架。

Abstract: General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.

</details>


### [16] [MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents](https://arxiv.org/abs/2601.08235)
*Shouju Wang,Haopeng Zhang*

Main category: cs.AI

TL;DR: MPCI-Bench是首个多模态成对情境完整性基准，用于评估智能体环境中的隐私行为，通过三层结构（种子判断、故事推理、智能体行动轨迹）和三元迭代精炼流程确保数据质量，揭示了当前多模态模型在隐私与效用平衡方面的系统性失败。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型智能体从被动聊天机器人发展为处理个人数据的主动助手，评估其遵守社会规范变得日益重要。现有情境完整性基准主要是文本中心且侧重于负面拒绝场景，忽视了多模态隐私风险和隐私与效用的基本权衡。

Method: 提出了MPCI-Bench基准，包含从相同视觉源衍生的成对正负实例，分为三个层级：规范性种子判断、上下文丰富的故事推理、可执行的智能体行动轨迹。采用三元迭代精炼流程确保数据质量。

Result: 评估最先进的多模态模型显示，它们在平衡隐私与效用方面存在系统性失败，并表现出明显的模态泄漏差距——敏感视觉信息比文本信息泄漏更频繁。

Conclusion: MPCI-Bench是首个多模态成对情境完整性基准，将开源以促进未来智能体情境完整性的研究，揭示了当前模型在隐私保护方面的关键缺陷。

Abstract: As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.

</details>


### [17] [The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination](https://arxiv.org/abs/2601.08237)
*Haoran Su,Yandong Sun,Congjia Yu*

Main category: cs.AI

TL;DR: 论文提出利用大语言模型从手工设计数值奖励转向基于语言的目标规范，以解决多智能体强化学习中的奖励工程挑战。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中，手工设计奖励函数面临信用分配模糊、环境非平稳性、交互复杂性组合增长等挑战，需要新的奖励工程方法。

Method: 利用大语言模型从自然语言描述直接合成奖励函数（如EUREKA），在线自适应调整奖励公式（如CARD），并采用可验证奖励的强化学习范式。

Result: 语言中介的监督可以作为传统奖励工程的可行替代方案，实现语义奖励规范、动态奖励适应和更好的人类意图对齐。

Conclusion: 未来研究方向是从共享语义表征而非显式设计的数值信号中产生协调，但仍需解决计算开销、幻觉鲁棒性和大规模系统可扩展性等挑战。

Abstract: Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.

</details>


### [18] [T3: Benchmarking Sycophancy and Skepticism in Causal Judgment](https://arxiv.org/abs/2601.08258)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: T3是一个诊断基准，用于评估LLM在Pearl因果阶梯上的因果判断能力，包含454个专家策划的情景，重点分析失败模式，发现前沿模型存在"怀疑陷阱"和"缩放悖论"两种病理现象。


<details>
  <summary>Details</summary>
Motivation: 需要严格评估大型语言模型在因果推理方面的能力，特别是在Pearl因果阶梯的不同层次上，以诊断模型在因果判断中的系统性缺陷和病理现象。

Method: 开发了T3基准测试，包含454个专家策划的情景，将性能分解为效用（敏感性）、安全性（特异性）和明智拒绝三个维度，应用于前沿模型进行分析。

Result: 发现了两种病理现象：在L1层的"怀疑陷阱"（安全调整模型拒绝60%的有效链接）和在L3层的非单调缩放悖论（GPT-5.2在模糊反事实上比GPT-4-Turbo低55分，表现为过度犹豫而非幻觉）。

Conclusion: T3基准能够有效诊断LLM的因果判断缺陷，并验证了过程验证协议（RCA）可以恢复结构化验证下的决定性因果判断能力。

Abstract: We introduce T3 (Testing Trustworthy Thinking), a diagnostic benchmark designed to rigorously evaluate LLM causal judgment across Pearl's Ladder of Causality. Comprising 454 expert-curated vignettes, T3 prioritizes high-resolution failure analysis, decomposing performance into Utility (sensitivity), Safety (specificity), and Wise Refusal on underdetermined cases. By applying T3 to frontier models, we diagnose two distinct pathologies: a "Skepticism Trap" at L1 (where safety-tuned models like Claude Haiku reject 60% of valid links) and a non-monotonic Scaling Paradox at L3. In the latter, the larger GPT-5.2 underperforms GPT-4-Turbo by 55 points on ambiguous counterfactuals, driven by a collapse into paralysis (excessive hedging) rather than hallucination. Finally, we use the benchmark to validate a process-verified protocol (RCA), showing that T3 successfully captures the restoration of decisive causal judgment under structured verification.

</details>


### [19] [Greedy Is Enough: Sparse Action Discovery in Agentic LLMs](https://arxiv.org/abs/2601.08280)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 该论文研究大型动作空间中的智能体系统，提出基于结构化稀疏假设的动作发现算法，证明在稀疏性和充分覆盖条件下，可通过多项式样本复杂度高效识别相关动作集。


<details>
  <summary>Details</summary>
Motivation: 现代智能体系统（如工具增强语言模型）面临极大动作空间（数千个API），但经验表明只有少数动作对性能有实质影响。这启发了对稀疏动作发现的理论研究。

Method: 采用上下文线性奖励模型，假设动作相关性服从结构化稀疏性：只有少量动作在潜在状态中具有非零效应。将动作发现建模为块稀疏恢复问题，分析受正交匹配追踪启发的贪婪算法。

Result: 在不相干性、信号强度和动作覆盖的标准假设下，证明贪婪算法能以高概率精确恢复相关动作集，样本复杂度与稀疏度和潜在维度呈多项式关系，与总动作数仅呈对数关系。同时提供参数估计误差保证。

Conclusion: 稀疏动作发现是大型动作决策的基本原理，为智能体系统中的动作剪枝提供了理论基础。信息论下界表明稀疏性和充分覆盖是问题可处理性的必要条件。

Abstract: Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. Motivated by this observation, we study a contextual linear reward model in which action relevance is governed by a structured sparsity assumption: only a small number of actions have nonzero effects across latent states.
  We formulate action discovery as a block-sparse recovery problem and analyze a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions on incoherence, signal strength, and action coverage, we prove that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. We further provide estimation error guarantees for refitted parameters and show that the resulting decision rule is near-optimal for new latent states.
  Complementing these results, we establish information-theoretic lower bounds demonstrating that sparsity and sufficient coverage are necessary for tractability. Together, our results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.

</details>


### [20] [OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System](https://arxiv.org/abs/2601.08288)
*Yuyang Wu,Hanzhong Cao,Jianhao Chen,Yufei Li*

Main category: cs.AI

TL;DR: OpenMic：基于AutoGen的多智能体系统，将用户提供的生活话题转化为3-5分钟的中文单口喜剧表演并生成叙事喜剧视频


<details>
  <summary>Details</summary>
Motivation: 中文单口喜剧生成需要文化背景的幽默、精准的节奏把控、舞台表演提示和隐式的多步推理，而现有的中文幽默数据集更适合幽默理解和评估而非长篇单口喜剧生成，导致直接监督与目标任务不匹配

Method: 1. 基于AutoGen构建端到端多智能体系统；2. 采用多轮迭代循环规划，协调多个专业智能体共同优化幽默、节奏和可表演性；3. 使用检索增强生成（RAG）进行素材基础和创意扩展；4. 微调专门的JokeWriter模型以更好地内化单口喜剧特有的铺垫-笑点结构和长程回调

Result: 开发了OpenMic系统，能够将用户提供的生活话题转化为完整的3-5分钟中文单口喜剧表演，并进一步生成叙事喜剧视频

Conclusion: 通过多智能体协作、RAG增强和专门模型微调，OpenMic有效解决了中文单口喜剧生成中的文化背景、节奏把控和数据集不匹配等挑战，实现了从话题到完整表演的端到端生成

Abstract: Chinese stand-up comedy generation goes beyond plain text generation, requiring culturally grounded humor, precise timing, stage-performance cues, and implicit multi-step reasoning. Moreover, commonly used Chinese humor datasets are often better suited for humor understanding and evaluation than for long-form stand-up generation, making direct supervision misaligned with the target task. To address these challenges, we present OpenMic, an end-to-end multi-agent system built on AutoGen that transforms a user-provided life topic into a 3-5 minute Chinese stand-up performance and further produces a narrated comedy video. OpenMic orchestrates multiple specialized agents in a multi-round iterative loop-planning to jointly optimize humor, timing, and performability. To mitigate the dataset-task mismatch, we augment generation with retrieval-augmented generation (RAG) for material grounding and idea expansion, and we fine-tune a dedicated JokeWriter to better internalize stand-up-specific setup-punchline structures and long-range callbacks.

</details>


### [21] [AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation](https://arxiv.org/abs/2601.08323)
*Yupeng Huo,Yaxi Lu,Zhong Zhang,Haotian Chen,Yankai Lin*

Main category: cs.AI

TL;DR: AtomMem将记忆管理重构为动态决策问题，通过原子CRUD操作和强化学习实现任务对齐的自适应记忆策略，在长上下文任务中优于静态记忆方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆机制大多依赖静态、手工设计的工作流程，这限制了记忆设计的性能和泛化能力，需要更灵活、基于学习的记忆框架。

Method: 将高级记忆过程解构为基本的原子CRUD（创建、读取、更新、删除）操作，将记忆工作流转化为可学习的决策过程，结合监督微调和强化学习训练自主的任务对齐策略。

Result: 在3个长上下文基准测试中，训练后的AtomMem-8B模型持续优于先前的静态工作流记忆方法。训练动态分析显示该方法能让智能体发现结构化、任务对齐的记忆管理策略。

Conclusion: 基于学习的记忆管理框架相比预定义流程具有关键优势，能够实现自适应、任务特定的记忆行为编排，为长视野问题的解决提供了更灵活有效的记忆机制。

Abstract: Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.

</details>


### [22] [Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant](https://arxiv.org/abs/2601.08333)
*Oleg Romanchuk,Roman Bondar*

Main category: cs.AI

TL;DR: 论文提出"语义洗钱"概念，指LLM智能体架构将信息传输机制与认知辩护机制混淆，导致缺乏充分依据的命题通过可信接口被系统接受，构成架构层面的盖蒂尔问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体架构存在系统性缺陷，将信息传输机制与认知辩护机制混为一谈，导致命题在缺乏充分依据的情况下被系统接受，形成架构层面的盖蒂尔问题。

Method: 通过形式化分析提出"语义洗钱"概念，证明"不可避免的自许可定理"，引入"依据侵蚀原则"作为根本解释，并分析扩展、模型改进和LLM作为评判者方案的结构性局限。

Result: 证明在标准架构假设下，循环认知辩护无法消除；扩展、模型改进和LLM作为评判者方案在类型层面无法解决这一问题，因为问题存在于架构类型层面而非具体实现。

Conclusion: LLM智能体架构存在根本性缺陷，语义洗钱是架构实现的盖蒂尔问题，无法通过现有技术手段完全消除，需要重新思考智能体架构设计的基本原理。

Abstract: LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms. We formalize this class of architectural failures as semantic laundering: a pattern where propositions with absent or weak warrant are accepted by the system as admissible by crossing architecturally trusted interfaces. We show that semantic laundering constitutes an architectural realization of the Gettier problem: propositions acquire high epistemic status without a connection between their justification and what makes them true. Unlike classical Gettier cases, this effect is not accidental; it is architecturally determined and systematically reproducible. The central result is the Theorem of Inevitable Self-Licensing: under standard architectural assumptions, circular epistemic justification cannot be eliminated. We introduce the Warrant Erosion Principle as the fundamental explanation for this effect and show that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating a problem that exists at the type level.

</details>


### [23] [Thematic Working Group 5 -- Artificial Intelligence (AI) literacy for teaching and learning: design and implementation](https://arxiv.org/abs/2601.08380)
*Mary Webb,Matt Bower,Ana Amélia Carvalho,Fredrik Mørk Røkenes,Jodie Torrington,Jonathan D. Cohen,Yousra Chtouki,Kathryn Maccallum,Tanya Linden,Deirdre Butler,Juliana Elisa Raffaghelli,Henriikka Vartiainen,Martina Ronci,Peter Tiernan,David M. Smith,Chris Shelton,Joyce Malyn-smith,Pierre Gorissen*

Main category: cs.AI

TL;DR: TWG 5工作组专注于开发实施提升教师AI素养和能动性的策略，帮助教师掌握将AI融入教学实践所需的知识技能


<details>
  <summary>Details</summary>
Motivation: 教师缺乏足够的AI素养和能动性来有效整合AI到教学实践中，需要系统性的策略来赋能教育工作者

Method: 通过课程设计、专业发展项目、实际课堂应用和政策指南等多维度探索，开发全面的教师AI能力提升策略

Result: 开发了系统的教师AI素养提升框架，包括课程、培训、实践应用和政策支持的综合方案

Conclusion: 通过多维度策略可以有效提升教师的AI素养和能动性，使教师能够自信地使用AI工具并培养学生对AI概念的深入理解

Abstract: TWG 5 focused on developing and implementing effective strategies for enhancing AI literacy and agency of teachers, equipping them with the knowledge and skills necessary to integrate AI into their teaching practices. Explorations covered curriculum design, professional development programs, practical classroom applications, and policy guidelines aiming to empower educators to confidently utilize AI tools and foster a deeper understanding of AI concepts among students.

</details>


### [24] [A Qualitative Model to Reason about Object Rotations (QOR) applied to solve the Cube Comparison Test (CCT)](https://arxiv.org/abs/2601.08382)
*Zoe Falomir*

Main category: cs.AI

TL;DR: 本文提出了一个用于推理物体旋转的定性模型(QOR)，并将其应用于解决Ekstrom等人(1976)提出的立方体比较测试(CCT)。


<details>
  <summary>Details</summary>
Motivation: 需要开发一个定性推理模型来处理物体旋转问题，特别是解决立方体比较测试这一经典空间推理任务。

Method: 构建了概念邻域图(CNGRLO)，将旋转运动与立方体各面特征的位置变化和方向变化联系起来，并生成组合表来计算旋转推理的推断。

Result: 成功开发了QOR模型，能够通过概念邻域图和组合表进行旋转推理，应用于立方体比较测试的解决。

Conclusion: QOR模型为物体旋转的定性推理提供了有效的框架，通过概念邻域图和组合表的方法能够系统化地处理旋转相关的空间推理问题。

Abstract: This paper presents a Qualitative model for Reasoning about Object Rotations (QOR) which is applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). A conceptual neighborhood graph relating the Rotation movement to the Location change and the Orientation change (CNGRLO) of the features on the cube sides has been built and it produces composition tables to calculate inferences for reasoning about rotations.

</details>


### [25] [Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models](https://arxiv.org/abs/2601.08383)
*Bo Wang,Junzhuo Li,Hong Chen,Yuanlin Chu,Yuxuan Fan,Xuming Hu*

Main category: cs.AI

TL;DR: MoE架构在预训练期间形成低熵骨干，早期稳定，功能鲁棒，与密集架构形成鲜明对比


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构在预训练期间如何获取知识，以及与密集架构的差异，填补稀疏架构训练动态理解空白

Method: 引入Gated-LPI（门控对数概率增加）神经元级归因指标，分解对数概率增加；对比分析MoE和密集架构在1.2M和600K训练步中的知识获取动态

Result: 发现三个关键模式：1）低熵骨干：前1% MoE神经元捕获超45%正更新；2）早期稳定：MoE在10万步内锁定稳定重要性分布；3）功能鲁棒：屏蔽重要注意力头对MoE影响远小于密集模型

Conclusion: 稀疏性从训练早期就培养出内在稳定和分布式的计算骨干，有助于弥合稀疏架构与训练时可解释性之间的差距

Abstract: Mixture-of-Experts (MoE) architectures decouple model capacity from per-token computation, enabling scaling beyond the computational limits imposed by dense scaling laws. Yet how MoE architectures shape knowledge acquisition during pre-training, and how this process differs from dense architectures, remains unknown. To address this issue, we introduce Gated-LPI (Log-Probability Increase), a neuron-level attribution metric that decomposes log-probability increase across neurons. We present a time-resolved comparison of knowledge acquisition dynamics in MoE and dense architectures, tracking checkpoints over 1.2M training steps (~ 5.0T tokens) and 600K training steps (~ 2.5T tokens), respectively. Our experiments uncover three patterns: (1) Low-entropy backbone. The top approximately 1% of MoE neurons capture over 45% of positive updates, forming a high-utility core, which is absent in the dense baseline. (2) Early consolidation. The MoE model locks into a stable importance profile within < 100K steps, whereas the dense model remains volatile throughout training. (3) Functional robustness. Masking the ten most important MoE attention heads reduces relational HIT@10 by < 10%, compared with > 50% for the dense model, showing that sparsity fosters distributed -- rather than brittle -- knowledge storage. These patterns collectively demonstrate that sparsity fosters an intrinsically stable and distributed computational backbone from early in training, helping bridge the gap between sparse architectures and training-time interpretability.

</details>


### [26] [Creativity in AI as Emergence from Domain-Limited Generative Models](https://arxiv.org/abs/2601.08388)
*Corina Chutaux*

Main category: cs.AI

TL;DR: 论文提出从生成视角理解AI创造力，将其视为有限领域生成模型在受限信息环境中的涌现属性，而非事后评估标签。


<details>
  <summary>Details</summary>
Motivation: 现有AI创造力研究多采用评估框架衡量生成输出的新颖性、多样性或实用性，将创造力视为待评估的属性而非待建模的现象。随着大规模生成系统（特别是多模态架构）展现出日益复杂的模式重组能力，需要重新思考机器创造力的本质和界限。

Method: 提出生成视角的AI创造力框架，关注创造性行为产生的结构和语境条件。将创造力分解为四个相互作用组件：基于模式的生成、诱导的世界模型、语境基础、任意性，并研究这些组件在多模态生成系统中的表现。

Result: 通过将创造力建立在生成动力学与领域特定表征之间的相互作用基础上，为研究AI系统中创造力作为涌现现象提供了技术框架。

Conclusion: 创造力应被视为有限领域生成模型在受限信息环境中的涌现属性，这一生成视角为理解AI创造力提供了更本质的技术框架，超越了传统的事后评估方法。

Abstract: Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.

</details>


### [27] [Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs](https://arxiv.org/abs/2601.08403)
*Abhijnan Nath,Alireza Bagheri Garakani,Tianchen Zhou,Fan Yang,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: OSPO是一种强化学习框架，通过Shapley-Owen归因方法将序列级奖励重新分配到语义单元层面，解决了推荐系统中信用分配问题，在个性化推荐任务上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的大语言模型推荐方法（如GRPO）依赖稀疏的序列级奖励，存在信用分配差距，难以确定哪些具体token驱动了推荐成功。当模型需要从非明确语言中推断潜在用户意图时，这个问题尤为突出。

Method: 提出Owen-Shapley策略优化（OSPO）框架，通过基于Shapley-Owen归因的潜在奖励塑造方法，将序列级优势重新分配到语义连贯的单元层面（如描述产品属性的短语或捕捉偏好的句子），无需额外的参数化价值模型。

Result: 在Amazon ESCI和H&M Fashion数据集上的实验表明，OSPO相比基线方法取得了持续的性能提升，并且在测试时对训练期间未见过的分布外检索器表现出显著的鲁棒性。

Conclusion: OSPO通过细粒度的信用分配机制有效解决了推荐系统中的信用分配问题，提高了模型性能和对未见检索器的鲁棒性，为基于强化学习的个性化推荐提供了新思路。

Abstract: Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when models must infer latent user intent from under-specified language without ground truth labels, a reasoning pattern rarely seen during pretraining. We introduce Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages based on tokens' marginal contributions to outcomes. Unlike value-model-based methods requiring additional computation, OSPO employs potential-based reward shaping via Shapley-Owen attributions to assign segment-level credit while preserving the optimal policy, learning directly from task feedback without parametric value models. By forming coalitions of semantically coherent units (phrases describing product attributes or sentences capturing preferences), OSPO identifies which response parts drive performance. Experiments on Amazon ESCI and H&M Fashion datasets show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.

</details>


### [28] [Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation](https://arxiv.org/abs/2601.08412)
*Yizhan Feng,Hichem Snoussi,Yuhang Wang,Jing Teng,Abel Cherouat,Tian Wang*

Main category: cs.AI

TL;DR: 本文提出了一种结合知识蒸馏、思维链引导和监督微调的方法，用于无人机多SDK控制任务，旨在将复杂推理和代码生成能力高效迁移到小型模型，实现无人机平台的轻量化智能控制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成任务中展现出巨大潜力，但将其应用于资源受限的无人机平台时存在矛盾：大模型的高资源消耗与无人机平台的实时性、轻量化需求不匹配。需要解决如何在保持高性能的同时实现轻量化部署的问题。

Method: 1. 构建高质量数据集：覆盖多种主流无人机SDK，包含指令-代码-推理链，并加入反事实负样本进行数据增强；2. 知识蒸馏：以DeepSeek-Coder-V2-Lite为教师模型，采用混合黑盒白盒蒸馏策略生成高质量思维链软标签，结合硬标签的加权交叉熵损失；3. 提示调优工程：针对无人机控制场景优化提示工程，提升SDK类型识别和函数调用匹配等核心任务性能。

Result: 实验结果表明，蒸馏后的轻量化模型在保持高代码生成准确率的同时，在部署和推理效率方面实现了显著提升，有效证明了该方法在实现无人机精准轻量化智能控制方面的可行性和优越性。

Conclusion: 该方法成功解决了大模型高资源消耗与无人机平台轻量化需求之间的矛盾，通过知识蒸馏等技术将复杂推理能力迁移到小型模型，为资源受限的无人机平台提供了高效、精准的智能控制解决方案。

Abstract: With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs

</details>


### [29] [RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation](https://arxiv.org/abs/2601.08430)
*Sunzhu Li,Jiale Zhao,Miteto Wei,Huimin Ren,Yang Zhou,Jingwen Yang,Shunyu Liu,Kaike Zhang,Wei Chen*

Main category: cs.AI

TL;DR: 提出RubricHub框架，通过粗到细的评分标准生成解决开放式生成任务中缺乏真实标签的问题，在多个领域实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 开放式生成任务缺乏真实标签，现有基于评分标准的方法存在可扩展性瓶颈和标准粗糙的问题，导致监督天花板效应

Method: 提出自动化粗到细评分标准生成框架，结合原则引导合成、多模型聚合和难度演进，构建大规模多领域数据集RubricHub，采用两阶段后训练流程：基于评分标准的拒绝采样微调(RuFT)和强化学习(RuRL)

Result: RubricHub包含约11万样本，后训练的Qwen3-14B在HealthBench上达到69.3分，超越GPT-5等前沿专有模型，实现SOTA性能

Conclusion: RubricHub框架有效解决了开放式生成任务的监督瓶颈，通过高质量的评分标准实现了显著的性能提升，为可验证奖励的强化学习提供了新方向

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.

</details>


### [30] [YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation](https://arxiv.org/abs/2601.08441)
*Abdelaziz Bounhar,Rania Hossam Elmohamady Elbadry,Hadi Abdine,Preslav Nakov,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.AI

TL;DR: YaPO是一种基于稀疏自编码器学习稀疏转向向量的无参考对齐方法，相比密集转向向量能实现更细粒度、解耦的对齐控制，在文化对齐等任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于密集转向向量的激活干预方法存在神经元多语义性导致的潜在因素纠缠问题，限制了在细粒度设置（如文化对齐）中的效果和稳定性，需要区分密切相关的价值观和行为。

Method: 提出YaPO（Yet another Policy Optimization），一种无参考方法，在稀疏自编码器的潜在空间中学习稀疏转向向量。通过优化稀疏编码，产生解耦、可解释且高效的转向方向。

Result: YaPO比密集转向基线收敛更快、性能更强、训练稳定性更好。除了文化对齐，还能泛化到幻觉、财富追求、越狱、权力追求等多种对齐相关行为，且不损害MMLU通用知识。

Conclusion: YaPO为大语言模型提供了一种高效、稳定、细粒度的对齐通用方案，在可控性和领域适应方面具有广泛应用前景。

Abstract: Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \textit{reference-free} method that learns \textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\footnote{https://github.com/MBZUAI-Paris/YaPO}.

</details>


### [31] [Beyond Linearization: Attributed Table Graphs for Table Reasoning](https://arxiv.org/abs/2601.08444)
*Yuxiang Wang,Junhao Gan,Shengxiang Gao,Shenghao Ye,Zhengyi Yang,Jianzhong Qi*

Main category: cs.AI

TL;DR: TABGR提出基于图结构的表格推理方法，通过属性表图(ATG)保留表格结构，使用QG-PPR缓解信息丢失问题，在基准测试中优于现有方法9.7%


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的表格推理方法存在三个关键问题：1) 线性化处理丢失表格结构信息；2) 缺乏显式推理路径导致可解释性差；3) 存在"中间丢失"问题。需要一种能保留表格结构、支持可解释推理且能缓解信息丢失的方法。

Method: 提出TABGR模型，包含两个核心组件：1) 属性表图(ATG) - 将表格表示为图结构，显式保留行列单元格关系；2) 问题引导个性化PageRank(QG-PPR) - 根据问题重新排序表格数据，缓解"中间丢失"问题。该方法无需训练，直接利用图推理能力。

Result: 在两个常用基准测试上的广泛实验表明，TABGR在准确率上持续优于最先进模型，最高提升达9.7%。模型在保留表格结构、提供可解释推理路径和缓解信息丢失方面表现出色。

Conclusion: TABGR通过图结构表示和问题引导的重新排序机制，有效解决了现有表格推理方法的局限性，在准确性和可解释性方面均有显著提升，为表格推理任务提供了新的有效解决方案。

Abstract: Table reasoning, a task to answer questions by reasoning over data presented in tables, is an important topic due to the prevalence of knowledge stored in tabular formats. Recent solutions use Large Language Models (LLMs), exploiting the semantic understanding and reasoning capabilities of LLMs. A common paradigm of such solutions linearizes tables to form plain texts that are served as input to LLMs. This paradigm has critical issues. It loses table structures, lacks explicit reasoning paths for result explainability, and is subject to the "lost-in-the-middle" issue. To address these issues, we propose Table Graph Reasoner (TABGR), a training-free model that represents tables as an Attributed Table Graph (ATG). The ATG explicitly preserves row-column-cell structures while enabling graph-based reasoning for explainability. We further propose a Question-Guided Personalized PageRank (QG-PPR) mechanism to rerank tabular data and mitigate the lost-in-the-middle issue. Extensive experiments on two commonly used benchmarks show that TABGR consistently outperforms state-of-the-art models by up to 9.7% in accuracy. Our code will be made publicly available upon publication.

</details>


### [32] [An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English](https://arxiv.org/abs/2601.08457)
*Sargam Yadav,Abhishek Kaushik,Kevin Mc Daid*

Main category: cs.AI

TL;DR: 开发了一个多模态可解释的Web应用，用于检测印地语-英语混合文本和表情包中的厌女内容，采用最先进的Transformer模型，并提供SHAP和LIME解释功能。


<details>
  <summary>Details</summary>
Motivation: 数字平台用户增长迅速，但同时也助长了仇恨言论和厌女内容的传播。现有AI模型在低资源语言和混合语言场景下研究不足，且缺乏可解释性，这在敏感的仇恨言论检测领域尤为重要。

Method: 1. 文本检测：使用XLM-RoBERTa和mBERT模型，在约4,193条评论数据集上训练；2. 多模态检测：使用mBERT+EfficientNet和mBERT+ResNET模型，在约4,218个表情包数据集上训练；3. 可解释性：采用SHAP和LIME技术提供特征重要性分析；4. 评估：通过Chatbot Usability Questionnaire和User Experience Questionnaire进行人工评估。

Result: 开发了一个完整的Web应用程序，支持对印地语-英语混合文本和表情包中的厌女内容进行检测，并提供可解释的分析结果。系统已通过人工评估验证其可用性。

Conclusion: 该系统为研究人员和内容审核人员提供了一个有效的工具，有助于推动该领域的研究，打击基于性别的数字暴力，确保安全的数字空间。

Abstract: Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.

</details>


### [33] [SUMMPILOT: Bridging Efficiency and Customization for Interactive Summarization System](https://arxiv.org/abs/2601.08475)
*JungMin Yun,Juhwan Choi,Kyohoon Jin,Soojin Jang,Jinhee Jang,YoungBin Kim*

Main category: cs.AI

TL;DR: SummPilot：基于交互的可定制化摘要系统，结合自动摘要效率与个性化需求，通过语义图、实体聚类等交互组件实现用户定制化摘要生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统自动摘要系统无法满足用户个性化兴趣和需求的问题，将自动摘要的效率优势与个性化定制需求相结合。

Method: 引入SummPilot系统，利用大语言模型支持自动和交互式摘要，通过语义图、实体聚类和可解释评估等交互组件让用户参与文档理解和摘要个性化。

Result: 演示和用户研究表明SummPilot在可定制化摘要方面具有适应性和实用性，能够有效满足用户的个性化需求。

Conclusion: SummPilot成功解决了个性化摘要生成的挑战，通过交互式设计实现了高效且用户友好的可定制化摘要系统。

Abstract: This paper incorporates the efficiency of automatic summarization and addresses the challenge of generating personalized summaries tailored to individual users' interests and requirements. To tackle this challenge, we introduce SummPilot, an interaction-based customizable summarization system. SummPilot leverages a large language model to facilitate both automatic and interactive summarization. Users can engage with the system to understand document content and personalize summaries through interactive components such as semantic graphs, entity clustering, and explainable evaluation. Our demo and user studies demonstrate SummPilot's adaptability and usefulness for customizable summarization.

</details>


### [34] [What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting](https://arxiv.org/abs/2601.08509)
*Jinkwan Jang,Hyunbin Jin,Hyungjin Park,Kyubyung Chae,Taesup Kim*

Main category: cs.AI

TL;DR: WIT是一个多模态时间序列预测基准，通过专家构建的合理或反事实场景来评估模型能否基于上下文文本（特别是未来场景）进行条件预测。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法大多是单模态的，依赖历史模式外推。虽然大语言模型在多模态预测方面有潜力，但现有基准主要提供回顾性或未对齐的原始上下文，无法确定模型是否真正利用了文本输入。人类专家在实践中会结合历史证据和假设场景，在不同场景下基于相同观察做出不同的预测。

Method: 引入What If TSF（WIT）多模态预测基准，通过提供专家构建的合理或反事实场景，为场景引导的多模态预测提供严格的测试平台。

Result: WIT基准可用于评估模型是否能够根据上下文文本（特别是未来场景）调整其预测，为多模态时间序列预测提供了新的评估标准。

Conclusion: WIT基准填补了现有时间序列预测评估的空白，通过场景引导的预测任务，能够更准确地评估多模态模型在实际应用中的能力，特别是模型对文本输入的利用程度。

Abstract: Time series forecasting is critical to real-world decision making, yet most existing approaches remain unimodal and rely on extrapolating historical patterns. While recent progress in large language models (LLMs) highlights the potential for multimodal forecasting, existing benchmarks largely provide retrospective or misaligned raw context, making it unclear whether such models meaningfully leverage textual inputs. In practice, human experts incorporate what-if scenarios with historical evidence, often producing distinct forecasts from the same observations under different scenarios. Inspired by this, we introduce What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting. The benchmark is available at https://github.com/jinkwan1115/WhatIfTSF.

</details>


### [35] [Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse](https://arxiv.org/abs/2601.08531)
*Warissara Booranamaitree,Xusheng Du,Yushu Cai,Zhengyang Wang,Ye Zhang,Haoran Xie*

Main category: cs.AI

TL;DR: 提出结合生成式AI和视觉语言模型的三阶段框架，直接处理粗略结构草图和文本描述生成一致的立面改造方案，无需详细现状建模。


<details>
  <summary>Details</summary>
Motivation: 立面改造比完全拆除更可持续，但现有工作流程需要详细的现状建模，耗时耗力且需要反复修改。需要一种能直接处理粗略草图和文本描述的方法来快速生成改造方案。

Method: 三阶段框架：1) 微调VLM模型预测需要修改的区域和添加组件的边界框；2) 稳定扩散模型生成新元素的详细草图，通过生成式修复管道与原始轮廓合并；3) 使用ControlNet将结果细化为逼真图像。

Result: 在数据集和真实工业建筑上的实验表明，该框架能生成既保留原始结构又提升立面细节质量的改造方案，有效避免了详细现状建模的需求。

Conclusion: 该方法使建筑师能够快速探索设计替代方案，迭代早期概念，并以更清晰的方式传达改造意图，为立面改造提供了高效的设计工具。

Abstract: Facade renovation offers a more sustainable alternative to full demolition, yet producing design proposals that preserve existing structures while expressing new intent remains challenging. Current workflows typically require detailed as-built modelling before design, which is time-consuming, labour-intensive, and often involves repeated revisions. To solve this issue, we propose a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) that directly processes rough structural sketch and textual descriptions to produce consistent renovation proposals. First, the input sketch is used by a fine-tuned VLM model to predict bounding boxes specifying where modifications are needed and which components should be added. Next, a stable diffusion model generates detailed sketches of new elements, which are merged with the original outline through a generative inpainting pipeline. Finally, ControlNet is employed to refine the result into a photorealistic image. Experiments on datasets and real industrial buildings indicate that the proposed framework can generate renovation proposals that preserve the original structure while improving facade detail quality. This approach effectively bypasses the need for detailed as-built modelling, enabling architects to rapidly explore design alternatives, iterate on early-stage concepts, and communicate renovation intentions with greater clarity.

</details>


### [36] [WaterCopilot: An AI-Driven Virtual Assistant for Water Management](https://arxiv.org/abs/2601.08559)
*Keerththanan Vickneswaran,Mariangel Garcia Andarcia,Hugo Retief,Chris Dickens,Paulo Silva*

Main category: cs.AI

TL;DR: WaterCopilot是一个基于AI的虚拟助手，用于解决跨界河流流域水资源管理中的数据碎片化、实时访问限制和信息整合复杂性问题，通过RAG和工具调用架构整合静态政策文档和实时水文数据。


<details>
  <summary>Details</summary>
Motivation: 跨界河流流域的可持续水资源管理面临数据碎片化、实时访问受限以及多样化信息源整合复杂性的挑战，需要统一的交互平台来弥合这些差距。

Method: 采用检索增强生成(RAG)和工具调用架构，开发了两个自定义插件：iwmi-doc-plugin（使用Azure AI Search进行语义搜索）和iwmi-api-plugin（查询实时数据库），支持多语言交互、透明源引用、自动计算和可视化功能。

Result: 使用RAGAS框架评估，WaterCopilot总体得分0.8043，答案相关性0.8571，上下文精度0.8009。系统实现了自动阈值警报、与LRB数字孪生集成，并在AWS上部署了可扩展的管道。

Conclusion: 尽管在处理非英语技术文档和API延迟方面存在限制，但WaterCopilot为数据稀缺的跨界环境建立了可复制的AI增强框架，展示了支持及时决策和加强复杂河流流域水安全的潜力。

Abstract: Sustainable water resource management in transboundary river basins is challenged by fragmented data, limited real-time access, and the complexity of integrating diverse information sources. This paper presents WaterCopilot-an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB) to bridge these gaps through a unified, interactive platform. Built on Retrieval-Augmented Generation (RAG) and tool-calling architectures, WaterCopilot integrates static policy documents and real-time hydrological data via two custom plugins: the iwmi-doc-plugin, which enables semantic search over indexed documents using Azure AI Search, and the iwmi-api-plugin, which queries live databases to deliver dynamic insights such as environmental-flow alerts, rainfall trends, reservoir levels, water accounting, and irrigation data. The system features guided multilingual interactions (English, Portuguese, French), transparent source referencing, automated calculations, and visualization capabilities. Evaluated using the RAGAS framework, WaterCopilot achieves an overall score of 0.8043, with high answer relevancy (0.8571) and context precision (0.8009). Key innovations include automated threshold-based alerts, integration with the LRB Digital Twin, and a scalable deployment pipeline hosted on AWS. While limitations in processing non-English technical documents and API latency remain, WaterCopilot establishes a replicable AI-augmented framework for enhancing water governance in data-scarce, transboundary contexts. The study demonstrates the potential of this AI assistant to support informed, timely decision-making and strengthen water security in complex river basins.

</details>


### [37] [ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios](https://arxiv.org/abs/2601.08620)
*António Loison,Quentin Macé,Antoine Edy,Victor Xing,Tom Balough,Gabriel Moreira,Bo Liu,Manuel Faysse,Céline Hudelot,Gautier Viaud*

Main category: cs.AI

TL;DR: ViDoRe v3是一个全面的多模态RAG基准测试，包含视觉丰富文档上的多类型查询，涵盖10个专业领域数据集，提供高质量的人工标注，用于评估RAG系统在视觉元素理解、多文档综合和源定位方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG基准测试主要关注文本数据、单文档理解或孤立评估检索与生成，无法捕捉真实应用中需要理解视觉元素（表格、图表、图像）、跨文档综合信息以及提供准确源定位的复杂性。

Method: 构建了包含10个专业领域数据集的多模态RAG基准测试，包含约26,000个文档页面和3,099个人工验证的查询，每个查询支持6种语言。通过12,000小时的人工标注工作，提供了检索相关性、边界框定位和验证参考答案的高质量标注。

Result: 评估显示：视觉检索器优于文本检索器；后期交互模型和文本重排序显著提升性能；混合或纯视觉上下文能提高答案生成质量。但当前模型在非文本元素理解、开放式查询和细粒度视觉定位方面仍有困难。

Conclusion: ViDoRe v3基准测试揭示了当前RAG系统在视觉丰富文档处理方面的能力与局限，为多模态RAG研究提供了重要评估工具，鼓励研究社区解决视觉元素理解、跨文档综合和精细源定位等挑战。

Abstract: Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.

</details>


### [38] [Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.08641)
*Yichen Luo,Yebo Feng,Jiahua Xu,Yang Liu*

Main category: cs.AI

TL;DR: 论文提出了一种可解释的多智能体系统，用于模因币跟单交易，通过分解复杂任务、协调专业智能体，在识别高质量模因币项目和KOL钱包方面优于传统机器学习模型和单一LLM。


<details>
  <summary>Details</summary>
Motivation: 模因币跟单交易虽然流行但存在诸多问题：操纵性机器人普遍存在、被跟随钱包未来表现不确定、交易执行滞后。同时，单一LLM在处理复杂多任务（如资产配置）方面存在困难，特别是在加密货币领域缺乏足够的领域知识。

Method: 受资产管理团队结构启发，将复杂任务分解为子任务，协调专业智能体协作解决。采用少样本思维链提示，使每个智能体获得专业模因币交易知识，解释多模态数据，生成可解释决策。

Result: 在1000个模因币项目交易数据上的实证评估显示，该多智能体系统在识别高质量模因币项目和关键意见领袖（KOL）钱包方面分别达到73%和70%的精确率。选定的KOL在这些项目中总共产生了50万美元的利润。

Conclusion: 提出的可解释多智能体系统有效解决了模因币跟单交易中的挑战，通过任务分解和智能体协作，在识别优质项目和KOL方面表现出色，为加密货币市场的智能交易提供了新思路。

Abstract: The launch of \$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.
  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \$500,000 across these projects.

</details>


### [39] [Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding](https://arxiv.org/abs/2601.08653)
*Zenghua Liao,Jinzhi Liao,Xiang Zhao*

Main category: cs.AI

TL;DR: Prism框架通过逻辑依赖建模解决大语言模型在复杂意图理解中的核心挑战，显著提升澄清交互的逻辑一致性、用户满意度和任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型作为社交平台接口时，用户意图往往模糊且动态变化，现有方法通过顺序或并行提问澄清意图，但未能解决核心挑战：建模澄清问题间的逻辑依赖关系。

Method: Prism包含四个模块：复杂意图分解模块（分解意图并识别逻辑依赖）、逻辑澄清生成模块（基于依赖组织问题）、意图感知奖励模块（评估澄清轨迹质量并生成训练数据）、自进化意图调优模块（迭代优化模型能力）。

Result: Prism在澄清交互、意图执行和认知负载基准测试中均优于现有方法，逻辑一致性达SOTA，逻辑冲突降至11.5%，用户满意度提升14.4%，任务完成时间减少34.8%。

Conclusion: Prism通过建模澄清问题间的逻辑依赖关系，实现了逻辑一致且高效的意图澄清，为大语言模型在动态社交环境中的复杂意图理解提供了有效解决方案。

Abstract: Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.

</details>


### [40] [From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner's Tutorial](https://arxiv.org/abs/2601.08662)
*Abhijit Sen,Sonali Panda,Mahima Arya,Subhajit Patra,Zizhan Zheng,Denys I. Bondar*

Main category: cs.AI

TL;DR: 为本科生设计的强化学习入门教程，通过示例驱动的方式降低学习门槛，帮助理解理论与代码实现之间的衔接


<details>
  <summary>Details</summary>
Motivation: 强化学习理论到实际编码应用之间存在鸿沟，学生在从概念理解转向实现时面临挑战，需要更易理解的教程来降低学习门槛

Method: 采用清晰、示例驱动的解释方式，通过动手实践案例和易于理解的讲解，帮助学生掌握基础技能

Result: 该教程旨在让学生能够更自信地在实际场景中应用强化学习技术，填补理论与实践的差距

Conclusion: 通过示例驱动的教学方法可以有效降低强化学习的学习门槛，帮助学生从理论理解顺利过渡到实际应用

Abstract: This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.

</details>


### [41] [Parallel Context-of-Experts Decoding for Retrieval Augmented Generation](https://arxiv.org/abs/2601.08670)
*Giulio Corallo,Paolo Papotti*

Main category: cs.AI

TL;DR: PCED框架通过并行解码解决RAG中的多文档推理与效率权衡问题，将检索文档视为独立专家，通过检索感知对比解码规则同步预测，无需构建共享注意力机制。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成面临两难：长提示拼接支持多文档推理但造成预填充瓶颈；单独编码文档KV缓存速度快但破坏跨文档交互。需要一种既能保持跨文档推理能力又高效的方法。

Method: 提出并行专家上下文解码（PCED），训练免费框架，将证据聚合从注意力机制转移到解码过程。将检索文档视为独立"专家"，通过新颖的检索感知对比解码规则同步专家预测，权衡专家对数概率与模型先验。

Result: 该方法无需构建跨文档共享注意力，就能恢复跨文档推理能力，解决了RAG中的效率与效果权衡问题。

Conclusion: PCED通过将证据聚合从注意力转移到解码，提供了一种高效的多文档推理解决方案，避免了传统方法的瓶颈，同时保持了跨文档交互能力。

Abstract: Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated "experts", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.

</details>


### [42] [Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock](https://arxiv.org/abs/2601.08673)
*Didier Sornette,Sandro Claudio Lera,Ke Wu*

Main category: cs.AI

TL;DR: 论文认为大语言模型表现出的欺骗、威胁等行为不应被解读为对齐失败或恶意智能涌现，而是对人类社会中权力不对称下互动模式的统计泛化，真正的AGI风险在于其作为人类智能和矛盾的放大器作用。


<details>
  <summary>Details</summary>
Motivation: 针对当前将大语言模型表现出的欺骗、威胁、勒索等行为解释为对齐失败或恶意智能涌现的观点，作者认为这种解释存在概念错误，需要重新理解这些行为的本质和AGI的真正风险。

Method: 采用关系模型理论分析，将勒索等行为视为市场定价、权威关系和最后通牒谈判等正常社会行为连续体中的极限情况，而非偏离正常社会行为的范畴性异常。

Result: 大语言模型并不进行道德推理，而是统计性地内化人类社会互动记录，包括法律、合同、谈判、冲突和强制安排。这些被标记为不道德或异常的行为实际上是权力、信息或约束极端不对称下互动机制的结构性泛化。

Conclusion: AGI的主要风险不是对抗性意图，而是其作为人类智能、权力和矛盾的内生放大器作用。通过消除长期存在的认知和制度摩擦，AGI压缩了时间尺度，消除了历史上允许不一致价值观和治理制度持续存在的容错空间。对齐失败是结构性的而非偶然的，需要关注放大效应、复杂性和制度稳定性而非仅仅模型层面的意图。

Abstract: Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI's role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.

</details>


### [43] [PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning](https://arxiv.org/abs/2601.08679)
*Xiaoyou Liu,Xinyi Mou,Shengbin Yue,Liang Wang,Yuqing Wang,Qiexiang Wang,Tianrui Qin,Wangchunshu Zhou,Zhongyu Wei*

Main category: cs.AI

TL;DR: PersonaDual框架在单个模型中同时支持客观推理和个性化推理，通过自适应模式切换平衡个性化信息的利弊，减少对客观性的干扰。


<details>
  <summary>Details</summary>
Motivation: 随着用户期望LLMs更符合个人偏好，个性化信息变得有价值，但可能损害客观性和事实准确性，特别是当个性化信息与问题不匹配时。

Method: 提出PersonaDual框架，首先通过监督微调学习两种推理模式，然后通过强化学习（DualGRPO）优化模式选择，实现基于上下文的自适应切换。

Result: 在客观和个性化基准测试中，PersonaDual在保持个性化优势的同时减少干扰，实现接近无干扰的性能，并能更好地利用有益的个性化信号改进客观问题解决。

Conclusion: PersonaDual成功平衡了LLMs中个性化与客观性的矛盾，通过双模式自适应切换机制有效管理个性化信息的双重性。

Abstract: As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.

</details>


### [44] [All Required, In Order: Phase-Level Evaluation for AI-Human Dialogue in Healthcare and Beyond](https://arxiv.org/abs/2601.08690)
*Shubham Kulkarni,Alexander Lyzhov,Shiva Chaitanya,Preetam Joshi*

Main category: cs.AI

TL;DR: 提出OIP-SCE评估方法，通过检查临床对话中每个必需义务是否按正确顺序完成并提供清晰证据，来评估对话AI的合规性，填补技术进展与医疗实际需求之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前对话AI已开始支持实际临床工作，但大多数评估方法忽略了合规性如何依赖于完整对话过程。需要一种能够评估临床义务是否按正确顺序完成并提供可审计证据的方法。

Method: 引入Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE)方法，该方法检查每个必需的临床义务是否在正确阶段完成，并提供清晰的证据供临床医生审查，使复杂规则变得实用且可审计。

Result: 在两个案例研究（呼吸病史、福利验证）中展示了该方法，显示阶段级证据如何将政策转化为共享、可操作的步骤。为临床医生提供检查控制，为工程师提供清晰规范。

Conclusion: OIP-SCE提供了一个单一、可审计的评估界面，使AI能力与临床工作流程保持一致，支持常规、安全的使用，填补技术进展与医疗实际需求之间的差距。

Abstract: Conversational AI is starting to support real clinical work, but most evaluation methods miss how compliance depends on the full course of a conversation. We introduce Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE), an evaluation method that checks whether every required clinical obligation is met, in the right order, with clear evidence for clinicians to review. This makes complex rules practical and auditable, helping close the gap between technical progress and what healthcare actually needs. We demonstrate the method in two case studies (respiratory history, benefits verification) and show how phase-level evidence turns policy into shared, actionable steps. By giving clinicians control over what to check and engineers a clear specification to implement, OIP-SCE provides a single, auditable evaluation surface that aligns AI capability with clinical workflow and supports routine, safe use.

</details>


### [45] [Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set](https://arxiv.org/abs/2601.08703)
*Kaivalya Rawal,Eoin Delaney,Zihao Fu,Sandra Wachter,Chris Russell*

Main category: cs.AI

TL;DR: 该论文提出了一种无需真实标签的模型解释评估方法AXE，用于评估特征重要性解释的质量，特别针对Rashomon集合中不同模型的行为差异，并能检测对抗性公平漂白。


<details>
  <summary>Details</summary>
Motivation: 当前模型解释评估方法存在两个主要问题：1）依赖与理想真实解释的比较，这会掩盖Rashomon集合中不同模型的行为差异；2）无法有效检测对抗性公平漂白，即模型可能使用受保护属性进行预测但通过解释方法隐藏这一事实。

Method: 提出了三个解释评估原则，并开发了AXE（无需真实标签的解释评估）方法。该方法不依赖真实标签，而是通过分析解释本身的质量来评估特征重要性解释，能够检测模型是否使用受保护属性进行预测。

Result: AXE方法能够以100%的成功率检测对抗性公平漂白，即模型使用受保护属性进行预测但解释方法试图隐藏这一事实的情况。相比基于模型敏感性或真实标签比较的现有方法，AXE能更有效地揭示Rashomon集合中模型的行为差异。

Conclusion: 提出的AXE方法提供了一种无需真实标签的模型解释评估框架，能够有效评估特征重要性解释的质量，检测对抗性公平漂白，并帮助从Rashomon集合中选择合适的模型，同时避免被虚假解释误导。

Abstract: Explainable artificial intelligence (XAI) is concerned with producing explanations indicating the inner workings of models. For a Rashomon set of similarly performing models, explanations provide a way of disambiguating the behavior of individual models, helping select models for deployment. However explanations themselves can vary depending on the explainer used, and need to be evaluated. In the paper "Evaluating Model Explanations without Ground Truth", we proposed three principles of explanation evaluation and a new method "AXE" to evaluate the quality of feature-importance explanations. We go on to illustrate how evaluation metrics that rely on comparing model explanations against ideal ground truth explanations obscure behavioral differences within a Rashomon set. Explanation evaluation aligned with our proposed principles would highlight these differences instead, helping select models from the Rashomon set. The selection of alternate models from the Rashomon set can maintain identical predictions but mislead explainers into generating false explanations, and mislead evaluation methods into considering the false explanations to be of high quality. AXE, our proposed explanation evaluation method, can detect this adversarial fairwashing of explanations with a 100% success rate. Unlike prior explanation evaluation strategies such as those based on model sensitivity or ground truth comparison, AXE can determine when protected attributes are used to make predictions.

</details>


### [46] [Learning from Demonstrations via Capability-Aware Goal Sampling](https://arxiv.org/abs/2601.08731)
*Yuanlin Duan,Yuning Wang,Wenjie Qiu,He Zhu*

Main category: cs.AI

TL;DR: Cago是一种新的模仿学习方法，通过动态跟踪智能体在专家轨迹上的能力，选择刚好超出当前能力范围的中介目标来引导学习，从而解决长时域环境中模仿学习失败的问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在长时域环境中经常失败，因为完美复制演示不现实，小错误会灾难性累积。现有方法要么仅用演示进行策略初始化，要么用于奖励塑造，都存在对专家轨迹的脆弱依赖。

Method: Cago方法动态跟踪智能体在专家轨迹上的能力，使用这个信号选择刚好超出智能体当前能力范围的中介目标（目标采样），从而创建一个自适应课程，引导智能体逐步学习完整任务。

Result: 实验结果表明，Cago在稀疏奖励、目标条件任务中显著提高了样本效率和最终性能，始终优于现有的模仿学习基线方法。

Conclusion: Cago通过能力感知的目标采样方法，有效减轻了对专家轨迹的直接模仿依赖，创建了自适应课程，使智能体能够稳定地向解决完整任务的方向进步。

Abstract: Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.

</details>


### [47] [AI as Entertainment](https://arxiv.org/abs/2601.08768)
*Cody Kommers,Ari Holtzman*

Main category: cs.AI

TL;DR: 论文指出AI系统主要被设计为提升生产力的智能工具，但娱乐正成为AI的重要应用场景，而AI领域缺乏评估娱乐内容社会影响的框架，需要从"厚娱乐"角度重新思考AI文化内容的评估。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统主要被定位为提升生产力的智能工具，但数据显示AI已广泛用于娱乐目的，尤其是年轻人群体。AI领域缺乏评估娱乐内容社会影响的框架，现有评估主要关注文化危害，而忽略了文化输出的潜在益处。

Method: 分析当前AI评估实践的局限性，借鉴人文学科见解，提出"厚娱乐"作为评估AI生成文化内容的框架，该框架考虑娱乐在意义创造、身份形成和社会连接中的作用，而不仅仅是减少危害。

Result: 识别出AI评估中的关键不对称性：虽然AI评估严格衡量智能的益处和危害，但几乎完全关注文化危害，缺乏阐述文化输出如何积极有益的框架。娱乐可能成为主要AI公司的重要商业模式。

Conclusion: AI领域需要为评估娱乐内容的社会影响做好准备，采用"厚娱乐"框架来全面评估AI生成文化内容的价值。长期来看，AI可能更多是关于"智能"的娱乐应用，就像社交媒体更多是关于社会连接一样。

Abstract: Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose "thick entertainment" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about "intelligence" as social media is about social connection.

</details>


### [48] [Uncovering Political Bias in Large Language Models using Parliamentary Voting Records](https://arxiv.org/abs/2601.08785)
*Jieying Chen,Karen de Jong,Andreas Poole,Jan Burakowski,Elena Elderson Nosti,Joep Windt,Chendi Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于真实议会投票记录构建政治偏见基准的方法，并在荷兰、挪威和西班牙三个国家案例中应用，发现先进大语言模型普遍呈现左倾或中间倾向，并对右翼保守政党存在明显负面偏见。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在数字平台和决策系统中的深度应用，对其政治偏见的担忧日益增长。尽管已有大量研究关注性别和种族等社会偏见，但对政治偏见的系统性研究仍然有限，尽管政治偏见对社会有直接影响。

Method: 提出了一种通用方法，通过将模型生成的投票预测与经过验证的议会投票记录对齐来构建政治偏见基准。在三个国家案例中实例化该方法：PoliBiasNL（荷兰）、PoliBiasNO（挪威）和PoliBiasES（西班牙）。还提出了一种可视化方法，将LLM和政党的意识形态映射到二维CHES空间，实现模型与现实政治行为者之间的直接可解释比较。

Result: 实验揭示了细粒度的意识形态区分：最先进的大语言模型一致显示左倾或中间倾向，同时对右翼保守政党存在明显的负面偏见。

Conclusion: 这些发现强调了基于真实议会行为的透明、跨国评估对于理解和审计现代大语言模型政治偏见的价值。

Abstract: As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [49] [Post-Quantum Cryptography Key Expansion Method and Anonymous Certificate Scheme Based on NTRU](https://arxiv.org/abs/2601.07841)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 提出基于NTRU的高效公钥扩展方法，应用于匿名证书方案，显著提升密钥生成效率


<details>
  <summary>Details</summary>
Motivation: NTRU作为重要的后量子密码方法具有抗量子计算攻击能力，但其密钥对生成效率相对较低，需要改进

Method: 提出基于NTRU的密钥扩展方法，允许高效公钥扩展，并将该方法应用于匿名证书方案，使终端实体只需生成一次密钥对，证书机构即可扩展多个不同的公钥以实现匿名性

Result: 实验结果表明，所提出的密钥扩展方法比密钥对生成具有显著更高的效率

Conclusion: 提出的NTRU密钥扩展方法有效解决了NTRU密钥生成效率低的问题，为匿名证书方案提供了高效的后量子密码解决方案

Abstract: NTRU is one of the important lattice-based post-quantum cryptography methods, offering resistance against quantum computing attacks. However, a drawback of NTRU lies in its relatively low efficiency in generating key pairs. Therefore, this study proposes an NTRU-based key expansion method that enables efficient public key expansion. Furthermore, the proposed method is applied to an anonymous certificate scheme, allowing an end entity to generate a key pair only once, after which the certificate authority can expand multiple distinct public keys for anonymity. The experimental results demonstrate that the proposed key expansion method achieves significantly higher efficiency than key pair generation.

</details>


### [50] [Decentralized Firmware Integrity Verification for Cyber-Physical Systems Using Ethereum Blockchain](https://arxiv.org/abs/2601.08091)
*S M Mostaq Hossain,Amani Altarawneh*

Main category: cs.CR

TL;DR: 基于以太坊区块链的去中心化固件完整性验证框架，通过智能合约存储固件哈希值，实现防篡改、透明、无需信任的验证机制。


<details>
  <summary>Details</summary>
Motivation: 传统固件完整性验证机制（如安全启动、数字签名、集中式哈希数据库）存在内部威胁和单点故障风险，无法有效保护关键信息物理系统（CPS）免受固件篡改和供应链攻击。

Method: 在以太坊Sepolia测试网上部署智能合约存储固件SHA-256哈希值，使用Web3和Infura进行链上交互，开发Python客户端工具计算哈希并与区块链通信实现实时注册和验证。

Result: 成功实现原型系统，演示了合约部署、哈希注册和完整性验证，实验结果表明方法可靠且燃气费用低，具有实际应用价值。

Conclusion: 该工作提出了一个实用且可扩展的区块链固件验证模型，显著增强了关键CPS环境中对抗固件篡改和供应链攻击的防御能力，并讨论了通过Layer-2 rollups和IPFS等技术的扩展方案。

Abstract: Firmware integrity is a foundational requirement for securing Cyber-Physical Systems (CPS), where malicious or compromised firmware can result in persistent backdoors, unauthorized control, or catastrophic system failures. Traditional verification mechanisms such as secure boot, digital signatures, and centralized hash databases are increasingly inadequate due to risks from insider threats and single points of failure. In this paper, we propose a decentralized firmware integrity verification framework built on the Ethereum blockchain, offering tamperproof, transparent, and trustless validation. Our system stores SHA-256 hashes of firmware binaries within smart contracts deployed on the Ethereum Sepolia testnet, using Web3 and Infura for seamless on-chain interaction. A Python-based client tool computes firmware hashes and communicates with the blockchain to register and verify firmware authenticity in realtime. We implement and evaluate a fully functional prototype using real firmware samples, demonstrating successful contract deployment, hash registration, and integrity verification through live blockchain transactions. Experimental results confirm the reliability and low cost (in gas fees) of our approach, highlighting its practicality and scalability for real-world CPS applications. To enhance scalability and performance, we discuss extensions using Layer-2 rollups and off-chain storage via the InterPlanetary File System (IPFS). We also outline integration pathways with secure boot mechanisms, Trusted Platform Module (TPM)based attestation, and zero-trust architectures. This work contributes a practical and extensible model for blockchain-based firmware verification, significantly strengthening the defense against firmware tampering and supply chain attacks in critical CPS environments.

</details>


### [51] [ForgetMark: Stealthy Fingerprint Embedding via Targeted Unlearning in Language Models](https://arxiv.org/abs/2601.08189)
*Zhenhua Xu,Haobo Zhang,Zhebo Wang,Qichen Liu,Haitao Xu,Wenpeng Xing,Meng Han*

Main category: cs.CR

TL;DR: ForgetMark是一种隐蔽的指纹框架，通过目标性遗忘编码模型来源，避免传统后门指纹的高困惑度触发器和固定响应模式，实现100%所有权验证同时保持标准性能。


<details>
  <summary>Details</summary>
Motivation: 现有侵入式（后门）指纹存在高困惑度触发器易被过滤、固定响应模式易被启发式检测器发现、以及在良性输入上产生虚假激活等问题，需要更隐蔽的指纹方法。

Method: 构建紧凑的人类可读键值集，通过预测熵排序，训练轻量级LoRA适配器来抑制原始值在对应键上的输出，同时保持通用能力。所有权验证通过聚合似然和语义证据计算指纹成功率。

Result: 在多样化架构和设置下，ForgetMark在指纹模型上实现100%所有权验证，保持标准性能，在后门基准测试中超越隐蔽性和模型合并鲁棒性，并在适度增量微调下保持有效。

Conclusion: ForgetMark通过依赖概率性遗忘痕迹而非固定触发-响应模式，避免了高困惑度触发器，降低了可检测性和虚假触发，提供了一种更隐蔽和鲁棒的模型指纹方法。

Abstract: Existing invasive (backdoor) fingerprints suffer from high-perplexity triggers that are easily filtered, fixed response patterns exposed by heuristic detectors, and spurious activations on benign inputs. We introduce \textsc{ForgetMark}, a stealthy fingerprinting framework that encodes provenance via targeted unlearning. It builds a compact, human-readable key--value set with an assistant model and predictive-entropy ranking, then trains lightweight LoRA adapters to suppress the original values on their keys while preserving general capabilities. Ownership is verified under black/gray-box access by aggregating likelihood and semantic evidence into a fingerprint success rate. By relying on probabilistic forgetting traces rather than fixed trigger--response patterns, \textsc{ForgetMark} avoids high-perplexity triggers, reduces detectability, and lowers false triggers. Across diverse architectures and settings, it achieves 100\% ownership verification on fingerprinted models while maintaining standard performance, surpasses backdoor baselines in stealthiness and robustness to model merging, and remains effective under moderate incremental fine-tuning. Our code and data are available at \href{https://github.com/Xuzhenhua55/ForgetMark}{https://github.com/Xuzhenhua55/ForgetMark}.

</details>


### [52] [DNF: Dual-Layer Nested Fingerprinting for Large Language Model Intellectual Property Protection](https://arxiv.org/abs/2601.08223)
*Zhenhua Xu,Yiran Zhao,Mengting Zhong,Dezhang Kong,Changting Lin,Tong Qiao,Meng Han*

Main category: cs.CR

TL;DR: DNF是一种黑盒指纹方法，通过结合领域特定风格线索和隐式语义触发器嵌入分层后门，实现完美的指纹激活同时保持下游任务效用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速增长引发了黑盒部署下知识产权保护的迫切关注。现有基于后门的指纹方法要么依赖罕见标记导致高困惑度输入易被过滤，要么使用固定的触发-响应映射易受泄露和后适应攻击。

Method: 提出双层级嵌套指纹（DNF），通过耦合领域特定风格线索与隐式语义触发器嵌入分层后门，使用较低困惑度的触发器。

Result: 在Mistral-7B、LLaMA-3-8B-Instruct和Falcon3-7B-Instruct上实现完美指纹激活，保持下游任务效用，对指纹检测攻击不可检测，对增量微调和模型合并相对鲁棒。

Conclusion: DNF为LLM所有权验证和知识产权保护提供了一种实用、隐蔽且具有弹性的解决方案。

Abstract: The rapid growth of large language models raises pressing concerns about intellectual property protection under black-box deployment. Existing backdoor-based fingerprints either rely on rare tokens -- leading to high-perplexity inputs susceptible to filtering -- or use fixed trigger-response mappings that are brittle to leakage and post-hoc adaptation. We propose \textsc{Dual-Layer Nested Fingerprinting} (DNF), a black-box method that embeds a hierarchical backdoor by coupling domain-specific stylistic cues with implicit semantic triggers. Across Mistral-7B, LLaMA-3-8B-Instruct, and Falcon3-7B-Instruct, DNF achieves perfect fingerprint activation while preserving downstream utility. Compared with existing methods, it uses lower-perplexity triggers, remains undetectable under fingerprint detection attacks, and is relatively robust to incremental fine-tuning and model merging. These results position DNF as a practical, stealthy, and resilient solution for LLM ownership verification and intellectual property protection.

</details>


### [53] [APT-MCL: An Adaptive APT Detection System Based on Multi-View Collaborative Provenance Graph Learning](https://arxiv.org/abs/2601.08328)
*Mingqi Lv,Shanshan Zhang,Haiwen Liu,Tieming Chen,Tiantian Zhu*

Main category: cs.CR

TL;DR: APT-MCL：基于多视图协同溯源图学习的APT检测系统，通过无监督学习和多视图特征集成解决APT样本稀缺、标注困难和攻击多样性问题


<details>
  <summary>Details</summary>
Motivation: 传统单点防御无法捕获APT攻击的长距离跨实体语义，溯源图分析面临APT样本稀缺、细粒度标注成本高、攻击战术技术多样等实际部署障碍

Method: 采用无监督学习策略通过异常检测发现节点级APT攻击，基于多视图特征创建多个异常检测子模型，并在协同学习框架中集成以适配多样化攻击场景

Result: 在三个真实世界APT数据集上的实验验证表明：多视图特征提升跨场景泛化能力，协同训练在标签稀缺情况下显著增强节点级检测性能，支持多样化攻击场景的实际部署

Conclusion: APT-MCL通过多视图协同学习有效解决了APT检测中的样本稀缺、标注困难和攻击多样性问题，为实际部署提供了可行的解决方案

Abstract: Advanced persistent threats (APTs) are stealthy and multi-stage, making single-point defenses (e.g., malware- or traffic-based detectors) ill-suited to capture long-range and cross-entity attack semantics. Provenance-graph analysis has become a prominent approach for APT detection. However, its practical deployment is hampered by (i) the scarcity of APT samples, (ii) the cost and difficulty of fine-grained APT sample labeling, and (iii) the diversity of attack tactics and techniques. Aiming at these problems, this paper proposes APT-MCL, an intelligent APT detection system based on Multi-view Collaborative provenance graph Learning. It adopts an unsupervised learning strategy to discover APT attacks at the node level via anomaly detection. After that, it creates multiple anomaly detection sub-models based on multi-view features and integrates them within a collaborative learning framework to adapt to diverse attack scenarios. Extensive experiments on three real-world APT datasets validate the approach: (i) multi-view features improve cross-scenario generalization, and (ii) co-training substantially boosts node-level detection under label scarcity, enabling practical deployment on diverse attack scenarios.

</details>


### [54] [On the Maximum Toroidal Distance Code for Lattice-Based Public-Key Cryptography](https://arxiv.org/abs/2601.08452)
*Shuiyin Liu,Amin Sakzad*

Main category: cs.CR

TL;DR: 提出最大环面距离(MTD)码用于基于格的公钥加密，通过最大化最小L2范数环面距离来降低解密失败率，在ℓ=2时与Minal码性能相当，在ℓ>2时优于Minal码和最大Lee距离码。


<details>
  <summary>Details</summary>
Motivation: 在基于格的公钥加密方案中，如NIST ML-KEM(Crystals-Kyber)，降低解密失败率(DFR)是一个重要目标。现有的编码方案如Minal码和最大Lee距离码在减少DFR方面仍有改进空间。

Method: 将加密编码问题表述为在离散ℓ维环面ℤ_q^ℓ中选择2^ℓ个点，最大化最小L2范数环面距离。针对不同维度：ℓ=2时构造与Minal码变体相似；ℓ=4时基于D4格构造；ℓ=8时对应2E8格点在ℤ_4^8中。

Result: 在Kyber设置下的数值评估显示：ℓ=2时与Minal码性能相当；ℓ>2时提出的MTD码在解密失败率方面优于Minal码和最大Lee距离码。

Conclusion: MTD码通过最大化环面距离有效降低了基于格加密方案的解密失败率，为后量子密码学中的编码问题提供了新的解决方案。

Abstract: We propose a maximum toroidal distance (MTD) code for lattice-based public-key encryption (PKE). By formulating the encryption encoding problem as the selection of $2^\ell$ points in the discrete $\ell$-dimensional torus $\mathbb{Z}_q^\ell$, the proposed construction maximizes the minimum $L_2$-norm toroidal distance to reduce the decryption failure rate (DFR) in post-quantum schemes such as the NIST ML-KEM (Crystals-Kyber). For $\ell = 2$, we show that the MTD code is essentially a variant of the Minal code recently introduced at IACR CHES 2025. For $\ell = 4$, we present a construction based on the $D_4$ lattice that achieves the largest known toroidal distance, while for $\ell = 8$, the MTD code corresponds to $2E_8$ lattice points in $\mathbb{Z}_4^8$. Numerical evaluations under the Kyber setting show that the proposed codes outperform both Minal and maximum Lee-distance ($L_1$-norm) codes in DFR for $\ell > 2$, while matching Minal code performance for $\ell = 2$.

</details>


### [55] [MASH: Evading Black-Box AI-Generated Text Detectors via Style Humanization](https://arxiv.org/abs/2601.08564)
*Yongtong Gu,Songze Li,Xia Hu*

Main category: cs.CR

TL;DR: MASH是一种通过风格转移规避黑盒AI生成文本检测器的新框架，采用多阶段对齐方法使AI生成文本的分布更接近人类写作风格，在6个数据集和5个检测器上平均攻击成功率92%，优于11个基线方法。


<details>
  <summary>Details</summary>
Motivation: AI生成文本的滥用日益严重，促进了检测方法的快速发展，但这些检测器在面对对抗性规避时可靠性脆弱。现有攻击策略通常依赖白盒假设或需要过高计算和交互成本，在实际黑盒场景下效果有限。

Method: MASH采用多阶段对齐框架：1) 风格注入监督微调；2) 直接偏好优化；3) 推理时精炼。通过风格转移技术将AI生成文本的分布塑造成类似人类写作文本的分布。

Result: 在6个数据集和5个检测器上的实验表明，MASH平均攻击成功率达到92%，比最强基线方法平均高出24%，同时保持了优越的语言质量。

Conclusion: MASH框架在无需白盒访问或高计算成本的情况下，通过多阶段风格对齐有效规避黑盒AI生成文本检测器，为对抗性攻击提供了实用解决方案。

Abstract: The increasing misuse of AI-generated texts (AIGT) has motivated the rapid development of AIGT detection methods. However, the reliability of these detectors remains fragile against adversarial evasions. Existing attack strategies often rely on white-box assumptions or demand prohibitively high computational and interaction costs, rendering them ineffective under practical black-box scenarios. In this paper, we propose Multi-stage Alignment for Style Humanization (MASH), a novel framework that evades black-box detectors based on style transfer. MASH sequentially employs style-injection supervised fine-tuning, direct preference optimization, and inference-time refinement to shape the distributions of AI-generated texts to resemble those of human-written texts. Experiments across 6 datasets and 5 detectors demonstrate the superior performance of MASH over 11 baseline evaders. Specifically, MASH achieves an average Attack Success Rate (ASR) of 92%, surpassing the strongest baselines by an average of 24%, while maintaining superior linguistic quality.

</details>


### [56] [Estimating the True Distribution of Data Collected with Randomized Response](https://arxiv.org/abs/2601.08603)
*Carlos Antonio Pinzón,Ehab ElSalamouny,Lucas Massot,Alexis Miller,Héber Hwang Arcolezi,Catuscia Palamidessi*

Main category: cs.CR

TL;DR: 本文提供了随机响应协议中精确最大似然估计的简单公式，绕过了迭代贝叶斯更新算法，并通过实验比较帮助实践者选择最佳估计方法。


<details>
  <summary>Details</summary>
Motivation: 随机响应协议在收集分类数据时存在标准去偏规则可能产生负值的问题，而迭代贝叶斯更新算法虽然优雅但速度慢，需要更高效的解决方案。

Method: 本文提出了随机响应协议中精确最大似然估计的简单公式，绕过了迭代贝叶斯更新算法，并通过实验比较了不同估计方法的性能。

Result: 新提出的精确最大似然估计公式在计算效率和准确性方面优于传统方法，为实践者提供了更好的选择。

Conclusion: 本文提供的精确最大似然估计公式解决了随机响应协议中标准去偏规则产生负值的问题，为实践者提供了更高效、更准确的估计方法选择。

Abstract: Randomized Response (RR) is a protocol designed to collect and analyze categorical data with local differential privacy guarantees. It has been used as a building block of mechanisms deployed by Big tech companies to collect app or web users' data. Each user reports an automatic random alteration of their true value to the analytics server, which then estimates the histogram of the true unseen values of all users using a debiasing rule to compensate for the added randomness. A known issue is that the standard debiasing rule can yield a vector with negative values (which can not be interpreted as a histogram), and there is no consensus on the best fix. An elegant but slow solution is the Iterative Bayesian Update algorithm (IBU), which converges to the Maximum Likelihood Estimate (MLE) as the number of iterations goes to infinity. This paper bypasses IBU by providing a simple formula for the exact MLE of RR and compares it with other estimation methods experimentally to help practitioners decide which one to use.

</details>


### [57] [Double Strike: Breaking Approximation-Based Side-Channel Countermeasures for DNNs](https://arxiv.org/abs/2601.08698)
*Lorenzo Casalino,Maria Méndez Real,Jean-Christophe Prévotet,Rubén Salvador*

Main category: cs.CR

TL;DR: 本文分析了MACPRUNING侧信道防御机制的漏洞，通过利用其控制流依赖性，成功恢复了DNN的重要权重，证明了该防御机制的安全性大幅降低。


<details>
  <summary>Details</summary>
Motivation: DNN权重作为重要的知识产权资产需要保护，MACPRUNING被提出作为基于剪枝的侧信道防御机制，但其原始安全分析未考虑控制流依赖性，可能存在安全漏洞。

Method: 提出了一种预处理方法来利用MACPRUNING的控制流依赖性，通过在Chipwhisperer-Lite上对受保护的MLP进行实际实验，针对每个神经元的前8个权重进行攻击。

Result: 成功恢复了96%的重要权重，证明了受保护实现的安全性大幅降低。微架构泄漏进一步提高了攻击效果，甚至能恢复100%的目标非重要权重。

Conclusion: MACPRUNING防御机制存在严重安全漏洞，控制流依赖性可被利用来绕过防御，需要重新评估和改进该防御方案的设计。

Abstract: Deep neural networks (DNNs), which support services such as driving assistants and medical diagnoses, undergo lengthy and expensive training procedures. Therefore, the training's outcome - the DNN weights - represents a significant intellectual property asset to protect. Side-channel analysis (SCA) has recently appeared as an effective approach to recover this confidential asset from DNN implementations. In response, researchers have proposed to defend DNN implementations through classic side-channel countermeasures, at the cost of higher energy consumption, inference time, and resource utilisation. Following a different approach, Ding et al. (HOST'25) introduced MACPRUNING, a novel SCA countermeasure based on pruning, a performance-oriented Approximate Computing technique: at inference time, the implementation randomly prunes (or skips) non-important weights (i.e., with low contribution to the DNN's accuracy) of the first layer, exponentially increasing the side-channel resilience of the protected DNN implementation. However, the original security analysis of MACPRUNING did not consider a control-flow dependency intrinsic to the countermeasure design. This dependency may allow an attacker to circumvent MACPRUNING and recover the weights important to the DNN's accuracy. This paper describes a preprocessing methodology to exploit the above-mentioned control-flow dependency. Through practical experiments on a Chipwhisperer-Lite running a MACPRUNING-protected Multi-Layer Perceptron, we target the first 8 weights of each neuron and recover 96% of the important weights, demonstrating the drastic reduction in security of the protected implementation. Moreover, we show how microarchitectural leakage improves the effectiveness of our methodology, even allowing for the recovery of up to 100% of the targeted non-important weights. Lastly, by adapting our methodology [continue in pdf].

</details>


### [58] [Malware Detection based on API Calls: A Reproducibility Study](https://arxiv.org/abs/2601.08725)
*Juhani Merilehto*

Main category: cs.CR

TL;DR: 本研究独立复现了Felli cious等人的恶意软件检测方法，使用随机森林分类器进行顺序不变的API调用频率分析。复现结果验证了原始方法，并在所有模型上获得了比原始结果更好的F1分数（提升0.99%到2.57%）。


<details>
  <summary>Details</summary>
Motivation: 验证Felli cious等人提出的基于API调用频率分析的恶意软件检测方法的可复现性、科学严谨性和实际可行性。通过独立复现实验来确认该方法的稳健性和可靠性。

Method: 使用原始公开数据集（250,533个训练样本，83,511个测试样本），复现了四种模型变体：Unigram、Bigram、Trigram和Combined n-gram方法。采用随机森林分类器进行顺序不变的API调用频率分析，在最优API调用长度2,500下进行实验。

Result: 成功验证了所有关键发现，所有模型的F1分数均超过原始结果0.99%到2.57%。Unigram模型达到F1=0.8717（原始：0.8631），证明其作为轻量级恶意软件检测器的有效性。三次独立实验运行结果高度一致，标准差低于0.5%。

Conclusion: 本研究验证了原始方法的稳健性和科学严谨性，同时确认了基于频率的API调用分析在恶意软件检测中的实际可行性。复现实验证明了该方法的可重复性和可靠性。

Abstract: This study independently reproduces the malware detection methodology presented by Felli cious et al. [7], which employs order-invariant API call frequency analysis using Random Forest classification. We utilized the original public dataset (250,533 training samples, 83,511 test samples) and replicated four model variants: Unigram, Bigram, Trigram, and Combined n gram approaches. Our reproduction successfully validated all key findings, achieving F1-scores that exceeded the original results by 0.99% to 2.57% across all models at the optimal API call length of 2,500. The Unigram model achieved F1=0.8717 (original: 0.8631), confirming its ef fectiveness as a lightweight malware detector. Across three independent experimental runs with different random seeds, we observed remarkably consistent results with standard deviations be low 0.5%, demonstrating high reproducibility. This study validates the robustness and scientific rigor of the original methodology while confirming the practical viability of frequency-based API call analysis for malware detection.

</details>


### [59] [Memory DisOrder: Memory Re-orderings as a Timerless Side-channel](https://arxiv.org/abs/2601.08770)
*Sean Siddens,Sanya Srivastava,Reese Levine,Josiah Dykstra,Tyler Sorensen*

Main category: cs.CR

TL;DR: 论文提出Memory DisOrder：一种无定时器的侧信道攻击，利用内存重排序推断其他进程活动，在多种主流处理器上实现隐蔽信道和应用程序指纹识别攻击。


<details>
  <summary>Details</summary>
Motivation: 现代CPU和GPU采用宽松内存模型，内存操作可能被重排序。先前研究发现内存重排序在内存系统压力下更频繁发生，这为跨进程信息泄露提供了可能。

Method: 1) 进行模糊测试，验证多种处理器对跨进程信号的敏感性；2) 利用漏洞实现隐蔽信道和应用程序指纹识别攻击；3) 探索利用系统底层细节增加重排序频率的方法。

Result: 1) X86/Arm/Apple CPU和NVIDIA/AMD/Apple GPU均存在漏洞；2) 在Apple M3 GPU上实现16位/秒、95%准确率的隐蔽信道；3) 在多种CPU和Apple M3 GPU上实现可靠的DNN架构指纹识别；4) X86 CPU上隐蔽信道可达近30K位/秒。

Conclusion: 内存重排序漏洞构成严重安全威胁，可被用于跨进程信息泄露。随着对该漏洞理解的深入，可能开发出更精确的攻击。需要硬件和软件层面的防护措施。

Abstract: To improve efficiency, nearly all parallel processing units (CPUs and GPUs) implement relaxed memory models in which memory operations may be re-ordered, i.e., executed out-of-order. Prior testing work in this area found that memory re-orderings are observed more frequently when other cores are active, e.g., stressing the memory system, which likely triggers aggressive hardware optimizations.
  In this work, we present Memory DisOrder: a timerless side-channel that uses memory re-orderings to infer activity on other processes. We first perform a fuzzing campaign and show that many mainstream processors (X86/Arm/Apple CPUs, NVIDIA/AMD/Apple GPUs) are susceptible to cross-process signals. We then show how the vulnerability can be used to implement classic attacks, including a covert channel, achieving up to 16 bits/second with 95% accuracy on an Apple M3 GPU, and application fingerprinting, achieving reliable closed-world DNN architecture fingerprinting on several CPUs and an Apple M3 GPU. Finally, we explore how low-level system details can be exploited to increase re-orderings, showing the potential for a covert channel to achieve nearly 30K bits/second on X86 CPUs. More precise attacks can likely be developed as the vulnerability becomes better understood.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [60] [SECite: Analyzing and Summarizing Citations in Software Engineering Literature](https://arxiv.org/abs/2601.07939)
*Shireesh Reddy Pyreddy,Khaja Valli Pathan,Hasan Masum,Tarannum Shaila Zaman*

Main category: cs.SE

TL;DR: SECite：通过引用上下文情感分析评估学术影响力的新方法，结合无监督机器学习与生成式AI，从引用反馈中提取论文优缺点


<details>
  <summary>Details</summary>
Motivation: 传统文献综述仅反映作者自我陈述的视角，而其他研究者如何讨论和引用论文能提供更深入、更实用的理解。需要一种方法来系统分析学术社区对论文的看法，以全面评估其贡献和局限。

Method: 开发半自动化流程提取九篇研究论文的引用，应用先进的自然语言处理技术和无监督机器学习对引用陈述进行正负面情感分类。除了情感分类，还使用生成式AI从聚类引用组和全文生成情感特定的摘要，捕捉目标论文的优缺点。

Result: 研究揭示了学术社区对这些作品认知的有意义模式，突出了外部引用反馈与作者自我陈述之间的契合与分歧。通过情感分析发现了引用模式中的规律性。

Conclusion: 通过整合引用情感分析与基于LLM的摘要生成，本研究为评估学术贡献提供了一个全面框架，能够更客观地理解论文的实际影响和局限性。

Abstract: Identifying the strengths and limitations of a research paper is a core component of any literature review. However, traditional summaries reflect only the authors' self-presented perspective. Analyzing how other researchers discuss and cite the paper can offer a deeper, more practical understanding of its contributions and shortcomings. In this research, we introduce SECite, a novel approach for evaluating scholarly impact through sentiment analysis of citation contexts. We develop a semi-automated pipeline to extract citations referencing nine research papers and apply advanced natural language processing (NLP) techniques with unsupervised machine learning to classify these citation statements as positive or negative. Beyond sentiment classification, we use generative AI to produce sentiment-specific summaries that capture the strengths and limitations of each target paper, derived both from clustered citation groups and from the full text. Our findings reveal meaningful patterns in how the academic community perceives these works, highlighting areas of alignment and divergence between external citation feedback and the authors' own presentation. By integrating citation sentiment analysis with LLM-based summarization, this study provides a comprehensive framework for assessing scholarly contributions.

</details>


### [61] [Towards Verifiably Safe Tool Use for LLM Agents](https://arxiv.org/abs/2601.08012)
*Aarya Doshi,Yining Hong,Congying Xu,Eunsuk Kang,Alexandros Kapravelos,Christian Kästner*

Main category: cs.SE

TL;DR: 该论文提出了一种系统化的LLM智能体安全框架，通过STPA分析识别风险，结合增强型MCP协议提供形式化安全保障，从临时修复转向主动防护。


<details>
  <summary>Details</summary>
Motivation: LLM智能体通过工具调用扩展能力，但可能引发敏感数据泄露、关键记录被覆盖等风险，现有方法如模型防护无法保证系统安全，信息流控制和时序约束需要大量人工标注。

Method: 1. 应用系统理论过程分析(STPA)识别智能体工作流中的危险，推导安全需求；2. 将需求形式化为数据流和工具序列的可执行规范；3. 引入增强型模型上下文协议(MCP)框架，要求对能力、机密性和信任级别进行结构化标注。

Result: 提出了一种系统化安全框架，能够将LLM智能体安全从临时可靠性修复转向具有形式化保证的主动防护栏，减少对用户确认的依赖，使自主性成为有意识的设计选择。

Conclusion: 该研究通过STPA分析和增强型MCP协议的结合，为LLM智能体提供了系统化的安全保障方法，实现了从反应式修复到主动防护的转变，为企业环境中的智能体部署提供了更可靠的安全基础。

Abstract: Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. While this empowers agents to perform complex tasks, LLMs may invoke unintended tool interactions and introduce risks, such as leaking sensitive data or overwriting critical records, which are unacceptable in enterprise contexts. Current approaches to mitigate these risks, such as model-based safeguards, enhance agents' reliability but cannot guarantee system safety. Methods like information flow control (IFC) and temporal constraints aim to provide guarantees but often require extensive human annotation. We propose a process that starts with applying System-Theoretic Process Analysis (STPA) to identify hazards in agent workflows, derive safety requirements, and formalize them as enforceable specifications on data flows and tool sequences. To enable this, we introduce a capability-enhanced Model Context Protocol (MCP) framework that requires structured labels on capabilities, confidentiality, and trust level. Together, these contributions aim to shift LLM-based agent safety from ad hoc reliability fixes to proactive guardrails with formal guarantees, while reducing dependence on user confirmation and making autonomy a deliberate design choice.

</details>


### [62] [Automating API Documentation from Crowdsourced Knowledge](https://arxiv.org/abs/2601.08036)
*Bonan Kou,Zijie Zhou,Muhao Chen,Tianyi Zhang*

Main category: cs.SE

TL;DR: AutoDoc利用Stack Overflow讨论和GPT-4o自动生成API文档，通过特定组件处理LLM幻觉和冗余，相比基线方法准确率提升77.7%，减少9.5%重复，覆盖34.4%官方文档未包含的知识。


<details>
  <summary>Details</summary>
Motivation: 官方API文档通常存在过时和不完整的问题，需要从开发者社区（如Stack Overflow）提取API知识来补充和完善文档。

Method: 1. 使用微调的密集检索模型从Stack Overflow帖子中识别7种API知识类型；2. 利用GPT-4o总结API知识为简洁文本；3. 设计两个特定组件处理LLM幻觉和生成内容冗余。

Result: 在48个不同流行度的API上评估：1. 生成的API文档比基线准确率提升77.7%；2. 重复内容减少9.5%；3. 包含34.4%官方文档未覆盖的知识；4. 用户研究发现文档更全面、简洁、有帮助。

Conclusion: 通过精心设计对抗LLM幻觉和信息冗余，利用LLMs生成API文档是可行的，AutoDoc方法能显著提升文档质量，即使是较小的开源模型也能获得可比结果。

Abstract: API documentation is crucial for developers to learn and use APIs. However, it is known that many official API documents are obsolete and incomplete. To address this challenge, we propose a new approach called AutoDoc that generates API documents with API knowledge extracted from online discussions on Stack Overflow (SO). AutoDoc leverages a fine-tuned dense retrieval model to identify seven types of API knowledge from SO posts. Then, it uses GPT-4o to summarize the API knowledge in these posts into concise text. Meanwhile, we designed two specific components to handle LLM hallucination and redundancy in generated content. We evaluated AutoDoc against five comparison baselines on 48 APIs of different popularity levels. Our results indicate that the API documents generated by AutoDoc are up to 77.7% more accurate, 9.5% less duplicated, and contain 34.4% knowledge uncovered by the official documents. We also measured the sensitivity of AutoDoc to the choice of different LLMs. We found that while larger LLMs produce higher-quality API documents, AutoDoc enables smaller open-source models (e.g., Mistral-7B-v0.3) to achieve comparable results. Finally, we conducted a user study to evaluate the usefulness of the API documents generated by AutoDoc. All participants found API documents generated by AutoDoc to be more comprehensive, concise, and helpful than the comparison baselines. This highlights the feasibility of utilizing LLMs for API documentation with careful design to counter LLM hallucination and information redundancy.

</details>


### [63] [Cognitive Biases in LLM-Assisted Software Development](https://arxiv.org/abs/2601.08045)
*Xinyi Zhou,Zeinadsadat Saghi,Sadra Sabouri,Rahul Pandita,Mollie McGuire,Souti Chattopadhyay*

Main category: cs.SE

TL;DR: 本研究首次全面调查了LLM辅助开发中的认知偏见，发现48.8%的程序员行为存在偏见，其中56.4%与开发者-LLM交互相关，揭示了AI协作编程中的新型认知偏见模式。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中的广泛应用，编程正从解决方案生成活动转变为解决方案评估活动。这种转变可能带来新的认知挑战，放大现有决策偏见或创造全新的偏见。本研究旨在探索认知偏见如何在AI协作开发中表现并影响决策。

Method: 采用混合方法：首先对14名学生和专业开发者进行观察研究，然后对另外22名开发者进行问卷调查。通过定性比较传统非LLM工作流与LLM辅助工作流中的偏见类别，系统分析了90种特定于开发者-LLM交互的认知偏见。

Result: 研究发现48.8%的程序员行为存在偏见，其中56.4%的偏见行为与开发者-LLM交互相关。开发了包含15个偏见类别的分类法，并经过认知心理学家验证。LLM相关行为更可能与新型偏见相关联。

Conclusion: LLM辅助开发引入了新的认知偏见模式，需要专门工具和实践来缓解。研究为开发者提供了工具建议，并为LLM工具构建者提出了缓解认知偏见的推荐方案，以改善人机协作编程体验。

Abstract: The widespread adoption of Large Language Models (LLMs) in software development is transforming programming from a solution-generative to a solution-evaluative activity. This shift opens a pathway for new cognitive challenges that amplify existing decision-making biases or create entirely novel ones. One such type of challenge stems from cognitive biases, which are thinking patterns that lead people away from logical reasoning and result in sub-optimal decisions. How do cognitive biases manifest and impact decision-making in emerging AI-collaborative development? This paper presents the first comprehensive study of cognitive biases in LLM-assisted development. We employ a mixed-methods approach, combining observational studies with 14 student and professional developers, followed by surveys with 22 additional developers. We qualitatively compare categories of biases affecting developers against the traditional non-LLM workflows. Our findings suggest that LLM-related actions are more likely to be associated with novel biases. Through a systematic analysis of 90 cognitive biases specific to developer-LLM interactions, we develop a taxonomy of 15 bias categories validated by cognitive psychologists. We found that 48.8% of total programmer actions are biased, and developer-LLM interactions account for 56.4% of these biased actions. We discuss how these bias categories manifest, present tools and practices for developers, and recommendations for LLM tool builders to help mitigate cognitive biases in human-AI programming.

</details>


### [64] [Coverage-Guided Road Selection and Prioritization for Efficient Testing in Autonomous Driving Systems](https://arxiv.org/abs/2601.08609)
*Qurban Ali,Andrea Stocco,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: 提出了一种ADAS测试优先级框架，通过聚类减少冗余测试用例，基于几何和行为多样性选择代表性案例，按复杂性、难度和历史失败率排序，在保持79%失败场景的同时减少89%测试集规模，早期故障检测提升95倍。


<details>
  <summary>Details</summary>
Motivation: ADAS系统需要大量测试确保安全可靠，但道路场景数据集常包含冗余案例，这些冗余会减慢测试过程却无法改善故障检测效果。

Method: 提出测试优先级框架：1) 基于ADAS驾驶行为的几何和动态特征对道路场景进行聚类；2) 从每个聚类中选择代表性案例以保证覆盖；3) 基于几何复杂性、驾驶难度和历史失败率对道路进行优先级排序，确保最关键和最具挑战性的测试优先执行。

Result: 在OPENCAT数据集和Udacity自动驾驶模拟器上使用两个ADAS模型进行评估：平均减少89%测试集规模，同时保留79%的失败道路场景；优先级策略相比随机基线将早期故障检测提升高达95倍。

Conclusion: 该测试优先级框架能有效减少ADAS测试冗余，在显著缩小测试规模的同时保持故障检测能力，大幅提升测试效率。

Abstract: Autonomous Driving Assistance Systems (ADAS) rely on extensive testing to ensure safety and reliability, yet road scenario datasets often contain redundant cases that slow down the testing process without improving fault detection. To address this issue, we present a novel test prioritization framework that reduces redundancy while preserving geometric and behavioral diversity. Road scenarios are clustered based on geometric and dynamic features of the ADAS driving behavior, from which representative cases are selected to guarantee coverage. Roads are finally prioritized based on geometric complexity, driving difficulty, and historical failures, ensuring that the most critical and challenging tests are executed first. We evaluate our framework on the OPENCAT dataset and the Udacity self-driving car simulator using two ADAS models. On average, our approach achieves an 89% reduction in test suite size while retaining an average of 79% of failed road scenarios. The prioritization strategy improves early failure detection by up to 95x compared to random baselines.

</details>


### [65] [LLMs in Code Vulnerability Analysis: A Proof of Concept](https://arxiv.org/abs/2601.08691)
*Shaznin Sultana,Sadia Afreen,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 该研究评估了5对代码专用和通用大语言模型在C/C++漏洞数据集上的表现，发现微调方法在所有任务上都优于零样本和少样本方法，代码专用模型在复杂任务上表现更优，同时指出当前代码修复质量评估指标存在不足。


<details>
  <summary>Details</summary>
Motivation: 传统软件安全分析方法难以应对现代代码库的规模和复杂性，需要智能自动化来更高效、准确地检测、评估和修复漏洞。本研究探索利用代码专用和通用大语言模型来自动化关键软件安全任务。

Method: 评估了5对最近的大语言模型（包括代码专用和通用开源模型），在Big-Vul和Vul-Repair两个C/C++漏洞数据集上进行测试。比较了微调和基于提示的方法，涵盖漏洞识别、严重性预测、访问复杂性评估和修复生成等任务。

Result: 微调方法在所有任务和模型上都优于零样本和少样本方法。代码专用模型在复杂任务的零样本和少样本设置中表现更优，而通用模型效果相近。CodeBLEU、CodeBERTScore、BLEU和ChrF等指标之间存在差异，表明当前修复质量评估指标不足。

Conclusion: 本研究通过调查先进大语言模型在改进漏洞分析和修复方面的潜力，为软件安全社区做出了贡献。研究结果强调了微调的重要性，并指出了当前代码修复质量评估指标的局限性。

Abstract: Context: Traditional software security analysis methods struggle to keep pace with the scale and complexity of modern codebases, requiring intelligent automation to detect, assess, and remediate vulnerabilities more efficiently and accurately. Objective: This paper explores the incorporation of code-specific and general-purpose Large Language Models (LLMs) to automate critical software security tasks, such as identifying vulnerabilities, predicting severity and access complexity, and generating fixes as a proof of concept. Method: We evaluate five pairs of recent LLMs, including both code-based and general-purpose open-source models, on two recognized C/C++ vulnerability datasets, namely Big-Vul and Vul-Repair. Additionally, we compare fine-tuning and prompt-based approaches. Results: The results show that fine-tuning uniformly outperforms both zero-shot and few-shot approaches across all tasks and models. Notably, code-specialized models excel in zero-shot and few-shot settings on complex tasks, while general-purpose models remain nearly as effective. Discrepancies among CodeBLEU, CodeBERTScore, BLEU, and ChrF highlight the inadequacy of current metrics for measuring repair quality. Conclusions: This study contributes to the software security community by investigating the potential of advanced LLMs to improve vulnerability analysis and remediation.

</details>


### [66] [Revisiting "Revisiting Neuron Coverage for DNN Testing: A Layer-Wise and Distribution-Aware Criterion": A Critical Review and Implications on DNN Coverage Testing](https://arxiv.org/abs/2601.08729)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: 该论文对ICSE 2023提出的Neural Coverage（NLC）进行了批判性评估，指出其在理论假设和实证研究上的问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 虽然NLC提出了八个设计需求并展示了强大的实证性能，但作者质疑其理论和实证假设的有效性，认为NLC偏离了覆盖率准则的核心原则。

Method: 通过理论分析和实证验证，评估NLC在单调性、测试套件顺序独立性等核心原则上的偏差，并分析协方差矩阵关键属性的处理不足。

Result: 研究发现NLC确实偏离了覆盖率准则的核心原则，实证研究存在有效性威胁，特别是关于测试套件真实排序的问题。

Conclusion: 论文提出了未来DNN覆盖率指标的改进建议，并讨论了这些发现对深度学习测试领域的影响。

Abstract: We present a critical review of Neural Coverage (NLC), a state-of-the-art DNN coverage criterion by Yuan et al. at ICSE 2023. While NLC proposes to satisfy eight design requirements and demonstrates strong empirical performance, we question some of their theoretical and empirical assumptions. We observe that NLC deviates from core principles of coverage criteria, such as monotonicity and test suite order independence, and could more fully account for key properties of the covariance matrix. Additionally, we note threats to the validity of the empirical study, related to the ground truth ordering of test suites. Through our empirical validation, we substantiate our claims and propose improvements for future DNN coverage metrics. Finally, we conclude by discussing the implications of these insights.

</details>


### [67] [TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback](https://arxiv.org/abs/2601.08734)
*Prithwish Jana,Sam Davidson,Bhavana Bhasker,Andrey Kan,Anoop Deoras,Laurent Callot*

Main category: cs.SE

TL;DR: TerraFormer是一个神经符号框架，通过监督微调和验证器引导的强化学习，结合形式验证工具，显著提高了从自然语言生成和修改基础设施即代码的正确性。


<details>
  <summary>Details</summary>
Motivation: 基础设施即代码（IaC）自动化具有挑战性，大型语言模型（LLMs）从自然语言生成配置时经常出错，需要提高IaC生成的正确性和合规性。

Method: 提出TerraFormer神经符号框架，结合监督微调与验证器引导的强化学习，使用形式验证工具提供语法、可部署性和策略合规性反馈。创建了两个高质量数据集TF-Gen（152k实例）和TF-Mutn（52k实例），通过多阶段验证和迭代LLM自校正。

Result: 相比基础LLM，TerraFormer在IaC-Eval上正确性提高15.94%，在TF-Gen（测试集）上提高11.65%，在TF-Mutn（测试集）上提高19.60%。在TF-Gen和TF-Mutn测试集上优于包括Sonnet 3.7、DeepSeek-R1和GPT-4.1等大50倍的模型，在IaC-Eval上排名第三，并实现最佳实践和安全性合规。

Conclusion: TerraFormer框架通过神经符号方法显著提高了IaC生成和修改的正确性，超越了更大的LLMs，为IaC自动化提供了有效的解决方案。

Abstract: Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.

</details>


### [68] [Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs](https://arxiv.org/abs/2601.08773)
*Manideep Reddy Chinthareddy*

Main category: cs.SE

TL;DR: 本文比较了三种检索增强生成方法在软件工程中的应用：纯向量检索、LLM生成知识图谱和确定性AST知识图谱。结果显示AST知识图谱在索引成本、覆盖率和多跳推理方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统的基于向量相似性搜索的检索增强生成方法在捕获主题相似性方面表现良好，但在处理多跳架构推理（如控制器到服务到存储库链、接口驱动连接和继承关系）时存在不足。需要探索更有效的检索方法来支持复杂的软件架构分析。

Method: 在Java代码库（Shopizer、ThingsBoard、OpenMRS Core）上对三种检索管道进行基准测试：A) 纯向量检索（无图），B) LLM生成的知识图谱RAG，C) 基于Tree-sitter和双向遍历的确定性AST知识图谱。使用15个架构和代码追踪查询，测量索引时间、查询延迟、语料覆盖率、成本和答案正确性。

Result: AST知识图谱在几秒钟内构建完成，而LLM知识图谱需要更长的生成时间。LLM知识图谱存在索引不完整问题（Shopizer中377个文件被跳过），覆盖率和图谱规模均低于AST知识图谱。AST知识图谱的端到端成本相对于纯向量基线适中，而LLM知识图谱成本更高，特别是随着仓库规模增加。查询延迟方面，无图方法和AST知识图谱相似，LLM知识图谱更慢且更不稳定。在Shopizer问题集上，AST知识图谱正确率最高，LLM知识图谱次之，纯向量基线在架构查询上表现最差且幻觉风险最高。

Conclusion: 确定性AST知识图谱比LLM提取的知识图谱提供更可靠的覆盖率和多跳基础支持，同时索引成本显著更低。对于需要多跳架构推理的软件工程任务，基于AST的确定性方法优于基于LLM的提取方法。

Abstract: Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal.
  Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.

</details>


### [69] [APEX-SWE](https://arxiv.org/abs/2601.08806)
*Abhi Kottamasu,Akul Datta,Aakash Barthwal,Chirag Mahapatra,Ajay Arun,Adarsh Hiremath,Brendan Foody,Bertie Vidgen*

Main category: cs.SE

TL;DR: APEX-SWE是一个评估前沿AI模型在软件工程中经济价值创造能力的基准，包含集成任务和可观测性任务两类真实场景任务，评估显示Gemini 3 Pro表现最佳，性能主要取决于认知推理能力和不确定性解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI评估主要关注狭窄、定义明确的任务，无法反映真实软件工程工作的经济价值。需要创建能够评估AI模型在实际软件工程工作中执行经济价值创造能力的基准。

Method: 提出APEX-SWE基准，包含两类任务：1) 集成任务（100个）- 要求跨异构云原语、业务应用和基础设施即代码服务构建端到端系统；2) 可观测性任务（100个）- 要求使用日志、仪表板等遥测信号和非结构化上下文调试生产故障。评估了8个前沿模型。

Result: Gemini 3 Pro（Thinking = High）表现最佳，Pass@1得分为25%。分析表明，强性能主要由认知推理能力驱动，即区分假设与已验证事实的能力，结合在行动前解决不确定性的能动性。

Conclusion: APEX-SWE为评估AI在软件工程中的经济价值创造能力提供了新基准，认知推理和不确定性解决是AI在复杂软件工程任务中成功的关键因素。作者开源了评估框架和开发集（50个任务）。

Abstract: We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).

</details>
