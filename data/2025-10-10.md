<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.CR](#cs.CR) [Total: 21]
- [cs.AI](#cs.AI) [Total: 67]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Modeling Developer Burnout with GenAI Adoption](https://arxiv.org/abs/2510.07435)
*Zixuan Feng,Sadia Afroz,Anita Sarma*

Main category: cs.SE

TL;DR: 研究发现GenAI采用通过增加工作需求加剧开发者倦怠，但工作资源和积极认知可缓解此效应


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI采用与开发者倦怠的关系，弥补现有研究仅关注生产力提升而忽视对开发者福祉影响的不足

Method: 采用嵌入式混合方法研究设计，结合定量（442名开发者调查、PLS-SEM和回归分析）与定性（开放式回答分析）证据

Result: GenAI采用通过增加工作需求加剧倦怠，但工作资源和积极认知能缓解负面影响

Conclusion: GenAI采用既是挑战也是机遇，组织应关注工作资源建设以减轻负面影响

Abstract: Generative AI (GenAI) is rapidly reshaping software development workflows.
While prior studies emphasize productivity gains, the adoption of GenAI also
introduces new pressures that may harm developers' well-being. In this paper,
we investigate the relationship between the adoption of GenAI and developers'
burnout. We utilized the Job Demands--Resources (JD--R) model as the analytic
lens in our empirical study. We employed a concurrent embedded mixed-methods
research design, integrating quantitative and qualitative evidence. We first
surveyed 442 developers across diverse organizations, roles, and levels of
experience. We then employed Partial Least Squares--Structural Equation
Modeling (PLS-SEM) and regression to model the relationships among job demands,
job resources, and burnout, complemented by a qualitative analysis of
open-ended responses to contextualize the quantitative findings. Our results
show that GenAI adoption heightens burnout by increasing job demands, while job
resources and positive perceptions of GenAI mitigate these effects, reframing
adoption as an opportunity.

</details>


### [2] [HotBugs.jar: A Benchmark of Hot Fixes for Time-Critical Bugs](https://arxiv.org/abs/2510.07529)
*Carol Hanna,Federica Sarro,Mark Harman,Justyna Petke*

Main category: cs.SE

TL;DR: 提出了首个专门针对热修复的基准数据集HotBugs.jar，包含679个经过人工验证的真实热修复案例，其中110个可复现。


<details>
  <summary>Details</summary>
Motivation: 热修复是解决生产系统中紧急问题的关键变更，但现有评估基准缺乏专门针对热修复的数据集。

Method: 从10个Apache项目的19万次提交和15万份问题报告中筛选出746个候选热修复补丁，经过人工评估确认679个真实热修复，其中110个可复现测试。

Result: 构建了HotBugs.jar数据集，包含110个可复现案例和所有679个验证热修复，每个案例都包含错误版本、修复版本、测试套件和元数据。

Conclusion: 该基准数据集已被SBSE会议挑战赛道采用，为快速调试、自动修复和生产级弹性工具的研究提供了重要支持。

Abstract: Hot fixes are urgent, unplanned changes deployed to production systems to
address time-critical issues. Despite their importance, no existing evaluation
benchmark focuses specifically on hot fixes. We present HotBugs$.$jar, the
first dataset dedicated to real-world hot fixes. From an initial mining of 10
active Apache projects totaling over 190K commits and 150K issue reports, we
identified 746 software patches that met our hot-fix criteria. After manual
evaluation, 679 were confirmed as genuine hot fixes, of which 110 are
reproducible using a test suite. Building upon the Bugs$.$jar framework,
HotBugs$.$jar integrates these 110 reproducible cases and makes available all
679 manually validated hot fixes, each enriched with comprehensive metadata to
support future research. Each hot fix was systematically identified using Jira
issue data, validated by independent reviewers, and packaged in a reproducible
format with buggy and fixed versions, test suites, and metadata. HotBugs$.$jar
has already been adopted as the official challenge dataset for the Search-Based
Software Engineering (SBSE) Conference Challenge Track, demonstrating its
immediate impact. This benchmark enables the study and evaluation of tools for
rapid debugging, automated repair, and production-grade resilience in modern
software systems to drive research in this essential area forward.

</details>


### [3] [RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code](https://arxiv.org/abs/2510.07604)
*Yubo Bai,Tapti Palit*

Main category: cs.SE

TL;DR: RustAssure使用大型语言模型自动将C代码转译为Rust，并通过差异符号测试验证语义等价性，实现89.8%的C函数可编译为Rust，其中69.9%保持语义等价。


<details>
  <summary>Details</summary>
Motivation: Rust作为内存安全语言能显著提升软件安全性，但现有C代码库需要转译为Rust才能利用其安全优势。

Method: 使用LLM进行C到Rust的自动转译，结合提示工程技术生成惯用安全的Rust代码，并通过差异符号测试验证C和Rust代码的语义相似性。

Result: 在5个真实应用和库的评估中，系统能为89.8%的C函数生成可编译的Rust函数，其中69.9%的函数在C和Rust版本间产生了等价的符号返回值。

Conclusion: RustAssure证明了使用LLM自动转译C代码到Rust的可行性，并通过符号测试确保语义等价性，为利用Rust安全特性提供有效途径。

Abstract: Rust is a memory-safe programming language that significantly improves
software security. Existing codebases written in unsafe memory languages, such
as C, must first be transpiled to Rust to take advantage of Rust's improved
safety guarantees. RustAssure presents a system that uses Large Language Models
(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses
prompt engineering techniques to maximize the chances of the LLM generating
idiomatic and safe Rust code. Moreover, because LLMs often generate code with
subtle bugs that can be missed under traditional unit or fuzz testing,
RustAssure performs differential symbolic testing to establish the semantic
similarity between the original C and LLM-transpiled Rust code. We evaluated
RustAssure with five real-world applications and libraries, and showed that our
system is able to generate compilable Rust functions for 89.8% of all C
functions, of which 69.9% produced equivalent symbolic return values for both
the C and Rust functions.

</details>


### [4] [AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?](https://arxiv.org/abs/2510.07740)
*Dezhi Ran,Yuan Cao,Mengzhou Wu,Simin Chen,Yuzhe Guo,Jun Ren,Zihe Song,Hao Yu,Jialei Wei,Linyi Li,Wei Yang,Baishakhi Ray,Tao Xie*

Main category: cs.SE

TL;DR: APPFORGE是一个评估LLMs构建完整软件系统能力的基准测试，包含101个真实Android应用开发问题。测试显示当前LLMs在复杂系统开发方面表现不佳，最佳模型仅能开发18.8%功能正确的应用。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估函数级代码生成，但无法评估LLMs构建完整软件系统的能力。真实应用开发需要协调多个组件、管理状态生命周期和异步操作，现有基准无法满足这一需求。

Method: 构建APPFORGE基准：1) 从真实Android应用中提取101个开发问题；2) 使用多智能体系统自动总结应用功能并合成测试用例；3) 经过Android专家手动验证；4) 建立自动化评估框架。

Result: 评估12个主流LLMs，所有模型表现均不佳。最佳模型GPT-5仅能开发18.8%功能正确的应用，表明当前模型在处理复杂多组件软件工程挑战方面存在根本性局限。

Conclusion: 当前LLMs在构建完整软件系统方面能力有限，需要进一步研究来提升模型在复杂软件工程任务中的表现。APPFORGE为未来研究提供了可复现的评估基准。

Abstract: Large language models (LLMs) have demonstrated remarkable capability in
function-level code generation tasks. Unlike isolated functions, real-world
applications demand reasoning over the entire software system: developers must
orchestrate how different components interact, maintain consistency across
states over time, and ensure the application behaves correctly within the
lifecycle and framework constraints. Yet, no existing benchmark adequately
evaluates whether LLMs can bridge this gap and construct entire software
systems from scratch. To address this gap, we propose APPFORGE, a benchmark
consisting of 101 software development problems drawn from real-world Android
apps. Given a natural language specification detailing the app functionality, a
language model is tasked with implementing the functionality into an Android
app from scratch. Developing an Android app from scratch requires understanding
and coordinating app states, lifecycle management, and asynchronous operations,
calling for LLMs to generate context-aware, robust, and maintainable code. To
construct APPFORGE, we design a multi-agent system to automatically summarize
the main functionalities from app documents and navigate the app to synthesize
test cases validating the functional correctness of app implementation.
Following rigorous manual verification by Android development experts, APPFORGE
incorporates the test cases within an automated evaluation framework that
enables reproducible assessment without human intervention, making it easily
adoptable for future research. Our evaluation on 12 flagship LLMs show that all
evaluated models achieve low effectiveness, with the best-performing model
(GPT-5) developing only 18.8% functionally correct applications, highlighting
fundamental limitations in current models' ability to handle complex,
multi-component software engineering challenges.

</details>


### [5] [Interleaved Learning and Exploration: A Self-Adaptive Fuzz Testing Framework for MLIR](https://arxiv.org/abs/2510.07815)
*Zeyu Sun,Jingjing Liang,Weiyi Wang,Chenyao Suo,Junjie Chen,Fanjiang Xu*

Main category: cs.SE

TL;DR: FLEX是一个基于神经网络的MLIR自适应模糊测试框架，通过扰动采样和反馈驱动的增强循环，能够自主生成高质量测试用例，显著提升MLIR的bug检测能力。


<details>
  <summary>Details</summary>
Motivation: MLIR作为现代编译器框架的基础技术，其正确性和鲁棒性验证面临挑战。现有的基于模板或规则突变的模糊测试方法难以生成足够多样且语义有效的测试用例，无法有效检测MLIR复杂代码空间中的深层bug。

Method: FLEX采用神经网络进行程序生成，结合扰动采样策略促进多样性，并通过反馈驱动的增强循环迭代改进模型，利用崩溃和非崩溃测试案例来学习有效语法和语义。

Result: 在30天测试中，FLEX发现了80个未知bug（包括多个新根本原因和解析器bug）；在24小时固定版本比较中，检测到53个bug（比最佳基线多3.5倍），代码覆盖率达到28.2%，比次优工具高42%。

Conclusion: FLEX通过神经程序生成和自适应学习机制，显著提升了MLIR模糊测试的效果，消融研究证实了扰动生成和多样性增强在框架有效性中的关键作用。

Abstract: MLIR (Multi-Level Intermediate Representation) has rapidly become a
foundational technology for modern compiler frameworks, enabling extensibility
across diverse domains. However, ensuring the correctness and robustness of
MLIR itself remains challenging. Existing fuzzing approaches-based on manually
crafted templates or rule-based mutations-struggle to generate sufficiently
diverse and semantically valid test cases, making it difficult to expose subtle
or deep-seated bugs within MLIR's complex and evolving code space. In this
paper, we present FLEX, a novel self-adaptive fuzzing framework for MLIR. FLEX
leverages neural networks for program generation, a perturbed sampling strategy
to encourage diversity, and a feedback-driven augmentation loop that
iteratively improves its model using both crashing and non-crashing test cases.
Starting from a limited seed corpus, FLEX progressively learns valid syntax and
semantics and autonomously produces high-quality test inputs. We evaluate FLEX
on the upstream MLIR compiler against four state-of-the-art fuzzers. In a
30-day campaign, FLEX discovers 80 previously unknown bugs-including multiple
new root causes and parser bugs-while in 24-hour fixed-revision comparisons, it
detects 53 bugs (over 3.5x as many as the best baseline) and achieves 28.2%
code coverage, outperforming the next-best tool by 42%. Ablation studies
further confirm the critical role of both perturbed generation and diversity
augmentation in FLEX's effectiveness.

</details>


### [6] [Bug Histories as Sources of Compiler Fuzzing Mutators](https://arxiv.org/abs/2510.07834)
*Lingjun Liu,Feiran Qin,Owolabi Legunsen,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: IssueMut从编译器bug历史中提取变异器，用于改进编译器模糊测试，在GCC和LLVM中发现了65个新bug。


<details>
  <summary>Details</summary>
Motivation: 编译器bug影响重大，现有变异模糊测试器缺乏利用bug历史作为变异器来源的方法。

Method: 自动从bug报告中挖掘变异器，并将其集成到现有变异编译器模糊测试器中。

Result: 从1760个GCC和LLVM bug报告中挖掘587个变异器，发现28个GCC新bug和37个LLVM新bug，其中60个被确认或修复。

Conclusion: bug历史包含丰富信息，编译器模糊测试器应充分利用这些信息来提高bug检测效果。

Abstract: Bugs in compilers, which are critical infrastructure today, can have outsized
negative impacts. Mutational fuzzers aid compiler bug detection by
systematically mutating compiler inputs, i.e., programs. Their effectiveness
depends on the quality of the mutators used. Yet, no prior work used compiler
bug histories as a source of mutators. We propose IssueMut, the first approach
for extracting compiler fuzzing mutators from bug histories. Our insight is
that bug reports contain hints about program elements that induced compiler
bugs; they can guide fuzzers towards similar bugs. IssueMut uses an automated
method to mine mutators from bug reports and retrofit such mutators into
existing mutational compiler fuzzers. Using IssueMut, we mine 587 mutators from
1760 GCC and LLVM bug reports. Then, we run IssueMut on these compilers, with
all their test inputs as seed corpora. We find that "bug history" mutators are
effective: they find new bugs that a state-of-the-art mutational compiler
fuzzer misses-28 in GCC and 37 in LLVM. Of these, 60 were confirmed or fixed,
validating our idea that bug histories have rich information that compiler
fuzzers should leverage.

</details>


### [7] [An AUTOSAR-Aligned Architectural Study of Vulnerabilities in Automotive SoC Software](https://arxiv.org/abs/2510.07941)
*Srijita Basu,Haraldsson Bengt,Miroslaw Staron,Christian Berger,Jennifer Horkoff,Magnus Almgren*

Main category: cs.SE

TL;DR: 分析180个公开报告的汽车SoC漏洞，识别16个根本原因和56个受影响软件模块，研究AUTOSAR对齐架构中的漏洞模式和缓解延迟，为汽车CPS平台安全提供改进策略。


<details>
  <summary>Details</summary>
Motivation: 汽车SoC软件架构在实时安全关键环境中存在安全挑战，缺乏对AUTOSAR对齐架构中SoC漏洞根源和影响的系统性分析。

Method: 分析180个公开报告的汽车SoC漏洞，映射到符合AUTOSAR分层抽象和服务导向原则的代表性SoC软件架构模型。

Result: 识别16个根本原因和56个受影响软件模块，发现主导漏洞模式和补丁延迟较长的关键模块。

Conclusion: 为基于SoC的车辆平台提供了改进检测、优先级排序和定位策略的可操作见解，以增强汽车CPS平台安全性。

Abstract: Cooperative, Connected and Automated Mobility (CCAM) are complex
cyber-physical systems (CPS) that integrate computation, communication, and
control in safety-critical environments. At their core, System-on-Chip (SoC)
platforms consolidate processing units, communication interfaces, AI
accelerators, and security modules into a single chip. AUTOSAR (AUTomotive Open
System ARchitecture) standard was developed in the automotive domain to better
manage this complexity, defining layered software structures and interfaces to
facilitate reuse of HW/SW components. However, in practice, this integrated SoC
software architecture still poses security challenges, particularly in
real-time, safety-critical environments. Recent reports highlight a surge in
SoC-related vulnerabilities, yet systematic analysis of their root causes and
impact within AUTOSAR-aligned architectures is lacking. This study fills that
gap by analyzing 180 publicly reported automotive SoC vulnerabilities, mapped
to a representative SoC software architecture model that is aligned with
AUTOSAR principles for layered abstraction and service orientation. We identify
16 root causes and 56 affected software modules, and examine mitigation delays
across Common Weakness Enumeration (CWE) categories and architectural layers.
We uncover dominant vulnerability patterns and critical modules with prolonged
patch delays, and provide actionable insights for securing automotive CPS
platforms, including guides for improved detection, prioritization, and
localization strategies for SoC software architectures in SoC-based vehicle
platforms.

</details>


### [8] [Past, Present, and Future of Bug Tracking in the Generative AI Era](https://arxiv.org/abs/2510.08005)
*Utku Boran Torun,Mehmet Taha Demircan,Mahmut Furkan Gön,Eray Tüzün*

Main category: cs.SE

TL;DR: 提出基于LLM的AI驱动bug追踪框架，通过自动化减少修复时间和人工开销，从自然语言报告到自动分类、复现、定位和补丁生成。


<details>
  <summary>Details</summary>
Motivation: 传统bug追踪系统依赖人工报告、复现、分类和解决，涉及多方协调，沟通成本高且响应缓慢，影响用户体验和修复效率。

Method: 构建AI驱动的bug追踪框架，用户用自然语言报告问题，AI代理精炼报告、尝试复现、请求补充信息，自动分类并处理无效报告，对有效问题进行定位并分配给开发者，LLM生成候选补丁并由人工监督确保正确性。

Result: 该框架将自动化集成到bug追踪的每个阶段，加速响应时间，改善协作，增强软件维护实践。

Conclusion: AI驱动的bug追踪框架能够显著提升bug处理效率，实现更高效、以用户为中心的软件维护未来。

Abstract: Traditional bug tracking systems rely heavily on manual reporting,
reproduction, triaging, and resolution, each carried out by different
stakeholders such as end users, customer support, developers, and testers. This
division of responsibilities requires significant coordination and widens the
communication gap between non-technical users and technical teams, slowing the
process from bug discovery to resolution. Moreover, current systems are highly
asynchronous; users often wait hours or days for a first response, delaying
fixes and contributing to frustration. This paper examines the evolution of bug
tracking, from early paper-based reporting to today's web-based and SaaS
platforms. Building on this trajectory, we propose an AI-powered bug tracking
framework that augments existing tools with intelligent, large language model
(LLM)-driven automation. Our framework addresses two main challenges: reducing
time-to-fix and minimizing human overhead. Users report issues in natural
language, while AI agents refine reports, attempt reproduction, and request
missing details. Reports are then classified, invalid ones resolved through
no-code fixes, and valid ones localized and assigned to developers. LLMs also
generate candidate patches, with human oversight ensuring correctness. By
integrating automation into each phase, our framework accelerates response
times, improves collaboration, and strengthens software maintenance practices
for a more efficient, user-centric future.

</details>


### [9] [Building Whitespace-Sensitive Languages Using Whitespace-Insensitive Components](https://arxiv.org/abs/2510.08200)
*Alexander Hellwig,Nico Jansen,Bernhard Rumpe*

Main category: cs.SE

TL;DR: 提出了一种通过预处理语言构件来构建空格敏感语言的技术，使用模块化的空格不敏感语言模块，旨在提高语言组件的可重用性。


<details>
  <summary>Details</summary>
Motivation: 软件语言工程中，模块化语言组件的可重用性受到空格敏感和空格不敏感语言集成差距的严重限制，导致库无法重用，空格敏感语言需要从头开发。

Method: 在解析前预处理语言构件，使用模块化的空格不敏感语言模块来构建空格敏感语言，通过重构简化版Python进行评估。

Result: 成功展示了该技术能够提高现有语言组件的可重用性。

Conclusion: 该方法能够减少开发时间并提高软件语言的整体质量，通过解决空格敏感和空格不敏感语言之间的集成问题来促进语言组件的重用。

Abstract: In Software Language Engineering, there is a trend towards reusability by
composing modular language components. However, this reusability is severely
inhibited by a gap in integrating whitespace-sensitive and
whitespace-insensitive languages. There is currently no consistent procedure
for seamlessly reusing such language components in both cases, such that
libraries often cannot be reused, and whitespacesensitive languages are
developed from scratch. This paper presents a technique for using modular,
whitespaceinsensitive language modules to construct whitespace sensitive
languages by pre-processing language artifacts before parsing. The approach is
evaluated by reconstructing a simplified version of the programming language
Python. Our solution aims to increase the reusability of existing language
components to reduce development time and increase the overall quality of
software languages.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [10] [PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware Targeted Circuit PatcHing](https://arxiv.org/abs/2510.07452)
*Anthony Hughes,Vasisht Duddu,N. Asokan,Nikolaos Aletras,Ning Ma*

Main category: cs.CR

TL;DR: 提出PATCH方法，通过识别和编辑语言模型中负责PII泄漏的计算电路，在保持模型效用的同时显著降低个人身份信息泄漏风险。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制如差分隐私虽然能减少PII泄漏，但会导致模型效用大幅下降。研究发现语言模型中存在特定的PII泄漏电路，这为针对性防御提供了可能。

Method: 使用电路发现技术识别语言模型中负责PII泄漏的计算电路，然后直接编辑这些电路来减少泄漏。PATCH还可与差分隐私结合使用。

Result: PATCH将PII泄漏召回率降低高达65%，与差分隐私结合后可将剩余泄漏降至0.01%。相比现有方法，PATCH实现了更好的隐私-效用权衡。

Conclusion: PII泄漏电路在现有防御机制下仍然存在，而PATCH能有效减轻其影响，为语言模型隐私保护提供了更优的解决方案。

Abstract: Language models (LMs) may memorize personally identifiable information (PII)
from training data, enabling adversaries to extract it during inference.
Existing defense mechanisms such as differential privacy (DP) reduce this
leakage, but incur large drops in utility. Based on a comprehensive study using
circuit discovery to identify the computational circuits responsible PII
leakage in LMs, we hypothesize that specific PII leakage circuits in LMs should
be responsible for this behavior. Therefore, we propose PATCH (Privacy-Aware
Targeted Circuit PatcHing), a novel approach that first identifies and
subsequently directly edits PII circuits to reduce leakage. PATCH achieves
better privacy-utility trade-off than existing defenses, e.g., reducing recall
of PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to
reduce recall of residual leakage of an LM to as low as 0.01%. Our analysis
shows that PII leakage circuits persist even after the application of existing
defense mechanisms. In contrast, PATCH can effectively mitigate their impact.

</details>


### [11] [Comparison of Fully Homomorphic Encryption and Garbled Circuit Techniques in Privacy-Preserving Machine Learning Inference](https://arxiv.org/abs/2510.07457)
*Kalyan Cheerla,Lotfi Ben Othmane,Kirill Morozov*

Main category: cs.CR

TL;DR: 比较全同态加密(FHE)和混淆电路(GC)在安全神经网络推理中的性能表现，发现GC执行更快、内存消耗更低，而FHE支持非交互式推理


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在医疗、金融等敏感领域的应用，数据隐私和模型保密性日益重要，需要隐私保护机器学习技术

Method: 使用微软SEAL库的CKKS方案实现FHE，使用IntelLabs的TinyGarble2.0框架实现GC，在两层神经网络上进行比较评估

Result: GC在执行速度和内存消耗方面表现更好，而FHE支持非交互式推理，两者存在权衡关系

Conclusion: FHE和GC在安全神经网络推理中各具优势，需要根据具体应用场景选择合适的隐私保护技术

Abstract: Machine Learning (ML) is making its way into fields such as healthcare,
finance, and Natural Language Processing (NLP), and concerns over data privacy
and model confidentiality continue to grow. Privacy-preserving Machine Learning
(PPML) addresses this challenge by enabling inference on private data without
revealing sensitive inputs or proprietary models. Leveraging Secure Computation
techniques from Cryptography, two widely studied approaches in this domain are
Fully Homomorphic Encryption (FHE) and Garbled Circuits (GC). This work
presents a comparative evaluation of FHE and GC for secure neural network
inference. A two-layer neural network (NN) was implemented using the CKKS
scheme from the Microsoft SEAL library (FHE) and the TinyGarble2.0 framework
(GC) by IntelLabs. Both implementations are evaluated under the semi-honest
threat model, measuring inference output error, round-trip time, peak memory
usage, communication overhead, and communication rounds. Results reveal a
trade-off: modular GC offers faster execution and lower memory consumption,
while FHE supports non-interactive inference.

</details>


### [12] [A Secure Authentication-Driven Protected Data Collection Protocol in Internet of Things](https://arxiv.org/abs/2510.07462)
*Maryam Ataei Nezhad,Hamid Barati,Ali Barati*

Main category: cs.CR

TL;DR: 本文提出了一种三阶段的安全通信方法，用于保护物联网中的隐私和信息安全，通过星型结构、多跳加密和认证协议来提高安全性。


<details>
  <summary>Details</summary>
Motivation: 物联网的快速发展带来了隐私和信息安全的严重挑战，黑客窃取用户信息的问题阻碍了物联网的普及，需要有效的安全解决方案。

Method: 采用三阶段方法：1) 在集群内构建星型结构并共享唯一密钥；2) 多跳方式传输数据，每跳使用不同密钥加密并更新密钥；3) 使用认证协议确保集群间通信安全，防止恶意节点。

Result: 使用NS2软件模拟显示，该方法在能耗、端到端延迟、灵活性、数据包投递率和存活节点数量方面均优于其他方法。

Conclusion: 提出的三阶段安全通信方法能有效提高物联网系统的安全性，在多个性能指标上都有显著改善。

Abstract: Internet of Things means connecting different devices through the Internet.
The Internet of things enables humans to remotely manage and control the
objects they use with the Internet infrastructure. After the advent of the
Internet of Things in homes, organizations, and private companies, privacy and
information security are the biggest concern. This issue has challenged the
spread of the Internet of things as news of the users theft of information by
hackers intensified. The proposed method in this paper consists of three
phases. In the first phase, a star structure is constructed within each
cluster, and a unique key is shared between each child and parent to encrypt
and secure subsequent communications. The second phase is for intracluster
communications, in which members of the cluster send their data to the cluster
head in a multi hop manner. Also, in this phase, the data is encrypted with
different keys in each hop, and at the end of each connection, the keys are
updated to ensure data security. The third phase is to improve the security of
inter cluster communications using an authentication protocol. In this way, the
cluster heads are authenticated before sending information to prevent malicious
nodes in the network. The proposed method is also simulated using NS2 software.
The results showed that the proposed method has improved in terms of energy
consumption, end-to-end delay, flexibility, packet delivery rate, and the
number of alive nodes compared to other methods.

</details>


### [13] [MIRANDA: short signatures from a leakage-free full-domain-hash scheme](https://arxiv.org/abs/2510.07479)
*Alain Couvreur,Thomas Debris-Alazard,Philippe Gaborit,Adrien Vinçotte*

Main category: cs.CR

TL;DR: Miranda是基于矩阵码的首个全域哈希签名方案，采用GPV范式提供强安全性保证。该方案使用简单的陷门结构，无需拒绝采样，签名大小仅90字节，公钥约2.6MB。


<details>
  <summary>Details</summary>
Motivation: 开发一个基于矩阵码的GPV范式签名方案，避免复杂的拒绝采样过程，简化实现难度，同时保持强安全性。

Method: 使用Gabidulin码作为矩阵空间，基于可解码码的子码构造陷门，通过少量均匀比特的抽取确保签名不泄露陷门信息。

Result: 在128位经典安全级别下，签名大小仅90字节，公钥大小约2.6MB，实现了高效的签名方案。

Conclusion: Miranda是首个基于矩阵码的GPV签名方案，具有简单的陷门结构和实现，在保持安全性的同时提供了紧凑的签名尺寸。

Abstract: We present $\mathsf{Miranda}$, the first family of full-domain-hash
signatures based on matrix codes. This signature scheme fulfils the paradigm of
Gentry, Peikert and Vaikuntanathan ($\mathsf{GPV}$), which gives strong
security guarantees. Our trapdoor is very simple and generic: if we propose it
with matrix codes, it can actually be instantiated in many other ways since it
only involves a subcode of a decodable code (or lattice) in a unique decoding
regime of parameters. Though $\mathsf{Miranda}$ signing algorithm relies on a
decoding task where there is exactly one solution, there are many possible
signatures given a message to sign and we ensure that signatures are not
leaking information on their underlying trapdoor by means of a very simple
procedure involving the drawing of a small number of uniform bits. In
particular $\mathsf{Miranda}$ does not use a rejection sampling procedure which
makes its implementation a very simple task contrary to other
$\mathsf{GPV}$-like signatures schemes such as $\mathsf{Falcon}$ or even
$\mathsf{Wave}$.
  We instantiate $\mathsf{Miranda}$ with the famous family of Gabidulin codes
represented as spaces of matrices and we study thoroughly its security (in the
EUF-CMA security model). For~$128$ bits of classical security, the signature
sizes are as low as~$90$ bytes and the public key sizes are in the order
of~$2.6$ megabytes.

</details>


### [14] [EMPalm: Exfiltrating Palm Biometric Data via Electromagnetic Side-Channels](https://arxiv.org/abs/2510.07533)
*Haowen Xu,Tianya Zhao,Xuyu Wang,Lei Ma,Jun Dai,Alexander Wyglinski,Xiaoyan Sun*

Main category: cs.CR

TL;DR: EMPalm攻击框架通过窃听电磁信号，能够隐蔽地恢复手掌生物特征图像（掌纹和掌静脉），并在实际手掌识别系统中实现65.30%的平均欺骗成功率。


<details>
  <summary>Details</summary>
Motivation: 手掌识别系统在运行时会无意中发射电磁信号，研究发现这些电磁辐射会泄露手掌生物特征信息，因此开发EMPalm攻击框架来利用这一漏洞。

Method: 首先分离两种模态的交错传输，识别并组合它们的有效频带，然后重建图像。为了进一步提高保真度，使用扩散模型来恢复每个领域特有的细粒度生物特征。

Result: 在7个原型和2个商用设备上评估，EMPalm能够以高视觉保真度恢复手掌生物特征，SSIM达0.79，PSNR达29.88 dB，FID低至6.82。在4个最先进的手掌识别模型上，对100个不同用户的6000个样本实现了65.30%的平均欺骗成功率。

Conclusion: 手掌识别系统的电磁信号泄露构成了严重的安全威胁，EMPalm攻击框架证明了从这些信号中恢复手掌生物特征的可行性，对生物识别系统的安全性提出了重要警示。

Abstract: Palm recognition has emerged as a dominant biometric authentication
technology in critical infrastructure. These systems operate in either
single-modal form, using palmprint or palmvein individually, or dual-modal
form, fusing the two modalities. Despite this diversity, they share similar
hardware architectures that inadvertently emit electromagnetic (EM) signals
during operation. Our research reveals that these EM emissions leak palm
biometric information, motivating us to develop EMPalm--an attack framework
that covertly recovers both palmprint and palmvein images from eavesdropped EM
signals. Specifically, we first separate the interleaved transmissions of the
two modalities, identify and combine their informative frequency bands, and
reconstruct the images. To further enhance fidelity, we employ a diffusion
model to restore fine-grained biometric features unique to each domain.
Evaluations on seven prototype and two commercial palm acquisition devices show
that EMPalm can recover palm biometric information with high visual fidelity,
achieving SSIM scores up to 0.79, PSNR up to 29.88 dB, and FID scores as low as
6.82 across all tested devices, metrics that collectively demonstrate strong
structural similarity, high signal quality, and low perceptual discrepancy. To
assess the practical implications of the attack, we further evaluate it against
four state-of-the-art palm recognition models, achieving a model-wise average
spoofing success rate of 65.30% over 6,000 samples from 100 distinct users.

</details>


### [15] [A Minrank-based Encryption Scheme à la Alekhnovich-Regev](https://arxiv.org/abs/2510.07584)
*Thomas Debris-Alazard,Philippe Gaborit,Romaric Neveu,Olivier Ruatta*

Main category: cs.CR

TL;DR: 本文提出了基于stationary-MinRank问题的公钥加密方案，这是首个仅依赖MinRank问题难度的加密方案，具有强大的安全保证。


<details>
  <summary>Details</summary>
Motivation: Alekhnovich和Regev方案是首个仅基于随机线性码解码和LWE平均难度的公钥加密方案，但缺乏仅基于MinRank问题的加密方案。本文旨在构建仅依赖MinRank问题难度的加密方案。

Method: 通过证明stationary-MinRank问题具有搜索到决策的归约特性，对Alekhnovich和Regev方案进行适配，构建基于stationary-MinRank问题的加密方案。

Result: 成功构建了仅依赖stationary-MinRank问题难度的加密方案，性能优于Alekhnovich和Regev原始方案，略低于FrodoKEM，但具有改进潜力。

Conclusion: 该方案为构建仅基于MinRank问题难度的加密方案提供了部分答案，具有实际应用价值，且可通过增加结构进一步优化性能。

Abstract: Introduced in 2003 and 2005, Alekhnovich and Regev' schemes were the first
public-key encryptions whose security is only based on the average hardness of
decoding random linear codes and LWE, without other security assumptions. Such
security guarantees made them very popular, being at the origin of the now
standardized HQC or Kyber.
  We present an adaptation of Alekhnovich and Regev' encryption scheme whose
security is only based on the hardness of a slight variation of MinRank, the
so-called stationary-MinRank problem. We succeeded to reach this strong
security guarantee by showing that stationary-MinRank benefits from a
search-to-decision reduction. Our scheme therefore brings a partial answer to
the long-standing open question of building an encryption scheme whose security
relies solely on the hardness of MinRank.
  Finally, we show after a thoroughly security analysis that our scheme is
practical and competitive with other encryption schemes admitting such strong
security guarantees. Our scheme is slightly less efficient than FrodoKEM, but
much more efficient than Alekhnovich and Regev' original schemes, with
possibilities of improvements by considering more structure, in the same way as
HQC and Kyber.

</details>


### [16] [Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs](https://arxiv.org/abs/2510.07697)
*Man Hu,Xinyi Wu,Zuofeng Suo,Jinbo Feng,Linghui Meng,Yanhao Jia,Anh Tuan Luu,Shuai Zhao*

Main category: cs.CR

TL;DR: 本文首次全面综述了针对大语言模型推理能力的后门攻击，提出了新的分类法将推理后门攻击分为关联型、被动型和主动型，并讨论了相应的防御策略和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型推理能力的提升，虽然提高了下游任务性能，但也带来了新的安全风险，攻击者可能利用推理能力进行后门攻击。现有调查缺乏对针对LLM推理能力的后门攻击和防御的深入分析。

Method: 提出了一个新的分类法，从统一视角总结现有方法，将推理后门攻击分为三类：关联型、被动型和主动型，并分析了其底层机制和方法框架。

Result: 建立了针对LLM推理能力的后门攻击的全面分析框架，识别了现有方法的共同特征和差异，为理解此类攻击提供了系统视角。

Conclusion: 这项工作为安全可信的LLM社区发展提供了新视角，为后续研究奠定了基础，强调需要进一步探索针对推理能力的后门攻击防御机制。

Abstract: With the rise of advanced reasoning capabilities, large language models
(LLMs) are receiving increasing attention. However, although reasoning improves
LLMs' performance on downstream tasks, it also introduces new security risks,
as adversaries can exploit these capabilities to conduct backdoor attacks.
Existing surveys on backdoor attacks and reasoning security offer comprehensive
overviews but lack in-depth analysis of backdoor attacks and defenses targeting
LLMs' reasoning abilities. In this paper, we take the first step toward
providing a comprehensive review of reasoning-based backdoor attacks in LLMs by
analyzing their underlying mechanisms, methodological frameworks, and
unresolved challenges. Specifically, we introduce a new taxonomy that offers a
unified perspective for summarizing existing approaches, categorizing
reasoning-based backdoor attacks into associative, passive, and active. We also
present defense strategies against such attacks and discuss current challenges
alongside potential directions for future research. This work offers a novel
perspective, paving the way for further exploration of secure and trustworthy
LLM communities.

</details>


### [17] [ANCORA: Accurate Intrusion Recovery for Web Applications](https://arxiv.org/abs/2510.07806)
*Yihao Peng,Biao Ma,Hai Wan,Xibin Zhao*

Main category: cs.CR

TL;DR: ANCORA是一个精确的Web应用入侵恢复系统，通过隔离恶意请求的完整系统调用序列，分别追踪文件修改和数据库操作，实现选择性回滚和重放，完全移除攻击影响同时保留合法数据。


<details>
  <summary>Details</summary>
Motivation: 现代Web应用恢复面临关键困境：粗粒度快照回滚会导致合法用户数据丢失，而精确移除攻击影响在高并发环境中难以实现，因为难以将文件和数据库修改归因于特定攻击请求。

Method: ANCORA首先隔离恶意请求触发的完整系统调用序列。对于文件修改，构建溯源图揭示所有修改（包括漏洞利用产生的进程）。对于数据库操作，引入新颖的时空锚点，使用请求的网络连接元组和活动时间窗口精确定位数据库操作。最后执行统一回滚和选择性重放恢复。

Result: 在10个Web应用和20个基于CVE的攻击场景中评估，并发连接数达150。ANCORA实现99.9%的恢复准确率，最坏情况下响应延迟增加19.8%、QPS降低17.8%，恢复吞吐量为110.7个数据库操作/秒和27.2个受影响文件/秒。

Conclusion: ANCORA能够精确识别恶意文件和数据库操作，通过回滚到干净快照并选择性重放合法操作，完全移除攻击影响同时有效保留合法数据，在可管理开销下实现高精度恢复。

Abstract: Modern web application recovery presents a critical dilemma. Coarse-grained
snapshot rollbacks cause unacceptable data loss for legitimate users.
Surgically removing an attack's impact is hindered by a fundamental challenge
in high-concurrency environments: it is difficult to attribute resulting file
and database modifications to a specific attack-related request. We present
ANCORA, a system for precise intrusion recovery in web applications without
invasive instrumentation. ANCORA first isolates the full sequence of syscalls
triggered by a single malicious request. Based on this sequence, ANCORA
addresses file and database modifications separately. To trace file changes, it
builds a provenance graph that reveals all modifications, including those by
exploit-spawned processes. To attribute database operations, a more difficult
challenge due to connection pooling, ANCORA introduces a novel spatiotemporal
anchor. This anchor uses the request's network connection tuple and active time
window to pinpoint exact database operations. With all malicious file and
database operations precisely identified, ANCORA performs a unified rewind and
selective replay recovery. It reverts the system to a clean snapshot taken
before the attack, then selectively re-applies only legitimate operations to
both the file system and database. This completely removes the attack's effects
while preserving concurrent legitimate data. We evaluated ANCORA on 10 web
applications and 20 CVE-based attack scenarios with concurrency up to 150
connections. Experiments demonstrate ANCORA achieves 99.9% recovery accuracy
with manageable overhead: up to 19.8% response latency increase and 17.8% QPS
decrease in worst cases, and recovery throughput of 110.7 database operations
per second and 27.2 affected files per second, effectively preserving
legitimate data.

</details>


### [18] [Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents](https://arxiv.org/abs/2510.07809)
*Renhua Ding,Xiao Yang,Zhengwei Fang,Jun Luo,Kun He,Jun Zhu*

Main category: cs.CR

TL;DR: 提出了一种针对大型视觉语言模型驱动的移动代理的实用隐蔽一次性越狱攻击，通过应用内提示注入实现，利用ADB操作时暴露的恶意UI文本触发攻击。


<details>
  <summary>Details</summary>
Motivation: 现有UI级攻击依赖显眼的UI覆盖层、高权限或不切实际的威胁模型，缺乏隐蔽性和实际应用性，需要研究更实用的攻击方法。

Method: 框架包含三个关键组件：低权限感知链目标定位、隐蔽的用户不可见激活机制、以及一次性提示有效性算法(HG-IDA*)，通过字符级迭代深化搜索实现关键词级去毒化。

Result: 在多个LVLM后端评估中，单次攻击场景下观察到高规划(82.5%)和执行(75.0%)劫持成功率，特别是在GPT-4o模型上表现突出。

Conclusion: 这些发现揭示了当前移动代理存在基本安全漏洞，对自主智能手机操作具有直接的安全影响。

Abstract: Large vision-language models (LVLMs) enable autonomous mobile agents to
operate smartphone user interfaces, yet vulnerabilities to UI-level attacks
remain critically understudied. Existing research often depends on conspicuous
UI overlays, elevated permissions, or impractical threat models, limiting
stealth and real-world applicability. In this paper, we present a practical and
stealthy one-shot jailbreak attack that leverages in-app prompt injections:
malicious applications embed short prompts in UI text that remain inert during
human interaction but are revealed when an agent drives the UI via ADB (Android
Debug Bridge). Our framework comprises three crucial components: (1)
low-privilege perception-chain targeting, which injects payloads into malicious
apps as the agent's visual inputs; (2) stealthy user-invisible activation, a
touch-based trigger that discriminates agent from human touches using physical
touch attributes and exposes the payload only during agent operation; and (3)
one-shot prompt efficacy, a heuristic-guided, character-level
iterative-deepening search algorithm (HG-IDA*) that performs one-shot,
keyword-level detoxification to evade on-device safety filters. We evaluate
across multiple LVLM backends, including closed-source services and
representative open-source models within three Android applications, and we
observe high planning and execution hijack rates in single-shot scenarios
(e.g., GPT-4o: 82.5% planning / 75.0% execution). These findings expose a
fundamental security vulnerability in current mobile agents with immediate
implications for autonomous smartphone operation.

</details>


### [19] [Decentralised Blockchain Management Through Digital Twins](https://arxiv.org/abs/2510.07901)
*Georgios Diamantopoulos,Nikos Tziritas,Rami Bahsoon,Georgios Theodoropoulos*

Main category: cs.CR

TL;DR: 提出基于数字孪生的动态去中心化区块链管理机制，通过二级区块链系统实现去中心化决策，在保持去中心化的同时实现动态管理。


<details>
  <summary>Details</summary>
Motivation: 当前区块链治理方案在控制与去中心化之间存在权衡，需要一种既能保持去中心化又能实现动态管理的解决方案。

Method: 利用多个由利益相关者控制的数字孪生，组织在二级区块链系统中进行去中心化决策，并将决策传播到被管理的区块链。

Result: 模拟评估显示该机制能快速达成决策共识，并以最小开销重新配置主区块链。

Conclusion: 该机制成功实现了在保持去中心化的前提下对区块链系统进行动态管理，解决了控制与去中心化之间的权衡问题。

Abstract: The necessity of blockchain systems to remain decentralised limits current
solutions to blockchain governance and dynamic management, forcing a trade-off
between control and decentralisation. In light of the above, this work proposes
a dynamic and decentralised blockchain management mechanism based on digital
twins. To ensure decentralisation, the proposed mechanism utilises multiple
digital twins that the system's stakeholders control. To facilitate
decentralised decision-making, the twins are organised in a secondary
blockchain system that orchestrates agreement on, and propagation of decisions
to the managed blockchain. This enables the management of blockchain systems
without centralised control. A preliminary evaluation of the performance and
impact of the overheads introduced by the proposed mechanism is conducted
through simulation. The results demonstrate the proposed mechanism's ability to
reach consensus on decisions quickly and reconfigure the primary blockchain
with minimal overhead.

</details>


### [20] [From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses](https://arxiv.org/abs/2510.07968)
*Xiangtao Meng,Tianshuo Cong,Li Wang,Wenyu Chen,Zheng Li,Shanqing Guo,Xiaoyun Wang*

Main category: cs.CR

TL;DR: 本研究提出了CrossRiskEval框架，首次系统评估了LLM防御策略在安全、公平和隐私三个风险维度之间的意外交互作用，揭示了防御措施可能对其他风险维度产生负面影响的严重副作用。


<details>
  <summary>Details</summary>
Motivation: 现有LLM防御策略研究大多孤立评估单一风险，忽视了防御措施在多个风险维度间的复杂交互作用，可能导致部署防御时产生意外负面后果。

Method: 提出CrossRiskEval综合评估框架，对14个部署了12种不同防御策略的LLM进行实证研究，并通过细粒度的神经元级分析揭示其底层机制。

Result: 研究发现：1）安全防御可能抑制对偏倚或隐私相关敏感查询的直接响应，但仍会放大间接隐私泄露或偏倚输出；2）公平防御增加滥用和隐私泄露风险；3）隐私防御通常损害安全性并加剧偏倚。神经元分析揭示了存在冲突纠缠神经元。

Conclusion: 需要向整体性、交互感知的防御策略评估范式转变，以全面理解LLM防御措施在多风险维度间的复杂交互作用。

Abstract: Large Language Models (LLMs) have shown remarkable performance across various
applications, but their deployment in sensitive domains raises significant
concerns. To mitigate these risks, numerous defense strategies have been
proposed. However, most existing studies assess these defenses in isolation,
overlooking their broader impacts across other risk dimensions. In this work,
we take the first step in investigating unintended interactions caused by
defenses in LLMs, focusing on the complex interplay between safety, fairness,
and privacy. Specifically, we propose CrossRiskEval, a comprehensive evaluation
framework to assess whether deploying a defense targeting one risk
inadvertently affects others. Through extensive empirical studies on 14
defense-deployed LLMs, covering 12 distinct defense strategies, we reveal
several alarming side effects: 1) safety defenses may suppress direct responses
to sensitive queries related to bias or privacy, yet still amplify indirect
privacy leakage or biased outputs; 2) fairness defenses increase the risk of
misuse and privacy leakage; 3) privacy defenses often impair safety and
exacerbate bias. We further conduct a fine-grained neuron-level analysis to
uncover the underlying mechanisms of these phenomena. Our analysis reveals the
existence of conflict-entangled neurons in LLMs that exhibit opposing
sensitivities across multiple risk dimensions. Further trend consistency
analysis at both task and neuron levels confirms that these neurons play a key
role in mediating the emergence of unintended behaviors following defense
deployment. We call for a paradigm shift in LLM risk evaluation, toward
holistic, interaction-aware assessment of defense strategies.

</details>


### [21] [Composition Law of Conjugate Observables in Random Permutation Sorting Systems](https://arxiv.org/abs/2510.08013)
*Yurang R. Kuang*

Main category: cs.CR

TL;DR: 发现随机置换排序系统中共轭可观测量的基本组成定律，通过功能关系连接离散置换计数和连续时间，实现熵纯化，将微架构时序波动转化为均匀随机性。


<details>
  <summary>Details</summary>
Motivation: 为从通用计算中生成可证明均匀的随机性提供理论基础，从涌现的计算动力学中确保密码学纯度。

Method: 建立连接时序分布特征函数和置换计数概率生成函数的功能关系框架，通过几何收敛实现熵纯化。

Result: 实验验证获得每字节超过7.9998比特的香农熵，在不同平台上实现卡方均匀性，建立了具有明确边界的收敛定理。

Conclusion: 该组成定律为从通用计算生成可证明均匀随机性提供了通用基础，能够从涌现的计算动力学中确保密码学纯度。

Abstract: We present the discovery of a fundamental composition law governing conjugate
observables in the Random Permutation Sorting System (RPSS). The law links the
discrete permutation count Np and the continuous elapsed time T through a
functional relation connecting the characteristic function of timing
distributions to the probability generating function of permutation counts.
This framework enables entropy purification, transforming microarchitectural
timing fluctuations into uniform randomness via geometric convergence. We
establish convergence theorems with explicit bounds and validate the results
experimentally, achieving Shannon entropy above 7.9998 bits per byte and
chi-square uniformity across diverse platforms. The composition law provides a
universal foundation for generating provably uniform randomness from
general-purpose computation, securing cryptographic purity from emergent
computational dynamics.

</details>


### [22] [A Novel Ensemble Learning Approach for Enhanced IoT Attack Detection: Redefining Security Paradigms in Connected Systems](https://arxiv.org/abs/2510.08084)
*Hikmat A. M. Abdeljaber,Md. Alamgir Hossain,Sultan Ahmad,Ahmed Alsanad,Md Alimul Haque,Sudan Jha,Jabeen Nazeer*

Main category: cs.CR

TL;DR: 提出一种新颖的集成学习架构，结合Extra Trees分类器和超参数优化，在多个IoT数据集上实现高效的攻击检测。


<details>
  <summary>Details</summary>
Motivation: IoT设备的快速扩张带来了严重的安全漏洞，使系统更容易遭受复杂网络攻击，需要更有效的检测方法。

Method: 采用集成学习架构，应用Extra Trees分类器，结合全面的预处理和超参数优化技术。

Result: 在CICIoT2023、IoTID20等多个基准数据集上表现出色，实现了高召回率、准确率和精确度，错误率极低。

Conclusion: 该方法为保护连接设备免受不断演变的网络威胁提供了有效且可扩展的解决方案，为未来研究奠定了坚实基础。

Abstract: The rapid expansion of Internet of Things (IoT) devices has transformed
industries and daily life by enabling widespread connectivity and data
exchange. However, this increased interconnection has introduced serious
security vulnerabilities, making IoT systems more exposed to sophisticated
cyber attacks. This study presents a novel ensemble learning architecture
designed to improve IoT attack detection. The proposed approach applies
advanced machine learning techniques, specifically the Extra Trees Classifier,
along with thorough preprocessing and hyperparameter optimization. It is
evaluated on several benchmark datasets including CICIoT2023, IoTID20,
BotNeTIoT L01, ToN IoT, N BaIoT, and BoT IoT. The results show excellent
performance, achieving high recall, accuracy, and precision with very low error
rates. These outcomes demonstrate the model efficiency and superiority compared
to existing approaches, providing an effective and scalable method for securing
IoT environments. This research establishes a solid foundation for future
progress in protecting connected devices from evolving cyber threats.

</details>


### [23] [LLM-Assisted Web Measurements](https://arxiv.org/abs/2510.08101)
*Simone Bozzolan,Stefano Calzavara,Lorenzo Cazzaro*

Main category: cs.CR

TL;DR: 该论文研究使用大语言模型(LLMs)进行有针对性的网络测量研究，通过LLMs的语义理解能力对网站进行分类，解决了现有网站排名列表缺乏语义信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有流行的网站排名列表缺乏语义标签和分类信息，使得研究人员难以针对特定类别的网站进行测量研究，需要依赖临时技术来偏置数据集。

Method: 基于先前文献确定关键网站分类任务，构建数据集系统评估不同LLMs在这些任务上的性能，然后进行LLM辅助的网络测量研究，并严格评估研究推断的有效性。

Result: LLMs在多个分类场景中表现出色，能够作为分析网络安全和隐私趋势的实用工具。

Conclusion: LLMs可以作为实现有针对性的网络测量研究的实用工具，通过其语义理解能力帮助分析网络安全和隐私趋势。

Abstract: Web measurements are a well-established methodology for assessing the
security and privacy landscape of the Internet. However, existing top lists of
popular websites commonly used as measurement targets are unlabeled and lack
semantic information about the nature of the sites they include. This
limitation makes targeted measurements challenging, as researchers often need
to rely on ad-hoc techniques to bias their datasets toward specific categories
of interest. In this paper, we investigate the use of Large Language Models
(LLMs) as a means to enable targeted web measurement studies through their
semantic understanding capabilities. Building on prior literature, we identify
key website classification tasks relevant to web measurements and construct
datasets to systematically evaluate the performance of different LLMs on these
tasks. Our results demonstrate that LLMs may achieve strong performance across
multiple classification scenarios. We then conduct LLM-assisted web measurement
studies inspired by prior work and rigorously assess the validity of the
resulting research inferences. Our results demonstrate that LLMs can serve as a
practical tool for analyzing security and privacy trends on the Web.

</details>


### [24] [TracE2E: Easily Deployable Middleware for Decentralized Data Traceability](https://arxiv.org/abs/2510.08225)
*Daniel Pressensé,Elisavet Kozyri*

Main category: cs.CR

TL;DR: TracE2E是一个用Rust编写的中间件，提供跨多节点的数据可解释性和合规性，通过包装Rust标准库的IO模块实现轻松集成。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量应用修改，TracE2E旨在通过轻量级集成方式为现有和未来应用提供数据溯源和合规性保障。

Method: 通过中介进程的输入输出来记录溯源信息，并基于记录的溯源信息强制执行数据保护策略（如机密性、完整性）。

Result: TracE2E能够跨节点一致地记录溯源信息，其合规层可以适应多种策略的执行。

Conclusion: TracE2E提供了一种无需大量应用修改的数据溯源和合规性解决方案，具有实际部署的可行性。

Abstract: This paper presents TracE2E, a middleware written in Rust, that can provide
both data explainability and compliance across multiple nodes. By mediating
inputs and outputs of processes, TracE2E records provenance information and
enforces data-protection policies (e.g., confidentiality, integrity) that
depend on the recorded provenance. Unlike existing approaches that necessitate
substantial application modifications, TracE2E is designed for easy integration
into existing and future applications through a wrapper of the Rust standard
library's IO module. We describe how TracE2E consistently records provenance
information across nodes, and we demonstrate how the compliance layer of
TracE2E can accommodate the enforcement of multiple policies.

</details>


### [25] [Systematic Assessment of Cache Timing Vulnerabilities on RISC-V Processors](https://arxiv.org/abs/2510.08272)
*Cédrick Austa,Jan Tobias Mühlberg,Jean-Michel Dricot*

Main category: cs.CR

TL;DR: 将x86-64缓存时序漏洞基准测试移植到RISC-V，并评估三个商用RISC-V处理器（T-Head C910、SiFive U54和U74）的安全性能。


<details>
  <summary>Details</summary>
Motivation: RISC-V架构日益流行，但缺乏评估具体处理器实现安全性的工具，而x86-64和ARM已有专门的微架构侧信道漏洞测试工具。

Method: 将Intel x86-64的缓存时序漏洞基准测试套件移植到RISC-V架构，并使用该基准测试评估三个商用RISC-V处理器。

Result: C910处理器表现出更多不同的时序类型，表明其存在更多微架构漏洞源；37.5%的漏洞在所有处理器中都存在，仅6.8%的漏洞在所有核心中都不存在。

Conclusion: 移植的基准测试有助于RISC-V处理器设计者早期识别泄漏源，并支持开发相应的防护措施。

Abstract: While interest in the open RISC-V instruction set architecture is growing,
tools to assess the security of concrete processor implementations are lacking.
There are dedicated tools and benchmarks for common microarchitectural
side-channel vulnerabilities for popular processor families such as Intel
x86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in
porting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities
to RISC-V. We then use this benchmark to evaluate the security of three
commercially available RISC-V processors, the T-Head C910 and the SiFive U54
and U74 cores. We observe that the C910 processor exhibits more distinct timing
types than the other processors, leading to the assumption that code running on
the C910 would be exposed to more microarchitectural vulnerability sources. In
addition, our evaluation reveals that $37.5\%$ of the vulnerabilities covered
by the benchmark exist in all processors, while only $6.8\%$ are absent from
all cores. Our work, in particular the ported benchmark, aims to support RISC-V
processor designers to identify leakage sources early in their designs and to
support the development of countermeasures.

</details>


### [26] [New Machine Learning Approaches for Intrusion Detection in ADS-B](https://arxiv.org/abs/2510.08333)
*Mikaëla Ngamboé,Jean-Simon Marrocco,Jean-Yves Ouattara,José M. Fernandez,Gabriela Nicolescu*

Main category: cs.CR

TL;DR: 本研究评估了基于Transformer和xLSTM的深度学习入侵检测系统在ADS-B协议安全中的应用，采用迁移学习策略，xLSTM模型在检测性能上表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着航空交通管理对易受攻击的ADS-B协议的依赖增加，确保其安全性变得至关重要，需要改进基于AI的入侵检测系统。

Method: 使用Transformer编码器和扩展LSTM网络两种深度学习模型，采用迁移学习策略：先在良性ADS-B消息上进行预训练，再用包含篡改消息的标记数据进行微调。

Result: xLSTM模型F1分数达98.9%，优于Transformer模型的94.3%，在检测渐进式攻击方面表现更好。xLSTM推理延迟7.26秒，Transformer为2.1秒。

Conclusion: xLSTM模型在检测性能上优于Transformer模型，其延迟在SSR刷新间隔内可接受，但可能对时间关键操作有限制。Transformer延迟更低但检测性能较差。

Abstract: With the growing reliance on the vulnerable Automatic Dependent
Surveillance-Broadcast (ADS-B) protocol in air traffic management (ATM),
ensuring security is critical. This study investigates emerging machine
learning models and training strategies to improve AI-based intrusion detection
systems (IDS) for ADS-B. Focusing on ground-based ATM systems, we evaluate two
deep learning IDS implementations: one using a transformer encoder and the
other an extended Long Short-Term Memory (xLSTM) network, marking the first
xLSTM-based IDS for ADS-B. A transfer learning strategy was employed, involving
pre-training on benign ADS-B messages and fine-tuning with labeled data
containing instances of tampered messages. Results show this approach
outperforms existing methods, particularly in identifying subtle attacks that
progressively undermine situational awareness. The xLSTM-based IDS achieves an
F1-score of 98.9%, surpassing the transformer-based model at 94.3%. Tests on
unseen attacks validated the generalization ability of the xLSTM model.
Inference latency analysis shows that the 7.26-second delay introduced by the
xLSTM-based IDS fits within the Secondary Surveillance Radar (SSR) refresh
interval (5-12 s), although it may be restrictive for time-critical operations.
While the transformer-based IDS achieves a 2.1-second latency, it does so at
the cost of lower detection performance.

</details>


### [27] [A Haskell to FHE Transpiler](https://arxiv.org/abs/2510.08343)
*Anne Müller,Mohd Kashif,Nico Döttling*

Main category: cs.CR

TL;DR: 开发了Haskell到布尔电路的转译器，支持全同态加密，并实现电路自动并行化评估，在PIR和AES应用中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 全同态加密需要将程序表示为布尔或算术电路，但低级表示使应用开发复杂，需要减少高級语言到电路转换的负担。

Method: 为Haskell开发转译器生成布尔电路，并实现电路分层并行化评估器。

Result: 在16线程下评估时间28秒，100线程下8秒，自动并行化方法在某些情况下优于手动并行化。

Conclusion: 成功扩展了支持FHE的高級语言范围，证明了自动并行化在FHE评估中的有效性。

Abstract: Fully Homomorphic Encryption (FHE) enables the evaluation of programs
directly on encrypted data. However, because only basic operations can be
performed on ciphertexts, programs must be expressed as boolean or arithmetic
circuits. This low-level representation makes implementing applications for FHE
significantly more cumbersome than writing code in a high-level language. To
reduce this burden, several transpilers have been developed that translate
high-level code into circuit representations. In this work, we extend the range
of high-level languages that can target FHE by introducing a transpiler for
Haskell, which converts Haskell programs into Boolean circuits suitable for
homomorphic evaluation. Our second contribution is the automatic
parallelization of these generated circuits. We implement an evaluator that
executes gates in parallel by parallelizing each layer of the circuit. We
demonstrate the effectiveness of our approach on two key applications: Private
Information Retrieval (PIR) and the AES encryption standard. Prior work has
parallelized AES encryption manually. We demonstrate that the automated method
outperforms some but not all manual parallelizations of AES evaluations under
FHE. We achieve an evaluation time of 28 seconds for a parallel execution with
16 threads and an evaluation time of 8 seconds for a parallel execution with
100 threads

</details>


### [28] [ExPrESSO: Zero-Knowledge backed Extensive Privacy Preserving Single Sign-on](https://arxiv.org/abs/2510.08355)
*Kaustabh Barman,Fabian Piper,Sanjeet Raj Pandey,Axel Kuepper*

Main category: cs.CR

TL;DR: 提出了一种基于零知识证明的OIDC集成机制，让用户通过SSO认证时无需透露服务提供商信息，保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 标准化的SSO系统（如OIDC）无法保证用户隐私，身份提供商可以追踪用户活动。

Method: 利用Groth的zk-SNARK技术证明用户订阅的服务提供商成员资格而不透露具体身份，采用去中心化和可验证的方法设置系统前提条件。

Result: 实现了高安全目标，同时保持最小的存储和延迟成本，证明该研究可用于生产环境。

Conclusion: 提出的零知识证明机制成功解决了SSO系统中的用户隐私问题，为生产部署提供了可行的解决方案。

Abstract: User authentication is one of the most important aspects for secure
communication between services and end-users over the Internet. Service
providers leverage Single-Sign On (SSO) to make it easier for their users to
authenticate themselves. However, standardized systems for SSO, such as OIDC,
do not guarantee user privacy as identity providers can track user activities.
We propose a zero-knowledge-based mechanism that integrates with OIDC to let
users authenticate through SSO without revealing information about the service
provider. Our system leverages Groth's zk-SNARK to prove membership of
subscribed service providers without revealing their identity. We adopt a
decentralized and verifiable approach to set up the prerequisites of our
construction that further secures and establishes trust in the system. We set
up high security targets and achieve them with minimal storage and latency
cost, proving that our research can be adopted for production.

</details>


### [29] [Rethinking Provenance Completeness with a Learning-Based Linux Scheduler](https://arxiv.org/abs/2510.08479)
*Jinsong Mao,Benjamin E. Ujcich,Shiqing Ma*

Main category: cs.CR

TL;DR: 提出了Venus，一个专门为溯源数据收集设计的Linux学习型调度器，通过强化学习优化资源分配，解决溯源系统中的'超级生产者威胁'问题。


<details>
  <summary>Details</summary>
Motivation: 现有溯源收集系统存在'超级生产者威胁'，即溯源数据生成可能过载系统，导致安全相关事件被丢弃，使攻击者能够隐藏其行为。资源隔离方法无法完全解决硬件依赖和性能限制问题。

Method: 利用操作系统内核调度器来缓解该威胁，开发了Venus学习型调度器，通过强化学习学习溯源任务行为并动态优化资源分配。

Result: Venus显著提高了溯源收集系统的完整性和效率，与传统调度相比，在保持合理开销的同时，在某些情况下甚至比默认Linux调度器提高了整体运行时间。

Conclusion: 操作系统内核调度器可以有效缓解溯源系统中的'超级生产者威胁'，学习型调度器Venus为溯源数据收集提供了更好的完整性和效率保障。

Abstract: Provenance plays a critical role in maintaining traceability of a system's
actions for root cause analysis of security threats and impacts. Provenance
collection is often incorporated into the reference monitor of systems to
ensure that an audit trail exists of all events, that events are completely
captured, and that logging of such events cannot be bypassed. However, recent
research has questioned whether existing state-of-the-art provenance collection
systems fail to ensure the security guarantees of a true reference monitor due
to the 'super producer threat' in which provenance generation can overload a
system to force the system to drop security-relevant events and allow an
attacker to hide their actions. One approach towards solving this threat is to
enforce resource isolation, but that does not fully solve the problems
resulting from hardware dependencies and performance limitations.
  In this paper, we show how an operating system's kernel scheduler can
mitigate this threat, and we introduce Venus, a learned scheduler for Linux
specifically designed for provenance. Unlike conventional schedulers that
ignore provenance completeness requirements, Venus leverages reinforcement
learning to learn provenance task behavior and to dynamically optimize resource
allocation. We evaluate Venus's efficacy and show that Venus significantly
improves both the completeness and efficiency of provenance collection systems
compared to traditional scheduling, while maintaining reasonable overheads and
even improving overall runtime in certain cases compared to the default Linux
scheduler.

</details>


### [30] [AI-Driven Post-Quantum Cryptography for Cyber-Resilient V2X Communication in Transportation Cyber-Physical Systems](https://arxiv.org/abs/2510.08496)
*Akid Abrar,Sagar Dasgupta,Mizanur Rahman,Ahmad Alsharif*

Main category: cs.CR

TL;DR: 本章探讨了将后量子密码学（PQC）与人工智能（AI）结合，以增强交通信息物理系统（TCPS）在量子计算时代的通信安全。


<details>
  <summary>Details</summary>
Motivation: 量子计算对传统密码方法构成重大威胁，TCPS需要量子攻击抗性的安全措施来保护敏感数据传输。

Method: 通过AI优化PQC算法选择、资源分配和实时威胁适应，提出AI驱动的PQC方法。

Result: AI增强的PQC方法能够在不影响系统性能的前提下，提高TCPS通信的安全性和弹性。

Conclusion: 结合AI和PQC是确保TCPS在量子时代保持安全通信的关键策略。

Abstract: Transportation Cyber-Physical Systems (TCPS) integrate physical elements,
such as transportation infrastructure and vehicles, with cyber elements via
advanced communication technologies, allowing them to interact seamlessly. This
integration enhances the efficiency, safety, and sustainability of
transportation systems. TCPS rely heavily on cryptographic security to protect
sensitive information transmitted between vehicles, transportation
infrastructure, and other entities within the transportation ecosystem,
ensuring data integrity, confidentiality, and authenticity. Traditional
cryptographic methods have been employed to secure TCPS communications, but the
advent of quantum computing presents a significant threat to these existing
security measures. Therefore, integrating Post-Quantum Cryptography (PQC) into
TCPS is essential to maintain secure and resilient communications. While PQC
offers a promising approach to developing cryptographic algorithms resistant to
quantum attacks, artificial intelligence (AI) can enhance PQC by optimizing
algorithm selection, resource allocation, and adapting to evolving threats in
real-time. AI-driven PQC approaches can improve the efficiency and
effectiveness of PQC implementations, ensuring robust security without
compromising system performance. This chapter introduces TCPS communication
protocols, discusses the vulnerabilities of corresponding communications to
cyber-attacks, and explores the limitations of existing cryptographic methods
in the quantum era. By examining how AI can strengthen PQC solutions, the
chapter presents cyber-resilient communication strategies for TCPS.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [31] [Truth-Aware Decoding: A Program-Logic Approach to Factual Language Generation](https://arxiv.org/abs/2510.07331)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.AI

TL;DR: Truth-Aware Decoding (TAD) 是一种验证导向的解码方案，通过知识库对齐神经语言生成，在解码时使用语义守卫减少幻觉而不牺牲吞吐量。


<details>
  <summary>Details</summary>
Motivation: 将大规模经验模型与形式验证相结合，解决神经语言生成中的幻觉问题，确保生成内容与知识库的一致性。

Method: 基于约束的语义学，在解码时使用语义守卫进行筛选，采用多智能体操作演算和经过验证的Lean构件来认证实现行为。

Result: 数值和算法案例研究证实，该方法能有效减少幻觉，同时保持吞吐量，在形式验证和实证模型之间建立了实用桥梁。

Conclusion: TAD为神经语言生成提供了可验证的语义保障，通过知识感知的安全质量量化事实风险，实现了概率程序语义在序列模型中的实际应用。

Abstract: This paper introduces Truth-Aware Decoding (TAD), a verification-oriented
decoding scheme that aligns neural language generation with knowledge bases.
Situated in the tradition of probabilistic program semantics for sequence
models, TAD augments modern instruction-tuned systems with a lattice of
semantic guards that operate at decode time. Our contributions are fourfold:
(i) a constraint-based semantics that renders oracle filtering as a
program-logic judgment, (ii) a proof that greedy selection enjoys local
likelihood dominance under sound and complete guards (Theorem 2.7), (iii) an
entropy-style invariant that quantifies factual risk via knowledge-aware safe
mass, and (iv) a multi-agent operational calculus with verified Lean artefacts
to certify implementation behaviour. Numerical and algorithmic case studies
confirm that the resulting guardrails reduce hallucinations without sacrificing
throughput, yielding a pragmatic bridge between large-scale empirical models
and formal verification.

</details>


### [32] [L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)](https://arxiv.org/abs/2510.07363)
*Tianxiang Xu,Zhichao Wen,Xinyu Zhao,Jun Wang,Yan Li,Chang Liu*

Main category: cs.AI

TL;DR: L2M-AID是一个基于LLM增强的多智能体强化学习的自主工业防御框架，通过LLM作为语义桥梁将非结构化遥测数据转化为上下文感知状态表示，使智能体能够推理攻击意图而非仅匹配模式，显著提升了工业物联网安全性能。


<details>
  <summary>Details</summary>
Motivation: 工业物联网(IIoT)的日益集成使关键网络物理系统面临复杂多阶段攻击，传统防御缺乏上下文感知能力，无法有效应对这些威胁。

Method: 采用LLM驱动的多智能体强化学习框架，使用LLM作为语义桥梁将非结构化遥测数据转化为丰富的上下文状态表示，结合MAPPO算法学习复杂协作策略，奖励函数平衡安全目标与操作需求。

Result: 在SWaT基准数据集和基于MITRE ATT&CK for ICS框架的合成数据集上，L2M-AID显著优于传统IDS、深度学习异常检测器和单智能体RL基线，检测率达到97.2%，误报率降低80%以上，响应时间提升4倍，同时保持物理过程稳定性。

Conclusion: L2M-AID为保护关键国家基础设施提供了一个强大的新范式，通过LLM与多智能体强化学习的深度融合实现了自适应和弹性安全。

Abstract: The increasing integration of Industrial IoT (IIoT) exposes critical
cyber-physical systems to sophisticated, multi-stage attacks that elude
traditional defenses lacking contextual awareness. This paper introduces
L2M-AID, a novel framework for Autonomous Industrial Defense using
LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team
of collaborative agents, each driven by a Large Language Model (LLM), to
achieve adaptive and resilient security. The core innovation lies in the deep
fusion of two AI paradigms: we leverage an LLM as a semantic bridge to
translate vast, unstructured telemetry into a rich, contextual state
representation, enabling agents to reason about adversary intent rather than
merely matching patterns. This semantically-aware state empowers a Multi-Agent
Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative
strategies. The MARL reward function is uniquely engineered to balance security
objectives (threat neutralization) with operational imperatives, explicitly
penalizing actions that disrupt physical process stability. To validate our
approach, we conduct extensive experiments on the benchmark SWaT dataset and a
novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework.
Results demonstrate that L2M-AID significantly outperforms traditional IDS,
deep learning anomaly detectors, and single-agent RL baselines across key
metrics, achieving a 97.2% detection rate while reducing false positives by
over 80% and improving response times by a factor of four. Crucially, it
demonstrates superior performance in maintaining physical process stability,
presenting a robust new paradigm for securing critical national infrastructure.

</details>


### [33] [Base Models Know How to Reason, Thinking Models Learn When](https://arxiv.org/abs/2510.07364)
*Constantin Venhoff,Iván Arcuschin,Philip Torr,Arthur Conmy,Neel Nanda*

Main category: cs.AI

TL;DR: 思考模型（如DeepSeek R1）通过激活基础模型中已有的推理机制，在适当时间引发思考级推理链，而非学习全新能力，可恢复91%性能差距且仅引导12%的token。


<details>
  <summary>Details</summary>
Motivation: 探究思考模型性能提升的原因：是学习全新推理能力还是重新利用基础模型已有能力。

Method: 提出混合模型，在基础模型中适时激活推理机制；引入无监督自底向上方法发现可解释推理行为；在三个基础模型和四个思考模型上使用GSM8K和MATH500数据集测试。

Result: 混合模型无需权重更新即可恢复高达91%到思考模型的性能差距，仅引导12%的token；提供因果测试方法验证基础模型推理机制有效性。

Conclusion: 预训练阶段模型已获得大部分推理机制，后训练教会模型在适当时机高效部署这些机制，实现推理计算的高效利用。

Abstract: Why do thinking language models like DeepSeek R1 outperform their base
counterparts? Despite consistent performance gains, it remains unclear to what
extent thinking models learn entirely new reasoning capabilities or repurpose
pre-existing base model ones. In this work, we propose a hybrid model where we
activate reasoning mechanisms in base models at the right time to elicit
thinking-model-level reasoning chains, implying that thinking models exploit
already existing capabilities. To ground our analysis, we introduce an
unsupervised, bottom-up approach for uncovering human-interpretable reasoning
behaviors in thinking models. This approach provides an unbiased method to
discover reasoning behaviors without imposing manual or LLM-derived
assumptions. Across three base and four thinking models, using GSM8K and
MATH500, our hybrid model recovers up to 91% of the performance gap to thinking
models without any weight updates while steering only 12% of tokens.
Concretely, our empirical setup provides a simple, causal way to test the
effectiveness of existing reasoning mechanisms in base models by invoking them
directly and measuring the resulting task performance. More broadly, these
results reframe our understanding of how thinking models are trained:
pre-training is when models acquire most of their reasoning mechanisms, and
post-training teaches efficient deployment of these mechanisms at the right
time, enabling efficient use of their inference-time compute.

</details>


### [34] [Position: AI Will Transform Neuropsychology Through Mental Health Digital Twins for Dynamic Mental Health Care, Especially for ADHD](https://arxiv.org/abs/2510.07409)
*Neil Natarajan,Sruthi Viswanathan,Xavier Roberts-Gaal,Michelle Marie Martel*

Main category: cs.AI

TL;DR: 提出从静态心理健康诊断转向AI驱动的连续评估，以ADHD为例，探索生成式AI如何解决神经心理学容量限制，实现个性化纵向护理路径，并引入心理健康数字孪生(MHDTs)作为变革性框架。


<details>
  <summary>Details</summary>
Motivation: 静态解决方案无法满足动态思维需求，当前心理健康诊断存在容量限制，需要更个性化、连续的护理方法。

Method: 使用生成式AI进行频繁的低水平体验采样，促进跨护理路径的诊断协调，建立心理健康数字孪生(MHDTs)作为持续更新的计算模型。

Result: AI能够有效进行患者体验采样，为动态适应个体需求和病情变化提供支持，提高治疗的可及性和有效性。

Conclusion: AI驱动的连续评估和心理健康数字孪生框架有望变革心理健康护理，实现更个性化、动态适应的治疗路径。

Abstract: Static solutions don't serve a dynamic mind. Thus, we advocate a shift from
static mental health diagnostic assessments to continuous, artificial
intelligence (AI)-driven assessment. Focusing on
Attention-Deficit/Hyperactivity Disorder (ADHD) as a case study, we explore how
generative AI has the potential to address current capacity constraints in
neuropsychology, potentially enabling more personalized and longitudinal care
pathways. In particular, AI can efficiently conduct frequent, low-level
experience sampling from patients and facilitate diagnostic reconciliation
across care pathways. We envision a future where mental health care benefits
from continuous, rich, and patient-centered data sampling to dynamically adapt
to individual patient needs and evolving conditions, thereby improving both
accessibility and efficacy of treatment. We further propose the use of mental
health digital twins (MHDTs) - continuously updated computational models that
capture individual symptom dynamics and trajectories - as a transformative
framework for personalized mental health care. We ground this framework in
empirical evidence and map out the research agenda required to refine and
operationalize it.

</details>


### [35] [ProSEA: Problem Solving via Exploration Agents](https://arxiv.org/abs/2510.07423)
*William Nguyen,Vinh Luong,Christopher Nguyen*

Main category: cs.AI

TL;DR: ProSEA是一个模块化的多智能体框架，通过探索和计划演化实现迭代问题解决，在FinanceBench基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体大多局限于静态规划和脆弱交互，缺乏真正的协作或自适应推理能力。

Method: 采用分层架构，由管理器智能体协调领域专家智能体，分解任务并根据失败尝试的结构化反馈进行自适应重新规划。

Result: 在FinanceBench基准测试中，即使没有人类反馈，ProSEA也优于最先进的基线方法，在推理密集型任务中表现出稳健性能。

Conclusion: ProSEA有潜力成为更透明、自适应和人类对齐的AI智能体的基础框架。

Abstract: Large language models (LLMs) have empowered AI agents to tackle increasingly
complex tasks. However, most existing agents remain limited to static planning
and brittle interactions, falling short of true collaboration or adaptive
reasoning. We introduce ProSEA, a modular, general-purpose multi-agent
framework designed for iterative problem solving through exploration and plan
evolution. ProSEA features a hierarchical architecture in which a Manager Agent
orchestrates domain-specialized Expert Agents, decomposes tasks, and adaptively
replans based on structured feedback from failed attempts. Unlike prior
systems, ProSEA agents report not only success or failure but also detailed
reasons for failure and newly discovered constraints, enabling dynamic plan
refinement informed by exploratory traces. The framework operates autonomously
but supports seamless integration with human collaborators when needed.
Experiments on the challenging FinanceBench benchmark demonstrate that ProSEA,
even without human feedback, outperforms state-of-the-art baselines and
achieves robust performance across reasoning-heavy tasks. These results
underscore ProSEA's potential as a foundation for more transparent, adaptive,
and human-aligned AI agents.

</details>


### [36] [Less is More: Strategic Expert Selection Outperforms Ensemble Complexity in Traffic Forecasting](https://arxiv.org/abs/2510.07426)
*Walid Guettala,Yufan Zhao,László Gulyás*

Main category: cs.AI

TL;DR: TESTAM+是一个增强的时空交通预测框架，通过引入新的空间语义专家整合物理道路拓扑和数据驱动特征相似性，在保持计算效率的同时显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测模型如TESTAM缺乏对物理道路网络拓扑的显式整合，限制了其空间建模能力。需要开发能够同时考虑物理拓扑和数据特征相似性的混合方法。

Method: 提出TESTAM+框架，引入空间语义专家，通过混合图构建将物理道路拓扑与数据驱动特征相似性相结合，并采用策略性专家选择而非简单的集成聚合。

Result: 在METR LA数据集上MAE降低1.3%(3.10 vs 3.14)，在PEMS BAY上提升4.1%(1.65 vs 1.72)。最优配置相比MegaCRN在METR LA上MAE降低11.5%(2.99 vs 3.38)，推理延迟比完整四专家模型降低53.1%。

Conclusion: 少量但策略性设计的专家模型优于复杂的多专家集成，在保持计算效率的同时实现了最先进的性能，适合实时部署。

Abstract: Traffic forecasting is fundamental to intelligent transportation systems,
enabling congestion mitigation and emission reduction in increasingly complex
urban environments. While recent graph neural network approaches have advanced
spatial temporal modeling, existing mixture of experts frameworks like Time
Enhanced Spatio Temporal Attention Model (TESTAM) lack explicit incorporation
of physical road network topology, limiting their spatial capabilities. We
present TESTAM+, an enhanced spatio temporal forecasting framework that
introduces a novel SpatioSemantic Expert integrating physical road topology
with data driven feature similarity through hybrid graph construction. TESTAM+
achieves significant improvements over TESTAM: 1.3% MAE reduction on METR LA
(3.10 vs. 3.14) and 4.1% improvement on PEMS BAY (1.65 vs. 1.72). Through
comprehensive ablation studies, we discover that strategic expert selection
fundamentally outperforms naive ensemble aggregation. Individual experts
demonstrate remarkable effectiveness: the Adaptive Expert achieves 1.63 MAE on
PEMS BAY, outperforming the original three expert TESTAM (1.72 MAE), while the
SpatioSemantic Expert matches this performance with identical 1.63 MAE. The
optimal Identity + Adaptive configuration achieves an 11.5% MAE reduction
compared to state of the art MegaCRN on METR LA (2.99 vs. 3.38), while reducing
inference latency by 53.1% compared to the full four expert TESTAM+. Our
findings reveal that fewer, strategically designed experts outperform complex
multi expert ensembles, establishing new state of the art performance with
superior computational efficiency for real time deployment.

</details>


### [37] [Traceability and Accountability in Role-Specialized Multi-Agent LLM Pipelines](https://arxiv.org/abs/2510.07614)
*Amine Barrak*

Main category: cs.AI

TL;DR: 该论文研究了基于大语言模型的可追踪、可问责的多智能体系统，通过Planner->Executor->Critic流水线结构，分析错误传播路径并量化各角色的表现，显著提升了系统准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的序列多智能体系统虽然能自动化复杂软件任务，但由于错误会从一个阶段悄悄传递到下一阶段，导致系统难以信任。需要建立可追踪和可问责的流水线系统。

Method: 采用Planner->Executor->Critic流水线结构，评估了三种最先进LLM的八种配置，在三个基准测试上分析错误起始点、传播路径和修复方法。

Result: 研究发现：(1)添加结构化、可问责的交接显著提高准确性；(2)模型具有明确的角色特定优势和风险；(3)准确性-成本-延迟权衡是任务相关的，异构流水线通常最有效。

Conclusion: 提供了一种实用的、数据驱动的方法，用于设计、追踪和调试可靠、可预测和可问责的多智能体系统。

Abstract: Sequential multi-agent systems built with large language models (LLMs) can
automate complex software tasks, but they are hard to trust because errors
quietly pass from one stage to the next. We study a traceable and accountable
pipeline, meaning a system with clear roles, structured handoffs, and saved
records that let us trace who did what at each step and assign blame when
things go wrong. Our setting is a Planner -> Executor -> Critic pipeline. We
evaluate eight configurations of three state-of-the-art LLMs on three
benchmarks and analyze where errors start, how they spread, and how they can be
fixed. Our results show: (1) adding a structured, accountable handoff between
agents markedly improves accuracy and prevents the failures common in simple
pipelines; (2) models have clear role-specific strengths and risks (e.g.,
steady planning vs. high-variance critiquing), which we quantify with repair
and harm rates; and (3) accuracy-cost-latency trade-offs are task-dependent,
with heterogeneous pipelines often the most efficient. Overall, we provide a
practical, data-driven method for designing, tracing, and debugging reliable,
predictable, and accountable multi-agent systems.

</details>


### [38] [TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering](https://arxiv.org/abs/2510.07432)
*Penghang Liu,Elizabeth Fons,Svitlana Vyetrenko,Daniel Borrajo,Vamsi Potluru,Manuela Veloso*

Main category: cs.AI

TL;DR: 提出了TS-Agent，一个时间序列推理代理，通过将LLM用于证据收集和推理，同时将统计信息提取委托给时间序列分析工具，解决了LLM在时间序列推理中的幻觉和知识泄漏问题。


<details>
  <summary>Details</summary>
Motivation: LLM在时间序列推理任务中表现不佳，输出常受幻觉或知识泄漏影响，需要专门的方法来处理时间序列数据。

Method: TS-Agent使用原子操作符与原始数值序列交互，记录证据日志，通过自批判和最终质量门控迭代优化推理，避免多模态对齐训练。

Result: 在基准测试中，TS-Agent在理解任务上达到SOTA LLM水平，在推理任务上显著改进，特别是在零样本设置下优于依赖记忆的现有模型。

Conclusion: 该方法通过保留时间序列原生形式、确保可解释性和可验证性，有效缓解了知识泄漏和幻觉问题，为时间序列推理提供了新思路。

Abstract: Large language models (LLMs) have shown strong abilities in reasoning and
problem solving, but recent studies reveal that they still struggle with time
series reasoning tasks, where outputs are often affected by hallucination or
knowledge leakage. In this work we propose TS-Agent, a time series reasoning
agent that leverages LLMs strictly for what they excel at, i.e., gathering
evidence and synthesizing it into conclusions through step-by-step reasoning,
while delegating the extraction of statistical and structural information to
time series analytical tools. Instead of mapping time series into text tokens,
images, or embeddings, our agent interacts with raw numeric sequences through
atomic operators, records outputs in an explicit evidence log, and iteratively
refines its reasoning under the guidance of a self-critic and a final quality
gate. This design avoids multi-modal alignment training, preserves the native
form of time series, ensures interpretability and verifiability, and mitigates
knowledge leakage or hallucination. Empirically, we evaluate the agent on
established benchmarks. Our experiments show that TS-Agent achieves performance
comparable to state-of-the-art LLMs on understanding benchmarks, and delivers
significant improvements on reasoning tasks, where existing models often rely
on memorization and fail in zero-shot settings.

</details>


### [39] [ExpertAgent: Enhancing Personalized Education through Dynamic Planning and Retrieval-Augmented Long-Chain Reasoning](https://arxiv.org/abs/2510.07456)
*Binrong Zhu,Guiran Liu,Nina Jiang*

Main category: cs.AI

TL;DR: 提出了ExpertAgent智能代理框架，通过动态规划学习内容和策略，基于持续更新的学生模型提供个性化教育，减少大语言模型幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI在教育中缺乏实时适应性、个性化和内容可靠性的问题。

Method: 开发ExpertAgent智能代理框架，基于验证的课程知识库，通过动态规划学习内容和策略，结合持续更新的学生模型。

Result: 提供了主动个性化的学习体验，克服传统静态学习内容的限制，实时优化教学策略和学习体验。

Conclusion: ExpertAgent框架能够有效提升教育AI的可靠性、可信度和个性化水平，减少大语言模型的幻觉风险。

Abstract: The application of advanced generative artificial intelligence in education
is often constrained by the lack of real-time adaptability, personalization,
and reliability of the content. To address these challenges, we propose
ExpertAgent - an intelligent agent framework designed for personalized
education that provides reliable knowledge and enables highly adaptive learning
experiences. Therefore, we developed ExpertAgent, an innovative learning agent
that provides users with a proactive and personalized learning experience.
ExpertAgent dynamic planning of the learning content and strategy based on a
continuously updated student model. Therefore, overcoming the limitations of
traditional static learning content to provide optimized teaching strategies
and learning experience in real time. All instructional content is grounded in
a validated curriculum repository, effectively reducing hallucination risks in
large language models and improving reliability and trustworthiness.

</details>


### [40] [Evaluation of LLMs for Process Model Analysis and Optimization](https://arxiv.org/abs/2510.07489)
*Akhil Kumar,Jianliang Leon Zhao,Om Dobariya*

Main category: cs.AI

TL;DR: LLMs（如ChatGPT）能够理解BPMN流程模型图像，通过自然语言界面进行对话式交互，发现语法和逻辑错误，并在语法、逻辑和语义层面进行深度推理。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在理解业务流程模型、发现错误和深度推理方面的能力，探索其作为业务流程设计助手的作用。

Method: 使用多个LLMs（包括ChatGPT o3模型）在零样本设置下测试其对BPMN流程模型图像的理解能力，通过自然语言界面进行交互式分析。

Result: 未经训练的LLMs能够有效理解BPMN流程模型，在语法、逻辑和语义层面智能回答查询，不同LLMs在准确性和有效性方面表现各异。

Conclusion: LLMs可以作为业务流程设计者和用户的有价值助手，表现出类人化的推理特性，具备深度分析和优化流程的能力。

Abstract: In this paper, we report our experience with several LLMs for their ability
to understand a process model in an interactive, conversational style, find
syntactical and logical errors in it, and reason with it in depth through a
natural language (NL) interface. Our findings show that a vanilla, untrained
LLM like ChatGPT (model o3) in a zero-shot setting is effective in
understanding BPMN process models from images and answering queries about them
intelligently at syntactic, logic, and semantic levels of depth. Further,
different LLMs vary in performance in terms of their accuracy and
effectiveness. Nevertheless, our empirical analysis shows that LLMs can play a
valuable role as assistants for business process designers and users. We also
study the LLM's "thought process" and ability to perform deeper reasoning in
the context of process analysis and optimization. We find that the LLMs seem to
exhibit anthropomorphic properties.

</details>


### [41] [Optimizing Ethical Risk Reduction for Medical Intelligent Systems with Constraint Programming](https://arxiv.org/abs/2510.07491)
*Clotilde Brayé,Aurélien Bricout,Arnaud Gotlieb,Nadjib Lazaar,Quentin Vallet*

Main category: cs.AI

TL;DR: 本文研究了医疗智能系统的伦理风险降低优化问题，通过混合整数规划、可满足性和约束编程三种方法来解决风险分配问题，确保符合可信AI的伦理要求。


<details>
  <summary>Details</summary>
Motivation: 医疗智能系统被归类为高风险系统，需要正式的风险管理流程来确保符合可信AI的伦理要求，因此需要优化风险降低策略。

Method: 将问题形式化为约束优化任务，使用Minizinc约束建模语言，并比较混合整数规划、可满足性和约束编程三种解决范式的性能。

Result: 通过实验研究分析了三种方法在性能、表达能力和可扩展性方面的表现，并识别了方法的局限性。

Conclusion: 从方法论的局限性出发，提出了将Minizinc模型整合到完整的可信AI伦理风险管理流程中的未来工作方向。

Abstract: Medical Intelligent Systems (MIS) are increasingly integrated into healthcare
workflows, offering significant benefits but also raising critical safety and
ethical concerns. According to the European Union AI Act, most MIS will be
classified as high-risk systems, requiring a formal risk management process to
ensure compliance with the ethical requirements of trustworthy AI. In this
context, we focus on risk reduction optimization problems, which aim to reduce
risks with ethical considerations by finding the best balanced assignment of
risk assessment values according to their coverage of trustworthy AI ethical
requirements. We formalize this problem as a constrained optimization task and
investigate three resolution paradigms: Mixed Integer Programming (MIP),
Satisfiability (SAT), and Constraint Programming(CP).Our contributions include
the mathematical formulation of this optimization problem, its modeling with
the Minizinc constraint modeling language, and a comparative experimental study
that analyzes the performance, expressiveness, and scalability of each approach
to solving. From the identified limits of the methodology, we draw some
perspectives of this work regarding the integration of the Minizinc model into
a complete trustworthy AI ethical risk management process for MIS.

</details>


### [42] [CompassLLM: A Multi-Agent Approach toward Geo-Spatial Reasoning for Popular Path Query](https://arxiv.org/abs/2510.07516)
*Md. Nazmul Islam Ananto,Shamit Fatin,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: CompassLLM是一个基于大型语言模型的多智能体框架，用于解决流行路径查询问题，通过搜索和生成两阶段流程，在真实和合成数据集上表现出优越的准确性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 传统算法和机器学习方法在流行路径查询中需要模型训练、参数调整和数据更新时的重新训练，而大型语言模型在空间和图推理方面展现出强大能力，因此探索如何将这些模型应用于地理空间问题具有重要意义。

Method: CompassLLM采用多智能体框架，包含两阶段流程：SEARCH阶段识别流行路径，GENERATE阶段在历史轨迹数据中不存在现有路径时合成新路径。

Result: 在真实和合成数据集上的实验表明，CompassLLM在SEARCH阶段表现出优越的准确性，在GENERATE阶段具有竞争力的性能，同时成本效益高。

Conclusion: CompassLLM成功将大型语言模型的推理能力引入地理空间领域，为流行路径查询提供了一种无需模型训练和参数调整的有效解决方案。

Abstract: The popular path query - identifying the most frequented routes between
locations from historical trajectory data - has important applications in urban
planning, navigation optimization, and travel recommendations. While
traditional algorithms and machine learning approaches have achieved success in
this domain, they typically require model training, parameter tuning, and
retraining when accommodating data updates. As Large Language Models (LLMs)
demonstrate increasing capabilities in spatial and graph-based reasoning, there
is growing interest in exploring how these models can be applied to geo-spatial
problems.
  We introduce CompassLLM, a novel multi-agent framework that intelligently
leverages the reasoning capabilities of LLMs into the geo-spatial domain to
solve the popular path query. CompassLLM employs its agents in a two-stage
pipeline: the SEARCH stage that identifies popular paths, and a GENERATE stage
that synthesizes novel paths in the absence of an existing one in the
historical trajectory data. Experiments on real and synthetic datasets show
that CompassLLM demonstrates superior accuracy in SEARCH and competitive
performance in GENERATE while being cost-effective.

</details>


### [43] [Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization](https://arxiv.org/abs/2510.07517)
*Hyeong Kyu Choi,Xiaojin Zhu,Yixuan Li*

Main category: cs.AI

TL;DR: 提出了一个原则性框架来量化和减轻多智能体辩论中的身份偏见，通过响应匿名化和定义身份偏见系数来解决智能体的谄媚和自我偏见问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明多智能体辩论中的智能体存在身份驱动的谄媚（盲目跟随同伴观点）和自我偏见（固执坚持自己先前输出）问题，这削弱了辩论的可靠性。

Method: 1. 将辩论动态形式化为身份加权的贝叶斯更新过程；2. 提出响应匿名化方法，通过移除提示中的身份标记使智能体无法区分"自我"和"同伴"；3. 定义身份偏见系数来量化智能体跟随同伴与跟随自己的频率。

Result: 跨多个模型、数据集和辩论轮次的实证研究表明身份偏见普遍存在，谄媚比自我偏见更为常见。响应匿名化有效减少了偏见。

Conclusion: 需要"掩盖"身份以确保多智能体辩论系统基于内容而非来源身份进行推理，身份偏见是影响辩论可靠性的重要因素。

Abstract: Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning
by letting multiple agents exchange answers and then aggregate their opinions.
Yet recent studies reveal that agents are not neutral: they are prone to
identity-driven sycophancy and self-bias, uncritically adopting a peer's view
or stubbornly adhering to their own prior output, undermining the reliability
of debate. In this work, we present the first principled framework that joins
sycophancy and self-bias to mitigate and quantify identity bias in MAD. First,
we formalize the debate dynamics as an identity-weighted Bayesian update
process. Second, we propose response anonymization: by removing identity
markers from prompts, agents cannot distinguish "self" from "peer", which
forces equal weights on agent identity, thereby reducing bias. Third, we define
the Identity Bias Coefficient (IBC), a principled metric that measures how
often an agent follows a peer versus itself. Empirical studies across multiple
models, datasets and debate rounds confirm that identity bias is widespread,
with sycophancy far more common than self-bias. Our findings highlight the need
to "mask" identity to ensure that MAD systems reason based on content rather
than source identity. Code is released in
https://github.com/deeplearning-wisc/MAD-identity-bias.

</details>


### [44] [An Evaluation Study of Hybrid Methods for Multilingual PII Detection](https://arxiv.org/abs/2510.07551)
*Harshit Rajgarhia,Suryam Gupta,Asif Shaik,Gulipalli Praveen Kumar,Y Santhoshraj,Sanka Nithya Tanvy Nishitha,Abhishek Mukherji*

Main category: cs.AI

TL;DR: RECAP是一个混合框架，结合确定性正则表达式和上下文感知大语言模型，用于在13种低资源语言中检测个人身份信息，性能优于微调NER模型82%和零样本LLMs 17%。


<details>
  <summary>Details</summary>
Motivation: 低资源语言中的个人身份信息检测面临语言多样性和标注数据有限的挑战，需要可扩展的解决方案来满足隐私合规要求。

Method: 采用混合框架，结合正则表达式和上下文感知LLMs，通过三阶段精炼流程进行消歧和过滤，支持300多种实体类型而无需重新训练。

Result: 使用nervaluate基准测试，系统在加权F1分数上优于微调NER模型82%，优于零样本LLMs 17%。

Conclusion: RECAP为合规应用中的PII检测提供了一个可扩展且适应性强的解决方案。

Abstract: The detection of Personally Identifiable Information (PII) is critical for
privacy compliance but remains challenging in low-resource languages due to
linguistic diversity and limited annotated data. We present RECAP, a hybrid
framework that combines deterministic regular expressions with context-aware
large language models (LLMs) for scalable PII detection across 13 low-resource
locales. RECAP's modular design supports over 300 entity types without
retraining, using a three-phase refinement pipeline for disambiguation and
filtering. Benchmarked with nervaluate, our system outperforms fine-tuned NER
models by 82% and zero-shot LLMs by 17% in weighted F1-score. This work offers
a scalable and adaptable solution for efficient PII detection in
compliance-focused applications.

</details>


### [45] [Benchmarking is Broken -- Don't Let AI be its Own Judge](https://arxiv.org/abs/2510.07575)
*Zerui Cheng,Stella Wohnig,Ruchika Gupta,Samiul Alam,Tassallah Abdullahi,João Alves Ribeiro,Christian Nielsen-Garcia,Saif Mir,Siran Li,Jason Orender,Seyed Ali Bahrainian,Daniel Kirste,Aaron Gokaslan,Mikołaj Glinka,Carsten Eickhoff,Ruben Wolff*

Main category: cs.AI

TL;DR: 当前AI评估存在数据污染、选择性报告等系统性问题，需要构建统一、实时、质量可控的基准测试框架PeerBench，通过密封执行、题库轮换和延迟透明等机制来重建评估可信度。


<details>
  <summary>Details</summary>
Motivation: AI市场快速发展但评估体系混乱，存在数据污染、选择性报告等问题，导致难以区分真实进展与夸大宣传，损害科学信号和公众信任，亟需建立可信的评估范式。

Method: 提出PeerBench框架，采用社区治理模式，包含密封执行、题库银行与滚动更新、延迟透明等核心机制，确保评估过程的公正性和可信度。

Result: PeerBench提供了一个系统性的解决方案，能够有效应对当前AI评估中的关键漏洞，为构建可信的AI进步衡量标准奠定基础。

Conclusion: 当前放任自流的AI评估方法不可持续，需要向统一、实时、质量可控的基准测试范式转变，PeerBench为此提供了可行的技术蓝图和治理框架。

Abstract: The meteoric rise of Artificial Intelligence (AI), with its rapidly expanding
market capitalization, presents both transformative opportunities and critical
challenges. Chief among these is the urgent need for a new, unified paradigm
for trustworthy evaluation, as current benchmarks increasingly reveal critical
vulnerabilities. Issues like data contamination and selective reporting by
model developers fuel hype, while inadequate data quality control can lead to
biased evaluations that, even if unintentionally, may favor specific
approaches. As a flood of participants enters the AI space, this "Wild West" of
assessment makes distinguishing genuine progress from exaggerated claims
exceptionally difficult. Such ambiguity blurs scientific signals and erodes
public confidence, much as unchecked claims would destabilize financial markets
reliant on credible oversight from agencies like Moody's.
  In high-stakes human examinations (e.g., SAT, GRE), substantial effort is
devoted to ensuring fairness and credibility; why settle for less in evaluating
AI, especially given its profound societal impact? This position paper argues
that the current laissez-faire approach is unsustainable. We contend that true,
sustainable AI advancement demands a paradigm shift: a unified, live, and
quality-controlled benchmarking framework robust by construction, not by mere
courtesy and goodwill. To this end, we dissect the systemic flaws undermining
today's AI evaluation, distill the essential requirements for a new generation
of assessments, and introduce PeerBench, a community-governed, proctored
evaluation blueprint that embodies this paradigm through sealed execution, item
banking with rolling renewal, and delayed transparency. Our goal is to pave the
way for evaluations that can restore integrity and deliver genuinely
trustworthy measures of AI progress.

</details>


### [46] [AgentAsk: Multi-Agent Systems Need to Ask](https://arxiv.org/abs/2510.07593)
*Bohan Lin,Kuo Yang,Yingchuan Lai,Yudong Zhang,Chen Zhang,Guibin Zhang,Xinlei Yu,Miao Yu,Xu Wang,Yang Wang*

Main category: cs.AI

TL;DR: AgentAsk是一个轻量级即插即用的澄清模块，通过插入最小必要问题来阻止多智能体系统中边缘级错误的传播，显著提升准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统通过协作分工有望增强问题解决能力，但由于边缘级错误级联（微小不准确在消息传递中传播）而经常表现不如单智能体基线。

Method: 采用三阶段流程：(i)从整理的失败轨迹中提取边缘级判断到紧凑策略；(ii)监督策略决定何时/问什么/问谁/如何问；(iii)使用E-GRPO强化学习目标在线优化，平衡准确性、延迟和成本。

Result: 在数学、推理和编码基准测试中，AgentAsk持续提升公共多智能体实现的准确性和鲁棒性，同时保持最小开销，延迟和额外成本均低于5%，接近强评估器性能。

Conclusion: 除了经验改进外，贡献了边缘级错误的系统分类和链路局部干预的实用方法，为更可靠的基于LLM的多智能体系统提供了可扩展路径。

Abstract: Multi-agent systems built on large language models (LLMs) promise enhanced
problem-solving capabilities through collaborative division of labor. However,
they frequently underperform single-agent baselines due to edge-level error
cascades: minor inaccuracies at one message handoff propagate across the entire
chain. We propose AgentAsk, a lightweight and plug-and-play clarification
module that treats every inter-agent message as a potential failure point and
inserts minimally necessary questions to arrest error propagation. AgentAsk
follows a three-stage pipeline: (i) distilling edge-level judgments from
curated failure traces into a compact policy, (ii) supervising the policy to
determine when/what/whom/how to ask, and (iii) optimizing online with E-GRPO, a
reinforcement learning objective that balances accuracy, latency, and cost. The
module is architecture-agnostic and easy to integrate into existing
orchestration. Across math, reasoning, and coding benchmarks, AgentAsk
consistently improves accuracy and robustness over public multi-agent
implementations while keeping overhead minimal, with latency and extra cost all
less than 5%, approaching the performance of a strong evaluator. Beyond
empirical improvements, we contribute a principled taxonomy of edge-level
errors and a practical recipe for link-local intervention, offering a scalable
pathway toward more reliable LLM-based multi-agent systems.

</details>


### [47] [A Case for Leveraging Generative AI to Expand and Enhance Training in the Provision of Mental Health Services](https://arxiv.org/abs/2510.07623)
*Hannah R. Lawrence,Shannon Wiltsey Stirman,Samuel Dorison,Taedong Yun,Megan Jones Bell*

Main category: cs.AI

TL;DR: 本文主张将生成式AI应用于心理健康服务培训，而非直接作为治疗聊天机器人，认为这是更低风险、高影响力的应用场景。


<details>
  <summary>Details</summary>
Motivation: 当前对AI在心理健康领域的投资和讨论过度集中于治疗聊天机器人，但作者认为生成式AI在培训心理健康服务提供者方面具有更大潜力且风险更低。

Method: 通过实际案例研究，展示了生成式AI如何改进退伍军人相互支持的心理健康培训。

Result: 生成式AI成功提升了心理健康服务培训的效果和规模，特别是在退伍军人群体中。

Conclusion: 应该投资于利用生成式AI来支持心理健康服务提供者的培训，这是比直接应用聊天机器人更安全有效的策略。

Abstract: Generative artificial intelligence (Generative AI) is transforming
healthcare. With this evolution comes optimism regarding the impact it will
have on mental health, as well as concern regarding the risks that come with
generative AI operating in the mental health domain. Much of the investment in,
and academic and public discourse about, AI-powered solutions for mental health
has focused on therapist chatbots. Despite the common assumption that chatbots
will be the most impactful application of GenAI to mental health, we make the
case here for a lower-risk, high impact use case: leveraging generative AI to
enhance and scale training in mental health service provision. We highlight key
benefits of using generative AI to help train people to provide mental health
services and present a real-world case study in which generative AI improved
the training of veterans to support one another's mental health. With numerous
potential applications of generative AI in mental health, we illustrate why we
should invest in using generative AI to support training people in mental
health service provision.

</details>


### [48] [Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models](https://arxiv.org/abs/2510.07632)
*Yinglun Zhu,Jiancheng Zhang,Fuzhi Tang*

Main category: cs.AI

TL;DR: 论文揭示了现有评估指标系统性低估了AI模型的组合推理能力，提出了组匹配分数和测试时匹配算法，显著提升了模型在组合推理基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标对前沿AI模型的组合推理能力存在系统性低估，导致模型在基准测试中表现不佳，需要更准确的评估方法和性能提升技术。

Method: 引入组匹配分数以更好地利用组结构，并提出测试时匹配(TTM)算法——一种无需外部监督的迭代自改进方法。

Result: 新方法显著提升了模型表现：SigLIP-B16超越所有先前结果和GPT-4，GPT-4.1首次超过人类在Winoground上的表现；TTM使SigLIP-B16在MMVP-VLM上超越GPT-4.1，在16个数据集变体上均取得一致改进。

Conclusion: 通过改进评估方法和引入TTM算法，可以显著提升AI模型的组合推理能力，缩小与人类表现的差距，推动组合推理研究的前沿发展。

Abstract: Frontier AI models have achieved remarkable progress, yet recent studies
suggest they struggle with compositional reasoning, often performing at or
below random chance on established benchmarks. We revisit this problem and show
that widely used evaluation metrics systematically underestimate model
capability. To address this, we introduce a group matching score that better
exploits group structure and reveals substantial hidden capability in both
contrastive vision-language models (VLMs) and multimodal large language models
(MLLMs). Moreover, simply overfitting to the induced group matchings at test
time transfers this hidden capability into higher scores under standard
evaluation metrics, closing much of the reported gap. This adjustment enables
SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first
result surpassing estimated human performance on Winoground.
  Building on this insight, we propose Test-Time Matching (TTM), an iterative,
self-improving algorithm that further bootstraps model performance without any
external supervision. TTM delivers additional, non-trivial improvements: for
example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a
new state of the art. Importantly, TTM remains broadly effective even on
benchmarks without metric-induced effects or group structures, achieving
relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16
dataset variants spanning diverse setups, our experiments demonstrate that TTM
consistently improves model performance and advances the frontier of
compositional reasoning.

</details>


### [49] [Safely Exploring Novel Actions in Recommender Systems via Deployment-Efficient Policy Learning](https://arxiv.org/abs/2510.07635)
*Haruka Kiyohara,Yusuke Narita,Yuta Saito,Kei Tateno,Takuma Udagawa*

Main category: cs.AI

TL;DR: 提出Safe OPG方法保证推荐系统中探索新项目的安全性，并进一步开发DEPLOY框架来平衡安全保证与探索效率


<details>
  <summary>Details</summary>
Motivation: 现实推荐系统中频繁添加新项目，现有离线策略学习方法在探索新项目时存在安全隐患

Method: 首先开发基于高置信度离线策略评估的Safe OPG方法，然后提出DEPLOY框架利用安全边际和多轮部署逐步放松安全约束

Result: Safe OPG几乎总能满足安全要求，但过于保守；DEPLOY框架能在保证安全的同时有效探索新项目

Conclusion: 提出的框架能够在保证推荐系统安全实现的同时，有效探索新项目

Abstract: In many real recommender systems, novel items are added frequently over time.
The importance of sufficiently presenting novel actions has widely been
acknowledged for improving long-term user engagement. A recent work builds on
Off-Policy Learning (OPL), which trains a policy from only logged data,
however, the existing methods can be unsafe in the presence of novel actions.
Our goal is to develop a framework to enforce exploration of novel actions with
a guarantee for safety. To this end, we first develop Safe Off-Policy Policy
Gradient (Safe OPG), which is a model-free safe OPL method based on a high
confidence off-policy evaluation. In our first experiment, we observe that Safe
OPG almost always satisfies a safety requirement, even when existing methods
violate it greatly. However, the result also reveals that Safe OPG tends to be
too conservative, suggesting a difficult tradeoff between guaranteeing safety
and exploring novel actions. To overcome this tradeoff, we also propose a novel
framework called Deployment-Efficient Policy Learning for Safe User
Exploration, which leverages safety margin and gradually relaxes safety
regularization during multiple (not many) deployments. Our framework thus
enables exploration of novel actions while guaranteeing safe implementation of
recommender systems.

</details>


### [50] [Multimodal Safety Evaluation in Generative Agent Social Simulations](https://arxiv.org/abs/2510.07709)
*Alhim Vera,Karen Sanchez,Carlos Hinojosa,Haidar Bin Hamid,Donghoon Kim,Bernard Ghanem*

Main category: cs.AI

TL;DR: 该论文提出了一个可重复的仿真框架，用于评估生成式智能体在多模态环境中的安全性、连贯性和信任度，发现当前模型在纠正不安全计划方面成功率仅为55%，且45%的不安全行为在被误导性视觉信息误导时被接受。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型和视觉语言模型使智能体能够在丰富环境中自主行动并追求目标，但它们在跨模态安全性、连贯性和信任推理方面的能力仍然有限，需要系统评估。

Method: 引入可重复的仿真框架，配备分层记忆、动态规划、多模态感知能力，并使用SocialMetrics行为指标套件量化计划修订、不安全到安全转换以及网络信息扩散。

Result: 实验显示智能体在多模态矛盾检测方面表现良好，但在将局部修订与全局安全对齐方面失败，不安全计划纠正成功率仅55%。Claude、GPT-4o mini和Qwen-VL模型的不安全到安全转换率分别为75%、55%和58%。

Conclusion: 当前架构在多模态安全方面存在关键限制，特别是智能体倾向于过度信任图像，为研究多模态安全性、连贯性和社交动态提供了可重复平台。

Abstract: Can generative agents be trusted in multimodal environments? Despite advances
in large language and vision-language models that enable agents to act
autonomously and pursue goals in rich settings, their ability to reason about
safety, coherence, and trust across modalities remains limited. We introduce a
reproducible simulation framework for evaluating agents along three dimensions:
(1) safety improvement over time, including iterative plan revisions in
text-visual scenarios; (2) detection of unsafe activities across multiple
categories of social situations; and (3) social dynamics, measured as
interaction counts and acceptance ratios of social exchanges. Agents are
equipped with layered memory, dynamic planning, multimodal perception, and are
instrumented with SocialMetrics, a suite of behavioral and structural metrics
that quantifies plan revisions, unsafe-to-safe conversions, and information
diffusion across networks. Experiments show that while agents can detect direct
multimodal contradictions, they often fail to align local revisions with global
safety, reaching only a 55 percent success rate in correcting unsafe plans.
Across eight simulation runs with three models - Claude, GPT-4o mini, and
Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75,
55, and 58 percent, respectively. Overall performance ranged from 20 percent in
multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such
as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted
when paired with misleading visuals, showing a strong tendency to overtrust
images. These findings expose critical limitations in current architectures and
provide a reproducible platform for studying multimodal safety, coherence, and
social dynamics.

</details>


### [51] [Control Synthesis of Cyber-Physical Systems for Real-Time Specifications through Causation-Guided Reinforcement Learning](https://arxiv.org/abs/2510.07715)
*Xiaochen Tang,Zhenya Zhang,Miaomiao Zhang,Jie An*

Main category: cs.AI

TL;DR: 提出一种基于STL在线因果监测的奖励生成方法，通过实时监控系统行为与STL规范的符合程度，计算满足或违反的定量距离，为深度强化学习提供更稳定高效的奖励框架。


<details>
  <summary>Details</summary>
Motivation: 现有STL引导的强化学习方法使用全局评估的稀疏奖励，无法准确累积局部变化，导致训练不收敛和性能不稳定。需要一种能够反映瞬时状态动态的连续奖励生成机制。

Method: 采用STL在线因果监测，在每个控制步骤持续监控系统行为，计算满足或违反STL规范的定量距离，并提供平滑近似以克服因果语义的不连续性，使其适用于深度强化学习方法。

Result: 在Gym环境中的连续控制基准测试表明，该方法在性能上优于现有的STL引导强化学习方法，提供了更鲁棒和高效的奖励生成框架。

Conclusion: 基于在线因果语义的STL引导强化学习方法能够有效解决稀疏奖励问题，提高训练稳定性和收敛性能，为实时安全关键系统的控制合成提供了更可靠的解决方案。

Abstract: In real-time and safety-critical cyber-physical systems (CPSs), control
synthesis must guarantee that generated policies meet stringent timing and
correctness requirements under uncertain and dynamic conditions. Signal
temporal logic (STL) has emerged as a powerful formalism of expressing
real-time constraints, with its semantics enabling quantitative assessment of
system behavior. Meanwhile, reinforcement learning (RL) has become an important
method for solving control synthesis problems in unknown environments. Recent
studies incorporate STL-based reward functions into RL to automatically
synthesize control policies. However, the automatically inferred rewards
obtained by these methods represent the global assessment of a whole or partial
path but do not accumulate the rewards of local changes accurately, so the
sparse global rewards may lead to non-convergence and unstable training
performances. In this paper, we propose an online reward generation method
guided by the online causation monitoring of STL. Our approach continuously
monitors system behavior against an STL specification at each control step,
computing the quantitative distance toward satisfaction or violation and
thereby producing rewards that reflect instantaneous state dynamics.
Additionally, we provide a smooth approximation of the causation semantics to
overcome the discontinuity of the causation semantics and make it
differentiable for using deep-RL methods. We have implemented a prototype tool
and evaluated it in the Gym environment on a variety of continuously controlled
benchmarks. Experimental results show that our proposed STL-guided RL method
with online causation semantics outperforms existing relevant STL-guided RL
methods, providing a more robust and efficient reward generation framework for
deep-RL.

</details>


### [52] [oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning](https://arxiv.org/abs/2510.07731)
*Ruiling Xu,Yifan Zhang,Qingyun Wang,Carl Edwards,Heng Ji*

Main category: cs.AI

TL;DR: 提出了oMeBench，首个大规模专家标注的有机反应机理推理基准，包含10,000多个机理步骤标注，并开发了oMeS动态评估框架来精确评估LLM的化学推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在化学任务中表现出潜力，但尚不清楚其是否具备真正的化学推理能力，包括生成有效中间体、保持化学一致性和遵循逻辑连贯的多步路径。

Method: 构建oMeBench基准数据集，包含机理步骤、中间体、类型标签和难度评级；提出oMeS评估框架，结合步骤级逻辑和化学相似性进行动态评估。

Result: 当前模型显示出有前景的化学直觉，但在正确和一致的多步推理方面存在困难；通过提示策略和在数据集上微调专家模型，性能比领先闭源模型提高50%。

Conclusion: oMeBench为推进AI系统实现真正的化学推理提供了严谨基础，当前模型在化学推理方面仍有提升空间。

Abstract: Organic reaction mechanisms are the stepwise elementary reactions by which
reactants form intermediates and products, and are fundamental to understanding
chemical reactivity and designing new molecules and reactions. Although large
language models (LLMs) have shown promise in understanding chemical tasks such
as synthesis design, it is unclear to what extent this reflects genuine
chemical reasoning capabilities, i.e., the ability to generate valid
intermediates, maintain chemical consistency, and follow logically coherent
multi-step pathways. We address this by introducing oMeBench, the first
large-scale, expert-curated benchmark for organic mechanism reasoning in
organic chemistry. It comprises over 10,000 annotated mechanistic steps with
intermediates, type labels, and difficulty ratings. Furthermore, to evaluate
LLM capability more precisely and enable fine-grained scoring, we propose oMeS,
a dynamic evaluation framework that combines step-level logic and chemical
similarity. We analyze the performance of state-of-the-art LLMs, and our
results show that although current models display promising chemical intuition,
they struggle with correct and consistent multi-step reasoning. Notably, we
find that using prompting strategy and fine-tuning a specialist model on our
proposed dataset increases performance by 50% over the leading closed-source
model. We hope that oMeBench will serve as a rigorous foundation for advancing
AI systems toward genuine chemical reasoning.

</details>


### [53] [SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation](https://arxiv.org/abs/2510.07733)
*Minh-Anh Nguye,Minh-Duc Nguyen,Nguyen Thi Ha Lan,Kieu Hai Dang,Nguyen Tien Dong,Le Duy Dung*

Main category: cs.AI

TL;DR: SurveyG是一个基于LLM的智能体框架，通过整合分层引用图来生成更全面、结构更好的综述论文，解决了现有方法忽视论文间结构关系的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的综述生成方法通常直接从大量相关论文中提取内容进行总结，但忽视了论文间的结构关系，导致生成的综述缺乏连贯的分类体系和深层上下文理解。

Method: 提出SurveyG框架，整合分层引用图（包含基础层、发展层和前沿层），通过横向层内搜索和纵向跨层深度遍历生成多级摘要，并经过多智能体验证确保一致性、覆盖面和事实准确性。

Result: 实验表明，SurveyG在人类专家评估和LLM作为评判者的评估中均优于现有最先进框架，生成的综述更全面且更好地符合领域知识分类体系。

Conclusion: SurveyG通过整合结构化和上下文知识，能够生成质量更高的综述论文，为自动化综述生成提供了更有效的方法。

Abstract: Large language models (LLMs) are increasingly adopted for automating survey
paper generation \cite{wang2406autosurvey, liang2025surveyx,
yan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing
approaches typically extract content from a large collection of related papers
and prompt LLMs to summarize them directly. However, such methods often
overlook the structural relationships among papers, resulting in generated
surveys that lack a coherent taxonomy and a deeper contextual understanding of
research progress. To address these shortcomings, we propose \textbf{SurveyG},
an LLM-based agent framework that integrates \textit{hierarchical citation
graph}, where nodes denote research papers and edges capture both citation
dependencies and semantic relatedness between their contents, thereby embedding
structural and contextual knowledge into the survey generation process. The
graph is organized into three layers: \textbf{Foundation},
\textbf{Development}, and \textbf{Frontier}, to capture the evolution of
research from seminal works to incremental advances and emerging directions. By
combining horizontal search within layers and vertical depth traversal across
layers, the agent produces multi-level summaries, which are consolidated into a
structured survey outline. A multi-agent validation stage then ensures
consistency, coverage, and factual accuracy in generating the final survey.
Experiments, including evaluations by human experts and LLM-as-a-judge,
demonstrate that SurveyG outperforms state-of-the-art frameworks, producing
surveys that are more comprehensive and better structured to the underlying
knowledge taxonomy of a field.

</details>


### [54] [Haibu Mathematical-Medical Intelligent Agent:Enhancing Large Language Model Reliability in Medical Tasks via Verifiable Reasoning Chains](https://arxiv.org/abs/2510.07748)
*Yilun Zhang,Dexing Kong*

Main category: cs.AI

TL;DR: MMIA是一个基于LLM的医疗智能代理架构，通过形式化可验证的推理过程确保可靠性，在医疗管理任务中实现超过98%的错误检测率和低于1%的误报率，并能通过RAG模式降低85%的处理成本。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在医疗领域容易产生事实和逻辑错误的问题，因为这种错误在高风险医疗环境中是不可接受的。

Method: 递归分解复杂医疗任务为原子步骤，自动审核推理链的逻辑一致性和证据可追溯性，采用"引导"模式存储已验证推理链作为"定理"，后续任务使用RAG进行高效解决。

Result: 在四个医疗管理领域验证，错误检测率超过98%，误报率低于1%，显著优于基线LLMs，RAG模式预计可降低85%平均处理成本。

Conclusion: MMIA的可验证推理框架是创建可信赖、透明且成本效益高的AI系统的重要一步，使LLM技术在医疗关键应用中变得可行。

Abstract: Large Language Models (LLMs) show promise in medicine but are prone to
factual and logical errors, which is unacceptable in this high-stakes field. To
address this, we introduce the "Haibu Mathematical-Medical Intelligent Agent"
(MMIA), an LLM-driven architecture that ensures reliability through a formally
verifiable reasoning process. MMIA recursively breaks down complex medical
tasks into atomic, evidence-based steps. This entire reasoning chain is then
automatically audited for logical coherence and evidence traceability, similar
to theorem proving. A key innovation is MMIA's "bootstrapping" mode, which
stores validated reasoning chains as "theorems." Subsequent tasks can then be
efficiently solved using Retrieval-Augmented Generation (RAG), shifting from
costly first-principles reasoning to a low-cost verification model. We
validated MMIA across four healthcare administration domains, including DRG/DIP
audits and medical insurance adjudication, using expert-validated benchmarks.
Results showed MMIA achieved an error detection rate exceeding 98% with a false
positive rate below 1%, significantly outperforming baseline LLMs. Furthermore,
the RAG matching mode is projected to reduce average processing costs by
approximately 85% as the knowledge base matures. In conclusion, MMIA's
verifiable reasoning framework is a significant step toward creating
trustworthy, transparent, and cost-effective AI systems, making LLM technology
viable for critical applications in medicine.

</details>


### [55] [From Noisy to Native: LLM-driven Graph Restoration for Test-Time Graph Domain Adaptation](https://arxiv.org/abs/2510.07762)
*Xiangwei Lv,JinLuan Yang,Wang Lin,Jingyuan Chen,Beishui Liao*

Main category: cs.AI

TL;DR: 提出GRAIL框架，将测试时图域适应重新定义为生成式图恢复问题，利用大语言模型将目标图恢复到与源域对齐的状态，解决了传统方法依赖源域数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图域适应方法严重依赖源域数据，但在隐私和安全考虑下源域数据往往不可用。测试时图域适应(TT-GDA)旨在不访问源域示例的情况下进行知识迁移，这是一个重要但具有挑战性的问题。

Method: 提出GRAIL框架：1) 将节点表示压缩为紧凑潜在特征，使用图扩散过程建模图恢复；2) 量化模块将恢复特征编码为离散标记；3) 微调LLM作为生成式恢复器，将"噪声"目标图转换为"原生"图；4) 引入强化学习过程，使用对齐和置信度奖励指导恢复质量。

Result: 在多个数据集上的广泛实验证明了该方法的有效性，能够成功将目标图恢复到与源域对齐的状态。

Conclusion: 通过将TT-GDA重新定义为生成式图恢复问题，并利用LLM的生成能力，GRAIL框架在不访问源域数据的情况下实现了有效的图域适应，为解决隐私敏感场景下的域适应问题提供了新思路。

Abstract: Graph domain adaptation (GDA) has achieved great attention due to its
effectiveness in addressing the domain shift between train and test data. A
significant bottleneck in existing graph domain adaptation methods is their
reliance on source-domain data, which is often unavailable due to privacy or
security concerns. This limitation has driven the development of Test-Time
Graph Domain Adaptation (TT-GDA), which aims to transfer knowledge without
accessing the source examples. Inspired by the generative power of large
language models (LLMs), we introduce a novel framework that reframes TT-GDA as
a generative graph restoration problem, "restoring the target graph to its
pristine, source-domain-like state". There are two key challenges: (1) We need
to construct a reasonable graph restoration process and design an effective
encoding scheme that an LLM can understand, bridging the modality gap. (2) We
need to devise a mechanism to ensure the restored graph acquires the intrinsic
features of the source domain, even without access to the source data. To
ensure the effectiveness of graph restoration, we propose GRAIL, that restores
the target graph into a state that is well-aligned with the source domain.
Specifically, we first compress the node representations into compact latent
features and then use a graph diffusion process to model the graph restoration
process. Then a quantization module encodes the restored features into discrete
tokens. Building on this, an LLM is fine-tuned as a generative restorer to
transform a "noisy" target graph into a "native" one. To further improve
restoration quality, we introduce a reinforcement learning process guided by
specialized alignment and confidence rewards. Extensive experiments demonstrate
the effectiveness of our approach across various datasets.

</details>


### [56] [An approach for systematic decomposition of complex llm tasks](https://arxiv.org/abs/2510.07772)
*Tianle Zhou,Jiakai Xu,Guanhong Liu,Jiaxiang Liu,Haonan Wang,Eugene Wu*

Main category: cs.AI

TL;DR: 提出ACONIC框架，通过建模任务为约束问题并利用形式化复杂度度量来指导分解，显著提升LLM在复杂任务上的性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂任务上存在可靠性问题，现有分解方法依赖启发式、代理或手动分解

Method: 引入ACONIC框架，将任务建模为约束问题，使用形式化复杂度度量指导分解过程

Result: 在组合优化(SATBench)和LLM数据库查询任务(Spider)上，通过基于复杂度的分解，代理性能提升10-40个百分点

Conclusion: ACONIC框架提供了一种系统化的分解方法，能显著提高LLM在复杂任务上的可靠性

Abstract: Large Language Models (LLMs) suffer from reliability issues on complex tasks,
as existing decomposition methods are heuristic and rely on agent or manual
decomposition. This work introduces a novel, systematic decomposition framework
that we call Analysis of CONstraint-Induced Complexity (ACONIC), which models
the task as a constraint problem and leveraging formal complexity measures to
guide decomposition. On combinatorial (SATBench) and LLM database querying
tasks (Spider), we find that by decomposing the tasks following the measure of
complexity, agent can perform considerably better (10-40 percentage point).

</details>


### [57] [GCPO: When Contrast Fails, Go Gold](https://arxiv.org/abs/2510.07790)
*Hao Wu,Wei Liu*

Main category: cs.AI

TL;DR: 提出了Group Contrastive Policy Optimization (GCPO)方法，通过引入外部标准参考答案来解决GRPO等算法在样本全错或全对时无法有效学习的问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法如GRPO存在局限性：模型生成回答的上限完全由模型自身决定，无法从全错或全对的样本中获取知识。

Method: GCPO方法引入外部标准参考答案，当模型无法解决问题时，参考答案提供正确响应，引导模型向明确正确的更新方向学习。

Result: GCPO在多个基准数据集上取得了优异结果，相比基线模型有显著提升。

Conclusion: GCPO方法能够充分利用每个样本，提高训练效率，并让模型在训练过程中学习参考答案的解题策略，从而增强推理泛化能力。

Abstract: Reinforcement learning has been widely applied to enhance the reasoning
capabilities of large language models. Extending the inference limits of
smaller models has become a prominent research focus. However, algorithms such
as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the
upper bound of a model's rollout responses is entirely determined by the model
itself, preventing the acquisition of knowledge from samples that are either
all incorrect or all correct. In this paper, we introduce Group Contrastive
Policy Optimization (GCPO), a method that incorporates external standard
reference answers. When the model cannot solve a problem, the reference answer
supplies the correct response, steering the model toward an unequivocally
accurate update direction. This approach offers two main advantages: (1) it
improves training efficiency by fully utilizing every sample; (2) it enables
the model to emulate the problem solving strategy of the reference answer
during training, thereby enhancing generalization in reasoning. GCPO achieves
outstanding results across multiple benchmark datasets, yielding substantial
improvements over the baseline model. Our code is available at:
https://github.com/AchoWu/GCPO.

</details>


### [58] [Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games](https://arxiv.org/abs/2510.07813)
*Valerio La Gatta,Dolev Mutzari,Sarit Kraus,VS Subrahmanian*

Main category: cs.AI

TL;DR: 提出了SHADOW框架，通过强化学习平衡追击者在对抗环境中的信息获取与暴露风险，实现了比基线方法更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 在对抗环境中，智能体面临信息获取与暴露风险的关键权衡：通信可以获取敌方位置信息，但会暴露自身位置增加被攻击风险。

Method: 提出PEEC博弈框架，使用SHADOW多头部序列强化学习框架，整合连续导航控制、离散通信决策和对手行为建模。

Result: SHADOW追击者相比6个竞争基线方法获得更高成功率，消融研究确认时间序列建模和对手建模对有效决策至关重要。

Conclusion: 学习到的策略在不同通信风险和物理不对称条件下具有良好的泛化能力，证明了SHADOW框架在对抗环境中的有效性。

Abstract: Adversarial environments require agents to navigate a key strategic
trade-off: acquiring information enhances situational awareness, but may
simultaneously expose them to threats. To investigate this tension, we
formulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursuer
agent must decide when to communicate in order to obtain the evader's position.
Each communication reveals the pursuer's location, increasing the risk of being
targeted. Both agents learn their movement policies via reinforcement learning,
while the pursuer additionally learns a communication policy that balances
observability and risk. We propose SHADOW (Strategic-communication Hybrid
Action Decision-making under partial Observation for Warfare), a multi-headed
sequential reinforcement learning framework that integrates continuous
navigation control, discrete communication actions, and opponent modeling for
behavior prediction. Empirical evaluations show that SHADOW pursuers achieve
higher success rates than six competitive baselines. Our ablation study
confirms that temporal sequence modeling and opponent modeling are critical for
effective decision-making. Finally, our sensitivity analysis reveals that the
learned policies generalize well across varying communication risks and
physical asymmetries between agents.

</details>


### [59] [An LLM-Powered Cooperative Framework for Large-Scale Multi-Vehicle Navigation](https://arxiv.org/abs/2510.07825)
*Yuping Zhou,Siqi Lai,Jindong Han,Hao Liu*

Main category: cs.AI

TL;DR: CityNav是一个基于LLM的分层框架，用于大规模多车辆动态导航，通过全局交通分配代理和局部导航代理的协同优化，在城市级路网中显著提升交通效率和缓解拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有的路径搜索算法和强化学习方法难以扩展到城市级网络，无法有效捕捉城市交通的非线性、随机性和耦合动态特性。

Method: 提出分层框架：全局交通分配代理协调区域间战略交通流分布，局部导航代理生成与全局指令一致的局部自适应路线；引入协同推理优化机制，采用双奖励结构进行联合训练。

Result: 在四个不同规模的真实路网（最大160万条道路和43万个交叉口）上的实验表明，CityNav在城际旅行效率和拥堵缓解方面持续优于九种经典路径搜索和基于RL的基线方法。

Conclusion: LLM能够实现可扩展、自适应和协同的城市级交通导航，为复杂城市环境中的智能大规模车辆路由提供了基础。

Abstract: The rise of Internet of Vehicles (IoV) technologies is transforming traffic
management from isolated control to a collective, multi-vehicle process. At the
heart of this shift is multi-vehicle dynamic navigation, which requires
simultaneously routing large fleets under evolving traffic conditions. Existing
path search algorithms and reinforcement learning methods struggle to scale to
city-wide networks, often failing to capture the nonlinear, stochastic, and
coupled dynamics of urban traffic. To address these challenges, we propose
CityNav, a hierarchical, LLM-powered framework for large-scale multi-vehicle
navigation. CityNav integrates a global traffic allocation agent, which
coordinates strategic traffic flow distribution across regions, with local
navigation agents that generate locally adaptive routes aligned with global
directives. To enable effective cooperation, we introduce a cooperative
reasoning optimization mechanism, in which agents are jointly trained with a
dual-reward structure: individual rewards promote per-vehicle efficiency, while
shared rewards encourage network-wide coordination and congestion reduction.
Extensive experiments on four real-world road networks of varying scales (up to
1.6 million roads and 430,000 intersections) and traffic datasets demonstrate
that CityNav consistently outperforms nine classical path search and RL-based
baselines in city-scale travel efficiency and congestion mitigation. Our
results highlight the potential of LLMs to enable scalable, adaptive, and
cooperative city-wide traffic navigation, providing a foundation for
intelligent, large-scale vehicle routing in complex urban environments. Our
project is available at https://github.com/usail-hkust/CityNav.

</details>


### [60] [FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning](https://arxiv.org/abs/2510.07852)
*Shuangyan Deng,Haizhou Peng,Jiachen Xu,Rui Mao,Ciprian Doru Giurcăneanu,Jiamou Liu*

Main category: cs.AI

TL;DR: FinMR是一个高质量、知识密集的多模态数据集，专门设计用于评估专业分析师级别的金融推理能力，包含3200多个精心策划的问答对，涵盖15个金融主题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在金融等专业领域的严格评估受到缺乏专业级知识强度、详细注释和高级推理复杂性的数据集的限制。

Method: 构建FinMR数据集，包含3200多个专家标注的问答对，涵盖15个金融主题，整合了复杂数学推理、高级金融知识和多类型图像解释任务。

Result: 通过对领先的闭源和开源MLLMs进行基准测试，发现这些模型与专业金融分析师之间存在显著性能差距，特别是在精确图像分析、复杂金融公式准确应用和深度上下文金融理解方面。

Conclusion: FinMR通过提供丰富多样的视觉内容和详细解释性注释，成为评估和推进多模态金融推理达到专业分析师水平的重要基准工具。

Abstract: Multimodal Large Language Models (MLLMs) have made substantial progress in
recent years. However, their rigorous evaluation within specialized domains
like finance is hindered by the absence of datasets characterized by
professional-level knowledge intensity, detailed annotations, and advanced
reasoning complexity. To address this critical gap, we introduce FinMR, a
high-quality, knowledge-intensive multimodal dataset explicitly designed to
evaluate expert-level financial reasoning capabilities at a professional
analyst's standard. FinMR comprises over 3,200 meticulously curated and
expertly annotated question-answer pairs across 15 diverse financial topics,
ensuring broad domain diversity and integrating sophisticated mathematical
reasoning, advanced financial knowledge, and nuanced visual interpretation
tasks across multiple image types. Through comprehensive benchmarking with
leading closed-source and open-source MLLMs, we highlight significant
performance disparities between these models and professional financial
analysts, uncovering key areas for model advancement, such as precise image
analysis, accurate application of complex financial formulas, and deeper
contextual financial understanding. By providing richly varied visual content
and thorough explanatory annotations, FinMR establishes itself as an essential
benchmark tool for assessing and advancing multimodal financial reasoning
toward professional analyst-level competence.

</details>


### [61] [Augur: Modeling Covariate Causal Associations in Time Series via Large Language Models](https://arxiv.org/abs/2510.07858)
*Zhiqing Cui,Binwu Wang,Qingxiang Liu,Yeqiang Wang,Zhengyang Zhou,Yuxuan Liang,Yang Wang*

Main category: cs.AI

TL;DR: Augur是一个完全基于LLM的时间序列预测框架，利用LLM的因果推理能力发现变量间的有向因果关系，通过师生架构实现准确且可解释的预测。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时间序列预测方法存在架构边缘化、依赖粗糙统计文本提示、缺乏可解释性等局限性，需要更有效的LLM驱动解决方案。

Method: 采用两阶段师生架构：教师LLM通过启发式搜索和成对因果检验推断有向因果图；学生代理精炼图结构，并在高置信度因果关联上进行微调，使用丰富的文本提示进行预测。

Result: 在真实世界数据集上与25个基线方法对比，Augur实现了有竞争力的性能，并展现出强大的零样本泛化能力。

Conclusion: Augur框架通过利用LLM的因果推理能力，在保持预测准确性的同时提供了透明可追溯的变量交互推理，为时间序列预测提供了新的有效途径。

Abstract: Large language models (LLM) have emerged as a promising avenue for time
series forecasting, offering the potential to integrate multimodal data.
However, existing LLM-based approaches face notable limitations-such as
marginalized role in model architectures, reliance on coarse statistical text
prompts, and lack of interpretability. In this work, we introduce Augur, a
fully LLM driven time series forecasting framework that exploits LLM causal
reasoning to discover and use directed causal associations among covariates.
Augur uses a two stage teacher student architecture where a powerful teacher
LLM infers a directed causal graph from time series using heuristic search
together with pairwise causality testing. A lightweight student agent then
refines the graph and fine tune on high confidence causal associations that are
encoded as rich textual prompts to perform forecasting. This design improves
predictive accuracy while yielding transparent, traceable reasoning about
variable interactions. Extensive experiments on real-world datasets with 25
baselines demonstrate that Augur achieves competitive performance and robust
zero-shot generalization.

</details>


### [62] [Understanding DeepResearch via Reports](https://arxiv.org/abs/2510.07861)
*Tianyu Fan,Xinyao Niu,Yuxiang Zheng,Fengji Zhang,Chengen Huang,Bei Chen,Junyang Lin,Chao Huang*

Main category: cs.AI

TL;DR: 提出了DeepResearch-ReportEval评估框架，用于系统评估DeepResearch智能体的研究能力，通过质量、冗余性和事实性三个维度来衡量研究报告的质量。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以全面评估DeepResearch系统的整体研究能力，因为这类系统需要综合多种来源、生成见解并呈现连贯发现，这些能力难以通过简单验证来衡量。

Method: 采用LLM-as-a-Judge方法，构建包含100个精心设计的查询和12个现实世界类别的标准化基准，系统评估研究报告的质量、冗余性和事实性。

Result: 对四个领先商业系统的评估揭示了不同的设计理念和性能权衡，为DeepResearch从信息助手向智能研究伙伴的演进提供了基础性见解。

Conclusion: DeepResearch-ReportEval框架填补了DeepResearch系统评估的重要空白，为系统性能比较和能力演进提供了标准化方法。

Abstract: DeepResearch agents represent a transformative AI paradigm, conducting
expert-level research through sophisticated reasoning and multi-tool
integration. However, evaluating these systems remains critically challenging
due to open-ended research scenarios and existing benchmarks that focus on
isolated capabilities rather than holistic performance. Unlike traditional LLM
tasks, DeepResearch systems must synthesize diverse sources, generate insights,
and present coherent findings, which are capabilities that resist simple
verification. To address this gap, we introduce DeepResearch-ReportEval, a
comprehensive framework designed to assess DeepResearch systems through their
most representative outputs: research reports. Our approach systematically
measures three dimensions: quality, redundancy, and factuality, using an
innovative LLM-as-a-Judge methodology achieving strong expert concordance. We
contribute a standardized benchmark of 100 curated queries spanning 12
real-world categories, enabling systematic capability comparison. Our
evaluation of four leading commercial systems reveals distinct design
philosophies and performance trade-offs, establishing foundational insights as
DeepResearch evolves from information assistants toward intelligent research
partners. Source code and data are available at:
https://github.com/HKUDS/DeepResearch-Eval.

</details>


### [63] [Towards Meaningful Transparency in Civic AI Systems](https://arxiv.org/abs/2510.07889)
*Dave Murray-Rust,Kars Alfrink,Cristina Zaga*

Main category: cs.AI

TL;DR: 本文提出了"有意义透明度"概念，旨在让公众能够理解并参与影响其生活的AI系统，将理解与行动可能性联系起来。


<details>
  <summary>Details</summary>
Motivation: 当前AI透明度实践主要关注技术对象，难以被公众理解，无法连接到行动可能性，也无法洞察决策的广泛社会物质背景。

Method: 基于现有以人为中心的AI透明度方法，结合社会技术系统视角，发展公民AI系统的有意义透明度概念。

Result: 开发了有意义透明度的概念框架，强调透明度应使公众能够参与影响其生活的AI系统。

Conclusion: 有意义透明度是解决公民AI系统问题的关键，它连接了理解与行动，使公众能够真正参与决策过程。

Abstract: Artificial intelligence has become a part of the provision of governmental
services, from making decisions about benefits to issuing fines for parking
violations. However, AI systems rarely live up to the promise of neutral
optimisation, creating biased or incorrect outputs and reducing the agency of
both citizens and civic workers to shape the way decisions are made.
Transparency is a principle that can both help subjects understand decisions
made about them and shape the processes behind those decisions. However,
transparency as practiced around AI systems tends to focus on the production of
technical objects that represent algorithmic aspects of decision making. These
are often difficult for publics to understand, do not connect to potential for
action, and do not give insight into the wider socio-material context of
decision making. In this paper, we build on existing approaches that take a
human-centric view on AI transparency, combined with a socio-technical systems
view, to develop the concept of meaningful transparency for civic AI systems:
transparencies that allow publics to engage with AI systems that affect their
lives, connecting understanding with potential for action.

</details>


### [64] [Profit Mirage: Revisiting Information Leakage in LLM-based Financial Agents](https://arxiv.org/abs/2510.07920)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: 该论文揭示了LLM金融代理存在"利润幻象"问题，即回测表现优秀但实际表现差，原因是LLM存在信息泄露。作者提出了FactFin框架和FinLake-Bench基准来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: LLM金融代理在回测中表现出色但实际表现差，主要原因是LLM存在信息泄露问题，导致模型记忆了历史结果而非学习真正的因果关系。

Method: 提出了FactFin框架，包含四个核心组件：策略代码生成器、检索增强生成、蒙特卡洛树搜索和反事实模拟器，通过反事实扰动迫使LLM学习因果驱动因素。

Result: 实验表明该方法在所有基线方法中表现出更好的样本外泛化能力，实现了优越的风险调整后性能。

Conclusion: FactFin框架能有效缓解LLM金融代理的信息泄露问题，提升模型的泛化能力和实际交易表现。

Abstract: LLM-based financial agents have attracted widespread excitement for their
ability to trade like human experts. However, most systems exhibit a "profit
mirage": dazzling back-tested returns evaporate once the model's knowledge
window ends, because of the inherent information leakage in LLMs. In this
paper, we systematically quantify this leakage issue across four dimensions and
release FinLake-Bench, a leakage-robust evaluation benchmark. Furthermore, to
mitigate this issue, we introduce FactFin, a framework that applies
counterfactual perturbations to compel LLM-based agents to learn causal drivers
instead of memorized outcomes. FactFin integrates four core components:
Strategy Code Generator, Retrieval-Augmented Generation, Monte Carlo Tree
Search, and Counterfactual Simulator. Extensive experiments show that our
method surpasses all baselines in out-of-sample generalization, delivering
superior risk-adjusted performance.

</details>


### [65] [Enabling Personalized Long-term Interactions in LLM-based Agents through Persistent Memory and User Profiles](https://arxiv.org/abs/2510.07925)
*Rebecca Westhäußer,Wolfgang Minker,Sebatian Zepf*

Main category: cs.AI

TL;DR: 提出了一个集成持久记忆、动态协调、自我验证和演进用户档案的框架，用于实现基于LLM的个性化长期交互代理。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为AI代理控制单元时缺乏个性化交互能力，RAG虽然提升了上下文感知但无法结合用户特定数据，现有个性化研究多停留在概念层面而缺乏技术实现。

Method: 基于统一的个性化定义建立技术需求，结合多代理协作、多源检索等AI模式，构建包含持久记忆、动态协调、自我验证和演进用户档案的框架。

Result: 在三个公共数据集上评估了检索准确性、响应正确性和BertScore指标，并通过5天试点用户研究获得关于感知个性化的初步用户反馈。

Conclusion: 研究表明集成持久记忆和用户档案有潜力提升LLM代理的自适应性和感知个性化，为未来工作提供了指导方向。

Abstract: Large language models (LLMs) increasingly serve as the central control unit
of AI agents, yet current approaches remain limited in their ability to deliver
personalized interactions. While Retrieval Augmented Generation enhances LLM
capabilities by improving context-awareness, it lacks mechanisms to combine
contextual information with user-specific data. Although personalization has
been studied in fields such as human-computer interaction or cognitive science,
existing perspectives largely remain conceptual, with limited focus on
technical implementation. To address these gaps, we build on a unified
definition of personalization as a conceptual foundation to derive technical
requirements for adaptive, user-centered LLM-based agents. Combined with
established agentic AI patterns such as multi-agent collaboration or
multi-source retrieval, we present a framework that integrates persistent
memory, dynamic coordination, self-validation, and evolving user profiles to
enable personalized long-term interactions. We evaluate our approach on three
public datasets using metrics such as retrieval accuracy, response correctness,
or BertScore. We complement these results with a five-day pilot user study
providing initial insights into user feedback on perceived personalization. The
study provides early indications that guide future work and highlights the
potential of integrating persistent memory and user profiles to improve the
adaptivity and perceived personalization of LLM-based agents.

</details>


### [66] [Agent-Based Genetic Algorithm for Crypto Trading Strategy Optimization](https://arxiv.org/abs/2510.07943)
*Qiushi Tian,Churong Liang,Kairan Hong,Runnan Li*

Main category: cs.AI

TL;DR: 提出CGA-Agent混合框架，结合遗传算法与多智能体协调机制，用于加密货币交易策略参数优化，在动态金融环境中实现自适应优化。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场具有极端波动性、非平稳动态和复杂微观结构模式，传统参数优化方法难以应对这些挑战。

Method: 开发CGA-Agent框架，集成遗传算法与智能多智能体协调机制，结合实时市场微观结构智能和自适应策略性能反馈，动态引导进化过程。

Result: 在三种加密货币上的实证评估显示，在总回报和风险调整指标上都实现了系统性和统计显著的性能改进。

Conclusion: 该框架超越了静态优化方法的局限性，为动态金融环境中的交易策略优化提供了有效解决方案。

Abstract: Cryptocurrency markets present formidable challenges for trading strategy
optimization due to extreme volatility, non-stationary dynamics, and complex
microstructure patterns that render conventional parameter optimization methods
fundamentally inadequate. We introduce Cypto Genetic Algorithm Agent
(CGA-Agent), a pioneering hybrid framework that synergistically integrates
genetic algorithms with intelligent multi-agent coordination mechanisms for
adaptive trading strategy parameter optimization in dynamic financial
environments. The framework uniquely incorporates real-time market
microstructure intelligence and adaptive strategy performance feedback through
intelligent mechanisms that dynamically guide evolutionary processes,
transcending the limitations of static optimization approaches. Comprehensive
empirical evaluation across three cryptocurrencies demonstrates systematic and
statistically significant performance improvements on both total returns and
risk-adjusted metrics.

</details>


### [67] [TaoSR-SHE: Stepwise Hybrid Examination Reinforcement Learning Framework for E-commerce Search Relevance](https://arxiv.org/abs/2510.07972)
*Pengkun Jiao,Yiming Jin,Jianhui Yang,Chenhe Dong,Zerui Huang,Shaowei Yao,Xiaojiang Zhou,Dan Ou,Haihong Tang*

Main category: cs.AI

TL;DR: 提出了TaoSR-SHE框架，通过逐步混合检查强化学习解决电商搜索相关性分析中的推理一致性和泛化问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有训练范式存在局限性：SFT和DPO在长尾查询上泛化能力差，缺乏细粒度逐步监督；RLVR存在稀疏反馈问题，无法纠正错误中间步骤。

Method: 核心是逐步奖励策略优化(SRPO)，结合生成式逐步奖励模型和人工标注离线验证器的混合奖励，采用多样化数据过滤和多阶段课程学习。

Result: 在真实搜索基准测试中，TaoSR-SHE在推理质量和相关性预测准确性方面均优于SFT、DPO、GRPO等基线方法。

Conclusion: 该框架在大型电商环境中提高了推理质量和相关性预测准确性，同时增强了可解释性和鲁棒性。

Abstract: Query-product relevance analysis is a foundational technology in e-commerce
search engines and has become increasingly important in AI-driven e-commerce.
The recent emergence of large language models (LLMs), particularly their
chain-of-thought (CoT) reasoning capabilities, offers promising opportunities
for developing relevance systems that are both more interpretable and more
robust. However, existing training paradigms have notable limitations: SFT and
DPO suffer from poor generalization on long-tail queries and from a lack of
fine-grained, stepwise supervision to enforce rule-aligned reasoning. In
contrast, reinforcement learning with verification rewards (RLVR) suffers from
sparse feedback, which provides insufficient signal to correct erroneous
intermediate steps, thereby undermining logical consistency and limiting
performance in complex inference scenarios.
  To address these challenges, we introduce the Stepwise Hybrid Examination
Reinforcement Learning framework for Taobao Search Relevance (TaoSR-SHE). At
its core is Stepwise Reward Policy Optimization (SRPO), a reinforcement
learning algorithm that leverages step-level rewards generated by a hybrid of a
high-quality generative stepwise reward model and a human-annotated offline
verifier, prioritizing learning from critical correct and incorrect reasoning
steps. TaoSR-SHE further incorporates two key techniques: diversified data
filtering to encourage exploration across varied reasoning paths and mitigate
policy entropy collapse, and multi-stage curriculum learning to foster
progressive capability growth. Extensive experiments on real-world search
benchmarks show that TaoSR-SHE improves both reasoning quality and
relevance-prediction accuracy in large-scale e-commerce settings, outperforming
SFT, DPO, GRPO, and other baselines, while also enhancing interpretability and
robustness.

</details>


### [68] [VoiceAgentBench: Are Voice Assistants ready for agentic tasks?](https://arxiv.org/abs/2510.07978)
*Dhruv Jain,Harshit Shukla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.AI

TL;DR: 提出了VoiceAgentBench基准测试，用于评估语音语言模型在真实语音代理场景中的表现，包含5500多个合成语音查询，支持英语、印地语和5种印度语言，评估工具选择准确性、结构一致性和工具调用正确性。


<details>
  <summary>Details</summary>
Motivation: 现有语音基准测试主要关注转录或问答等孤立能力，缺乏对多语言文化理解和对抗鲁棒性的系统性评估，无法全面评估语音代理场景。

Method: 创建包含单工具调用、多工具工作流、多轮交互和安全评估的综合基准，使用基于说话人嵌入的采样算法最大化声学和说话人多样性，支持多种印度语言。

Result: 实验揭示了当前语音语言模型在上下文工具编排任务、印度语言泛化和对抗鲁棒性方面存在显著差距和关键限制。

Conclusion: VoiceAgentBench基准测试暴露了现有语音语言模型在真实代理场景中的不足，特别是在多语言文化理解和对抗鲁棒性方面需要改进。

Abstract: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants
capable of understanding natural spoken queries and performing complex tasks.
However, existing speech benchmarks primarily focus on isolated capabilities
such as transcription, or question-answering, and do not systematically
evaluate agentic scenarios encompassing multilingual and cultural
understanding, as well as adversarial robustness. To address this, we introduce
VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in
realistic spoken agentic settings. It comprises over 5,500 synthetic spoken
queries, including dialogues grounded in Indian context, covering single-tool
invocations, multi-tool workflows, multi-turn interactions, and safety
evaluations. The benchmark supports English, Hindi, and 5 other Indian
languages, reflecting real-world linguistic and cultural diversity. We simulate
speaker variability using a novel sampling algorithm that selects audios for
TTS voice conversion based on its speaker embeddings, maximizing acoustic and
speaker diversity. Our evaluation measures tool selection accuracy, structural
consistency, and the correctness of tool invocations, including adversarial
robustness. Our experiments reveal significant gaps in contextual tool
orchestration tasks, Indic generalization, and adversarial robustness, exposing
critical limitations of current SpeechLMs.

</details>


### [69] [ReInAgent: A Context-Aware GUI Agent Enabling Human-in-the-Loop Mobile Task Navigation](https://arxiv.org/abs/2510.07988)
*Haitao Jia,Ming He,Zimo Yin,Likang Wu,Jianping Fan,Jitao Sang*

Main category: cs.AI

TL;DR: ReInAgent是一个上下文感知的多代理框架，通过动态信息管理和人机协作解决移动GUI代理在信息困境中的适应性问题


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理过于强调自主操作，忽视了用户参与的重要性，导致在模糊、动态变化和冲突的任务场景中无法满足真实用户需求

Method: 集成三个专门代理：信息管理代理负责基于槽位的信息管理和主动用户交互，决策代理负责冲突感知规划，反思代理负责任务反思和信息一致性验证

Result: 在涉及信息困境的复杂任务中，ReInAgent比Mobile-Agent-v2实现了25%更高的成功率，能更有效地解决信息困境并产生更符合用户偏好的结果

Conclusion: 通过持续上下文信息分析和持续的用户-代理协作，ReInAgent克服了现有方法依赖清晰静态任务假设的局限性，实现了在复杂现实场景中更自适应和可靠的移动任务导航

Abstract: Mobile GUI agents exhibit substantial potential to facilitate and automate
the execution of user tasks on mobile phones. However, exist mobile GUI agents
predominantly privilege autonomous operation and neglect the necessity of
active user engagement during task execution. This omission undermines their
adaptability to information dilemmas including ambiguous, dynamically evolving,
and conflicting task scenarios, leading to execution outcomes that deviate from
genuine user requirements and preferences. To address these shortcomings, we
propose ReInAgent, a context-aware multi-agent framework that leverages dynamic
information management to enable human-in-the-loop mobile task navigation.
ReInAgent integrates three specialized agents around a shared memory module: an
information-managing agent for slot-based information management and proactive
interaction with the user, a decision-making agent for conflict-aware planning,
and a reflecting agent for task reflection and information consistency
validation. Through continuous contextual information analysis and sustained
user-agent collaboration, ReInAgent overcomes the limitation of existing
approaches that rely on clear and static task assumptions. Consequently, it
enables more adaptive and reliable mobile task navigation in complex,
real-world scenarios. Experimental results demonstrate that ReInAgent
effectively resolves information dilemmas and produces outcomes that are more
closely aligned with genuine user preferences. Notably, on complex tasks
involving information dilemmas, ReInAgent achieves a 25% higher success rate
than Mobile-Agent-v2.

</details>


### [70] [Language Models Do Not Embed Numbers Continuously](https://arxiv.org/abs/2510.08009)
*Alex O. Davies,Roussel Nzoyem,Nirav Ajmeri,Telmo M. Silva Filho*

Main category: cs.AI

TL;DR: 研究发现语言模型在表示数值时存在非连续性和噪声问题，尽管数值重建精度高，但主成分分析显示嵌入空间的大部分变化与简单数值输入空间正交。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否真正连续地表示数值，以及它们在处理数值时的内在表示机制。

Method: 使用来自OpenAI、Google Gemini和Voyage AI的模型，通过线性重建和主成分分析评估嵌入空间的预期属性。

Result: 虽然数值重建精度高（R²≥0.95），但主成分只能解释嵌入空间变化的很小部分，表明许多嵌入组件与简单数值输入空间正交。随着小数精度的增加，线性重建和解释方差都会恶化。

Conclusion: 语言模型在表示数值时引入显著噪声且非连续，这对需要高数值精度、大数值范围或混合符号数值的应用领域具有重要影响。

Abstract: Recent research has extensively studied how large language models manipulate
integers in specific arithmetic tasks, and on a more fundamental level, how
they represent numeric values. These previous works have found that language
model embeddings can be used to reconstruct the original values, however, they
do not evaluate whether language models actually model continuous values as
continuous. Using expected properties of the embedding space, including linear
reconstruction and principal component analysis, we show that language models
not only represent numeric spaces as non-continuous but also introduce
significant noise. Using models from three major providers (OpenAI, Google
Gemini and Voyage AI), we show that while reconstruction is possible with high
fidelity ($R^2 \geq 0.95$), principal components only explain a minor share of
variation within the embedding space. This indicates that many components
within the embedding space are orthogonal to the simple numeric input space.
Further, both linear reconstruction and explained variance suffer with
increasing decimal precision, despite the ordinal nature of the input space
being fundamentally unchanged. The findings of this work therefore have
implications for the many areas where embedding models are used, in-particular
where high numerical precision, large magnitudes or mixed-sign values are
common.

</details>


### [71] [PEAR: Phase Entropy Aware Reward for Efficient Reasoning](https://arxiv.org/abs/2510.08026)
*Chen Huang,Wei Lu,Wenxuan Zhang*

Main category: cs.AI

TL;DR: PEAR是一种基于阶段熵感知的奖励机制，通过在思维阶段惩罚过高熵值、在答案阶段允许适度探索，实现推理长度的自适应控制，在保持准确性的同时显著缩短响应长度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成的推理链往往过长且包含冗余步骤，增加了推理成本并降低了可用性。如何在保持准确性的同时控制推理长度是一个开放挑战。

Method: 提出阶段熵感知奖励机制PEAR，将不同推理阶段的熵值纳入奖励设计：在思维阶段惩罚过高熵值以减少冗余探索，在答案阶段允许适度熵值以保持灵活性。

Result: 在四个基准测试上的广泛实验表明，PEAR能持续减少响应长度，同时在不同模型规模下保持竞争力准确率，并展现出超出训练分布的强鲁棒性。

Conclusion: PEAR通过阶段熵感知机制有效平衡了推理的简洁性和性能，为自适应控制推理长度提供了无需显式长度目标或刚性截断规则的新方法。

Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on complex
reasoning tasks by generating detailed chain-of-thought (CoT) explanations.
However, these responses are often excessively long, containing redundant
reasoning steps that inflate inference cost and reduce usability. Controlling
the length of generated reasoning without sacrificing accuracy remains an open
challenge. Through a systematic empirical analysis, we reveal a consistent
positive correlation between model entropy and response length at different
reasoning stages across diverse LRMs: the thinking phase exhibits higher
entropy, reflecting exploratory behavior of longer responses, while the final
answer phase shows lower entropy, indicating a more deterministic solution.This
observation suggests that entropy at different reasoning stages can serve as a
control knob for balancing conciseness and performance. Based on this insight,
this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism
that incorporating phase-dependent entropy into the reward design. Instead of
treating all tokens uniformly, PEAR penalize excessive entropy during the
thinking phase and allowing moderate exploration at the final answer phase,
which encourages models to generate concise reasoning traces that retain
sufficient flexibility to solve the task correctly. This enables adaptive
control of response length without relying on explicit length targets or rigid
truncation rules. Extensive experiments across four benchmarks demonstrate that
PEAR consistently reduces response length while sustaining competitive accuracy
across model scales. In addition, PEAR demonstrates strong out-of-distribution
(OOD) robustness beyond the training distribution. Our code is available at:
https://github.com/iNLP-Lab/PEAR.

</details>


### [72] [AILoRA: Function-Aware Asymmetric Initialization for Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2510.08034)
*Xiaoshuang Ji,Zhendong Zhao,Xiaoyan Gu,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.AI

TL;DR: 提出AILoRA方法，通过函数感知的非对称低秩先验改进LoRA，针对自注意力机制中W^Q和W^V矩阵的不同功能特性进行差异化初始化，提升微调性能和收敛效率。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然在参数高效微调中表现优异，但仍面临性能次优和收敛缓慢的问题。研究发现自注意力机制中W^Q和W^V矩阵具有不同的参数特性，需要针对性的优化策略。

Method: AILoRA采用函数感知的非对称低秩先验，对W^Q注入主成分以保持任务适应能力，对W^V注入次要成分以保留通用特征表示，实现差异化初始化。

Result: 该方法能够更好地捕捉注意力参数的专业化角色，显著提升微调性能和收敛效率。

Conclusion: 通过考虑自注意力机制中不同投影矩阵的功能差异，AILoRA为参数高效微调提供了更精细的优化策略，在保持参数效率的同时改善了模型性能。

Abstract: Parameter-efficient finetuning (PEFT) aims to mitigate the substantial
computational and memory overhead involved in adapting large-scale pretrained
models to diverse downstream tasks. Among numerous PEFT strategies, Low-Rank
Adaptation (LoRA) has emerged as one of the most widely adopted approaches due
to its robust empirical performance and low implementation complexity. In
practical deployment, LoRA is typically applied to the $W^Q$ and $W^V$
projection matrices of self-attention modules, enabling an effective trade-off
between model performance and parameter efficiency. While LoRA has achieved
considerable empirical success, it still encounters challenges such as
suboptimal performance and slow convergence. To address these limitations, we
introduce \textbf{AILoRA}, a novel parameter-efficient method that incorporates
function-aware asymmetric low-rank priors. Our empirical analysis reveals that
the projection matrices $W^Q$ and $W^V$ in the self-attention mechanism exhibit
distinct parameter characteristics, stemming from their functional differences.
Specifically, $W^Q$ captures task-specific semantic space knowledge essential
for attention distributions computation, making its parameters highly sensitive
to downstream task variations. In contrast, $W^V$ encodes token-level feature
representations that tend to remain stable across tasks and layers. Leveraging
these insights, AILoRA performs a function-aware initialization by injecting
the principal components of $W^Q$ to retain task-adaptive capacity, and the
minor components of $W^V$ to preserve generalizable feature representations.
This asymmetric initialization strategy enables LoRA modules to better capture
the specialized roles of attention parameters, thereby enhancing both
finetuning performance and convergence efficiency.

</details>


### [73] [LinguaSim: Interactive Multi-Vehicle Testing Scenario Generation via Natural Language Instruction Based on Large Language Models](https://arxiv.org/abs/2510.08046)
*Qingyuan Shi,Qingwen Meng,Hao Cheng,Qing Xu,Jianqiang Wang*

Main category: cs.AI

TL;DR: LinguaSim是一个基于大语言模型的框架，能将自然语言转换为真实、交互式的3D场景，确保动态车辆交互和输入描述与生成场景的准确对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的场景生成方法难以平衡指令遵循准确性与真实世界驾驶环境的真实性，常常为了降低场景描述复杂度而牺牲真实性，限制在2D或开放循环模拟中。

Method: 提出LinguaSim框架，包含反馈校准模块来提升生成精度，通过自然语言与闭环交互模拟的桥梁，约束对抗性车辆行为。

Result: 实验显示LinguaSim能生成不同关键程度的场景（危险描述ACT：0.072秒 vs 安全描述3.532秒；舒适度0.654 vs 0.764），其精炼模块有效降低初始输出的过度攻击性，碰撞率从46.9%降至6.3%。

Conclusion: LinguaSim框架能够创建高保真场景，增强安全测试和训练，更好地匹配用户意图。

Abstract: The generation of testing and training scenarios for autonomous vehicles has
drawn significant attention. While Large Language Models (LLMs) have enabled
new scenario generation methods, current methods struggle to balance command
adherence accuracy with the realism of real-world driving environments. To
reduce scenario description complexity, these methods often compromise realism
by limiting scenarios to 2D, or open-loop simulations where background vehicles
follow predefined, non-interactive behaviors. We propose LinguaSim, an
LLM-based framework that converts natural language into realistic, interactive
3D scenarios, ensuring both dynamic vehicle interactions and faithful alignment
between the input descriptions and the generated scenarios. A feedback
calibration module further refines the generation precision, improving fidelity
to user intent. By bridging the gap between natural language and closed-loop,
interactive simulations, LinguaSim constrains adversarial vehicle behaviors
using both the scenario description and the autonomous driving model guiding
them. This framework facilitates the creation of high-fidelity scenarios that
enhance safety testing and training. Experiments show LinguaSim can generate
scenarios with varying criticality aligned with different natural language
descriptions (ACT: 0.072 s for dangerous vs. 3.532 s for safe descriptions;
comfortability: 0.654 vs. 0.764), and its refinement module effectively reduces
excessive aggressiveness in LinguaSim's initial outputs, lowering the crash
rate from 46.9% to 6.3% to better match user intentions.

</details>


### [74] [Multi-Condition Conformal Selection](https://arxiv.org/abs/2510.08075)
*Qingyang Hao,Wenbo Liao,Bingyi Jing,Hongxin Wei*

Main category: cs.AI

TL;DR: 提出了多条件符合选择（MCCS）算法，将符合选择扩展到多条件场景，实现了有限样本下的FDR控制。


<details>
  <summary>Details</summary>
Motivation: 在药物发现、精准医学等资源受限应用中，需要从大规模数据集中选择高质量候选者。现有符合选择方法仅限于单阈值场景，无法满足实际中多条件选择的需求。

Method: 提出了MCCS算法，包括：为合取条件设计具有区域单调性的新非符合性分数；为析取条件使用全局Benjamini-Hochberg程序；建立理论保证的有限样本FDR控制。

Result: 广泛实验验证了MCCS相对于基线的优越性，在不同条件组合、现实世界模态和多任务可扩展性方面表现出良好的泛化能力。

Conclusion: MCCS方法能够在各种多条件环境中实现严格的FDR控制选择，解决了现有方法的局限性。

Abstract: Selecting high-quality candidates from large-scale datasets is critically
important in resource-constrained applications such as drug discovery,
precision medicine, and the alignment of large language models. While conformal
selection methods offer a rigorous solution with False Discovery Rate (FDR)
control, their applicability is confined to single-threshold scenarios (i.e., y
> c) and overlooks practical needs for multi-condition selection, such as
conjunctive or disjunctive conditions. In this work, we propose the
Multi-Condition Conformal Selection (MCCS) algorithm, which extends conformal
selection to scenarios with multiple conditions. In particular, we introduce a
novel nonconformity score with regional monotonicity for conjunctive conditions
and a global Benjamini-Hochberg (BH) procedure for disjunctive conditions,
thereby establishing finite-sample FDR control with theoretical guarantees. The
integration of these components enables the proposed method to achieve rigorous
FDR-controlled selection in various multi-condition environments. Extensive
experiments validate the superiority of MCCS over baselines, its
generalizability across diverse condition combinations, different real-world
modalities, and multi-task scalability.

</details>


### [75] [AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment](https://arxiv.org/abs/2510.08081)
*Xiaochong Lan,Jie Feng,Yinxing Liu,Xinlei Shi,Yong Li*

Main category: cs.AI

TL;DR: AutoQual是一个基于LLM的智能体框架，用于自动发现可解释特征，在评论质量评估任务中表现出色，能提升用户体验和转化率。


<details>
  <summary>Details</summary>
Motivation: 在线评论质量评估对电商平台至关重要，但质量是领域依赖且动态变化的概念。传统方法难以跨领域扩展，深度学习模型缺乏可解释性且可能过度关注语义而非质量。

Method: 提出AutoQual框架，模仿人类研究过程：通过反思迭代生成特征假设，通过自主工具实现操作化，并在持久记忆中积累经验。

Result: 在拥有亿级用户的大型在线平台上部署，A/B测试显示平均每位用户查看评论数增加0.79%，评论读者转化率提升0.27%。

Conclusion: AutoQual成功将数据中的隐性知识转化为显性可计算特征，不仅适用于评论质量评估，还可作为通用框架用于其他领域。

Abstract: Ranking online reviews by their intrinsic quality is a critical task for
e-commerce platforms and information services, impacting user experience and
business outcomes. However, quality is a domain-dependent and dynamic concept,
making its assessment a formidable challenge. Traditional methods relying on
hand-crafted features are unscalable across domains and fail to adapt to
evolving content patterns, while modern deep learning approaches often produce
black-box models that lack interpretability and may prioritize semantics over
quality. To address these challenges, we propose AutoQual, an LLM-based agent
framework that automates the discovery of interpretable features. While
demonstrated on review quality assessment, AutoQual is designed as a general
framework for transforming tacit knowledge embedded in data into explicit,
computable features. It mimics a human research process, iteratively generating
feature hypotheses through reflection, operationalizing them via autonomous
tool implementation, and accumulating experience in a persistent memory. We
deploy our method on a large-scale online platform with a billion-level user
base. Large-scale A/B testing confirms its effectiveness, increasing average
reviews viewed per user by 0.79% and the conversion rate of review readers by
0.27%.

</details>


### [76] [From Ethical Declarations to Provable Independence: An Ontology-Driven Optimal-Transport Framework for Certifiably Fair AI Systems](https://arxiv.org/abs/2510.08086)
*Sukriti Bhattacharya,Chitro Majumdar*

Main category: cs.AI

TL;DR: 提出一个可证明公平的AI框架，通过系统性地移除敏感信息及其代理，使用OWL 2 QL本体工程定义敏感属性并通过逻辑推理推断代理，构建捕获偏见模式的sigma代数G，然后通过Delbaen Majumdar最优传输获得公平表示。


<details>
  <summary>Details</summary>
Motivation: 克服当前偏见缓解方法的局限性，确保AI系统的完全公平性，特别是在如贷款审批等任务中，代理变量（如邮政编码）可能揭示种族等敏感信息。

Method: 使用OWL 2 QL本体工程形式化定义敏感属性，通过逻辑推理推断代理变量，构建sigma代数G捕获偏见结构，然后应用Delbaen Majumdar最优传输生成与G独立的变量，同时最小化L2距离以保持准确性。

Result: 该方法保证了真正的独立性而非仅仅去相关，通过将偏见建模为sigma代数间的依赖关系，将本体知识编译为可测量结构，并使用最优传输作为唯一的公平变换。

Conclusion: 该方法提供了一个可认证且数学基础扎实的方法，用于构建可信赖的AI系统，确保在敏感任务中的完全公平性。

Abstract: This paper presents a framework for provably fair AI that overcomes the
limits of current bias mitigation methods by systematically removing all
sensitive information and its proxies. Using ontology engineering in OWL 2 QL,
it formally defines sensitive attributes and infers their proxies through
logical reasoning, constructing a sigma algebra G that captures the full
structure of biased patterns. Fair representations are then obtained via
Delbaen Majumdar optimal transport, which generates variables independent of G
while minimizing L2 distance to preserve accuracy. This guarantees true
independence rather than mere decorrelation. By modeling bias as dependence
between sigma algebras, compiling ontological knowledge into measurable
structures, and using optimal transport as the unique fair transformation, the
approach ensures complete fairness in tasks like loan approval, where proxies
such as ZIP code reveal race. The result is a certifiable and mathematically
grounded method for trustworthy AI.

</details>


### [77] [Can Risk-taking AI-Assistants suitably represent entities](https://arxiv.org/abs/2510.08114)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Amirhossein Farshi Sotoudeh*

Main category: cs.AI

TL;DR: 该研究调查了语言模型在风险厌恶可操纵性方面的表现，发现虽然某些模型与人类行为有一定对齐，但仍存在显著差异，需要改进生物中心的可操纵性测量方法。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地融入AI驱动的决策支持系统，理解其风险行为对于负责任部署至关重要，需要防止系统无意中将用户推向风险决策或嵌入隐藏偏见。

Method: 研究考察语言模型在不同经济场景下复制人类风险偏好的能力，重点关注性别特定态度、不确定性、基于角色的决策以及风险厌恶的可操纵性。

Result: DeepSeek Reasoner和Gemini-2.0-flash-lite等语言模型显示出与人类行为的一定对齐，但显著差异表明需要改进生物中心的可操纵性测量方法。

Conclusion: 需要进一步改进模型设计，确保AI系统更准确地复制人类风险偏好，从而在风险管理环境中提高其有效性，增强AI助手在风险管理中的适用性。

Abstract: Responsible AI demands systems whose behavioral tendencies can be effectively
measured, audited, and adjusted to prevent inadvertently nudging users toward
risky decisions or embedding hidden biases in risk aversion. As language models
(LMs) are increasingly incorporated into AI-driven decision support systems,
understanding their risk behaviors is crucial for their responsible deployment.
This study investigates the manipulability of risk aversion (MoRA) in LMs,
examining their ability to replicate human risk preferences across diverse
economic scenarios, with a focus on gender-specific attitudes, uncertainty,
role-based decision-making, and the manipulability of risk aversion. The
results indicate that while LMs such as DeepSeek Reasoner and
Gemini-2.0-flash-lite exhibit some alignment with human behaviors, notable
discrepancies highlight the need to refine bio-centric measures of
manipulability. These findings suggest directions for refining AI design to
better align human and AI risk preferences and enhance ethical decision-making.
The study calls for further advancements in model design to ensure that AI
systems more accurately replicate human risk preferences, thereby improving
their effectiveness in risk management contexts. This approach could enhance
the applicability of AI assistants in managing risk.

</details>


### [78] [Prepared mind, fast response: A temporal decoupling framework for adaptive knowledge orchestration in open-domain dialogue](https://arxiv.org/abs/2510.08175)
*Jinling Gan,Churong Liang,Runnan Li*

Main category: cs.AI

TL;DR: PMFR提出了一种异步知识编排框架，通过解耦知识检索与响应生成，在保持对话质量的同时大幅降低延迟（95.3%延迟减少）。


<details>
  <summary>Details</summary>
Motivation: 解决开放域对话系统中延迟与质量的根本矛盾：轻量模型延迟低但知识不足，工具增强代理知识丰富但同步执行导致交互阻塞。

Method: 采用三组件架构：知识充足性评估器（实时评估）、轻量响应生成器（即时交互）、异步知识精炼代理（后台知识增强），通过智能触发机制实现异步知识编排。

Result: 在TopiOCQA上评估，PMFR实现95.3%延迟降低（23.38s→1.09s），同时保持与重量级同步基线相当的响应质量（GEval-C: 0.613 vs. 0.620）。

Conclusion: PMFR通过时间解耦框架成功解决了延迟与质量的矛盾，为开放域对话系统提供了既快速又知识丰富的解决方案。

Abstract: The latency-quality tradeoff is a fundamental constraint in open-domain
dialogue AI systems, since comprehensive knowledge access necessitates
prohibitive response delays. Contemporary approaches offer two inadequate
solutions: lightweight instruct models achieve sub-second latency but lack
reasoning depth, while tool-augmented ReAct agents enhance factuality through
external knowledge at the cost of synchronous execution that blocks interaction
during retrieval processes. PMFR is thus proposed, with a temporal decoupling
framework that fundamentally resolves the contradiction through asynchronous
knowledge orchestration. PMFR employs three coordinated components: (1) a
Knowledge Adequacy Evaluator for real-time sufficiency assessment, (2) a
Lightweight Response Generator for immediate user interaction, and (3) an
Asynchronous Knowledge Refinement Agent for background knowledge enhancement.
This architecture maintains continuous conversational flow while progressively
enriching knowledge coverage through intelligent triggering mechanisms.
Evaluation results on TopiOCQA demonstrate PMFR outperforms brute-force
scaling: PMFR achieves 95.3% latency reduction (23.38s -> 1.09s) while
preserving response quality comparable to heavyweight synchronous baselines
(GEval-C: 0.613 vs. 0.620).

</details>


### [79] [R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?](https://arxiv.org/abs/2510.08189)
*Yi Lu,Jianing Wang,Linsen Guo,Wei He,Hongyin Tang,Tao Gui,Xuanjing Huang,Xuezhi Cao,Wei Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 提出了R-HORIZON方法和基准，用于评估和增强大型推理模型在长视野复杂推理任务中的能力，发现现有模型在长视野推理中存在显著性能下降，并通过强化学习验证奖励方法有效提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注即时、单视野任务，无法充分评估模型理解和响应复杂长视野场景的能力，需要更全面的评估方法来推动大型推理模型的发展。

Method: 提出R-HORIZON方法，通过查询组合刺激长视野推理行为，构建包含复杂多步推理任务的基准，并使用强化学习验证奖励方法进行模型训练。

Result: 评估发现即使最先进的推理模型在长视野任务中性能显著下降，模型有效推理长度有限且难以在多个问题间合理分配思考资源。使用R-HORIZON数据进行强化学习训练后，在标准推理任务上准确率提升了7.5。

Conclusion: R-HORIZON为增强和评估大型推理模型的长视野推理能力提供了一个可扩展、可控且低成本的范式。

Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,
DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought
(CoT). However, existing benchmarks mainly focus on immediate, single-horizon
tasks, failing to adequately evaluate models' ability to understand and respond
to complex, long-horizon scenarios. To address this incomplete evaluation of
Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to
stimulate long-horizon reasoning behaviors in LRMs through query composition.
Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising
complex multi-step reasoning tasks with interdependent problems that span long
reasoning horizons. Through comprehensive evaluation of LRMs using the
R-HORIZON benchmark, we find that even the most advanced LRMs suffer
significant performance degradation. Our analysis reveals that LRMs exhibit
limited effective reasoning length and struggle to allocate thinking budget
across multiple problems appropriately. Recognizing these limitations, we use
R-HORIZON to construct long-horizon reasoning data for reinforcement learning
with verified rewards (RLVR). Compared to training with single-horizon data,
RLVR with R-HORIZON not only substantially improves performance on the
multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning
tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as
a scalable, controllable, and low-cost paradigm for enhancing and evaluating
the long-horizon reasoning capabilities of LRMs.

</details>


### [80] [Measuring What Matters: The AI Pluralism Index](https://arxiv.org/abs/2510.08193)
*Rashid Mushkani*

Main category: cs.AI

TL;DR: 提出了AI多元性指数(AIPI)，这是一个透明、基于证据的评估工具，用于衡量AI系统的多元治理程度，涵盖参与式治理、包容性与多样性、透明度和问责制四个支柱。


<details>
  <summary>Details</summary>
Motivation: 当前AI开发和治理集中在少数公司和国家手中，可能导致技术编码狭隘利益并限制公众参与。虽然语言、视觉和编码能力基准很常见，但缺乏可公开审计的多元治理衡量标准。

Method: 开发了AIPI评估框架，通过验证公共制品和独立评估中的可验证实践，明确处理"未知"证据，报告下限分数和已知分数。建立了可复现的评估流程，整合结构化网络和存储库分析、外部评估和专家访谈。

Result: 报告了试点提供商结果，并将AIPI与邻近的透明度、安全和治理框架进行对比。评估了可靠性，包括评分者间一致性、覆盖率报告、跨指数相关性和敏感性分析。

Conclusion: AIPI旨在引导激励机制向多元实践倾斜，并为政策制定者、采购者和公众提供可比较的证据，促进AI系统的多元治理。

Abstract: Artificial intelligence systems increasingly mediate knowledge,
communication, and decision making. Development and governance remain
concentrated within a small set of firms and states, raising concerns that
technologies may encode narrow interests and limit public agency. Capability
benchmarks for language, vision, and coding are common, yet public, auditable
measures of pluralistic governance are rare. We define AI pluralism as the
degree to which affected stakeholders can shape objectives, data practices,
safeguards, and deployment. We present the AI Pluralism Index (AIPI), a
transparent, evidence-based instrument that evaluates producers and system
families across four pillars: participatory governance, inclusivity and
diversity, transparency, and accountability. AIPI codes verifiable practices
from public artifacts and independent evaluations, explicitly handling
"Unknown" evidence to report both lower-bound ("evidence") and known-only
scores with coverage. We formalize the measurement model; implement a
reproducible pipeline that integrates structured web and repository analysis,
external assessments, and expert interviews; and assess reliability with
inter-rater agreement, coverage reporting, cross-index correlations, and
sensitivity analysis. The protocol, codebook, scoring scripts, and evidence
graph are maintained openly with versioned releases and a public adjudication
process. We report pilot provider results and situate AIPI relative to adjacent
transparency, safety, and governance frameworks. The index aims to steer
incentives toward pluralistic practice and to equip policymakers, procurers,
and the public with comparable evidence.

</details>


### [81] [The Tournament Tree Method for preference elicitation in Multi-criteria decision-making](https://arxiv.org/abs/2510.08197)
*Diego García-Zamora,Álvaro Labella,José Rui Figueira*

Main category: cs.AI

TL;DR: 提出了锦标赛树方法(TTM)，一种新的成对比较框架，只需m-1次比较就能获得完整、互反且一致的比较矩阵，显著降低认知负担和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统成对比较方法需要m(m-1)/2次比较，认知负担重，存在不一致风险，且计算复杂度高。

Method: TTM方法包括三个阶段：(i)使用减少的目标比较集获取专家判断，(ii)构建一致的成对比较矩阵，(iii)从结果矩阵推导全局价值尺度。

Result: 该方法确保设计一致性，最小化认知努力，将偏好建模维度从m(m-1)/2减少到m个参数，并与经典卡牌方法兼容。

Conclusion: TTM方法有效解决了传统成对比较方法的局限性，并开发了基于Web的工具展示其实际应用价值。

Abstract: Pairwise comparison methods, such as Fuzzy Preference Relations and Saaty's
Multiplicative Preference Relations, are widely used to model expert judgments
in multi-criteria decision-making. However, their application is limited by the
high cognitive load required to complete $m(m-1)/2$ comparisons, the risk of
inconsistency, and the computational complexity of deriving consistent value
scales. This paper proposes the Tournament Tree Method (TTM), a novel
elicitation and evaluation framework that overcomes these limitations. The TTM
requires only $m-1$ pairwise comparisons to obtain a complete, reciprocal, and
consistent comparison matrix. The method consists of three phases: (i)
elicitation of expert judgments using a reduced set of targeted comparisons,
(ii) construction of the consistent pairwise comparison matrix, and (iii)
derivation of a global value scale from the resulting matrix. The proposed
approach ensures consistency by design, minimizes cognitive effort, and reduces
the dimensionality of preference modeling from $m(m-1)/2$ to $m$ parameters.
Furthermore, it is compatible with the classical Deck of Cards method, and thus
it can handle interval and ratio scales. We have also developed a web-based
tool that demonstrates its practical applicability in real decision-making
scenarios.

</details>


### [82] [DODO: Causal Structure Learning with Budgeted Interventions](https://arxiv.org/abs/2510.08207)
*Matteo Gregorini,Chiara Boldrini,Lorenzo Valerio*

Main category: cs.AI

TL;DR: DODO算法让智能体通过重复干预自主学习环境的因果结构，在噪声存在下准确推断因果有向无环图，相比观测方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要依赖复杂相关性，缺乏因果理解。让AI具备因果意识能提升性能，深入理解环境机制。

Method: 智能体在因果DAG控制的环境中交互，通过干预和因果推断技术分析观测变化的统计显著性来学习因果结构。

Result: DODO在除最有限资源条件外的所有情况下都优于观测方法，常能以零误差重建因果图结构，在最挑战配置中比最佳基线高0.25 F1分。

Conclusion: DODO算法能有效让智能体自主学习环境因果结构，提升因果推理能力。

Abstract: Artificial Intelligence has achieved remarkable advancements in recent years,
yet much of its progress relies on identifying increasingly complex
correlations. Enabling causality awareness in AI has the potential to enhance
its performance by enabling a deeper understanding of the underlying mechanisms
of the environment. In this paper, we introduce DODO, an algorithm defining how
an Agent can autonomously learn the causal structure of its environment through
repeated interventions. We assume a scenario where an Agent interacts with a
world governed by a causal Directed Acyclic Graph (DAG), which dictates the
system's dynamics but remains hidden from the Agent. The Agent's task is to
accurately infer the causal DAG, even in the presence of noise. To achieve
this, the Agent performs interventions, leveraging causal inference techniques
to analyze the statistical significance of observed changes. Results show
better performance for DODO, compared to observational approaches, in all but
the most limited resource conditions. DODO is often able to reconstruct with as
low as zero errors the structure of the causal graph. In the most challenging
configuration, DODO outperforms the best baseline by +0.25 F1 points.

</details>


### [83] [Selection, Reflection and Self-Refinement: Revisit Reasoning Tasks via a Causal Lens](https://arxiv.org/abs/2510.08222)
*Yunlong Deng,Boyang Sun,Yan Li,Lingjing Kong,Zeyu Tang,Kun Zhang,Guangyi Chen*

Main category: cs.AI

TL;DR: 本文从因果视角重新审视推理任务，将其建模为选择机制，提出SR²框架通过潜在变量反馈来学习密集依赖关系，在推理准确性上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型即使经过大规模预训练和后训练，在推理任务上仍然表现不可靠。本文旨在从因果角度理解推理任务在潜在空间中的行为，为解决推理挑战提供见解。

Method: 提出SR²框架，包含三个关键模块：反思表示学习、依赖自精炼和周期性中间对齐。该框架将推理任务建模为选择机制，利用估计的潜在变量作为反馈来学习潜在表示之间的密集依赖关系。

Result: 实验显示该方法在推理准确性上带来显著提升，例如在数独和迷宫任务上，使用8倍更少的参数实现了超过10%的性能改进。

Conclusion: 从因果视角将推理任务建模为选择机制，并通过SR²框架学习潜在变量间的密集依赖关系，能够有效提升推理任务的性能，为复杂推理问题提供了新的解决思路。

Abstract: Due to their inherent complexity, reasoning tasks have long been regarded as
rigorous benchmarks for assessing the capabilities of machine learning models,
especially large language models (LLMs). Although humans can solve these tasks
with ease, existing models, even after extensive pre-training and post-training
at scale, still fail to perform reasoning reliably. In this paper, we revisit
reasoning tasks from a causal perspective, seeking to understand their behavior
in latent space and to offer insights for addressing their challenges.
Specifically, we cast reasoning tasks as a selection mechanism, in which
high-level logical concepts function as selection operators on the given
observations, such as, identifying the correct answer in a math problem or
filling the appropriate entry in Sudoku. We emphasize two key properties of
this formulation that shed light on the difficulty of reasoning tasks. First,
the latent space exceeds the observation space in complexity, even when the
correct answer is fully determined by the observed input. Second, the latent
variables, corresponding to logical thought, are densely structured and exhibit
strong dependencies. Building on this formulation, we introduce a framework,
called SR$^2$, that incorporates the estimated latent variables as feedback
into the selection mechanism, thereby facilitating the learning of dense
dependencies among latent representations. The framework consists of three key
modules: reflective representation learning, dependency self-refinement, and
periodic intermediate alignment. Experimentally, we show that our approach
yields significant gains in reasoning accuracy, for example, attaining over
10$\%$ improvement in performance with 8$\times$ fewer parameters on the Sudoku
and Maze tasks over the recent advances.

</details>


### [84] [Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness](https://arxiv.org/abs/2510.08238)
*Jiyang Qiu,Xinbei Ma,Yunqing Xu,Zhuosheng Zhang,Hai Zhao*

Main category: cs.AI

TL;DR: 提出了Chain-of-Trigger Backdoor (CoTri)多步后门攻击方法，针对LLM智能体进行长期控制，攻击成功率接近100%且误触发率接近零，同时意外地提升了智能体在良性任务上的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在现实应用中的快速部署，其安全性和鲁棒性引发严重担忧。传统后门攻击仅限于单步控制，需要开发针对长期智能体控制的多步后门攻击方法。

Method: CoTri基于有序触发序列，从初始触发开始，后续触发从环境中提取，实现多步操作以引导智能体偏离预期任务。该方法通过训练数据建模环境的随机性来植入后门。

Result: 实验显示CoTri攻击成功率接近完美，误触发率接近零。由于训练数据建模了环境的随机性，CoTri的植入反而提升了智能体在良性任务上的性能，并增强了其对环境干扰的鲁棒性。在视觉语言模型上的验证证实了该方法对多模态智能体的可扩展性。

Conclusion: CoTri实现了智能体内稳定、多步的控制，同时提高了其固有鲁棒性和任务能力，这使得攻击更加隐蔽，带来了潜在的安全风险。

Abstract: The rapid deployment of large language model (LLM)-based agents in real-world
applications has raised serious concerns about their trustworthiness. In this
work, we reveal the security and robustness vulnerabilities of these agents
through backdoor attacks. Distinct from traditional backdoors limited to
single-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a
multi-step backdoor attack designed for long-horizon agentic control. CoTri
relies on an ordered sequence. It starts with an initial trigger, and
subsequent ones are drawn from the environment, allowing multi-step
manipulation that diverts the agent from its intended task. Experimental
results show that CoTri achieves a near-perfect attack success rate (ASR) while
maintaining a near-zero false trigger rate (FTR). Due to training data modeling
the stochastic nature of the environment, the implantation of CoTri
paradoxically enhances the agent's performance on benign tasks and even
improves its robustness against environmental distractions. We further validate
CoTri on vision-language models (VLMs), confirming its scalability to
multimodal agents. Our work highlights that CoTri achieves stable, multi-step
control within agents, improving their inherent robustness and task
capabilities, which ultimately makes the attack more stealthy and raises
potential safty risks.

</details>


### [85] [Co-TAP: Three-Layer Agent Interaction Protocol Technical Report](https://arxiv.org/abs/2510.08263)
*Shunyu An,Miao Wang,Yongchao Li,Dong Wan,Lina Wang,Ling Qin,Liqin Gao,Congyao Fan,Zhiyong Mao,Jiange Pu,Wenji Xia,Dong Zhao,Rui Hu,Ji Lu,Guiyue Zhou,Baoyu Tang,Yanqin Gao,Yongsheng Du,Daigang Xu,Lingjun Huang,Baoli Wang,Xiwen Zhang,Luyao Wang,Shilong Liu*

Main category: cs.AI

TL;DR: Co-TAP是一个三层智能体交互协议框架，包含HAI、UAP和MEK三个核心协议，旨在解决多智能体系统在互操作性、交互协作和知识共享方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统面临的三大核心挑战：互操作性、交互协作和知识共享，为构建高效、可扩展和智能的多智能体应用提供工程基础和理论指导。

Method: 设计了三层协议：HAI（人机交互协议）标准化事件驱动通信；UAP（统一智能体协议）通过统一服务发现和协议转换实现异构智能体互联；MEK（记忆-提取-知识协议）建立标准化认知链实现知识共享。

Result: 构建了一个完整的协议框架，能够确保实时性能、可靠性、协同交互，实现异构智能体无缝互联，并为集体智能的实现奠定基础。

Conclusion: Co-TAP协议框架为下一代高效、可扩展和智能的多智能体应用提供了坚实的工程基础和理论指导。

Abstract: This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer
agent interaction protocol designed to address the challenges faced by
multi-agent systems across the three core dimensions of Interoperability,
Interaction and Collaboration, and Knowledge Sharing. We have designed and
proposed a layered solution composed of three core protocols: the Human-Agent
Interaction Protocol (HAI), the Unified Agent Protocol (UAP), and the
Memory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction
layer, standardizing the flow of information between users, interfaces, and
agents by defining a standardized, event-driven communication paradigm. This
ensures the real-time performance, reliability, and synergy of interactions. As
the core of the infrastructure layer, UAP is designed to break down
communication barriers among heterogeneous agents through unified service
discovery and protocol conversion mechanisms, thereby enabling seamless
interconnection and interoperability of the underlying network. MEK, in turn,
operates at the cognitive layer. By establishing a standardized ''Memory (M) -
Extraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the
ability to learn from individual experiences and form shareable knowledge,
thereby laying the foundation for the realization of true collective
intelligence. We believe this protocol framework will provide a solid
engineering foundation and theoretical guidance for building the next
generation of efficient, scalable, and intelligent multi-agent applications.

</details>


### [86] [Symmetry-Aware Fully-Amortized Optimization with Scale Equivariant Graph Metanetworks](https://arxiv.org/abs/2510.08300)
*Bart Kuipers,Freek Byrman,Daniel Uyterlinde,Alejandro García-Castellanos*

Main category: cs.AI

TL;DR: ScaleGMNs通过利用缩放对称性在权重空间中直接优化神经网络，实现单次微调，减少迭代优化需求。


<details>
  <summary>Details</summary>
Motivation: 利用摊销优化加速相关优化问题的解决，通过学习利用问题实例间共享结构的映射。

Method: 使用尺度等变图元网络(ScaleGMNs)，直接在权重空间操作，实现现有模型的单次微调。

Result: 实证证明了该方法的有效性，并提供了理论结果：卷积神经网络中的缩放对称性诱导的规范自由度小于多层感知机。

Conclusion: 对称感知元网络是高效且可泛化的神经网络优化的强大方法。

Abstract: Amortized optimization accelerates the solution of related optimization
problems by learning mappings that exploit shared structure across problem
instances. We explore the use of Scale Equivariant Graph Metanetworks
(ScaleGMNs) for this purpose. By operating directly in weight space, ScaleGMNs
enable single-shot fine-tuning of existing models, reducing the need for
iterative optimization. We demonstrate the effectiveness of this approach
empirically and provide a theoretical result: the gauge freedom induced by
scaling symmetries is strictly smaller in convolutional neural networks than in
multi-layer perceptrons. This insight helps explain the performance differences
observed between architectures in both our work and that of Kalogeropoulos et
al. (2024). Overall, our findings underscore the potential of symmetry-aware
metanetworks as a powerful approach for efficient and generalizable neural
network optimization. Open-source code:
https://github.com/daniuyter/scalegmn_amortization

</details>


### [87] [First Try Matters: Revisiting the Role of Reflection in Reasoning Models](https://arxiv.org/abs/2510.08308)
*Liwei Kang,Yue Deng,Yao Xiao,Zhanfeng Mo,Wee Sun Lee,Lidong Bing*

Main category: cs.AI

TL;DR: 该论文系统分析了大型语言模型中的反思行为，发现反思主要是确认性的，很少改变初始答案。作者提出了一种基于问题感知的早期停止方法，可减少24.5%的推理标记，准确率仅下降2.9%。


<details>
  <summary>Details</summary>
Motivation: 研究反思行为对推理性能提升的实际贡献，因为目前不清楚反思在多大程度上真正改善了模型的推理能力。

Method: 分析了8个推理模型在5个数学数据集上的推理过程，构建了不同反思步数的SFT数据集进行训练，并提出了问题感知的早期停止方法来动态截断反思过程。

Result: 反思主要是确认性的，很少改变初始答案；训练更多反思步数主要提升首次答案正确率，而非通过反思纠正错误答案的能力；提出的方法可减少24.5%推理标记，准确率仅下降2.9%。

Conclusion: 反思在推理中的作用被高估，主要是确认性而非修正性；通过动态截断反思可以显著提高推理效率，同时保持较高的准确率。

Abstract: Large language models have recently demonstrated significant gains in
reasoning ability, often attributed to their capacity to generate longer chains
of thought and engage in reflective reasoning. However, the contribution of
reflections to performance improvement remains unclear. In this paper, we
systematically analyze the rollouts of eight reasoning models on five
mathematical datasets. We focus on reflective behaviours where the model has
already produced an answer but continues reflecting before finalizing its
output. Our analysis reveals that reflections are predominantly confirmatory
and rarely alter the model's initial answer, a pattern consistent across models
and datasets. To understand the role of reflections in training, we construct
supervised fine-tuning (SFT) datasets with varying amounts of reflection steps.
We observe that training models on rollouts with more reflection steps
primarily enhances first-answer correctness rather than the ability to correct
initially wrong answers through reflections. This motivates us to propose a
question-aware early-stopping method that enhances inference-time token
efficiency by stopping the reasoning process once a few plausible candidate
answers are generated, thereby reducing unnecessary reflection steps. Motivated
by this, we further propose to dynamically truncate the reflections after a
candidate answer has appeared during generation, which reduces reasoning tokens
by 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.

</details>


### [88] [Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries](https://arxiv.org/abs/2510.08325)
*Marius Dragoi,Ioana Pintilie,Florin Gogianu,Florin Brad*

Main category: cs.AI

TL;DR: 本文提出Cover@tau指标来替代Pass@k，用于更准确地评估语言模型在推理任务中的真实能力边界，避免随机猜测带来的误导。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法Pass@k在大采样预算下会高估基础模型的能力，因为离散答案空间中的随机猜测会导致成功率虚高，无法真实反映模型的推理边界。

Method: 提出Cover@tau指标，测量模型能够解决的问题中至少有tau比例补全正确的比例，通过可靠性阈值来区分真实推理和随机猜测。

Result: 使用Cover@tau评估多个RLVR模型，发现相对于Pass@1，不同算法的相对排名发生了变化，提供了对推理边界的不同视角。

Conclusion: Cover@tau能够更准确地捕捉模型的真实推理能力，避免随机猜测带来的评估偏差，为RLVR模型评估提供了更可靠的指标。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm to improve Large Language Models on reasoning tasks such as
coding, math or logic. To assess the reasoning boundary (the fraction of
problems a model can solve) researchers often report Pass@k at large sampling
budgets. Recent results reveal a crossover phenomenon: while RLVR models
outperform the base model at small k values, the base model usually outperforms
them when sampling a very large number of completions. This has been
interpreted as evidence that base models have a larger reasoning boundary. We
argue that on tasks with discrete answer spaces, such as math with numeric
outputs, Pass@k at large k reflects the increasingly higher chance of success
in the limit of the number of trials rather than genuine reasoning, and can
therefore be misleading. We propose Cover@tau, which measures the fraction of
problems that a model can solve for which at least a tau proportion of
completions are correct. Unlike Pass@k, Cover@tau captures reasoning under an
explicit reliability threshold: models that rely on random guessing degrade
rapidly as tau increases. We evaluate several RLVR models using Cover@tau-based
metrics and illustrate how the relative rankings of popular algorithms change
compared to Pass@1, offering a different perspective on reasoning boundaries.

</details>


### [89] [LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings](https://arxiv.org/abs/2510.08338)
*Benjamin F. Maier,Ulf Aslak,Luca Fiaschi,Nina Rismal,Kemble Fletcher,Christian C. Luhmann,Robbie Dow,Kli Pappas,Thomas V. Wiecki*

Main category: cs.AI

TL;DR: 提出语义相似度评分方法，通过LLM生成文本响应并映射到Likert分布，实现90%人类重测可靠性，同时保持真实响应分布。


<details>
  <summary>Details</summary>
Motivation: 传统消费者研究存在面板偏见和规模限制，LLM模拟消费者时直接获取数值评分会产生不现实的响应分布。

Method: 语义相似度评分方法：从LLM获取文本响应，通过嵌入相似度映射到参考陈述的Likert分布。

Result: 在57个个人护理产品调查数据集上测试，达到90%人类重测可靠性，KS相似度>0.85，同时提供丰富的定性反馈。

Conclusion: 该框架实现了可扩展的消费者研究模拟，同时保留了传统调查指标和可解释性。

Abstract: Consumer research costs companies billions annually yet suffers from panel
biases and limited scale. Large language models (LLMs) offer an alternative by
simulating synthetic consumers, but produce unrealistic response distributions
when asked directly for numerical ratings. We present semantic similarity
rating (SSR), a method that elicits textual responses from LLMs and maps these
to Likert distributions using embedding similarity to reference statements.
Testing on an extensive dataset comprising 57 personal care product surveys
conducted by a leading corporation in that market (9,300 human responses), SSR
achieves 90% of human test-retest reliability while maintaining realistic
response distributions (KS similarity > 0.85). Additionally, these synthetic
respondents provide rich qualitative feedback explaining their ratings. This
framework enables scalable consumer research simulations while preserving
traditional survey metrics and interpretability.

</details>


### [90] [QAgent: A modular Search Agent with Interactive Query Understanding](https://arxiv.org/abs/2510.08383)
*Yi Jiang,Lei Shen,Lujie Niu,Sendong Zhao,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 提出QAgent框架，通过强化学习训练搜索代理进行自适应检索，解决传统RAG在复杂查询理解和泛化能力方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在复杂查询理解方面存在不足，基于强化学习的搜索代理虽然前景广阔，但仍面临泛化和部署挑战。

Method: 采用模块化搜索代理进行多步决策，通过强化学习优化检索质量，支持准确的下游答案生成。

Result: 实验显示QAgent在问答任务中表现优异，可作为即插即用模块在实际应用中部署。

Conclusion: QAgent通过专注于有效检索的策略，增强了LLM应用的泛化能力，解决了传统RAG和端到端RL方法的局限性。

Abstract: Large language models (LLMs) excel at natural language tasks but are limited
by their static parametric knowledge, especially in knowledge-intensive task.
Retrieval-augmented generation (RAG) mitigates this by integrating external
information. However, (1) traditional RAG struggles with complex query
understanding, and (2) even search agents trained with reinforcement learning
(RL), despite their promise, still face generalization and deployment
challenges. To address these limitations, we propose QAgent, a unified agentic
RAG framework that employs a search agent for adaptive retrieval. This agent
optimizes its understanding of the query through interactive reasoning and
retrieval. To facilitate real-world application, we focus on modular search
agent for query understanding that are plug-and-play in complex systems.
Secifically, the agent follows a multi-step decision process trained with RL to
maximize retrieval quality and support accurate downstream answers. We further
analyze the strengths and weaknesses of end-to-end RL and propose a strategy
that focuses on effective retrieval, thereby enhancing generalization in LLM
applications. Experiments show QAgent excels at QA and serves as a
plug-and-play module for real-world deployment.

</details>


### [91] [Revisiting Hallucination Detection with Effective Rank-based Uncertainty](https://arxiv.org/abs/2510.08389)
*Rui Wang,Zeming Wei,Guanzhang Yue,Meng Sun*

Main category: cs.AI

TL;DR: 提出了一种基于隐藏状态有效秩的LLM幻觉检测方法，通过分析多输出和多层表示的谱特征来量化不确定性，无需额外知识或模块。


<details>
  <summary>Details</summary>
Motivation: 超越基础的不确定性驱动幻觉检测框架，解决LLM幻觉检测的根本挑战，为可信部署提供支持。

Method: 通过测量从多个模型输出和不同层导出的隐藏状态的有效秩来量化不确定性，基于表示谱分析提供可解释的见解。

Result: 广泛实验表明该方法能有效检测幻觉，并在各种场景中稳健泛化。

Conclusion: 为LLM真实性检测贡献了新的范式，结合了理论优雅性和实际效率。

Abstract: Detecting hallucinations in large language models (LLMs) remains a
fundamental challenge for their trustworthy deployment. Going beyond basic
uncertainty-driven hallucination detection frameworks, we propose a simple yet
powerful method that quantifies uncertainty by measuring the effective rank of
hidden states derived from multiple model outputs and different layers.
Grounded in the spectral analysis of representations, our approach provides
interpretable insights into the model's internal reasoning process through
semantic variations, while requiring no extra knowledge or additional modules,
thus offering a combination of theoretical elegance and practical efficiency.
Meanwhile, we theoretically demonstrate the necessity of quantifying
uncertainty both internally (representations of a single response) and
externally (different responses), providing a justification for using
representations among different layers and responses from LLMs to detect
hallucinations. Extensive experiments demonstrate that our method effectively
detects hallucinations and generalizes robustly across various scenarios,
contributing to a new paradigm of hallucination detection for LLM truthfulness.

</details>


### [92] [Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling](https://arxiv.org/abs/2510.08470)
*Bianca-Mihaela Ganescu,Suchir Salhan,Andrew Caines,Paula Buttery*

Main category: cs.AI

TL;DR: 提出轻量级解码器架构，通过动态门控、特征调制和对比学习，在有限数据下实现多模态信息融合，性能优于基线模型


<details>
  <summary>Details</summary>
Motivation: 在认知合理的数据量限制下训练视觉语言模型，需要重新思考模型如何整合多模态信息

Method: 使用动态门控实现语言和视觉线索的自适应融合，结合特征调制、通道注意力和辅助对比目标

Result: 在五个基准测试中表现优于多模态基线，动态门控发现可解释模式，对内容词偏好视觉线索，对功能词偏好语言线索

Conclusion: 动态门控是高效多模态学习的强大工具，即使在严格约束下也能提供可解释性和性能

Abstract: Training vision-language models on cognitively-plausible amounts of data
requires rethinking how models integrate multimodal information. Within the
constraints of the Vision track for the BabyLM Challenge 2025, we propose a
lightweight decoder-based architecture with (1) token-wise dynamic gating for
adaptive fusion of linguistic and visual cues, (2) feature modulation and
channel attention to maximise the utility of limited visual information and (3)
auxiliary contrastive objectives for visual grounding. Evaluation on five
benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows
competitive or superior performance to multimodal baselines. More notably, our
dynamic gate discovers interpretable patterns without explicit supervision,
favouring visual cues for content words and linguistic cues for function words.
While we identify limitations in the Challenge constraints, such as the
information bottleneck created by global image embeddings and training
instability from the dataset split, our findings establish dynamic gating as a
powerful tool for efficient multimodal learning, offering both interpretability
and performance even under severe constraints.

</details>


### [93] [AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents](https://arxiv.org/abs/2510.08511)
*Shangheng Du,Xiangchao Yan,Dengyang Jiang,Jiakang Yuan,Yusong Hu,Xin Li,Liang He,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: AutoMLGen是一个基于LLM的编码代理，通过集成领域知识库和蒙特卡洛图搜索来解决机器学习工程任务中的局限性，在MLE-Bench上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在AutoML和Kaggle竞赛等机器学习工程场景中缺乏细粒度领域先验知识，传统搜索方法限制了知识传递和搜索空间多样性，无法充分利用历史轨迹和跨分支信息共享。

Method: 提出AutoMLGen，集成领域知识库提供高质量先验指导，使用蒙特卡洛图搜索（MCGS）实现动态路径重组、历史轨迹重用和多解决方案融合，结合细粒度操作符集提高稳定性和收敛速度。

Result: 在MLE-Bench评估中，AutoMLGen在12小时预算（标准运行时间的一半）下，在平均奖牌率和有效提交率等多个维度上实现了最先进的性能。

Conclusion: AutoMLGen通过结合领域知识库和蒙特卡洛图搜索，有效解决了机器学习工程任务中的知识传递和搜索效率问题，显著提升了性能表现。

Abstract: Large language models (LLMs) have shown impressive performance in general
programming tasks. However, in Machine Learning Engineering (MLE) scenarios
such as AutoML and Kaggle competitions, achieving high performance depends
heavily on expert intervention and repeated adjustments rather than simply
generating correct code. When applied directly to these tasks, LLMs often lack
fine-grained domain priors, and existing MLE approaches that use linear or
tree-structured searches limit knowledge transfer to adjacent hierarchical
links. As a result, they cannot leverage past full trajectories or share
information across branches, limiting self-evolving ability and search space
diversity. To address these limitations, we introduce AutoMLGen, an LLM-based
coding agent that integrates a domain knowledge base for high-quality prior
guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS
retains the tree-guided exploration of MCTS while embedding a graph structure
into the expansion stage to enable dynamic path reorganization, historical
trajectory reuse, and multi-solution fusion to support both self-evolution and
collaborative learning. Combined with fine-grained operator sets, this design
improves stability and accelerates convergence. Evaluation on the MLE-Bench
shows that AutoMLGen achieves state-of-the-art performance in numerous
dimensions, such as the average medal rate and the valid submission rate, under
a 12-hour budget (half the standard runtime). The code is available at
https://github.com/Alpha-Innovator/InternAgent.

</details>


### [94] [CaRT: Teaching LLM Agents to Know When They Know Enough](https://arxiv.org/abs/2510.08517)
*Grace Liu,Yuxiao Qu,Jeff Schneider,Aarti Singh,Aviral Kumar*

Main category: cs.AI

TL;DR: CaRT方法通过反事实轨迹对和语言推理训练LLMs学习何时停止信息收集，在医疗诊断和数学解题任务中提高了信息收集效率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 许多任务需要模型在多轮交互中策略性地收集相关信息，然后才执行任务。策略性信息收集要求模型不仅知道如何有效获取信息，还要知道何时停止收集信息并做出决策，以避免过度思考或在行动时偏离方向。

Method: CaRT使用反事实轨迹对来微调LLMs，每个对包含一个适合终止的轨迹和其最小修改版本（不适合终止）。通过语言推理训练LLM解释终止决策的合理性，并将这种能力通过微调注入基础LLM。

Result: 在交互式医疗诊断和数学问题解决两个领域中，CaRT相比其他微调方法提高了信息收集效率和任务成功率。

Conclusion: CaRT方法能够有效教导LLMs何时停止寻求信息，在策略性信息收集任务中表现出色，提高了决策效率和准确性。

Abstract: Many tasks require learned models to strategically gather relevant
information over multiple rounds of interaction before actually acting on a
task. Strategic information gathering requires models to know not only how to
effectively acquire information, but also when to stop gathering information
and make a decision, in order to avoid overthinking or getting derailed when
acting. In this paper, we formalize this problem and introduce Counterfactuals
and Reasoning for Termination (CaRT), an approach for teaching LLMs when to
stop seeking information. To appropriately learn when to terminate, CaRT
fine-tunes LLMs using counterfactual pairs of trajectories, one where
termination is appropriate and a minimally modified version of the same
trajectory where it is not. It trains the LLM to explain the rationale for the
termination decision in either case via verbal reasoning, and imbues this
capability into the base LLM via fine-tuning. We instantiate CaRT in two
domains: interactive medical diagnosis and math problem solving. In both
domains, we find that CaRT improves the efficiency of information gathering and
task success rate compared to other fine-tuning methods.

</details>


### [95] [FlowSearch: Advancing deep research with dynamic structured knowledge flow](https://arxiv.org/abs/2510.08521)
*Yusong Hu,Runmin Ma,Yue Fan,Jinxin Shi,Zongsheng Cao,Yuhao Zhou,Jiakang Yuan,Xiangchao Yan,Wenlong Zhang,Lei Bai,Bo Zhang*

Main category: cs.AI

TL;DR: FlowSearch是一个多智能体框架，通过构建动态结构化知识流来驱动子任务执行和推理，在多个基准测试中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 深度研究任务需要广度和深度的思考，涉及导航多样化知识空间和推理复杂多步依赖关系，这对智能体系统提出了重大挑战

Method: 提出FlowSearch多智能体框架，能够战略性地规划和扩展知识流以实现并行探索和分层任务分解，并根据中间推理结果实时调整知识流

Result: 在GAIA、HLE、GPQA和TRQA等通用和科学基准测试中实现最先进性能，展示了在多学科研究场景中的有效性

Conclusion: FlowSearch展示了推进科学发现的潜力，代码已在GitHub上开源

Abstract: Deep research is an inherently challenging task that demands both breadth and
depth of thinking. It involves navigating diverse knowledge spaces and
reasoning over complex, multi-step dependencies, which presents substantial
challenges for agentic systems. To address this, we propose FlowSearch, a
multi-agent framework that actively constructs and evolves a dynamic structured
knowledge flow to drive subtask execution and reasoning. FlowSearch is capable
of strategically planning and expanding the knowledge flow to enable parallel
exploration and hierarchical task decomposition, while also adjusting the
knowledge flow in real time based on feedback from intermediate reasoning
outcomes and insights. FlowSearch achieves state-of-the-art performance on both
general and scientific benchmarks, including GAIA, HLE, GPQA and TRQA,
demonstrating its effectiveness in multi-disciplinary research scenarios and
its potential to advance scientific discovery. The code is available at
https://github.com/Alpha-Innovator/InternAgent.

</details>


### [96] [Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)
*Kai Zhang,Xiangchao Chen,Bo Liu,Tianci Xue,Zeyi Liao,Zhihan Liu,Xiyao Wang,Yuting Ning,Zhaorun Chen,Xiaohan Fu,Jian Xie,Yuxuan Sun,Boyu Gou,Qi Qi,Zihang Meng,Jianwei Yang,Ning Zhang,Xian Li,Ashish Shah,Dat Huynh,Hengduo Li,Zi Yang,Sara Cao,Lawrence Jang,Shuyan Zhou,Jiacheng Zhu,Huan Sun,Jason Weston,Yu Su,Yifan Wu*

Main category: cs.AI

TL;DR: 提出“早期经验”范式，通过智能体自身交互数据来改进语言智能体，无需奖励信号，在八个环境中验证了其提升效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言智能体主要依赖专家数据监督微调，存在数据范围窄、环境多样性有限的问题，难以通过强化学习从经验中学习。

Method: 提出早期经验范式，包含两种策略：隐式世界建模（利用收集的状态建立环境动态基础）和自我反思（从次优行动中学习改进推理和决策）。

Result: 在八个多样化环境中验证，方法持续提升有效性和跨领域泛化能力，在有可验证奖励的环境中为后续强化学习提供良好基础。

Conclusion: 早期经验是模仿学习与完全经验驱动智能体之间的实用桥梁，能显著改进语言智能体的学习能力。

Abstract: A long-term goal of language agents is to learn and improve through their own
experience, ultimately outperforming humans in complex, real-world tasks.
However, training agents from experience data with reinforcement learning
remains difficult in many environments, which either lack verifiable rewards
(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn
tool use). As a result, most current agents rely on supervised fine-tuning on
expert data, which is challenging to scale and generalizes poorly. This
limitation stems from the nature of expert demonstrations: they capture only a
narrow range of scenarios and expose the agent to limited environment
diversity. We address this limitation with a middle-ground paradigm we call
early experience: interaction data generated by the agent's own actions, where
the resulting future states serve as supervision without reward signals. Within
this paradigm we study two strategies of using such data: (1) Implicit world
modeling, which uses collected states to ground the policy in environment
dynamics; and (2) Self-reflection, where the agent learns from its suboptimal
actions to improve reasoning and decision-making. We evaluate across eight
diverse environments and multiple model families. Our approaches consistently
improve effectiveness and out-of-domain generalization, highlighting the value
of early experience. Moreover, in environments with verifiable rewards, our
results provide promising signals that early experience offers a strong
foundation for subsequent reinforcement learning, positioning it as a practical
bridge between imitation learning and fully experience-driven agents.

</details>


### [97] [How to Teach Large Multimodal Models New Skills](https://arxiv.org/abs/2510.08564)
*Zhen Zhu,Yiming Gong,Yao Xiao,Yaoyao Liu,Derek Hoiem*

Main category: cs.AI

TL;DR: 研究如何在保持大语言模型原有能力的同时教授新技能，发现选择性更新特定层可以显著减少遗忘现象。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在连续微调过程中出现的能力遗忘问题，避免学习新技能时丢失原有知识。

Method: 在三个模型家族上进行五个目标技能的连续微调，监控八个基准测试的通用能力，开发了两种选择性参数更新方法。

Result: 发现遗忘现象在后续阶段可以部分恢复，通过仅更新自注意力投影层或MLP的Gate&Up层，能在获得强目标技能的同时保持原有性能。

Conclusion: 选择性参数更新是有效的大语言模型持续学习的可行策略，能够平衡新技能学习和原有能力保持。

Abstract: How can we teach large multimodal models (LMMs) new skills without erasing
prior abilities? We study sequential fine-tuning on five target skills while
monitoring general ability on eight held-out benchmarks across three model
families. We observe that apparent "forgetting" on held-out tasks after narrow
fine-tuning can partly recover at later stages. We trace this behavior to a
measurable shift in the output token distribution, manifested through a simple
counting-bias probe that co-varies with forgetting. Guided by this picture, we
identify two simple, robust tuning recipes that learn strongly while limiting
drift: (i) updating only the self-attention projection layers, and (ii)
updating only the MLP Gate&Up while freezing the Down projection. Across models
and tasks, these choices deliver strong target gains while largely preserving
held-out performance. Code is available at
https://github.com/jessemelpolio/LMM_CL

</details>
