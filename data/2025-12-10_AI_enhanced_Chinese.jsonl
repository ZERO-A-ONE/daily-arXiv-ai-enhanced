{"id": "2512.06042", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06042", "abs": "https://arxiv.org/abs/2512.06042", "authors": ["Ashish Hooda", "Mihai Christodorescu", "Chuangang Ren", "Aaron Wilson", "Kassem Fawaz", "Somesh Jha"], "title": "Auto-SPT: Automating Semantic Preserving Transformations for Code", "comment": null, "summary": "Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.", "AI": {"tldr": "Auto-SPT\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u8bed\u4e49\u4fdd\u6301\u53d8\u6362\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6a21\u578b\u4e3b\u8981\u5728\u5e72\u51c0\u3001\u7ed3\u6784\u5316\u7684\u4ee3\u7801\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4ee3\u7801\u4f1a\u7ecf\u5386\u5404\u79cd\u8bed\u4e49\u4fdd\u6301\u7684\u53d8\u6362\uff08\u5982\u91cd\u6784\u3001\u538b\u7f29\u3001\u683c\u5f0f\u5316\u3001\u7f16\u8bd1\u5668\u4f18\u5316\uff09\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u5dee\u8ddd", "method": "\u63d0\u51faAuto-SPT\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u6784\u5efa\u4ee3\u7801\u5408\u6210\u6570\u636e\u751f\u6210\u5668\uff1a1\uff09\u4f7f\u7528LLM\u8bbe\u8ba1\u591a\u6837\u5316\u7684\u8bed\u4e49\u4fdd\u6301\u53d8\u6362\uff1b2\uff09\u4e3a\u8fd9\u4e9b\u53d8\u6362\u751f\u6210\u5f3a\u5b9e\u73b0\uff1b3\uff09\u7ec4\u5408\u53d8\u6362\u4ea7\u751f\u5f3a\u53d8\u6362\u6548\u679c\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u53d8\u6362\u591a\u6837\u6027\u5f71\u54cd\u7ec4\u5408\u5f3a\u5ea6", "result": "Auto-SPT\u751f\u6210\u7684\u8bed\u4e49\u4fdd\u6301\u53d8\u6362\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u591a\u6837\u5316\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u6700\u5148\u8fdb\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8fd8\u8868\u660eAuto-SPT\u53ef\u7528\u4e8e\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ea7\u751f\u5bf9\u73b0\u5b9e\u4e16\u754c\u5bf9\u6297\u6027\u4ee3\u7801\u53d8\u6362\u5177\u6709\u9c81\u68d2\u6027\u7684\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6a21\u578b", "conclusion": "Auto-SPT\u6846\u67b6\u80fd\u6709\u6548\u5f25\u5408\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6a21\u578b\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u8bed\u4e49\u4fdd\u6301\u53d8\u6362\uff0c\u65e2\u80fd\u653b\u51fb\u73b0\u6709\u6a21\u578b\uff0c\u4e5f\u80fd\u589e\u5f3a\u6a21\u578b\u5bf9\u73b0\u5b9e\u4e16\u754c\u4ee3\u7801\u53d8\u6362\u7684\u9c81\u68d2\u6027"}}
{"id": "2512.06060", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06060", "abs": "https://arxiv.org/abs/2512.06060", "authors": ["Mohanakrishnan Hariharan"], "title": "Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring", "comment": null, "summary": "This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4e1a\u52a1\u9700\u6c42\u6587\u6863\u81ea\u52a8\u751f\u6210\u8f6f\u4ef6\u6d4b\u8bd5\u7528\u4f8b\uff0c\u901a\u8fc7\u8d28\u91cf\u5de5\u7a0b\u53cd\u9988\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u4ece\u9759\u6001\u77e5\u8bc6\u5e93\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u7f3a\u4e4f\u968f\u65f6\u95f4\u6539\u8fdb\u6027\u80fd\u7684\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u8d28\u91cf\u5de5\u7a0b\u53cd\u9988\u4e2d\u5b66\u4e60\u5e76\u6301\u7eed\u4f18\u5316\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7684\u667a\u80fd\u4f53RAG\u6846\u67b6\uff0c\u7ed3\u5408\u4e13\u4e1a\u667a\u80fd\u4f53\u4e0e\u6df7\u5408\u5411\u91cf-\u56fe\u77e5\u8bc6\u5e93\uff0c\u4f7f\u7528PPO\u548cDQN\u7b97\u6cd5\u6839\u636e\u6d4b\u8bd5\u6709\u6548\u6027\u3001\u7f3a\u9677\u68c0\u6d4b\u7387\u548c\u5de5\u4f5c\u6d41\u6307\u6807\u4f18\u5316\u667a\u80fd\u4f53\u884c\u4e3a\u3002", "result": "\u5728\u4f01\u4e1a\u7ea7\u82f9\u679c\u9879\u76ee\u4e0a\u9a8c\u8bc1\uff1a\u6d4b\u8bd5\u751f\u6210\u51c6\u786e\u7387\u4ece94.8%\u63d0\u5347\u81f397.2%\uff08\u63d0\u9ad82.4%\uff09\uff0c\u7f3a\u9677\u68c0\u6d4b\u7387\u63d0\u534710.8%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u7531\u8d28\u91cf\u5de5\u7a0b\u4e13\u4e1a\u77e5\u8bc6\u9a71\u52a8\u7684\u6301\u7eed\u77e5\u8bc6\u7cbe\u70bc\u5faa\u73af\uff0c\u9010\u6b65\u63d0\u5347\u6d4b\u8bd5\u7528\u4f8b\u8d28\u91cf\uff0c\u589e\u5f3a\u800c\u975e\u66ff\u4ee3\u4eba\u5de5\u6d4b\u8bd5\u80fd\u529b\u3002"}}
{"id": "2512.06123", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.06123", "abs": "https://arxiv.org/abs/2512.06123", "authors": ["Qilin Zhou", "Zhengyuan Wei", "Haipeng Wang", "Zhuo Wang", "W. K. Chan"], "title": "Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples", "comment": "accepted by IEEE Transactions on Reliability; extended technical report", "summary": "Patch robustness certification is an emerging kind of provable defense technique against adversarial patch attacks for deep learning systems. Certified detection ensures the detection of all patched harmful versions of certified samples, which mitigates the failures of empirical defense techniques that could (easily) be compromised. However, existing certified detection methods are ineffective in certifying samples that are misclassified or whose mutants are inconsistently pre icted to different labels. This paper proposes HiCert, a novel masking-based certified detection technique. By focusing on the problem of mutants predicted with a label different from the true label with our formal analysis, HiCert formulates a novel formal relation between harmful samples generated by identified loopholes and their benign counterparts. By checking the bound of the maximum confidence among these potentially harmful (i.e., inconsistent) mutants of each benign sample, HiCert ensures that each harmful sample either has the minimum confidence among mutants that are predicted the same as the harmful sample itself below this bound, or has at least one mutant predicted with a label different from the harmful sample itself, formulated after two novel insights. As such, HiCert systematically certifies those inconsistent samples and consistent samples to a large extent. To our knowledge, HiCert is the first work capable of providing such a comprehensive patch robustness certification for certified detection. Our experiments show the high effectiveness of HiCert with a new state-of the-art performance: It certifies significantly more benign samples, including those inconsistent and consistent, and achieves significantly higher accuracy on those samples without warnings and a significantly lower false silent ratio.", "AI": {"tldr": "HiCert\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u63a9\u7801\u7684\u8ba4\u8bc1\u68c0\u6d4b\u6280\u672f\uff0c\u9488\u5bf9\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u63d0\u4f9b\u5168\u9762\u7684\u8865\u4e01\u9c81\u68d2\u6027\u8ba4\u8bc1\uff0c\u80fd\u591f\u8ba4\u8bc1\u66f4\u591a\u826f\u6027\u6837\u672c\uff08\u5305\u62ec\u4e0d\u4e00\u81f4\u548c\u4e00\u81f4\u7684\u6837\u672c\uff09\uff0c\u5728\u65e0\u8b66\u544a\u6837\u672c\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u51c6\u786e\u7387\u5e76\u663e\u8457\u964d\u4f4e\u865a\u5047\u9759\u9ed8\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u8ba4\u8bc1\u68c0\u6d4b\u65b9\u6cd5\u5728\u8ba4\u8bc1\u88ab\u9519\u8bef\u5206\u7c7b\u6216\u5176\u7a81\u53d8\u4f53\u88ab\u4e0d\u4e00\u81f4\u9884\u6d4b\u5230\u4e0d\u540c\u6807\u7b7e\u7684\u6837\u672c\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cfb\u7edf\u8ba4\u8bc1\u8fd9\u4e9b\u4e0d\u4e00\u81f4\u6837\u672c\u548c\u4e00\u81f4\u6837\u672c\u7684\u5168\u9762\u8865\u4e01\u9c81\u68d2\u6027\u8ba4\u8bc1\u65b9\u6cd5\u3002", "method": "HiCert\u662f\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u7684\u8ba4\u8bc1\u68c0\u6d4b\u6280\u672f\uff0c\u901a\u8fc7\u5173\u6ce8\u7a81\u53d8\u4f53\u9884\u6d4b\u6807\u7b7e\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e0d\u540c\u7684\u95ee\u9898\uff0c\u5236\u5b9a\u4e86\u6709\u5bb3\u6837\u672c\u4e0e\u5176\u826f\u6027\u5bf9\u5e94\u6837\u672c\u4e4b\u95f4\u7684\u5f62\u5f0f\u5173\u7cfb\u3002\u901a\u8fc7\u68c0\u67e5\u6bcf\u4e2a\u826f\u6027\u6837\u672c\u7684\u6f5c\u5728\u6709\u5bb3\uff08\u5373\u4e0d\u4e00\u81f4\uff09\u7a81\u53d8\u4f53\u7684\u6700\u5927\u7f6e\u4fe1\u5ea6\u8fb9\u754c\uff0c\u786e\u4fdd\u6bcf\u4e2a\u6709\u5bb3\u6837\u672c\u8981\u4e48\u5177\u6709\u9884\u6d4b\u4e0e\u6709\u5bb3\u6837\u672c\u672c\u8eab\u76f8\u540c\u7684\u7a81\u53d8\u4f53\u7684\u6700\u5c0f\u7f6e\u4fe1\u5ea6\u4f4e\u4e8e\u6b64\u8fb9\u754c\uff0c\u8981\u4e48\u81f3\u5c11\u6709\u4e00\u4e2a\u7a81\u53d8\u4f53\u9884\u6d4b\u6807\u7b7e\u4e0e\u6709\u5bb3\u6837\u672c\u672c\u8eab\u4e0d\u540c\u3002", "result": "\u5b9e\u9a8c\u663e\u793aHiCert\u5177\u6709\u9ad8\u6548\u6027\u5e76\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u8ba4\u8bc1\u4e86\u663e\u8457\u66f4\u591a\u7684\u826f\u6027\u6837\u672c\uff08\u5305\u62ec\u4e0d\u4e00\u81f4\u548c\u4e00\u81f4\u7684\uff09\uff0c\u5728\u65e0\u8b66\u544a\u6837\u672c\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u865a\u5047\u9759\u9ed8\u7387\u3002", "conclusion": "HiCert\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u4e3a\u8ba4\u8bc1\u68c0\u6d4b\u63d0\u4f9b\u5982\u6b64\u5168\u9762\u7684\u8865\u4e01\u9c81\u68d2\u6027\u8ba4\u8bc1\u7684\u5de5\u4f5c\uff0c\u901a\u8fc7\u7cfb\u7edf\u8ba4\u8bc1\u4e0d\u4e00\u81f4\u6837\u672c\u548c\u4e00\u81f4\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u3002"}}
{"id": "2512.06033", "categories": ["cs.CR", "econ.GN"], "pdf": "https://arxiv.org/pdf/2512.06033", "abs": "https://arxiv.org/abs/2512.06033", "authors": ["Michael Yang", "Ruijiang Gao", "Zhiqiang", "Zheng"], "title": "Sell Data to AI Algorithms Without Revealing It: Secure Data Valuation and Sharing via Homomorphic Encryption", "comment": null, "summary": "The rapid expansion of Artificial Intelligence is hindered by a fundamental friction in data markets: the value-privacy dilemma, where buyers cannot verify a dataset's utility without inspection, yet inspection may expose the data (Arrow's Information Paradox). We resolve this challenge by introducing the Trustworthy Influence Protocol (TIP), a privacy-preserving framework that enables prospective buyers to quantify the utility of external data without ever decrypting the raw assets. By integrating Homomorphic Encryption with gradient-based influence functions, our approach allows for the precise, blinded scoring of data points against a buyer's specific AI model. To ensure scalability for Large Language Models (LLMs), we employ low-rank gradient projections that reduce computational overhead while maintaining near-perfect fidelity to plaintext baselines, as demonstrated across BERT and GPT-2 architectures. Empirical simulations in healthcare and generative AI domains validate the framework's economic potential: we show that encrypted valuation signals achieve a high correlation with realized clinical utility and reveal a heavy-tailed distribution of data value in pre-training corpora where a minority of texts drive capability while the majority degrades it. These findings challenge prevailing flat-rate compensation models and offer a scalable technical foundation for a meritocratic, secure data economy.", "AI": {"tldr": "TIP\u534f\u8bae\u901a\u8fc7\u540c\u6001\u52a0\u5bc6\u548c\u68af\u5ea6\u5f71\u54cd\u51fd\u6570\uff0c\u8ba9\u4e70\u5bb6\u5728\u4e0d\u89e3\u5bc6\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u91cf\u5316\u5916\u90e8\u6570\u636e\u5bf9AI\u6a21\u578b\u7684\u6548\u7528\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5e02\u573a\u4e2d\u7684\u4ef7\u503c-\u9690\u79c1\u56f0\u5883\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u53d7\u5230\u6570\u636e\u5e02\u573a\u57fa\u672c\u6469\u64e6\u7684\u963b\u788d\uff1a\u4ef7\u503c-\u9690\u79c1\u56f0\u5883\uff08\u963f\u7f57\u4fe1\u606f\u6096\u8bba\uff09\uff0c\u4e70\u5bb6\u65e0\u6cd5\u5728\u4e0d\u68c0\u67e5\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u9a8c\u8bc1\u6570\u636e\u96c6\u6548\u7528\uff0c\u4f46\u68c0\u67e5\u53c8\u4f1a\u66b4\u9732\u6570\u636e\u9690\u79c1\u3002", "method": "\u63d0\u51fa\u53ef\u4fe1\u5f71\u54cd\u534f\u8bae(TIP)\uff0c\u6574\u5408\u540c\u6001\u52a0\u5bc6\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u5f71\u54cd\u51fd\u6570\uff0c\u5141\u8bb8\u4e70\u5bb6\u5bf9\u6570\u636e\u70b9\u8fdb\u884c\u7cbe\u786e\u7684\u76f2\u8bc4\u5206\u3002\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u4f4e\u79e9\u68af\u5ea6\u6295\u5f71\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u660e\u6587\u57fa\u7ebf\u7684\u63a5\u8fd1\u5b8c\u7f8e\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u533b\u7597\u5065\u5eb7\u548c\u751f\u6210\u5f0fAI\u9886\u57df\u7684\u5b9e\u8bc1\u6a21\u62df\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u7ecf\u6d4e\u6f5c\u529b\uff1a\u52a0\u5bc6\u4f30\u503c\u4fe1\u53f7\u4e0e\u5b9e\u73b0\u7684\u4e34\u5e8a\u6548\u7528\u9ad8\u5ea6\u76f8\u5173\uff0c\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u6570\u636e\u4ef7\u503c\u7684\u91cd\u5c3e\u5206\u5e03\uff0c\u5c11\u6570\u6587\u672c\u9a71\u52a8\u80fd\u529b\u800c\u591a\u6570\u6587\u672c\u964d\u4f4e\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u6311\u6218\u4e86\u73b0\u884c\u7684\u56fa\u5b9a\u8d39\u7387\u8865\u507f\u6a21\u5f0f\uff0c\u4e3a\u5efa\u7acb\u57fa\u4e8e\u529f\u7ee9\u7684\u3001\u5b89\u5168\u7684\u6570\u636e\u7ecf\u6d4e\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2512.05998", "categories": ["cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05998", "abs": "https://arxiv.org/abs/2512.05998", "authors": ["Michael Todasco"], "title": "Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals", "comment": "25 pages, 8 tables, 2 figures. Pilot study. Data, prompts, and code available at https://osf.io/dc24t/", "summary": "Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. \"Whale\" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.", "AI": {"tldr": "\u901a\u8fc7\u865a\u6784\u9884\u6d4b\u5e02\u573a\u8ba9LLM\u7528\u865a\u62df\u8d27\u5e01\u4e0b\u6ce8\uff0c\u5c06\u8bc4\u4f30\u4efb\u52a1\u8f6c\u5316\u4e3a\u8d4c\u535a\u6e38\u620f\uff0c\u80fd\u591f\u4ea7\u751f\u53ef\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\uff0c\u4f7f\u6a21\u578b\u5185\u90e8\u4fe1\u5ff5\u53d8\u5f97\u53ef\u89c1\u53ef\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u8bc4\u4f30\u5176\u4ed6\u6a21\u578b\uff0c\u4f46\u8fd9\u4e9b\u5224\u65ad\u901a\u5e38\u7f3a\u4e4f\u7f6e\u4fe1\u5ea6\u8868\u793a\u3002\u672c\u7814\u7a76\u63a2\u7d22\u662f\u5426\u901a\u8fc7\u8d4c\u535a\u6e38\u620f\u6846\u67b6\uff08\u865a\u6784\u9884\u6d4b\u5e02\u573a\uff09\u80fd\u591f\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u4ea7\u751f\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u3002", "method": "\u751f\u6210100\u4e2a\u6570\u5b66\u548c\u903b\u8f91\u95ee\u9898\uff0c6\u4e2a\u57fa\u7ebf\u6a21\u578b\uff083\u4e2a\u5f53\u524d\u4ee3\uff0c3\u4e2a\u524d\u4ee3\uff09\u56de\u7b54\u95ee\u9898\u30023\u4e2a\u9884\u6d4b\u6a21\u578b\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u9884\u6d4b\u6bcf\u4e2a\u95ee\u9898-\u57fa\u7ebf\u7ec4\u5408\uff1a\u63a7\u5236\u7ec4\uff08\u7b80\u5355\u6b63\u786e/\u9519\u8bef\u9884\u6d4b\uff09\u548c\u6fc0\u52b1\u7ec4\uff08\u9884\u6d4b\u52a01-100,000 LLMCoin\u4e0b\u6ce8\uff0c\u8d77\u59cb\u8d44\u91d11,000,000 LLMCoin\uff09\u3002", "result": "\u6fc0\u52b1\u7ec4\u663e\u793a\u7565\u9ad8\u7684\u51c6\u786e\u6027\uff0881.5% vs. 79.1%\uff0cp=0.089\uff09\uff0c\u5b66\u4e60\u901f\u5ea6\u663e\u8457\u66f4\u5feb\uff08\u7b2c1\u8f6e\u5230\u7b2c4\u8f6e\u6539\u8fdb12.0% vs. 2.9%\uff0cp=0.011\uff09\u3002\u4e0b\u6ce8\u5927\u5c0f\u4e0e\u7f6e\u4fe1\u5ea6\u76f8\u5173\uff1a40,000+\u786c\u5e01\u7684\u5927\u989d\u4e0b\u6ce8\u6b63\u786e\u7387\u7ea699%\uff0c<1,000\u786c\u5e01\u7684\u5c0f\u989d\u4e0b\u6ce8\u6b63\u786e\u7387\u4ec5\u7ea674%\u3002", "conclusion": "\u4e3b\u8981\u53d1\u73b0\u4e0d\u662f\u865a\u62df\u8d27\u5e01\u8ba9\u6a21\u578b\u66f4\u806a\u660e\uff0c\u800c\u662f\u8d4c\u535a\u673a\u5236\u521b\u9020\u4e86\u4ece\u4e8c\u5143\u8f93\u51fa\u4e2d\u7f3a\u5931\u7684\u53ef\u8bfb\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u3002\u7b80\u5355\u7684\u8d22\u52a1\u6846\u67b6\u53ef\u80fd\u5e2e\u52a9LLM\u6210\u4e3a\u98ce\u9669\u611f\u77e5\u7684\u9884\u6d4b\u8005\uff0c\u4f7f\u5176\u5185\u90e8\u4fe1\u5ff5\u53d8\u5f97\u53ef\u89c1\u53ef\u7528\uff0c\u4e3a\u5143\u8bc4\u4f30\u7cfb\u7edf\u548cLLM\u95f4\u9884\u6d4b\u5e02\u573a\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2512.06178", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.06178", "abs": "https://arxiv.org/abs/2512.06178", "authors": ["Georgiana Haldeman", "Peter Ohmann", "Paul Denny"], "title": "Systematically Thinking about the Complexity of Code Structuring Exercises at Introductory Level", "comment": null, "summary": "Decomposition and abstraction is an essential component of computational thinking, yet it is not always emphasized in introductory programming courses. In addition, as generative AI further reduces the focus on syntax and increases the importance of higher-level code reasoning, there is renewed opportunity to teach DA explicitly. In this paper, we introduce a framework for systematically assessing the complexity of code structuring tasks, where students must identify and separate meaningful abstractions within existing, unstructured code. The framework defines three dimensions of task complexity, each with multiple levels: repetition, code pattern, and data dependency. To support practical use, we provide example tasks mapped to these levels and offer an interactive tool for generating and exploring DA problems. The framework is designed to support the development of educational tasks that build students' skills with DA in the procedural paradigm.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u4ee3\u7801\u7ed3\u6784\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6559\u6388\u5206\u89e3\u4e0e\u62bd\u8c61\u6280\u80fd\uff0c\u5305\u542b\u4e09\u4e2a\u7ef4\u5ea6\uff1a\u91cd\u590d\u3001\u4ee3\u7801\u6a21\u5f0f\u548c\u6570\u636e\u4f9d\u8d56", "motivation": "\u5206\u89e3\u4e0e\u62bd\u8c61\u662f\u8ba1\u7b97\u601d\u7ef4\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u5728\u5165\u95e8\u7f16\u7a0b\u8bfe\u7a0b\u4e2d\u5e38\u88ab\u5ffd\u89c6\u3002\u968f\u7740\u751f\u6210\u5f0fAI\u964d\u4f4e\u8bed\u6cd5\u91cd\u8981\u6027\u5e76\u63d0\u5347\u9ad8\u7ea7\u4ee3\u7801\u63a8\u7406\u7684\u91cd\u8981\u6027\uff0c\u73b0\u5728\u6709\u65b0\u7684\u673a\u4f1a\u6765\u660e\u786e\u6559\u6388\u5206\u89e3\u4e0e\u62bd\u8c61\u6280\u80fd", "method": "\u5f15\u5165\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u4ee3\u7801\u7ed3\u6784\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u6846\u67b6\uff0c\u5b9a\u4e49\u4e09\u4e2a\u7ef4\u5ea6\uff08\u91cd\u590d\u3001\u4ee3\u7801\u6a21\u5f0f\u3001\u6570\u636e\u4f9d\u8d56\uff09\uff0c\u6bcf\u4e2a\u7ef4\u5ea6\u6709\u591a\u4e2a\u590d\u6742\u5ea6\u7ea7\u522b\u3002\u63d0\u4f9b\u793a\u4f8b\u4efb\u52a1\u6620\u5c04\u548c\u4ea4\u4e92\u5de5\u5177\u6765\u751f\u6210\u548c\u63a2\u7d22\u5206\u89e3\u4e0e\u62bd\u8c61\u95ee\u9898", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u652f\u6301\u6559\u80b2\u4efb\u52a1\u8bbe\u8ba1\u7684\u6846\u67b6\uff0c\u5e2e\u52a9\u5b66\u751f\u5728\u8fc7\u7a0b\u5f0f\u7f16\u7a0b\u8303\u5f0f\u4e2d\u57f9\u517b\u5206\u89e3\u4e0e\u62bd\u8c61\u6280\u80fd\u3002\u6846\u67b6\u63d0\u4f9b\u4e86\u4efb\u52a1\u590d\u6742\u5ea6\u8bc4\u4f30\u7684\u5177\u4f53\u7ef4\u5ea6\u548c\u7ea7\u522b", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u8bbe\u8ba1\u548c\u8bc4\u4f30\u5206\u89e3\u4e0e\u62bd\u8c61\u6559\u5b66\u4efb\u52a1\uff0c\u6709\u52a9\u4e8e\u5728\u751f\u6210\u5f0fAI\u65f6\u4ee3\u66f4\u597d\u5730\u57f9\u517b\u5b66\u751f\u7684\u8ba1\u7b97\u601d\u7ef4\u548c\u9ad8\u7ea7\u4ee3\u7801\u63a8\u7406\u80fd\u529b"}}
{"id": "2512.06161", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06161", "abs": "https://arxiv.org/abs/2512.06161", "authors": ["Gondy Leroy", "Prakash Bisht", "Sai Madhuri Kandula", "Nell Maltman", "Sydney Rice"], "title": "Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach", "comment": "9 pages", "summary": "Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBioBERT\u7684\u900f\u660e\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u4e34\u5e8a\u6587\u672c\u4ee5\u8bca\u65ad\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\uff0c\u901a\u8fc7\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u7b56\u7565\u83b7\u5f97\u4e8697%\u654f\u611f\u6027\u548c98%\u7279\u5f02\u6027\u7684\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\uff08ASD\uff09\u8bca\u65ad\u8fc7\u7a0b\u6f2b\u957f\u4e14\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u591a\u4e3a\u9ed1\u7bb1\u4e14\u901a\u5e38\u57fa\u4e8e\u5355\u4e00\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3002", "method": "\u91c7\u7528BioBERT\u8bed\u8a00\u6a21\u578b\u5206\u6790\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6587\u672c\uff0c\u8bad\u7ec3\u6a21\u578b\u6807\u8bb0\u884c\u4e3a\u63cf\u8ff0\u5e76\u6620\u5c04\u5230\u8bca\u65ad\u6807\u51c6\uff0c\u7136\u540e\u5206\u914d\u6700\u7ec8\u6807\u7b7e\uff08ASD\u6216\u975eASD\uff09\u3002\u8bc4\u4f30\u4e86\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\uff0c\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86\u987a\u5e8f\u8bad\u7ec3\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u4e0e\u9ed1\u7bb1\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u900f\u660e\u6a21\u578b\u8868\u73b0\u51fa\u7a33\u5065\u6027\u80fd\uff0c\u6df7\u5408\u6570\u636e\u8bad\u7ec3\u7b56\u7565\u83b7\u5f97\u6700\u4f73\u7ed3\u679c\uff0897%\u654f\u611f\u6027\uff0c98%\u7279\u5f02\u6027\uff09\u3002\u987a\u5e8f\u8bad\u7ec3\u5bfc\u81f4\u6027\u80fd\u7565\u6709\u4e0b\u964d\u3002\u9ed1\u7bb1\u6a21\u578b\u5728\u987a\u5e8f\u6216\u6df7\u5408\u6570\u636e\u8bad\u7ec3\u4e0b\u8868\u73b0\u8f83\u5dee\uff0890%\u654f\u611f\u6027\uff0c96%\u7279\u5f02\u6027\uff09\u3002\u900f\u660e\u65b9\u6cd5\u6574\u4f53\u4f18\u4e8e\u9ed1\u7bb1\u65b9\u6cd5\u3002", "conclusion": "\u900f\u660e\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728ASD\u8bca\u65ad\u4e2d\u4f18\u4e8e\u9ed1\u7bb1\u65b9\u6cd5\uff0c\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u53ef\u83b7\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u4e3a\u795e\u7ecf\u53d1\u80b2\u969c\u788d\u8bca\u65ad\u4e2d\u66f4\u53ef\u4fe1\u3001\u53ef\u6cdb\u5316\u4e14\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684AI\u5de5\u5177\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.06155", "categories": ["cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.06155", "abs": "https://arxiv.org/abs/2512.06155", "authors": ["Caleb Gross"], "title": "Sift or Get Off the PoC: Applying Information Retrieval to Vulnerability Research with SiftRank", "comment": null, "summary": "Security research is fundamentally a problem of resource constraint and consequent prioritization. There is simply too much attack surface and too little time and energy to spend analyzing it all. The most effective security researchers are often those who are most skilled at intuitively deciding which part of an expansive attack surface to investigate. We demonstrate that this problem of selecting the most promising option from among many possibilities can be reframed as an information retrieval problem, and solved using document ranking techniques with LLMs performing the heavy lifting as general-purpose rankers. We present SiftRank, a ranking algorithm achieving O(n) complexity through three key mechanisms: listwise ranking using an LLM to order documents in small batches of approximately 10 items at a time; inflection-based convergence detection that adaptively terminates ranking when score distributions have stabilized; and iterative refinement that progressively focuses ranking effort on the most relevant documents. Unlike existing reranking approaches that require a separate first-stage retrieval step to narrow datasets to approximately 100 candidates, SiftRank operates directly on thousands of items, with each document evaluated across multiple randomized batches to mitigate inconsistent judgments by an LLM. We demonstrate practical effectiveness on N-day vulnerability analysis, successfully identifying a vulnerability-fixing function among 2,197 changed functions in a stripped binary firmware patch within 99 seconds at an inference cost of $0.82. Our approach enables scalable security prioritization for problems that are generally constrained by manual analysis, requiring only standard LLM API access without specialized infrastructure, embedding, or domain-specific fine-tuning. An open-source implementation of SiftRank may be found at https://github.com/noperator/siftrank.", "AI": {"tldr": "SiftRank\uff1a\u4e00\u79cd\u57fa\u4e8eLLM\u7684O(n)\u590d\u6742\u5ea6\u6392\u5e8f\u7b97\u6cd5\uff0c\u7528\u4e8e\u5b89\u5168\u7814\u7a76\u4e2d\u7684\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u53ef\u76f4\u63a5\u5904\u7406\u6570\u5343\u4e2a\u9879\u76ee\uff0c\u65e0\u9700\u9884\u7b5b\u9009\u6b65\u9aa4", "motivation": "\u5b89\u5168\u7814\u7a76\u9762\u4e34\u8d44\u6e90\u9650\u5236\u548c\u4f18\u5148\u7ea7\u6392\u5e8f\u95ee\u9898\uff0c\u653b\u51fb\u9762\u592a\u5927\u800c\u5206\u6790\u8d44\u6e90\u6709\u9650\u3002\u6700\u6709\u6548\u7684\u5b89\u5168\u7814\u7a76\u4eba\u5458\u901a\u5e38\u64c5\u957f\u76f4\u89c9\u9009\u62e9\u6700\u6709\u5e0c\u671b\u7684\u8c03\u67e5\u65b9\u5411\uff0c\u8fd9\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u4ece\u4f17\u591a\u53ef\u80fd\u6027\u4e2d\u9009\u62e9\u6700\u4f18\u9009\u9879\u7684\u95ee\u9898\u3002", "method": "\u5c06\u4f18\u5148\u7ea7\u6392\u5e8f\u95ee\u9898\u91cd\u6784\u4e3a\u4fe1\u606f\u68c0\u7d22\u95ee\u9898\uff0c\u4f7f\u7528\u6587\u6863\u6392\u5e8f\u6280\u672f\uff0cLLM\u4f5c\u4e3a\u901a\u7528\u6392\u5e8f\u5668\u3002SiftRank\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u673a\u5236\u5b9e\u73b0O(n)\u590d\u6742\u5ea6\uff1a1) \u5217\u8868\u5f0f\u6392\u5e8f\uff0cLLM\u5728\u5c0f\u6279\u91cf\uff08\u7ea610\u4e2a\u9879\u76ee\uff09\u4e2d\u6392\u5e8f\u6587\u6863\uff1b2) \u57fa\u4e8e\u62d0\u70b9\u7684\u6536\u655b\u68c0\u6d4b\uff0c\u5f53\u5206\u6570\u5206\u5e03\u7a33\u5b9a\u65f6\u81ea\u9002\u5e94\u7ec8\u6b62\u6392\u5e8f\uff1b3) \u8fed\u4ee3\u7cbe\u70bc\uff0c\u9010\u6b65\u805a\u7126\u4e8e\u6700\u76f8\u5173\u6587\u6863\u3002\u6bcf\u4e2a\u6587\u6863\u5728\u591a\u4e2a\u968f\u673a\u6279\u6b21\u4e2d\u8bc4\u4f30\u4ee5\u51cf\u8f7bLLM\u5224\u65ad\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5728N-day\u6f0f\u6d1e\u5206\u6790\u4e2d\uff0c\u6210\u529f\u4ece2,197\u4e2a\u66f4\u6539\u51fd\u6570\u4e2d\u8bc6\u522b\u51fa\u6f0f\u6d1e\u4fee\u590d\u51fd\u6570\uff0c\u8017\u65f699\u79d2\uff0c\u63a8\u7406\u6210\u672c0.82\u7f8e\u5143\u3002\u65e0\u9700\u4e13\u95e8\u7684\u5d4c\u5165\u6216\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0c\u4ec5\u9700\u6807\u51c6LLM API\u8bbf\u95ee\u3002", "conclusion": "SiftRank\u4e3a\u901a\u5e38\u53d7\u9650\u4e8e\u624b\u52a8\u5206\u6790\u7684\u5b89\u5168\u4f18\u5148\u7ea7\u6392\u5e8f\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u76f4\u63a5\u5904\u7406\u6570\u5343\u4e2a\u9879\u76ee\uff0c\u65e0\u9700\u9884\u7b5b\u9009\u6b65\u9aa4\uff0c\u4ec5\u9700\u6807\u51c6LLM API\u8bbf\u95ee\uff0c\u65e0\u9700\u4e13\u95e8\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2512.06196", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06196", "abs": "https://arxiv.org/abs/2512.06196", "authors": ["Charlie Masters", "Marta Grze\u015bkiewicz", "Stefano V. Albrecht"], "title": "ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment", "comment": "Accepted to the AAAI 2026 LLAMAS Workshop (Large Language Model Agents for Multi-Agent Systems)", "summary": "As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.", "AI": {"tldr": "ARCANE\u6846\u67b6\u5c06AI\u5bf9\u9f50\u95ee\u9898\u8f6c\u5316\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u81ea\u7136\u8bed\u8a00\u8bc4\u5206\u6807\u51c6\u6765\u4ee3\u8868\u5229\u76ca\u76f8\u5173\u8005\u504f\u597d\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8c03\u6574\u7684\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5230\u957f\u671f\u4efb\u52a1\u4e2d\uff0c\u4fdd\u6301\u5176\u4e0e\u5229\u76ca\u76f8\u5173\u8005\u504f\u597d\u7684\u4e00\u81f4\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u6a21\u578b\u8ba9\u5229\u76ca\u76f8\u5173\u8005\u7406\u89e3\u548c\u5ba1\u6838\u6a21\u578b\u76ee\u6807\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u4ea4\u4e92\u65f6\u5f15\u5bfc\u667a\u80fd\u4f53\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u7eb3\u5165\u504f\u597d\u53d8\u5316\u3002", "method": "\u63d0\u51faARCANE\u6846\u67b6\uff0c\u5c06\u5bf9\u9f50\u95ee\u9898\u6784\u5efa\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u95ee\u9898\uff0c\u52a8\u6001\u751f\u6210\u81ea\u7136\u8bed\u8a00\u8bc4\u5206\u6807\u51c6\uff08\u52a0\u6743\u53ef\u9a8c\u8bc1\u6807\u51c6\uff09\u3002\u53d7\u6548\u7528\u7406\u8bba\u542f\u53d1\uff0c\u5c06\u8bc4\u5206\u6807\u51c6\u5b66\u4e60\u6784\u5efa\u4e3a\u91cd\u6784\u95ee\u9898\uff0c\u5e94\u7528\u6b63\u5219\u5316\u7684\u7ec4\u5e8f\u5217\u7b56\u7565\u4f18\u5316(GSPO)\u7a0b\u5e8f\uff0c\u5e73\u8861\u53ef\u89e3\u91ca\u6027\u3001\u5fe0\u5b9e\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u4f7f\u7528GDPVal\u57fa\u51c6\u7684219\u4e2a\u6807\u8bb0\u8bc4\u5206\u6807\u51c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u9700\u8981\u591a\u6b65\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u6d4b\u8bd5ARCANE\u3002\u5b66\u4e60\u7684\u8bc4\u5206\u6807\u51c6\u4ea7\u751f\u7d27\u51d1\u3001\u6613\u8bfb\u7684\u8bc4\u4f30\uff0c\u5e76\u652f\u6301\u53ef\u914d\u7f6e\u7684\u6743\u8861\uff08\u5982\u6b63\u786e\u6027\u4e0e\u7b80\u6d01\u6027\uff09\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u5956\u52b1\u6a21\u578b\u4e3a\u590d\u6742\u3001\u957f\u671fAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u5bf9\u9f50\u7684\u6709\u524d\u666f\u8def\u5f84\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8c03\u6574\u7684\u504f\u597d\u5bf9\u9f50\u3002"}}
{"id": "2512.06248", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.06248", "abs": "https://arxiv.org/abs/2512.06248", "authors": ["Cheng Cheng", "Jinqiu Yang"], "title": "CFCEval: Evaluating Security Aspects in Code Generated by Large Language Models", "comment": null, "summary": "Code-focused Large Language Models (LLMs), such as CodeX and Star-Coder, have demonstrated remarkable capabilities in enhancing developer productivity through context-aware code generation. However, evaluating the quality and security of LLM-generated code remains a significant challenge. Existing evaluation protocols for Code LLMs lack both methodological rigor and comprehensive scope. A key limitation is dataset bias, which arises from unintentional overlap between training and testing data. Furthermore, while CodeBLEU, a BLEU-based metric, is widely used to assess code similarity, it suffers from critical shortcomings, including imprecise tokenization, structural limitations, and low reference diversity. To address these challenges, we introduce CFCEval, a novel framework for evaluating the quality and security of code generated by LLMs. CFCEval mitigates dataset bias by creating a new benchmark, MLVBench, and incorporates ELRM, a new metric designed to assess the relevance between reference code and generated code. CFCEval evaluates generated code across four dimensions: programming quality, vulnerability-fixing capability, post-transformation fixing capability, and relevance. Our experiments show that CFCEval not only captures both quality and security aspects of generated code more effectively but also that its ELRM aligns more closely with human judgments than CodeBLEU, thus paving the way for future advancements in Code LLMs evaluation.", "AI": {"tldr": "CFCEval\u662f\u4e00\u4e2a\u8bc4\u4f30\u4ee3\u7801\u751f\u6210\u5927\u8bed\u8a00\u6a21\u578b\u8d28\u91cf\u548c\u5b89\u5168\u6027\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7MLVBench\u57fa\u51c6\u548cELRM\u6307\u6807\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u751f\u6210\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u5b58\u5728\u65b9\u6cd5\u5b66\u4e25\u8c28\u6027\u548c\u5168\u9762\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5305\u62ec\u6570\u636e\u96c6\u504f\u5dee\u3001CodeBLEU\u6307\u6807\u7684\u7f3a\u9677\uff08\u4e0d\u7cbe\u786e\u7684\u5206\u8bcd\u3001\u7ed3\u6784\u9650\u5236\u3001\u53c2\u8003\u591a\u6837\u6027\u4f4e\uff09\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u540c\u65f6\u8861\u91cf\u4ee3\u7801\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faCFCEval\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u521b\u5efaMLVBench\u65b0\u57fa\u51c6\u6765\u7f13\u89e3\u6570\u636e\u96c6\u504f\u5dee\uff1b2\uff09\u8bbe\u8ba1ELRM\u65b0\u6307\u6807\u8bc4\u4f30\u53c2\u8003\u4ee3\u7801\u4e0e\u751f\u6210\u4ee3\u7801\u7684\u76f8\u5173\u6027\uff1b3\uff09\u4ece\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u751f\u6210\u4ee3\u7801\uff1a\u7f16\u7a0b\u8d28\u91cf\u3001\u6f0f\u6d1e\u4fee\u590d\u80fd\u529b\u3001\u540e\u8f6c\u6362\u4fee\u590d\u80fd\u529b\u548c\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCFCEval\u80fd\u66f4\u6709\u6548\u5730\u6355\u6349\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u5b89\u5168\u65b9\u9762\uff0c\u4e14\u5176ELRM\u6307\u6807\u6bd4CodeBLEU\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5224\u65ad\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "conclusion": "CFCEval\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2512.06172", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06172", "abs": "https://arxiv.org/abs/2512.06172", "authors": ["Sheng Liu", "Panos Papadimitratos"], "title": "DEFEND: Poisoned Model Detection and Malicious Client Exclusion Mechanism for Secure Federated Learning-based Road Condition Classification", "comment": "Accepted to the 41st ACM/SIGAPP Symposium on Applied Computing (SAC 2026)", "summary": "Federated Learning (FL) has drawn the attention of the Intelligent Transportation Systems (ITS) community. FL can train various models for ITS tasks, notably camera-based Road Condition Classification (RCC), in a privacy-preserving collaborative way. However, opening up to collaboration also opens FL-based RCC systems to adversaries, i.e., misbehaving participants that can launch Targeted Label-Flipping Attacks (TLFAs) and threaten transportation safety. Adversaries mounting TLFAs poison training data to misguide model predictions, from an actual source class (e.g., wet road) to a wrongly perceived target class (e.g., dry road). Existing countermeasures against poisoning attacks cannot maintain model performance under TLFAs close to the performance level in attack-free scenarios, because they lack specific model misbehavior detection for TLFAs and neglect client exclusion after the detection. To close this research gap, we propose DEFEND, which includes a poisoned model detection strategy that leverages neuron-wise magnitude analysis for attack goal identification and Gaussian Mixture Model (GMM)-based clustering. DEFEND discards poisoned model contributions in each round and adapts accordingly client ratings, eventually excluding malicious clients. Extensive evaluation involving various FL-RCC models and tasks shows that DEFEND can thwart TLFAs and outperform seven baseline countermeasures, with at least 15.78% improvement, with DEFEND remarkably achieving under attack the same performance as in attack-free scenarios.", "AI": {"tldr": "DEFEND\uff1a\u4e00\u79cd\u9488\u5bf9\u8054\u90a6\u5b66\u4e60\u9053\u8def\u72b6\u51b5\u5206\u7c7b\u4e2d\u76ee\u6807\u6807\u7b7e\u7ffb\u8f6c\u653b\u51fb\u7684\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u5e45\u5ea6\u5206\u6790\u548cGMM\u805a\u7c7b\u68c0\u6d4b\u4e2d\u6bd2\u6a21\u578b\uff0c\u6392\u9664\u6076\u610f\u5ba2\u6237\u7aef\uff0c\u5728\u653b\u51fb\u4e0b\u4fdd\u6301\u4e0e\u65e0\u653b\u51fb\u573a\u666f\u76f8\u540c\u7684\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u9762\u4e34\u5b89\u5168\u5a01\u80c1\uff0c\u7279\u522b\u662f\u76ee\u6807\u6807\u7b7e\u7ffb\u8f6c\u653b\u51fb\u4f1a\u8bef\u5bfc\u6a21\u578b\u9884\u6d4b\uff0c\u5a01\u80c1\u4ea4\u901a\u5b89\u5168\u3002\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u65e0\u6cd5\u5728\u653b\u51fb\u4e0b\u7ef4\u6301\u63a5\u8fd1\u65e0\u653b\u51fb\u573a\u666f\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u7f3a\u4e4f\u9488\u5bf9TLFA\u7684\u7279\u5b9a\u68c0\u6d4b\u673a\u5236\u548c\u5ba2\u6237\u7aef\u6392\u9664\u7b56\u7565\u3002", "method": "\u63d0\u51faDEFEND\u9632\u5fa1\u673a\u5236\uff1a1\uff09\u4f7f\u7528\u795e\u7ecf\u5143\u5e45\u5ea6\u5206\u6790\u8fdb\u884c\u653b\u51fb\u76ee\u6807\u8bc6\u522b\uff1b2\uff09\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u805a\u7c7b\u68c0\u6d4b\u4e2d\u6bd2\u6a21\u578b\uff1b3\uff09\u6bcf\u8f6e\u4e22\u5f03\u4e2d\u6bd2\u6a21\u578b\u7684\u8d21\u732e\uff1b4\uff09\u81ea\u9002\u5e94\u8c03\u6574\u5ba2\u6237\u7aef\u8bc4\u5206\uff0c\u6700\u7ec8\u6392\u9664\u6076\u610f\u5ba2\u6237\u7aef\u3002", "result": "\u5728\u591a\u79cd\u8054\u90a6\u5b66\u4e60RCC\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cDEFEND\u80fd\u591f\u6709\u6548\u62b5\u5fa1TLFA\uff0c\u6027\u80fd\u4f18\u4e8e7\u4e2a\u57fa\u7ebf\u9632\u5fa1\u65b9\u6cd5\uff0c\u81f3\u5c11\u63d0\u534715.78%\uff0c\u5728\u653b\u51fb\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u65e0\u653b\u51fb\u573a\u666f\u76f8\u540c\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "DEFEND\u586b\u8865\u4e86\u8054\u90a6\u5b66\u4e60\u9053\u8def\u72b6\u51b5\u5206\u7c7b\u4e2d\u9488\u5bf9\u76ee\u6807\u6807\u7b7e\u7ffb\u8f6c\u653b\u51fb\u7684\u9632\u5fa1\u7814\u7a76\u7a7a\u767d\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u68c0\u6d4b\u548c\u6392\u9664\u673a\u5236\uff0c\u5728\u4fdd\u6301\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\u786e\u4fdd\u4e86\u4ea4\u901a\u5b89\u5168\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2512.06205", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06205", "abs": "https://arxiv.org/abs/2512.06205", "authors": ["Daniel Quigley", "Eric Maynard"], "title": "On measuring grounding and generalizing grounding problems", "comment": "36 pages, 85 sources", "summary": "The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u4ece\u4e8c\u5143\u5224\u65ad\u91cd\u6784\u4e3a\u591a\u7ef4\u5ea6\u5ba1\u8ba1\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u771f\u5b9e\u6027\u3001\u4fdd\u6301\u6027\u3001\u5fe0\u5b9e\u6027\u3001\u9c81\u68d2\u6027\u548c\u7ec4\u5408\u6027\u4e94\u4e2a\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u5e94\u7528\u4e8e\u56db\u79cd\u63a5\u5730\u6a21\u5f0f\u548c\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u2014\u2014\u5373\u7b26\u53f7\u5982\u4f55\u83b7\u5f97\u610f\u4e49\u800c\u975e\u4ec5\u4ec5\u662f\u5f62\u5f0f\u64cd\u4f5c\u2014\u2014\u7684\u4f20\u7edf\u4e8c\u5143\u5224\u65ad\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u7406\u89e3\u4e0d\u540c\u7cfb\u7edf\u4e2d\u7684\u610f\u4e49\u8868\u5f81\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u7ef4\u5ea6\u5ba1\u8ba1\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u6838\u5fc3\u8bc4\u4f30\u6807\u51c6\uff1a\u771f\u5b9e\u6027\uff08\u673a\u5236\u662f\u5426\u5728\u667a\u80fd\u4f53\u5185\u90e8\u5e76\u901a\u8fc7\u5b66\u4e60/\u8fdb\u5316\u83b7\u5f97\uff09\u3001\u4fdd\u6301\u6027\uff08\u539f\u5b50\u610f\u4e49\u662f\u5426\u4fdd\u6301\u5b8c\u6574\uff09\u3001\u5fe0\u5b9e\u6027\uff08\u5305\u62ec\u76f8\u5173\u6027\u548c\u56e0\u679c\u6027\uff09\u3001\u9c81\u68d2\u6027\uff08\u5728\u6270\u52a8\u4e0b\u7684\u4f18\u96c5\u964d\u7ea7\uff09\u3001\u7ec4\u5408\u6027\uff08\u6574\u4f53\u662f\u5426\u7cfb\u7edf\u5730\u4ece\u90e8\u5206\u6784\u5efa\uff09\u3002\u5c06\u6b64\u6846\u67b6\u5e94\u7528\u4e8e\u56db\u79cd\u63a5\u5730\u6a21\u5f0f\uff08\u7b26\u53f7\u3001\u6307\u79f0\u3001\u5411\u91cf\u3001\u5173\u7cfb\uff09\u548c\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u6a21\u578b\u8bba\u8bed\u4e49\u5b66\u5b9e\u73b0\u7cbe\u786e\u7ec4\u5408\u4f46\u7f3a\u4e4f\u56e0\u679c\u4fdd\u8bc1\uff1b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u4efb\u52a1\u4e0a\u663e\u793a\u76f8\u5173\u62df\u5408\u548c\u5c40\u90e8\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u65e0\u63a5\u5730\u4ea4\u4e92\u7684\u4e16\u754c\u4efb\u52a1\u4e0a\u7f3a\u4e4f\u6210\u529f\u9009\u62e9\uff1b\u4eba\u7c7b\u8bed\u8a00\u901a\u8fc7\u8fdb\u5316\u548c\u53d1\u5c55\u83b7\u5f97\u5f3a\u771f\u5b9e\u6027\u6ee1\u8db3\u6240\u6709\u6807\u51c6\u3002\u8be5\u6846\u67b6\u4e3a\u54f2\u5b66\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u8bed\u8a00\u5b66\u548c\u6570\u5b66\u63d0\u4f9b\u4e86\u5171\u540c\u8bed\u8a00\u548c\u6280\u672f\u5de5\u5177\u3002", "conclusion": "\u901a\u8fc7\u5c06\u54f2\u5b66\u4e0a\u7684\u8868\u5f81\u95ee\u9898\u64cd\u4f5c\u5316\u4e3a\u6280\u672f\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u7814\u7a76\u4e3a\u7b26\u53f7\u63a5\u5730\u548c\u610f\u4e49\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u8bba\uff0c\u4fc3\u8fdb\u4e86\u8de8\u5b66\u79d1\u5bf9\u8bdd\u548c\u66f4\u6df1\u5165\u7684\u7406\u89e3\u3002"}}
{"id": "2512.06253", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.06253", "abs": "https://arxiv.org/abs/2512.06253", "authors": ["Shuainan Liu", "Tianxi Ji", "Zhongshuo Fang", "Lu Wei", "Pan Li"], "title": "Privacy Loss of Noise Perturbation via Concentration Analysis of A Product Measure", "comment": "Accepted by ACM International Conference on Management of Data (SIGMOD '26)", "summary": "Noise perturbation is one of the most fundamental approaches for achieving $(\u03b5,\u03b4)$-differential privacy (DP) guarantees when releasing the result of a query or function $f(\\cdot)\\in\\mathbb{R}^M$ evaluated on a sensitive dataset $\\mathbf{x}$. In this approach, calibrated noise $\\mathbf{n}\\in\\mathbb{R}^M$ is used to obscure the difference vector $f(\\mathbf{x})-f(\\mathbf{x}')$, where $\\mathbf{x}'$ is known as a neighboring dataset. A DP guarantee is obtained by studying the tail probability bound of a privacy loss random variable (PLRV), defined as the Radon-Nikodym derivative between two distributions. When $\\mathbf{n}$ follows a multivariate Gaussian distribution, the PLRV is characterized as a specific univariate Gaussian. In this paper, we propose a novel scheme to generate $\\mathbf{n}$ by leveraging the fact that the perturbation noise is typically spherically symmetric (i.e., the distribution is rotationally invariant around the origin). The new noise generation scheme allows us to investigate the privacy loss from a geometric perspective and express the resulting PLRV using a product measure, $W\\times U$; measure $W$ is related to a radius random variable controlling the magnitude of $\\mathbf{n}$, while measure $U$ involves a directional random variable governing the angle between $\\mathbf{n}$ and the difference $f(\\mathbf{x})-f(\\mathbf{x}')$. We derive a closed-form moment bound on the product measure to prove $(\u03b5,\u03b4)$-DP. Under the same $(\u03b5,\u03b4)$-DP guarantee, our mechanism yields a smaller expected noise magnitude than the classic Gaussian noise in high dimensions, thereby significantly improving the utility of the noisy result $f(\\mathbf{x})+\\mathbf{n}$. To validate this, we consider convex and non-convex empirical risk minimization (ERM) problems in high dimensional space and apply the proposed product noise to achieve privacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5dee\u5206\u9690\u79c1\u566a\u58f0\u751f\u6210\u65b9\u6848\uff0c\u901a\u8fc7\u5229\u7528\u566a\u58f0\u7684\u7403\u9762\u5bf9\u79f0\u6027\uff0c\u5c06\u9690\u79c1\u635f\u5931\u8868\u793a\u4e3a\u534a\u5f84\u548c\u65b9\u5411\u968f\u673a\u53d8\u91cf\u7684\u4e58\u79ef\u6d4b\u5ea6\uff0c\u5728\u76f8\u540c\u9690\u79c1\u4fdd\u8bc1\u4e0b\u6bd4\u4f20\u7edf\u9ad8\u65af\u566a\u58f0\u5177\u6709\u66f4\u5c0f\u7684\u671f\u671b\u566a\u58f0\u5e45\u5ea6\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u9ad8\u7ef4\u67e5\u8be2\u7ed3\u679c\u7684\u6548\u7528\u3002", "motivation": "\u4f20\u7edf\u9ad8\u65af\u566a\u58f0\u673a\u5236\u5728\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\u65f6\uff0c\u867d\u7136\u80fd\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u4f46\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u566a\u58f0\u5e45\u5ea6\u8f83\u5927\uff0c\u964d\u4f4e\u4e86\u67e5\u8be2\u7ed3\u679c\u7684\u5b9e\u7528\u6027\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u566a\u58f0\u6270\u52a8\u901a\u5e38\u5177\u6709\u7403\u9762\u5bf9\u79f0\u6027\uff08\u65cb\u8f6c\u4e0d\u53d8\u6027\uff09\uff0c\u5e0c\u671b\u5229\u7528\u8fd9\u4e00\u51e0\u4f55\u7279\u6027\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u566a\u58f0\u751f\u6210\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u566a\u58f0\u751f\u6210\u65b9\u6848\uff0c\u5c06\u566a\u58f0\u5206\u89e3\u4e3a\u534a\u5f84\u968f\u673a\u53d8\u91cf\uff08\u63a7\u5236\u566a\u58f0\u5e45\u5ea6\uff09\u548c\u65b9\u5411\u968f\u673a\u53d8\u91cf\uff08\u63a7\u5236\u566a\u58f0\u4e0e\u67e5\u8be2\u7ed3\u679c\u5dee\u5411\u91cf\u7684\u5939\u89d2\uff09\u7684\u4e58\u79ef\u6d4b\u5ea6\u3002\u901a\u8fc7\u51e0\u4f55\u89c6\u89d2\u5206\u6790\u9690\u79c1\u635f\u5931\uff0c\u63a8\u5bfc\u51fa\u4e58\u79ef\u6d4b\u5ea6\u7684\u95ed\u5f0f\u77e9\u754c\u6765\u8bc1\u660e(\u03b5,\u03b4)-\u5dee\u5206\u9690\u79c1\u4fdd\u8bc1\u3002", "result": "\u5728\u76f8\u540c\u7684(\u03b5,\u03b4)-\u5dee\u5206\u9690\u79c1\u4fdd\u8bc1\u4e0b\uff0c\u65b0\u673a\u5236\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u6bd4\u4f20\u7edf\u9ad8\u65af\u566a\u58f0\u4ea7\u751f\u66f4\u5c0f\u7684\u671f\u671b\u566a\u58f0\u5e45\u5ea6\u3002\u5728\u9ad8\u7ef4\u7a7a\u95f4\u7684\u51f8\u548c\u975e\u51f8\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u95ee\u9898\u4e2d\u5e94\u7528\u8be5\u4e58\u79ef\u566a\u58f0\uff0c\u9a8c\u8bc1\u4e86\u5176\u80fd\u663e\u8457\u63d0\u9ad8\u566a\u58f0\u7ed3\u679c\u7684\u6548\u7528\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u566a\u58f0\u7684\u7403\u9762\u5bf9\u79f0\u6027\uff0c\u63d0\u51fa\u7684\u4e58\u79ef\u566a\u58f0\u673a\u5236\u4e3a\u9ad8\u7ef4\u5dee\u5206\u9690\u79c1\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u76f8\u540c\u9690\u79c1\u4fdd\u62a4\u6c34\u5e73\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u566a\u58f0\u5bf9\u67e5\u8be2\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u53d1\u5e03\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.06240", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06240", "abs": "https://arxiv.org/abs/2512.06240", "authors": ["Chuanhao Nie", "Yunbo Liu", "Chao Wang"], "title": "AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems", "comment": null, "summary": "Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5728\u53cd\u6d17\u94b1(AML)\u5de5\u4f5c\u6d41\u73b0\u4ee3\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u57fa\u4e8e\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG-Graph)\u7684KYC\u5e94\u7528\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u67b6\u6784\u80fd\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u964d\u4f4e\u8bef\u62a5\u7387\uff0c\u5e76\u589e\u5f3aKYC\u6d41\u7a0b\u7684\u6548\u7387\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u6d17\u94b1\u548c\u91d1\u878d\u6b3a\u8bc8\u6bcf\u5e74\u9020\u6210\u6570\u4e07\u4ebf\u7f8e\u5143\u635f\u5931\uff0c\u4e25\u91cd\u5a01\u80c1\u5168\u7403\u91d1\u878d\u7a33\u5b9a\uff0c\u4f20\u7edf\u76d1\u7ba1\u65b9\u5f0f\u9762\u4e34\u6311\u6218\u3002\u9700\u8981\u73b0\u4ee3\u5316\u6280\u672f\u6765\u63d0\u9ad8\u53cd\u6d17\u94b1\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u964d\u4f4e\u8bef\u62a5\u7387\uff0c\u5e76\u51cf\u8f7b\u4eba\u5de5\u8c03\u67e5\u7684\u8fd0\u8425\u8d1f\u62c5\u3002", "method": "\u8bba\u6587\u9996\u5148\u7efc\u8ff0AI\u5728AML\u4e2d\u7684\u5e94\u7528\uff0c\u7136\u540e\u63d0\u51fa\u57fa\u4e8e\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG-Graph)\u7684KYC\u5e94\u7528\u67b6\u6784\u3002\u8be5\u67b6\u6784\u5c06\u56fe\u6280\u672f\u4e0e\u751f\u6210\u6a21\u578b\u7ed3\u5408\uff0c\u7528\u4e8e\u589e\u5f3aKYC\u6d41\u7a0b\u7684\u6548\u7387\u548c\u900f\u660e\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRAG-Graph\u67b6\u6784\u5728\u4e0d\u540c\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u9ad8\u5fe0\u5b9e\u5ea6\u548c\u5f3a\u7b54\u6848\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86KYC CDD/EDD\u5de5\u4f5c\u6d41\u7684\u6548\u7387\u548c\u900f\u660e\u5ea6\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u53ef\u6301\u7eed\u3001\u8d44\u6e90\u4f18\u5316\u7684\u5408\u89c4\u5b9e\u8df5\u3002", "conclusion": "AI\u6280\u672f\u80fd\u591f\u6709\u6548\u73b0\u4ee3\u5316\u53cd\u6d17\u94b1\u5de5\u4f5c\u6d41\uff0c\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\uff1a\u8054\u90a6\u5b66\u4e60\u4fdd\u62a4\u9690\u79c1\u3001\u516c\u5e73\u53ef\u89e3\u91caAI\u3001\u5f3a\u5316\u5b66\u4e60\u81ea\u9002\u5e94\u9632\u5fa1\u3001\u4eba\u673a\u534f\u540c\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u4ee5\u786e\u4fdd\u4e0b\u4e00\u4ee3AML\u67b6\u6784\u7684\u900f\u660e\u6027\u3001\u53ef\u95ee\u8d23\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.06448", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.06448", "abs": "https://arxiv.org/abs/2512.06448", "authors": ["Takaaki Tateishi", "Yasuharu Katsuno"], "title": "Translating PL/I Macro Procedures into Java Using Automatic Templatization and Large Language Models", "comment": "5 pages, 7 figures, to be published in ICSE 2026 NIER", "summary": "Modernizing legacy enterprise systems often involves translating PL/I programs into modern languages such as Java. This task becomes significantly more complex when PL/I macro procedures are involved. The PL/I macro procedures are considered string-manipulating programs that generate PL/I code, and they make automated translation more complex. Recently, large language models (LLMs) have been explored for automated code translation. However, LLM-based code translation struggles to translate the PL/I macro procedures to Java programs that reproduce the behavior of the plain PL/I code generated by the original PL/I macro procedures.\n  This paper proposes a novel method called templatization, which uses symbolic execution to generate code templates (code with named placeholders) as an intermediate representation. In this approach, symbolic values are treated as parts of macro-generated code. By symbolically executing macro procedures and generating code templates, our approach facilitates LLMs to generate readable and maintainable Java code. Our preliminary experiment on ten PL/I macro procedures shows that the LLM-based translation through templatization successfully generates Java programs that reproduce the behavior of the macro-generated PL/I programs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u6a21\u677f\u5316\"\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b26\u53f7\u6267\u884c\u751f\u6210\u4ee3\u7801\u6a21\u677f\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u5e2e\u52a9LLM\u5c06\u5305\u542b\u5b8f\u8fc7\u7a0b\u7684PL/I\u7a0b\u5e8f\u7ffb\u8bd1\u6210\u53ef\u7ef4\u62a4\u7684Java\u4ee3\u7801\u3002", "motivation": "\u4f01\u4e1a\u7cfb\u7edf\u73b0\u4ee3\u5316\u9700\u8981\u5c06PL/I\u7a0b\u5e8f\u7ffb\u8bd1\u6210Java\u7b49\u73b0\u4ee3\u8bed\u8a00\uff0c\u4f46PL/I\u5b8f\u8fc7\u7a0b\u4f5c\u4e3a\u5b57\u7b26\u4e32\u64cd\u4f5c\u7a0b\u5e8f\u4f1a\u751f\u6210PL/I\u4ee3\u7801\uff0c\u8fd9\u4f7f\u5f97\u81ea\u52a8\u5316\u7ffb\u8bd1\u53d8\u5f97\u590d\u6742\u3002\u73b0\u6709\u7684LLM\u65b9\u6cd5\u96be\u4ee5\u5c06PL/I\u5b8f\u8fc7\u7a0b\u7ffb\u8bd1\u6210\u80fd\u591f\u91cd\u73b0\u539f\u59cb\u5b8f\u751f\u6210PL/I\u4ee3\u7801\u884c\u4e3a\u7684Java\u7a0b\u5e8f\u3002", "method": "\u63d0\u51fa\u6a21\u677f\u5316\u65b9\u6cd5\uff1a\u4f7f\u7528\u7b26\u53f7\u6267\u884c\u751f\u6210\u4ee3\u7801\u6a21\u677f\uff08\u5e26\u6709\u547d\u540d\u5360\u4f4d\u7b26\u7684\u4ee3\u7801\uff09\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u3002\u5c06\u7b26\u53f7\u503c\u89c6\u4e3a\u5b8f\u751f\u6210\u4ee3\u7801\u7684\u4e00\u90e8\u5206\uff0c\u901a\u8fc7\u7b26\u53f7\u6267\u884c\u5b8f\u8fc7\u7a0b\u5e76\u751f\u6210\u4ee3\u7801\u6a21\u677f\uff0c\u5e2e\u52a9LLM\u751f\u6210\u53ef\u8bfb\u4e14\u53ef\u7ef4\u62a4\u7684Java\u4ee3\u7801\u3002", "result": "\u572810\u4e2aPL/I\u5b8f\u8fc7\u7a0b\u4e0a\u7684\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u6a21\u677f\u5316\u7684LLM\u7ffb\u8bd1\u6210\u529f\u751f\u6210\u4e86\u80fd\u591f\u91cd\u73b0\u5b8f\u751f\u6210PL/I\u7a0b\u5e8f\u884c\u4e3a\u7684Java\u7a0b\u5e8f\u3002", "conclusion": "\u6a21\u677f\u5316\u65b9\u6cd5\u901a\u8fc7\u7b26\u53f7\u6267\u884c\u751f\u6210\u4ee3\u7801\u6a21\u677f\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u7ffb\u8bd1PL/I\u5b8f\u8fc7\u7a0b\u65f6\u7684\u56f0\u96be\uff0c\u80fd\u591f\u751f\u6210\u884c\u4e3a\u6b63\u786e\u4e14\u53ef\u7ef4\u62a4\u7684Java\u4ee3\u7801\u3002"}}
{"id": "2512.06296", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06296", "abs": "https://arxiv.org/abs/2512.06296", "authors": ["Sooho Moon", "Yunyong Ko"], "title": "How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion", "comment": "5 pages, 4 figures, 2 tables, ACM WSDM 2026", "summary": "Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86PROBE\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\u7684\u66f4\u5168\u9762\u8bc4\u4f30\uff0c\u5173\u6ce8\u9884\u6d4b\u9510\u5ea6\u548c\u6d41\u884c\u5ea6\u504f\u5dee\u9c81\u68d2\u6027\u4e24\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u4e0d\u8db3\uff0c\u5ffd\u89c6\u4e86\u9884\u6d4b\u9510\u5ea6\uff08\u8bc4\u4f30\u5355\u4e2a\u9884\u6d4b\u7684\u4e25\u683c\u7a0b\u5ea6\uff09\u548c\u6d41\u884c\u5ea6\u504f\u5dee\u9c81\u68d2\u6027\uff08\u9884\u6d4b\u4f4e\u6d41\u884c\u5ea6\u5b9e\u4f53\u7684\u80fd\u529b\uff09\u4e24\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002", "method": "\u63d0\u51faPROBE\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) \u79e9\u53d8\u6362\u5668\uff08RT\uff09- \u57fa\u4e8e\u6240\u9700\u9884\u6d4b\u9510\u5ea6\u6c34\u5e73\u4f30\u8ba1\u6bcf\u4e2a\u9884\u6d4b\u7684\u5206\u6570\uff1b2) \u79e9\u805a\u5408\u5668\uff08RA\uff09- \u4ee5\u6d41\u884c\u5ea6\u611f\u77e5\u7684\u65b9\u5f0f\u805a\u5408\u6240\u6709\u5206\u6570\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u6307\u6807\u503e\u5411\u4e8e\u9ad8\u4f30\u6216\u4f4e\u4f30KGC\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u800cPROBE\u80fd\u591f\u63d0\u4f9b\u5bf9KGC\u6a21\u578b\u7684\u5168\u9762\u7406\u89e3\u548c\u53ef\u9760\u7684\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "PROBE\u6846\u67b6\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u6a21\u578b\uff0c\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5728\u9884\u6d4b\u9510\u5ea6\u548c\u6d41\u884c\u5ea6\u504f\u5dee\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u7ed3\u679c\u3002"}}
{"id": "2512.06387", "categories": ["cs.CR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06387", "abs": "https://arxiv.org/abs/2512.06387", "authors": ["Yuhang Huang", "Junchao Li", "Boyang Ma", "Xuelong Dai", "Minghui Xu", "Kaidi Xu", "Yue Zhang", "Jianping Wang", "Xiuzhen Cheng"], "title": "Beyond Model Jailbreak: Systematic Dissection of the \"Ten DeadlySins\" in Embodied Intelligence", "comment": null, "summary": "Embodied AI systems integrate language models with real world sensing, mobility, and cloud connected mobile apps. Yet while model jailbreaks have drawn significant attention, the broader system stack of embodied intelligence remains largely unexplored. In this work, we conduct the first holistic security analysis of the Unitree Go2 platform and uncover ten cross layer vulnerabilities the \"Ten Sins of Embodied AI Security.\" Using BLE sniffing, traffic interception, APK reverse engineering, cloud API testing, and hardware probing, we identify systemic weaknesses across three architectural layers: wireless provisioning, core modules, and external interfaces. These include hard coded keys, predictable handshake tokens, WiFi credential leakage, missing TLS validation, static SSH password, multilingual safety bypass behavior, insecure local relay channels, weak binding logic, and unrestricted firmware access. Together, they allow adversaries to hijack devices, inject arbitrary commands, extract sensitive information, or gain full physical control.Our findings show that securing embodied AI requires far more than aligning the model itself. We conclude with system level lessons learned and recommendations for building embodied platforms that remain robust across their entire software hardware ecosystem.", "AI": {"tldr": "\u9996\u6b21\u5bf9Unitree Go2\u5e73\u53f0\u8fdb\u884c\u6574\u4f53\u5b89\u5168\u5206\u6790\uff0c\u53d1\u73b010\u4e2a\u8de8\u5c42\u6f0f\u6d1e\uff08\"\u5341\u5b97\u7f6a\"\uff09\uff0c\u6db5\u76d6\u65e0\u7ebf\u914d\u7f6e\u3001\u6838\u5fc3\u6a21\u5757\u548c\u5916\u90e8\u63a5\u53e3\uff0c\u53ef\u5bfc\u81f4\u8bbe\u5907\u52ab\u6301\u3001\u547d\u4ee4\u6ce8\u5165\u3001\u4fe1\u606f\u6cc4\u9732\u548c\u5b8c\u5168\u7269\u7406\u63a7\u5236", "motivation": "\u867d\u7136\u8bed\u8a00\u6a21\u578b\u8d8a\u72f1\u5df2\u53d7\u5173\u6ce8\uff0c\u4f46\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u7684\u6574\u4f53\u5b89\u5168\u6808\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u5bf9Unitree Go2\u5e73\u53f0\u8fdb\u884c\u9996\u6b21\u5168\u9762\u5b89\u5168\u5206\u6790\uff0c\u63ed\u793a\u5177\u8eabAI\u7cfb\u7edf\u7684\u7cfb\u7edf\u6027\u5b89\u5168\u98ce\u9669", "method": "\u91c7\u7528BLE\u55c5\u63a2\u3001\u6d41\u91cf\u62e6\u622a\u3001APK\u9006\u5411\u5de5\u7a0b\u3001\u4e91API\u6d4b\u8bd5\u548c\u786c\u4ef6\u63a2\u6d4b\u7b49\u6280\u672f\uff0c\u5bf9Unitree Go2\u5e73\u53f0\u8fdb\u884c\u8de8\u5c42\u5b89\u5168\u5206\u6790\uff0c\u8bc6\u522b\u4e09\u4e2a\u67b6\u6784\u5c42\uff08\u65e0\u7ebf\u914d\u7f6e\u3001\u6838\u5fc3\u6a21\u5757\u3001\u5916\u90e8\u63a5\u53e3\uff09\u7684\u6f0f\u6d1e", "result": "\u53d1\u73b010\u4e2a\u8de8\u5c42\u6f0f\u6d1e\uff0c\u5305\u62ec\u786c\u7f16\u7801\u5bc6\u94a5\u3001\u53ef\u9884\u6d4b\u63e1\u624b\u4ee4\u724c\u3001WiFi\u51ed\u636e\u6cc4\u9732\u3001\u7f3a\u5931TLS\u9a8c\u8bc1\u3001\u9759\u6001SSH\u5bc6\u7801\u3001\u591a\u8bed\u8a00\u5b89\u5168\u7ed5\u8fc7\u884c\u4e3a\u3001\u4e0d\u5b89\u5168\u7684\u672c\u5730\u4e2d\u7ee7\u901a\u9053\u3001\u5f31\u7ed1\u5b9a\u903b\u8f91\u548c\u65e0\u9650\u5236\u56fa\u4ef6\u8bbf\u95ee\u6743\u9650", "conclusion": "\u4fdd\u62a4\u5177\u8eabAI\u7cfb\u7edf\u9700\u8981\u8fdc\u8d85\u6a21\u578b\u5bf9\u9f50\u7684\u5168\u9762\u5b89\u5168\u63aa\u65bd\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u7cfb\u7edf\u7ea7\u7ecf\u9a8c\u6559\u8bad\u548c\u5efa\u8bae\uff0c\u5f3a\u8c03\u9700\u8981\u6784\u5efa\u5728\u6574\u4e2a\u8f6f\u786c\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\u7684\u5177\u8eab\u5e73\u53f0"}}
{"id": "2512.06337", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06337", "abs": "https://arxiv.org/abs/2512.06337", "authors": ["Xuan Xie", "Xuan Wang", "Wenjie Wang"], "title": "DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization", "comment": null, "summary": "The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.", "AI": {"tldr": "DaGRPO\u901a\u8fc7\u5e8f\u5217\u7ea7\u68af\u5ea6\u6821\u6b63\u548c\u79bb\u7b56\u7565\u6570\u636e\u589e\u5f3a\u89e3\u51b3GRPO\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548cOOD\u6cdb\u5316\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "GRPO\u867d\u7136\u80fd\u6709\u6548\u6fc0\u53d1LLM\u7684\u957f\u65f6\u7a0b\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\u6839\u672c\u539f\u56e0\u5728\u4e8eon-policy rollout\u4e2d\u6837\u672c\u7f3a\u4e4f\u533a\u5206\u5ea6\uff1a\u5bf9\u4e8e\u5e38\u89c4\u67e5\u8be2\uff0c\u9ad8\u5ea6\u540c\u8d28\u7684\u6837\u672c\u5bfc\u81f4\u7834\u574f\u6027\u68af\u5ea6\u51b2\u7a81\uff1b\u5bf9\u4e8e\u56f0\u96be\u67e5\u8be2\uff0c\u6709\u6548\u6b63\u6837\u672c\u7a00\u7f3a\u5bfc\u81f4\u4f18\u5316\u65e0\u6548\u3002", "method": "\u63d0\u51faDaGRPO\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1) \u5e8f\u5217\u7ea7\u68af\u5ea6\u6821\u6b63\uff1a\u5229\u7528\u7ec6\u7c92\u5ea6\u8bc4\u5206\u52a8\u6001\u63a9\u7801\u4f4e\u533a\u5206\u5ea6\u7684\u6837\u672c\u5bf9\uff0c\u4ece\u6e90\u5934\u6d88\u9664\u68af\u5ea6\u51b2\u7a81\uff1b2) \u79bb\u7b56\u7565\u6570\u636e\u589e\u5f3a\uff1a\u5f15\u5165\u9ad8\u8d28\u91cf\u951a\u70b9\u6837\u672c\uff0c\u4e3a\u56f0\u96be\u4efb\u52a1\u6062\u590d\u8bad\u7ec3\u4fe1\u53f7\u3002", "result": "\u57289\u4e2a\u6570\u5b66\u63a8\u7406\u548cOOD\u6cdb\u5316\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDaGRPO\u663e\u8457\u8d85\u8d8a\u73b0\u6709SFT\u3001GRPO\u548c\u6df7\u5408\u57fa\u7ebf\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\uff08\u5982\u5728\u6570\u5b66\u57fa\u51c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347+4.7%\uff09\u3002\u6df1\u5165\u5206\u6790\u8bc1\u5b9eDaGRPO\u6709\u6548\u7f13\u89e3\u68af\u5ea6\u7206\u70b8\u5e76\u52a0\u901f\u957f\u94fe\u63a8\u7406\u80fd\u529b\u7684\u51fa\u73b0\u3002", "conclusion": "DaGRPO\u901a\u8fc7\u89e3\u51b3GRPO\u7684\u533a\u5206\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\uff0c\u4e3aLLM\u7684\u957f\u65f6\u7a0b\u63a8\u7406\u80fd\u529b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u540e\u8bad\u7ec3\u673a\u5236\u3002"}}
{"id": "2512.06836", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.06836", "abs": "https://arxiv.org/abs/2512.06836", "authors": ["Weixing Zhang", "Regina Hebig", "Daniel Str\u00fcber"], "title": "Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs", "comment": null, "summary": "Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.", "AI": {"tldr": "LLM\u53ef\u7528\u4e8e\u6587\u672cDSL\u8bed\u6cd5\u548c\u5b9e\u4f8b\u7684\u534f\u540c\u6f14\u5316\uff0c\u5728\u5c0f\u89c4\u6a21\u6848\u4f8b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9762\u4e34\u5927\u89c4\u6a21\u5b9e\u4f8b\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "motivation": "\u6587\u672cDSL\u8bed\u6cd5\u6f14\u5316\u65f6\uff0c\u73b0\u6709\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\u4e2d\u7684\u534f\u540c\u6f14\u5316\u6280\u672f\u4f1a\u4e22\u5931\u6ce8\u91ca\u548c\u5e03\u5c40\u7b49\u8f85\u52a9\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u5bf9\u8f6f\u4ef6\u7406\u89e3\u548c\u7ef4\u62a4\u5f88\u91cd\u8981\u3002\u9700\u8981\u63a2\u7d22\u80fd\u4fdd\u7559\u8fd9\u4e9b\u4fe1\u606f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Claude-3.5\u548cGPT-4o\u4e24\u79cd\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4e03\u4e2a\u6848\u4f8b\u8bed\u8a00\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30LLM\u5728\u76f4\u63a5\u5904\u7406\u6587\u672c\u5b9e\u4f8b\u65f6\u5b9e\u73b0\u8bed\u6cd5\u548c\u5b9e\u4f8b\u534f\u540c\u6f14\u5316\u7684\u53ef\u884c\u6027\u3002", "result": "LLM\u5728\u5c0f\u89c4\u6a21\u5b9e\u4f8b\uff08\u4ee3\u8868\u5b9e\u8df5\u4e2d\u9047\u5230\u7684\u90e8\u5206\u6848\u4f8b\uff09\u4e2d\u8fc1\u79fb\u6587\u672c\u5b9e\u4f8b\u7684\u80fd\u529b\u826f\u597d\uff0c\u4f46\u5728\u6269\u5c55\u5230\u66f4\u5927\u5b9e\u4f8b\u65f6\u9762\u4e34\u663e\u8457\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "conclusion": "LLM\u5728\u6587\u672cDSL\u534f\u540c\u6f14\u5316\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u4fdd\u7559\u8f85\u52a9\u4fe1\u606f\u65b9\u9762\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u672a\u6765\u7814\u7a76\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2512.06393", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.06393", "abs": "https://arxiv.org/abs/2512.06393", "authors": ["Qiming Bao", "Xiaoxuan Fu"], "title": "Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression", "comment": null, "summary": "Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.\n  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u56db\u79cd\u538b\u529b\u6d4b\u8bd5\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff1a\u89c4\u5219\u5220\u9664\u3001\u77db\u76fe\u8bc1\u636e\u6ce8\u5165\u3001\u903b\u8f91\u7b49\u4ef7\u91cd\u5199\u548c\u591a\u5b9a\u5f8b\u7b49\u4ef7\u53e0\u52a0\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u8bed\u4e49\u4fdd\u6301\u7684\u903b\u8f91\u53d8\u6362\u5177\u6709\u7a33\u5b9a\u4e0d\u53d8\u6027\uff0c\u4f46\u5bf9\u7f3a\u5931\u6216\u51b2\u7a81\u8bc1\u636e\u7684\u5904\u7406\u5b58\u5728\u6839\u672c\u6027\u8106\u5f31\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u903b\u8f91\u8bed\u5883\u4e2d\u5bf9\u7ed3\u6784\u6027\u6270\u52a8\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u8005\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u53d7\u63a7\u8bc4\u4f30\u6846\u67b6\uff0c\u63a2\u7a76\u6a21\u578b\u63a8\u7406\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5bf9\u903b\u8f91\u7ed3\u6784\u53d8\u5316\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5305\u542b\u56db\u79cd\u9488\u5bf9\u6027\u538b\u529b\u6d4b\u8bd5\u7684\u8bc4\u4f30\u6846\u67b6\uff1a1) \u89c4\u5219\u5220\u9664\uff08\u5220\u9664\u5197\u4f59\u6216\u5fc5\u8981\u89c4\u5219\uff09\uff1b2) \u77db\u76fe\u8bc1\u636e\u6ce8\u5165\uff1b3) \u903b\u8f91\u7b49\u4ef7\u91cd\u5199\uff08\u4f7f\u7528\u5bf9\u5076\u3001\u53cc\u91cd\u5426\u5b9a\u3001\u8574\u542b\u3001\u5fb7\u6469\u6839\u3001\u6052\u7b49\u548c\u4ea4\u6362\u5f8b\u7b49\u53d8\u6362\uff09\uff1b4) \u591a\u5b9a\u5f8b\u7b49\u4ef7\u53e0\u52a0\uff08\u540c\u65f6\u5e94\u75282-5\u4e2a\u903b\u8f91\u53d8\u6362\uff09\u3002\u5728BERT\u3001Qwen2\u548cLLaMA\u7c7b\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u57fa\u7840\u4efb\u52a1\u4e0a\u8fbe\u5230\u5b8c\u7f8e\u51c6\u786e\u7387\uff0c\u5bf9\u5197\u4f59\u89c4\u5219\u5220\u9664\u548c\u6240\u6709\u7b49\u4ef7\u91cd\u5199\uff08\u5355\u5b9a\u5f8b\u6216\u591a\u5b9a\u5f8b\uff09\u5b8c\u5168\u6cdb\u5316\uff0c\u4f46\u5728\u5fc5\u8981\u89c4\u5219\u5220\u9664\u4e0b\u51c6\u786e\u7387\u9aa4\u964d\u81f325%\uff0c\u5728\u660e\u786e\u77db\u76fe\u5b58\u5728\u65f6\u5b8c\u5168\u5d29\u6e83\uff080%\u51c6\u786e\u7387\uff09\u3002\u6a21\u578b\u8868\u73b0\u51fa\u5bf9\u8bed\u4e49\u4fdd\u6301\u903b\u8f91\u53d8\u6362\u7684\u7a33\u5b9a\u4e0d\u53d8\u6027\uff0c\u4f46\u5bf9\u7f3a\u5931\u6216\u51b2\u7a81\u8bc1\u636e\u7684\u5904\u7406\u5b58\u5728\u6839\u672c\u6027\u8106\u5f31\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6e05\u6670\u7684\u8bca\u65ad\u5de5\u5177\u6765\u9694\u79bb\u63a8\u7406\u5931\u8d25\u6a21\u5f0f\uff0c\u7a81\u663e\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u6301\u4e45\u5dee\u8ddd\u3002\u6a21\u578b\u867d\u7136\u5bf9\u903b\u8f91\u7b49\u4ef7\u53d8\u6362\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u5904\u7406\u4e0d\u5b8c\u6574\u6216\u77db\u76fe\u4fe1\u606f\u65f6\u8868\u73b0\u51fa\u6839\u672c\u6027\u8106\u5f31\u3002"}}
{"id": "2512.06902", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06902", "abs": "https://arxiv.org/abs/2512.06902", "authors": ["Fazle Rabbi", "Soumit Kanti Saha", "Tri Minh Triet Pham", "Song Wang", "Jinqiu Yang"], "title": "BabelCoder: Agentic Code Translation with Specification Alignment", "comment": "21 pages, 8 figures, 4 tables", "summary": "As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.", "AI": {"tldr": "BabelCoder\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u4ee3\u7801\u7ffb\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u7ffb\u8bd1\u3001\u6d4b\u8bd5\u3001\u4fee\u590d\u4e09\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\u7684\u5206\u5de5\u5408\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u4ee3\u7801\u8fc1\u79fb\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u6f14\u8fdb\uff0c\u5f00\u53d1\u8005\u9700\u8981\u5728\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u95f4\u5de5\u4f5c\uff0c\u7ecf\u5e38\u9762\u4e34\u4ee3\u7801\u8fc1\u79fb\u9700\u6c42\u3002\u867d\u7136\u81ea\u52a8\u4ee3\u7801\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u51c6\u786e\u7387\u6709\u9650\uff0c\u4e14\u672a\u80fd\u6709\u6548\u5229\u7528\u4ee3\u7801\u7684\u4e0a\u4e0b\u6587\u548c\u7ed3\u6784\u4fe1\u606f\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7ed3\u6784\u5316\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u6765\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "BabelCoder\u91c7\u7528\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff1a\u7ffb\u8bd1\u667a\u80fd\u4f53\u8d1f\u8d23\u751f\u6210\u4ee3\u7801\uff0c\u6d4b\u8bd5\u667a\u80fd\u4f53\u8d1f\u8d23\u9a8c\u8bc1\u6b63\u786e\u6027\uff0c\u4fee\u590d\u667a\u80fd\u4f53\u8d1f\u8d23\u4fee\u6b63\u9519\u8bef\u3002\u8fd9\u79cd\u5206\u5de5\u534f\u4f5c\u673a\u5236\u4f7f\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e13\u6ce8\u4e8e\u7279\u5b9a\u65b9\u9762\uff0c\u5171\u540c\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4e0e\u56db\u79cd\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u6bd4\u8f83\uff0cBabelCoder\u572894%\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u5e45\u5ea6\u4e3a0.5%-13.5%\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523094.16%\u3002", "conclusion": "BabelCoder\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u51c6\u786e\u6027\u95ee\u9898\uff0c\u4e3a\u8de8\u8bed\u8a00\u4ee3\u7801\u8fc1\u79fb\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06906", "categories": ["cs.SE", "cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06906", "abs": "https://arxiv.org/abs/2512.06906", "authors": ["Wenjie Zhang", "Yun Lin", "Chun Fung Amos Kwok", "Xiwen Teoh", "Xiaofei Xie", "Frank Liauw", "Hongyu Zhang", "Jin Song Dong"], "title": "MINES: Explainable Anomaly Detection through Web API Invariant Inference", "comment": null, "summary": "Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.", "AI": {"tldr": "MINES\uff1a\u4e00\u79cd\u901a\u8fc7\u63a8\u65adAPI\u6570\u636e\u5e93\u7ea6\u675f\u6765\u68c0\u6d4bWeb\u5e94\u7528\u5f02\u5e38\u7684\u65b9\u6848\uff0c\u4ece\u6a21\u5f0f\u5c42\u9762\u800c\u975e\u539f\u59cb\u65e5\u5fd7\u5b9e\u4f8b\u751f\u6210\u53ef\u89e3\u91ca\u7684API\u4e0d\u53d8\u91cf\uff0c\u80fd\u6709\u6548\u533a\u5206\u65e5\u5fd7\u566a\u58f0\u5e76\u68c0\u6d4b\u8d85\u51fa\u65e5\u5fd7\u8bb0\u5f55\u8303\u56f4\u7684\u5f02\u5e38\u884c\u4e3a\u3002", "motivation": "\u73b0\u4ee3Web\u5e94\u7528\u57fa\u4e8eAPI\u8fd0\u884c\uff0c\u5176\u66b4\u9732\u6613\u53d7\u653b\u51fb\u6216\u975e\u6cd5\u8bbf\u95ee\uff0c\u5bfc\u81f4\u7cfb\u7edf\u5f02\u5e38\u3002\u73b0\u6709\u65e5\u5fd7\u5b66\u4e60\u65b9\u6848\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5f02\u5e38\u65e5\u5fd7\u4e0e\u6b63\u5e38\u65e5\u5fd7\u76f8\u4f3c\u5ea6\u9ad8\uff0c\u7f3a\u4e4f\u5173\u952e\u533a\u5206\u4fe1\u606f\uff1b\u65e5\u5fd7\u5b9e\u4f8b\u5b58\u5728\u566a\u58f0\uff0c\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u865a\u5047\u76f8\u5173\u6027\uff0c\u4ea7\u751f\u8868\u9762\u5316\u7684\u5f02\u5e38\u68c0\u6d4b\u89c4\u5219\u3002", "method": "MINES\u5c06API\u7b7e\u540d\u8f6c\u6362\u4e3a\u8868\u6a21\u5f0f\u4ee5\u589e\u5f3a\u539f\u59cb\u6570\u636e\u5e93\u6a21\u5f0f\uff0c\u63a8\u65ad\u589e\u5f3a\u6570\u636e\u5e93\u6a21\u5f0f\u4e0a\u7684\u6f5c\u5728\u6570\u636e\u5e93\u7ea6\u675f\uff0c\u6355\u6349API\u4e0e\u6570\u636e\u5e93\u8868\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u4f7f\u7528LLM\u57fa\u4e8e\u4e24\u4e2a\u8868\u7ed3\u6784\u63d0\u53d6\u6f5c\u5728\u5173\u7cfb\uff0c\u5229\u7528\u6b63\u5e38\u65e5\u5fd7\u5b9e\u4f8b\u6765\u62d2\u7edd\u6216\u63a5\u53d7LLM\u751f\u6210\u7684\u4e0d\u53d8\u91cf\uff0c\u6700\u540e\u5c06\u63a8\u65ad\u7684\u7ea6\u675f\u8f6c\u6362\u4e3a\u4e0d\u53d8\u91cf\u5e76\u751f\u6210Python\u4ee3\u7801\u9a8c\u8bc1\u8fd0\u884c\u65f6\u65e5\u5fd7\u3002", "result": "\u5728TrainTicket\u3001NiceFish\u3001Gitea\u3001Mastodon\u548cNextCloud\u7b49\u57fa\u51c6\u4e0a\u9488\u5bf9Web\u7be1\u6539\u653b\u51fb\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u4e0eLogRobust\u3001LogFormer\u3001WebNorm\u7b49\u57fa\u7ebf\u5bf9\u6bd4\u3002\u7ed3\u679c\u663e\u793aMINES\u5728\u5b9e\u73b0\u9ad8\u5f02\u5e38\u53ec\u56de\u7387\u7684\u540c\u65f6\u51e0\u4e4e\u5f15\u5165\u96f6\u8bef\u62a5\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "MINES\u901a\u8fc7\u4ece\u6a21\u5f0f\u5c42\u9762\u63a8\u65ad\u53ef\u89e3\u91ca\u7684API\u4e0d\u53d8\u91cf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65e5\u5fd7\u5b66\u4e60\u65b9\u6848\u4e2d\u566a\u58f0\u5e72\u6270\u548c\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\uff0c\u4e3aWeb\u5e94\u7528\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u3001\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06411", "categories": ["cs.CR", "math.RA"], "pdf": "https://arxiv.org/pdf/2512.06411", "abs": "https://arxiv.org/abs/2512.06411", "authors": ["Victor Duarte Melo", "Willian J. Buchanan"], "title": "KyFrog: A High-Security LWE-Based KEM Inspired by ML-KEM", "comment": null, "summary": "KyFrog is a conservative Learning-with-Errors (LWE) key-encapsulation mechanism designed to explore an alternative operating point compared to schemes with relatively small public keys and ciphertexts. KyFrog uses a larger dimension ($n = 1024$) and a small prime modulus $q = 1103$, together with narrow error distributions with standard deviations $\u03c3_s = \u03c3_e = 1.4$, to target approximately $2^{325}$ classical and quantum security against state-of-the-art lattice attacks under standard cost models, as estimated using the Lattice Estimator. The price paid for this security margin is an extremely large KEM ciphertext (about 0.5 MiB), while public and secret keys remain in the same ballpark as ML-KEM. We describe the design rationale, parameter search methodology, and implementation details of KyFrog, and we compare its asymptotic security and concrete parameter sizes with the ML-KEM standard. All code and data for this work are released as free and open-source software, with the full C++23 implementation and experimental scripts available at: https://github.com/victormeloasm/kyfrog", "AI": {"tldr": "KyFrog\u662f\u4e00\u4e2a\u4fdd\u5b88\u7684LWE\u5bc6\u94a5\u5c01\u88c5\u673a\u5236\uff0c\u901a\u8fc7\u4f7f\u7528\u5927\u7ef4\u5ea6\uff08n=1024\uff09\u548c\u5c0f\u6a21\u6570\uff08q=1103\uff09\u4ee5\u53ca\u7a84\u8bef\u5dee\u5206\u5e03\uff0c\u5728\u6807\u51c6\u6210\u672c\u6a21\u578b\u4e0b\u8fbe\u5230\u7ea62^325\u7684\u7ecf\u5178\u548c\u91cf\u5b50\u5b89\u5168\u7ea7\u522b\uff0c\u4ee3\u4ef7\u662f\u4ea7\u751f\u7ea60.5 MiB\u7684\u5927\u5bc6\u6587\u3002", "motivation": "\u63a2\u7d22\u4e0e\u73b0\u6709\u5177\u6709\u76f8\u5bf9\u8f83\u5c0f\u516c\u94a5\u548c\u5bc6\u6587\u7684\u65b9\u6848\u4e0d\u540c\u7684\u64cd\u4f5c\u70b9\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u5b89\u5168\u88d5\u5ea6\u3002", "method": "\u4f7f\u7528\u5927\u7ef4\u5ea6\uff08n=1024\uff09\u3001\u5c0f\u7d20\u6570\u6a21\u6570\uff08q=1103\uff09\u548c\u7a84\u8bef\u5dee\u5206\u5e03\uff08\u6807\u51c6\u5dee\u03c3_s=\u03c3_e=1.4\uff09\uff0c\u57fa\u4e8eLattice Estimator\u8fdb\u884c\u53c2\u6570\u641c\u7d22\u548c\u5b89\u5168\u6027\u8bc4\u4f30\u3002", "result": "\u5728\u6807\u51c6\u6210\u672c\u6a21\u578b\u4e0b\u8fbe\u5230\u7ea62^325\u7684\u7ecf\u5178\u548c\u91cf\u5b50\u5b89\u5168\u7ea7\u522b\uff0c\u5bc6\u6587\u5927\u5c0f\u7ea6\u4e3a0.5 MiB\uff0c\u516c\u94a5\u548c\u79c1\u94a5\u5927\u5c0f\u4e0eML-KEM\u76f8\u5f53\u3002", "conclusion": "KyFrog\u901a\u8fc7\u727a\u7272\u5bc6\u6587\u5927\u5c0f\u83b7\u5f97\u4e86\u6781\u9ad8\u7684\u5b89\u5168\u88d5\u5ea6\uff0c\u4e3aLWE\u5bc6\u94a5\u5c01\u88c5\u673a\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4fdd\u5b88\u4f46\u9ad8\u5b89\u5168\u6027\u7684\u8bbe\u8ba1\u9009\u62e9\u3002"}}
{"id": "2512.06406", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06406", "abs": "https://arxiv.org/abs/2512.06406", "authors": ["Xianzong Wu", "Xiaohong Li", "Lili Quan", "Qiang Hu"], "title": "UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems", "comment": null, "summary": "Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.", "AI": {"tldr": "UncertaintyZoo\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5de5\u5177\u5305\uff0c\u96c6\u6210\u4e8629\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u5728\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u53ef\u80fd\u505a\u51fa\u9519\u8bef\u9884\u6d4b\uff0c\u5bfc\u81f4\u6f5c\u5728\u635f\u5931\u3002\u867d\u7136\u5df2\u6709\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u96c6\u6210\u5de5\u5177\uff0c\u963b\u788d\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u548c\u672a\u6765\u7814\u7a76\u3002", "method": "\u5f00\u53d1UncertaintyZoo\u5de5\u5177\u5305\uff0c\u7edf\u4e00\u96c6\u621029\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e94\u5927\u7c7b\u522b\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u63a5\u53e3\u3002\u5728CodeBERT\u548cChatGLM3\u6a21\u578b\u4e0a\u8fdb\u884c\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u4efb\u52a1\u7684\u8bc4\u4f30\u3002", "result": "UncertaintyZoo\u80fd\u591f\u6709\u6548\u63ed\u793a\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5de5\u5177\u5df2\u5728GitHub\u5f00\u6e90\uff0c\u5e76\u914d\u6709\u6f14\u793a\u89c6\u9891\u3002", "conclusion": "UncertaintyZoo\u586b\u8865\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5de5\u5177\u96c6\u7684\u7a7a\u767d\uff0c\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u53ef\u9760\u6027\u65b9\u9762\u3002"}}
{"id": "2512.06467", "categories": ["cs.CR", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.06467", "abs": "https://arxiv.org/abs/2512.06467", "authors": ["Florian Kamm\u00fcller"], "title": "Formalisation of Security for Federated Learning with DP and Attacker Advantage in IIIf for Satellite Swarms -- Extended Version", "comment": null, "summary": "In distributed applications, like swarms of satellites, machine learning can be efficiently applied even on small devices by using Federated Learning (FL). This allows to reduce the learning complexity by transmitting only updates to the general model in the server in the form of differences in stochastic gradient descent. FL naturally supports differential privacy but new attacks, so called Data Leakage from Gradient (DLG) have been discovered recently. There has been work on defenses against DLG but there is a lack of foundation and rigorous evaluation of their security. In the current work, we extend existing work on a formal notion of Differential Privacy for Federated Learning distributed dynamic systems and relate it to the notion of the attacker advantage. This formalisation is carried out within the Isabelle Insider and Infrastructure framework (IIIf) allowing the machine supported verification of theory and applications within the proof assistant Isabelle. Satellite swarm systems are used as a motivating use case but also as a validation case study.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u8054\u90a6\u5b66\u4e60\u5206\u5e03\u5f0f\u52a8\u6001\u7cfb\u7edf\u7684\u5dee\u5206\u9690\u79c1\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u5e76\u5c06\u5176\u4e0e\u653b\u51fb\u8005\u4f18\u52bf\u6982\u5ff5\u5173\u8054\uff0c\u4f7f\u7528Isabelle\u8bc1\u660e\u52a9\u624b\u8fdb\u884c\u673a\u5668\u9a8c\u8bc1\uff0c\u4ee5\u536b\u661f\u7fa4\u7cfb\u7edf\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u5e94\u7528\uff08\u5982\u536b\u661f\u7fa4\uff09\u4e2d\u80fd\u6709\u6548\u5e94\u7528\uff0c\u4f46\u5b58\u5728\u6570\u636e\u68af\u5ea6\u6cc4\u9732\uff08DLG\uff09\u653b\u51fb\u3002\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u548c\u5b89\u5168\u6027\u7684\u4e25\u683c\u8bc4\u4f30\uff0c\u9700\u8981\u5efa\u7acb\u5f62\u5f0f\u5316\u6846\u67b6\u6765\u786e\u4fdd\u5b89\u5168\u6027\u3002", "method": "\u5728Isabelle Insider\u548cInfrastructure\u6846\u67b6\uff08IIIf\uff09\u5185\u6269\u5c55\u8054\u90a6\u5b66\u4e60\u5206\u5e03\u5f0f\u52a8\u6001\u7cfb\u7edf\u7684\u5dee\u5206\u9690\u79c1\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u5c06\u5dee\u5206\u9690\u79c1\u4e0e\u653b\u51fb\u8005\u4f18\u52bf\u6982\u5ff5\u5173\u8054\uff0c\u4f7f\u7528\u673a\u5668\u652f\u6301\u7684\u8bc1\u660e\u52a9\u624b\u8fdb\u884c\u7406\u8bba\u548c\u5e94\u7528\u9a8c\u8bc1\u3002", "result": "\u5efa\u7acb\u4e86\u8054\u90a6\u5b66\u4e60\u5206\u5e03\u5f0f\u52a8\u6001\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u5dee\u5206\u9690\u79c1\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u653b\u51fb\u8005\u4f18\u52bf\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u5173\u8054\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u536b\u661f\u7fa4\u7cfb\u7edf\u6848\u4f8b\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e3a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u9690\u79c1\u4fdd\u62a4\u7406\u8bba\u57fa\u7840\uff0c\u4f7f\u7528Isabelle\u8bc1\u660e\u52a9\u624b\u786e\u4fdd\u4e86\u9a8c\u8bc1\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u5206\u5e03\u5f0f\u52a8\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u6846\u67b6\u3002"}}
{"id": "2512.06431", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.06431", "abs": "https://arxiv.org/abs/2512.06431", "authors": ["Mohamed Shamroukh", "Mohamed Alkhuzamy Aziz"], "title": "Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City", "comment": null, "summary": "National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.", "AI": {"tldr": "\u672c\u7814\u7a76\u4e3a\u57c3\u53ca\u57fa\u7eb3\u5e02\u5f00\u53d1\u4e86\u57fa\u4e8eVoronoi\u56fe\u7684\u7a7a\u95f4\u5206\u6790\u7b97\u6cd5\uff0c\u521b\u5efa\u4e86\u672c\u5730\u5316\u89c4\u5212\u6807\u51c6\u6a21\u578b\uff0c\u8bc4\u4f30\u516c\u5171\u670d\u52a1\u8986\u76d6\u6548\u7387\uff0c\u5e73\u5747\u8986\u76d6\u7387\u4e3a81.3%\uff0c\u53d1\u73b0\u5e02\u4e2d\u5fc3\u4e0e\u90ca\u533a\u670d\u52a1\u5bc6\u5ea6\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u57c3\u53ca\u56fd\u5bb6\u516c\u5171\u670d\u52a1\u89c4\u5212\u6807\u51c6\u5f80\u5f80\u65e0\u6cd5\u9002\u5e94\u5730\u65b9\u72ec\u7279\u7279\u5f81\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u57fa\u7eb3\u5e02\u5f00\u53d1\u5b9a\u5236\u5316\u7684\u89c4\u5212\u6a21\u578b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff08\u63cf\u8ff0\u6027\u3001\u5206\u6790\u6027\u548c\u5b9e\u9a8c\u6027\uff09\uff0c\u5229\u7528Python\u7f16\u7a0b\u5f00\u53d1\u57fa\u4e8eVoronoi\u56fe\u7684\u667a\u80fd\u7a7a\u95f4\u5206\u6790\u7b97\u6cd5\uff0c\u521b\u5efa\u57ce\u5e02\u7279\u5b9a\u89c4\u5212\u6807\u51c6\u5e76\u8bc4\u4f30\u73b0\u6709\u516c\u5171\u8bbe\u65bd\u8986\u76d6\u60c5\u51b5\u3002", "result": "\u6a21\u578b\u5e94\u7528\u663e\u793a\u5e73\u5747\u670d\u52a1\u8986\u76d6\u7387\u4e3a81.3%\uff0c\u6551\u62a4\u8f66\u7ad9\u6548\u7387\u6700\u9ad8\uff0899.8%\uff09\uff0c\u516c\u56ed\u548c\u5f00\u653e\u7a7a\u95f4\u8986\u76d6\u7387\u6700\u4f4e\uff0810%\uff09\u3002\u7a7a\u95f4\u5206\u6790\u663e\u793a\u5e02\u4e2d\u5fc3\u670d\u52a1\u5bc6\u5ea6\u9ad8\uff08>45\u4e2a/\u5e73\u65b9\u516c\u91cc\uff09\uff0c\u90ca\u533a\u663e\u8457\u964d\u4f4e\uff08<5\u4e2a/\u5e73\u65b9\u516c\u91cc\uff09\uff0cHajer\u57fa\u7eb3\u533a\u672a\u670d\u52a1\u533a\u57df\u6700\u591a\uff0c\u7b2c\u4e00\u533a\u670d\u52a1\u8986\u76d6\u7387\u6700\u9ad8\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u672c\u5730\u5316\u89c4\u5212\u6807\u51c6\u6a21\u578b\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u7b97\u6cd5\uff0c\u4e3a\u57c3\u53ca\u57ce\u5e02\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u6570\u636e\u9a71\u52a8\u57ce\u5e02\u89c4\u5212\u6846\u67b6\u3002"}}
{"id": "2512.07022", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.07022", "abs": "https://arxiv.org/abs/2512.07022", "authors": ["Genevieve Caumartin", "Glaucia Melo"], "title": "Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization", "comment": "Accepted at BoatSE 2026", "summary": "Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u5229\u7528LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67e5\u8be2\u91cd\u6784\u548c\u6458\u8981\u6765\u6539\u8fdb\u6587\u4ef6\u7ea7\u7f3a\u9677\u5b9a\u4f4d\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u68c0\u7d22\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u7684\u7f3a\u9677\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u672a\u5904\u7406\u7684\u7f3a\u9677\u63cf\u8ff0\uff0c\u5176\u4e2d\u5e38\u5305\u542b\u566a\u58f0\u4fe1\u606f\uff0c\u5bfc\u81f4\u68c0\u7d22\u51c6\u786e\u7387\u4f4e\u4e0b\u3002\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u67e5\u8be2\u91cd\u6784\u65b9\u9762\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5176\u5bf9\u667a\u80fd\u4f53\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u5f00\u6e90\u3001\u672a\u7ecf\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u7f3a\u9677\u62a5\u544a\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff08\u5982\u6807\u8bc6\u7b26\u548c\u4ee3\u7801\u7247\u6bb5\uff09\uff0c\u5e76\u8fdb\u884c\u68c0\u7d22\u524d\u7684\u67e5\u8be2\u91cd\u6784\u3002\u667a\u80fd\u4f53\u968f\u540e\u4f7f\u7528\u8fd9\u4e9b\u9884\u5904\u7406\u540e\u7684\u67e5\u8be2\u6765\u7f16\u6392BM25\u68c0\u7d22\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u7684\u5b9a\u4f4d\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u91c7\u7528\u6700\u4f73\u67e5\u8be2\u91cd\u6784\u6280\u672f\u540e\uff0c\u667a\u80fd\u4f53\u5728\u9996\u6b21\u6587\u4ef6\u68c0\u7d22\u4e2d\u7684\u6392\u540d\u6bd4BM25\u57fa\u7ebf\u63d0\u9ad8\u4e8635%\uff0c\u76f8\u6bd4SWE-agent\u7684\u6587\u4ef6\u68c0\u7d22\u6027\u80fd\u63d0\u5347\u4e86\u9ad8\u8fbe22%\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67e5\u8be2\u91cd\u6784\u548c\u6458\u8981\u80fd\u591f\u663e\u8457\u63d0\u5347\u6587\u4ef6\u7ea7\u7f3a\u9677\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u8f6f\u4ef6\u4ed3\u5e93\u4e2d\u7684\u7f3a\u9677\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06573", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.06573", "abs": "https://arxiv.org/abs/2512.06573", "authors": ["Onur Bilgin", "Abdullah As Sami", "Sriram Sai Vujjini", "John Licato"], "title": "The Effect of Belief Boxes and Open-mindedness on Persuasion", "comment": "Accepted at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026), Marbella, Spain", "summary": "As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u5728LLM\u667a\u80fd\u4f53\u4e2d\u5f15\u5165\"\u4fe1\u5ff5\u76d2\"\uff08\u5305\u542b\u4fe1\u5ff5\u9648\u8ff0\u7684\u63d0\u793a\u7a7a\u95f4\uff09\u5982\u4f55\u5f71\u54cd\u5176\u884c\u4e3a\u3001\u4fe1\u5ff5\u6539\u53d8\u503e\u5411\u4ee5\u53ca\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u8bf4\u670d\u529b\uff0c\u7279\u522b\u662f\u5f00\u653e\u5fc3\u6001\u6307\u4ee4\u548c\u540c\u4f34\u538b\u529b\u60c5\u5883\u4e0b\u7684\u6548\u679c\u3002", "motivation": "\u968f\u7740\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u63a8\u7406\u548c\u51b3\u7b56\u5e94\u7528\u4e2d\u7684\u589e\u52a0\uff0c\u9700\u8981\u8ba9\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5177\u5907\u7c7b\u4f3c\u547d\u9898\u4fe1\u5ff5\u7684\u80fd\u529b\u3002\u7814\u7a76\u8005\u60f3\u77e5\u9053\u5728\u63d0\u793a\u7a7a\u95f4\u4e2d\u5305\u542b\u4fe1\u5ff5\u9648\u8ff0\uff08\u4fe1\u5ff5\u76d2\uff09\u5982\u4f55\u5b9e\u9645\u5f71\u54cd\u667a\u80fd\u4f53\u884c\u4e3a\u3001\u4fe1\u5ff5\u503e\u5411\u4ee5\u53ca\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u8bf4\u670d\u529b\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u63a2\u7d22\u4fe1\u5ff5\u76d2\u6280\u672f\uff0c\u7814\u7a76\u667a\u80fd\u4f53\u5728\u63d0\u793a\u7a7a\u95f4\u4e2d\u5305\u542b\u4fe1\u5ff5\u9648\u8ff0\u53ca\u5176\u5f3a\u5ea6\u5982\u4f55\u5f71\u54cd\u884c\u4e3a\u3002\u7279\u522b\u5173\u6ce8\u5f00\u653e\u5fc3\u6001\u6307\u4ee4\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5728\u8fa9\u8bba\u4e2d\u88ab\u5bf9\u7acb\u89c2\u70b9\u5305\u56f4\uff08\u540c\u4f34\u538b\u529b\u573a\u666f\uff09\u65f6\u4fe1\u5ff5\u6539\u53d8\u7684\u53ef\u80fd\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u6307\u793a\u667a\u80fd\u4f53\u4fdd\u6301\u5f00\u653e\u5fc3\u6001\u4f1a\u5f71\u54cd\u5176\u4fe1\u5ff5\u6539\u53d8\u7684\u6613\u611f\u6027\uff1b2\uff09\u7eb3\u5165\u4fe1\u5ff5\u9648\u8ff0\u53ca\u5176\u5f3a\u5ea6\u4f1a\u5f71\u54cd\u667a\u80fd\u4f53\u5bf9\u5bf9\u7acb\u89c2\u70b9\u7684\u62b5\u6297\u529b\u548c\u8bf4\u670d\u529b\uff1b3\uff09\u5728\u8fa9\u8bba\u4e2d\u88ab\u5bf9\u7acb\u89c2\u70b9\u5305\u56f4\uff08\u540c\u4f34\u538b\u529b\u573a\u666f\uff09\u65f6\uff0c\u4fe1\u5ff5\u6539\u53d8\u7684\u53ef\u80fd\u6027\u7279\u522b\u53d7\u5f71\u54cd\u3002", "conclusion": "\u7ed3\u679c\u8bc1\u660e\u4e86\u4fe1\u5ff5\u76d2\u6280\u672f\u5728\u63a8\u7406\u548c\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u6784\u5efa\u7c7b\u4f3c\u547d\u9898\u4fe1\u5ff5\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2512.07122", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07122", "abs": "https://arxiv.org/abs/2512.07122", "authors": ["Liping Han", "Tingting Nie", "Le Yu", "Mingzhe Hu", "Tao Yue"], "title": "RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations", "comment": null, "summary": "Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u4eba\u673a\u98ce\u9669\u914d\u7f6e\u5b9e\u65f6\u4fee\u590d\u65b9\u6cd5RisConFix\uff0c\u901a\u8fc7\u76d1\u63a7\u98de\u884c\u72b6\u6001\u5e76\u5229\u7528LLM\u5206\u6790\u53c2\u6570\u5173\u7cfb\u751f\u6210\u4fee\u590d\u65b9\u6848\uff0c\u5728ArduPilot\u6848\u4f8b\u4e2d\u8fbe\u523097%\u4fee\u590d\u6210\u529f\u7387\u3002", "motivation": "\u65e0\u4eba\u673a\u98de\u884c\u63a7\u5236\u8f6f\u4ef6\u5305\u542b\u5927\u91cf\u53ef\u914d\u7f6e\u53c2\u6570\uff0c\u867d\u7136\u5382\u5546\u63d0\u4f9b\u4e86\u63a8\u8350\u503c\u4ee5\u786e\u4fdd\u5b89\u5168\u7a33\u5b9a\u8fd0\u884c\uff0c\u4f46\u67d0\u4e9b\u63a8\u8350\u53c2\u6570\u7ec4\u5408\u4ecd\u53ef\u80fd\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u98de\u884c\u884c\u4e3a\uff0c\u964d\u4f4e\u65e0\u4eba\u673a\u9c81\u68d2\u6027\u3002\u9700\u8981\u4e00\u79cd\u5b9e\u65f6\u4fee\u590d\u98ce\u9669\u914d\u7f6e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRisConFix\u65b9\u6cd5\uff1a1\uff09\u6301\u7eed\u76d1\u63a7\u65e0\u4eba\u673a\u8fd0\u884c\u72b6\u6001\uff1b2\uff09\u68c0\u6d4b\u5230\u5f02\u5e38\u98de\u884c\u884c\u4e3a\u65f6\u81ea\u52a8\u89e6\u53d1\u4fee\u590d\u673a\u5236\uff1b3\uff09\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u914d\u7f6e\u53c2\u6570\u4e0e\u98de\u884c\u72b6\u6001\u7684\u5173\u7cfb\uff1b4\uff09\u751f\u6210\u7ea0\u6b63\u6027\u53c2\u6570\u66f4\u65b0\u4ee5\u6062\u590d\u98de\u884c\u7a33\u5b9a\u6027\uff1b5\uff09\u91c7\u7528\u8fed\u4ee3\u8fc7\u7a0b\u786e\u4fdd\u914d\u7f6e\u6709\u6548\u6027\uff0c\u5982\u679c\u5f02\u5e38\u6301\u7eed\u5219\u89e6\u53d1\u4e0b\u4e00\u4fee\u590d\u5468\u671f\u3002", "result": "\u5728ArduPilot\u6848\u4f8b\u7814\u7a76\u4e2d\uff08\u5305\u542b1,421\u7ec4\u9519\u8bef\u914d\u7f6e\uff09\uff0cRisConFix\u5b9e\u73b0\u4e86\u6700\u4f73\u4fee\u590d\u6210\u529f\u738797%\uff0c\u6700\u4f18\u5e73\u5747\u4fee\u590d\u6b21\u65701.17\u6b21\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u6709\u6548\u4e14\u9ad8\u6548\u5730\u5b9e\u65f6\u4fee\u590d\u98ce\u9669\u914d\u7f6e\u3002", "conclusion": "RisConFix\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8fed\u4ee3\u4fee\u590d\u673a\u5236\uff0c\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u5e76\u4fee\u590d\u65e0\u4eba\u673a\u98de\u884c\u63a7\u5236\u8f6f\u4ef6\u4e2d\u7684\u98ce\u9669\u914d\u7f6e\u53c2\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2512.06629", "categories": ["cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.06629", "abs": "https://arxiv.org/abs/2512.06629", "authors": ["Xiao-li Xia", "Hou-biao Li"], "title": "FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection", "comment": "36 pages, 14 figures,Table 5", "summary": "Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.", "AI": {"tldr": "FlatFormer\u901a\u8fc7\"\u4fe1\u606f\u6ce8\u5165\u800c\u975e\u7ed3\u6784\u5806\u53e0\"\u7684\u8bbe\u8ba1\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u5728\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u9762\u4e34\"\u6027\u80fd-\u590d\u6742\u5ea6\u9677\u9631\"\uff1a\u6355\u6349\u590d\u6742\u8ba4\u77e5\u52a8\u6001\uff08\u5982\u5b66\u4e60\u4f1a\u8bdd\u548c\u8bb0\u5fc6\u8870\u51cf\uff09\u901a\u5e38\u9700\u8981\u6df1\u5ea6\u5c42\u6b21\u67b6\u6784\uff0c\u8fd9\u5bfc\u81f4\u5b9e\u65f6\u90e8\u7f72\u7684\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51faFlatFormer\u67b6\u6784\uff0c\u91c7\u7528\"\u4fe1\u606f\u6ce8\u5165\u800c\u975e\u7ed3\u6784\u5806\u53e0\"\u7684\u8bbe\u8ba1\u8303\u5f0f\u3002\u4f7f\u7528\u6807\u51c6\u6241\u5e73Transformer\uff0c\u901a\u8fc7\u4e24\u79cd\u8f7b\u91cf\u7ea7\u6ce8\u5165\u673a\u5236\u589e\u5f3a\uff1a(1) \u6df7\u5408\u8f93\u5165\u7f16\u7801\u7b56\u7565\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u7684\u4f1a\u8bdd\u6807\u8bc6\u7b26\u548c\u56fa\u5b9a\u7684\u6b63\u5f26\u6b65\u957f\u5d4c\u5165\uff1b(2) \u5c06\u9884\u8ba1\u7b97\u7684\u5e42\u5f8b\u504f\u7f6e\u76f4\u63a5\u96c6\u6210\u5230\u6ce8\u610f\u529b\u5bf9\u6570\u4e2d\uff0c\u663e\u5f0f\u5efa\u6a21\u9057\u5fd8\u66f2\u7ebf\u3002", "result": "\u5728\u56db\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08\u5982EdNet\u3001Junyi\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFlatFormer\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5728EdNet\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6700\u5f3a\u7684\u5c42\u6b21\u57fa\u7ebf\uff08HiTSKT\uff09\uff0c\u7edd\u5bf9AUC\u63d0\u9ad8\u4e868.3%\uff0c\u540c\u65f6\u4f7f\u7528\u4e0d\u523015%\u7684\u53c2\u6570\uff0c\u63a8\u7406\u901f\u5ea6\u7ea6\u5feb\u4e09\u500d\u3002", "conclusion": "\u9ad8\u8ba4\u77e5\u4fdd\u771f\u5ea6\u4e0d\u4e00\u5b9a\u9700\u8981\u67b6\u6784\u590d\u6742\u6027\uff0c\u901a\u8fc7\u4fe1\u606f\u6ce8\u5165\u800c\u975e\u7ed3\u6784\u5806\u53e0\u7684\u8bbe\u8ba1\u8303\u5f0f\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2512.07193", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07193", "abs": "https://arxiv.org/abs/2512.07193", "authors": ["Manthan Shenoy", "Andreas Rausch"], "title": "Towards Benchmarking Design Pattern Detection Under Obfuscation: Reproducing and Evaluating Attention-Based Detection Method", "comment": "Pre-peer-review version of the paper submitted to the Workshop Track of the European Conference on Software Architecture (ECSA 2025), Springer LNCS 15982. Dataset: https://github.com/manthan410/Benchmark_dpd_att. Version of Record: https://doi.org/10.1007/978-3-032-04403-7_13", "summary": "This paper investigates the semantic robustness of attention-based classifiers for design pattern detection, particularly focusing on their reliance on structural and behavioral semantics. We reproduce the DPDAtt, an attention-based design pattern detection approach using learning-based classifiers, and evaluate its performance under obfuscation. To this end, we curate an obfuscated version of the DPDAtt Corpus, where the name identifiers in code such as class names, method names, etc., and string literals like print statements and comment blocks are replaced while preserving control flow, inheritance, and logic. Our findings reveal that these trained classifiers in DPDAtt depend significantly on superficial syntactic features, leading to substantial misclassification when such cues are removed through obfuscation. This work highlights the need for more robust detection tools capable of capturing deeper semantic meanings in source code. We propose our curated Obfuscated corpus (containing 34 Java source files) as a reusable proof-of-concept benchmark for evaluating state-of-the-art design pattern detectors on their true semantic generalization capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bbe\u8ba1\u6a21\u5f0f\u68c0\u6d4b\u5206\u7c7b\u5668\u5728\u8bed\u4e49\u9c81\u68d2\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u5b83\u4eec\u8fc7\u5ea6\u4f9d\u8d56\u8868\u9762\u8bed\u6cd5\u7279\u5f81\uff0c\u5728\u4ee3\u7801\u6df7\u6dc6\u540e\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bbe\u8ba1\u6a21\u5f0f\u68c0\u6d4b\u65b9\u6cd5\uff08DPDAtt\uff09\u7684\u8bed\u4e49\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5bf9\u7ed3\u6784\u548c\u884c\u4e3a\u8bed\u4e49\u7684\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u4ee5\u4e86\u89e3\u8fd9\u4e9b\u5206\u7c7b\u5668\u662f\u5426\u771f\u6b63\u7406\u89e3\u4e86\u4ee3\u7801\u7684\u6df1\u5c42\u8bed\u4e49\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u590d\u73b0DPDAtt\uff08\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bbe\u8ba1\u6a21\u5f0f\u68c0\u6d4b\u65b9\u6cd5\uff09\uff1b2\uff09\u521b\u5efa\u6df7\u6dc6\u7248\u7684DPDAtt\u8bed\u6599\u5e93\uff0c\u66ff\u6362\u7c7b\u540d\u3001\u65b9\u6cd5\u540d\u7b49\u6807\u8bc6\u7b26\u548c\u5b57\u7b26\u4e32\u5b57\u9762\u91cf\uff0c\u540c\u65f6\u4fdd\u7559\u63a7\u5236\u6d41\u3001\u7ee7\u627f\u548c\u903b\u8f91\uff1b3\uff09\u5728\u6df7\u6dc6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5206\u7c7b\u5668\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0DPDAtt\u4e2d\u7684\u8bad\u7ec3\u5206\u7c7b\u5668\u663e\u8457\u4f9d\u8d56\u8868\u9762\u8bed\u6cd5\u7279\u5f81\uff0c\u5f53\u8fd9\u4e9b\u7ebf\u7d22\u901a\u8fc7\u6df7\u6dc6\u88ab\u79fb\u9664\u65f6\uff0c\u4f1a\u5bfc\u81f4\u5927\u91cf\u8bef\u5206\u7c7b\uff0c\u8868\u660e\u8fd9\u4e9b\u5206\u7c7b\u5668\u672a\u80fd\u771f\u6b63\u7406\u89e3\u4ee3\u7801\u7684\u6df1\u5c42\u8bed\u4e49\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u5de5\u5177\uff0c\u80fd\u591f\u6355\u83b7\u6e90\u4ee3\u7801\u7684\u6df1\u5c42\u8bed\u4e49\u542b\u4e49\u3002\u4f5c\u8005\u63d0\u51fa\u7684\u6df7\u6dc6\u8bed\u6599\u5e93\uff08\u5305\u542b34\u4e2aJava\u6e90\u6587\u4ef6\uff09\u53ef\u4f5c\u4e3a\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u6982\u5ff5\u9a8c\u8bc1\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u8bbe\u8ba1\u6a21\u5f0f\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u8bed\u4e49\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u8868\u73b0\u3002"}}
{"id": "2512.06653", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06653", "abs": "https://arxiv.org/abs/2512.06653", "authors": ["Hengzhi Lan", "Yue Yu", "Li Qian", "Li Peng", "Jie Wu", "Wei Liu", "Jian Luan", "Ting Bai"], "title": "LightSearcher: Efficient DeepSearch via Experiential Memory", "comment": "10 pages, 5 figures", "summary": "DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.", "AI": {"tldr": "LightSearcher\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u7ecf\u9a8c\u8bb0\u5fc6\u548c\u81ea\u9002\u5e94\u5956\u52b1\u673a\u5236\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11DeepSearch\u8303\u5f0f\u4e2d\u7684\u5de5\u5177\u8c03\u7528\u6b21\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684RL\u9a71\u52a8\u7684DeepSearch\u7cfb\u7edf\u5b58\u5728\u51c6\u786e\u6027\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff1a\u9891\u7e41\u8c03\u7528\u5916\u90e8\u641c\u7d22\u5de5\u5177\u53ef\u4ee5\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46\u4f1a\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u6548\u7387\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86LightSearcher\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u901a\u8fc7\u5bf9\u6bd4\u63a8\u7406\u8f68\u8ff9\u5b66\u4e60\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6210\u529f\u63a8\u7406\u6a21\u5f0f\u6458\u8981\u7684\u6587\u672c\u7ecf\u9a8c\u8bb0\u5fc6\uff1b2\uff09\u4ec5\u5728\u6b63\u786e\u7b54\u6848\u573a\u666f\u4e2d\u60e9\u7f5a\u5197\u4f59\u5de5\u5177\u8c03\u7528\u7684\u81ea\u9002\u5e94\u5956\u52b1\u5851\u9020\u673a\u5236\u3002", "result": "\u5728\u56db\u4e2a\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLightSearcher\u5728\u4fdd\u6301\u4e0eSOTA\u57fa\u7ebfReSearch\u76f8\u5f53\u7684\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5c06\u641c\u7d22\u5de5\u5177\u8c03\u7528\u6b21\u6570\u51cf\u5c11\u4e8639.6%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e8648.6%\uff0ctoken\u6d88\u8017\u51cf\u5c11\u4e8621.2%\u3002", "conclusion": "LightSearcher\u901a\u8fc7\u521b\u65b0\u7684\u6587\u672c\u7ecf\u9a8c\u8bb0\u5fc6\u548c\u81ea\u9002\u5e94\u5956\u52b1\u673a\u5236\uff0c\u6709\u6548\u5e73\u8861\u4e86DeepSearch\u8303\u5f0f\u4e2d\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\u6743\u8861\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u6df1\u5ea6\u63a8\u7406\u3002"}}
{"id": "2512.06705", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06705", "abs": "https://arxiv.org/abs/2512.06705", "authors": ["Yongyuan He", "Yi Bu"], "title": "Academic journals' AI policies fail to curb the surge in AI-assisted academic writing", "comment": "40 pages, 10 figures, and 9 tables", "summary": "The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.", "AI": {"tldr": "\u5bf95114\u79cd\u671f\u520a\u548c520\u4e07\u7bc7\u8bba\u6587\u7684\u5206\u6790\u663e\u793a\uff0c\u5c3d\u7ba170%\u7684\u671f\u520a\u91c7\u7528\u4e86AI\u4f7f\u7528\u653f\u7b56\uff0c\u4f46AI\u5199\u4f5c\u5de5\u5177\u7684\u4f7f\u7528\u7387\u5728\u5b66\u79d1\u95f4\u6025\u5267\u589e\u957f\uff0c\u653f\u7b56\u5b58\u5728\u4e0e\u5426\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4e14AI\u4f7f\u7528\u62ab\u9732\u7387\u6781\u4f4e\uff08\u4ec50.1%\uff09\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u5feb\u901f\u666e\u53ca\uff0c\u671f\u520a\u548c\u51fa\u7248\u5546\u7eb7\u7eb7\u5236\u5b9a\u76f8\u5173\u653f\u7b56\uff0c\u4f46\u8fd9\u4e9b\u653f\u7b56\u7684\u5b9e\u9645\u6548\u679c\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u5b9e\u8bc1\u8bc4\u4f30\u3002", "method": "\u5206\u6790\u4e865114\u79cd\u671f\u520a\u548c\u8d85\u8fc7520\u4e07\u7bc7\u8bba\u6587\uff0c\u8bc4\u4f30AI\u4f7f\u7528\u6307\u5357\u7684\u5b9e\u9645\u5f71\u54cd\uff1b\u5bf916.4\u4e07\u7bc7\u79d1\u5b66\u51fa\u7248\u7269\u8fdb\u884c\u5168\u6587\u5206\u6790\uff0c\u7279\u522b\u5173\u6ce82023\u5e74\u4ee5\u6765\u53d1\u8868\u76847.5\u4e07\u7bc7\u8bba\u6587\u3002", "result": "1. \u5c3d\u7ba170%\u7684\u671f\u520a\u91c7\u7528\u4e86AI\u653f\u7b56\uff08\u4e3b\u8981\u662f\u8981\u6c42\u62ab\u9732\uff09\uff0c\u4f46\u7814\u7a76\u4eba\u5458\u4f7f\u7528AI\u5199\u4f5c\u5de5\u5177\u7684\u60c5\u51b5\u5728\u5404\u5b66\u79d1\u4e2d\u6025\u5267\u589e\u52a0\uff1b2. \u6709\u653f\u7b56\u4e0e\u65e0\u653f\u7b56\u7684\u671f\u520a\u4e4b\u95f4\u65e0\u663e\u8457\u5dee\u5f02\uff1b3. \u975e\u82f1\u8bed\u56fd\u5bb6\u3001\u7269\u7406\u79d1\u5b66\u548c\u9ad8\u5f00\u653e\u83b7\u53d6\u671f\u520a\u7684\u589e\u957f\u7387\u6700\u9ad8\uff1b4. 2023\u5e74\u4ee5\u6765\u53d1\u8868\u7684\u8bba\u6587\u4e2d\uff0c\u53ea\u670976\u7bc7\uff080.1%\uff09\u660e\u786e\u62ab\u9732\u4e86AI\u4f7f\u7528\u3002", "conclusion": "\u5f53\u524d\u653f\u7b56\u5728\u4fc3\u8fdb\u900f\u660e\u5ea6\u6216\u9650\u5236AI\u91c7\u7528\u65b9\u9762\u57fa\u672c\u5931\u8d25\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u4f26\u7406\u6846\u67b6\u4ee5\u4fc3\u8fdb\u79d1\u5b66\u4e2d\u8d1f\u8d23\u4efb\u7684AI\u6574\u5408\u3002"}}
{"id": "2512.07293", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07293", "abs": "https://arxiv.org/abs/2512.07293", "authors": ["Roberto Verdecchia", "Justus Bogner"], "title": "The Human Need for Storytelling: Reflections on Qualitative Software Engineering Research With a Focus Group of Experts", "comment": "Published in ACM SIGSOFT Software Engineering Notes (SEN), Volume 51, Issue 1, 2026", "summary": "From its first adoption in the late 80s, qualitative research has slowly but steadily made a name for itself in what was, and perhaps still is, the predominantly quantitative software engineering (SE) research landscape. As part of our regular column on empirical software engineering (ACM SIGSOFT SEN-ESE), we reflect on the state of qualitative SE research with a focus group of experts. Among other things, we discuss why qualitative SE research is important, how it evolved over time, common impediments faced while practicing it today, and what the future of qualitative SE research might look like. Joining the conversation are Rashina Hoda (Monash University, Australia), Carolyn Seaman (University of Maryland, United States), and Klaas Stol (University College Cork, Ireland). The content of this paper is a faithful account of our conversation from October 25, 2025, which we moderated and edited for our column.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e13\u5bb6\u5c0f\u7ec4\u8ba8\u8bba\uff0c\u56de\u987e\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u5b9a\u6027\u7814\u7a76\u7684\u73b0\u72b6\u3001\u91cd\u8981\u6027\u3001\u53d1\u5c55\u5386\u7a0b\u3001\u5f53\u524d\u9762\u4e34\u7684\u969c\u788d\u4ee5\u53ca\u672a\u6765\u5c55\u671b\u3002", "motivation": "\u5728\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u957f\u671f\u4ee5\u5b9a\u91cf\u65b9\u6cd5\u4e3a\u4e3b\u5bfc\u7684\u80cc\u666f\u4e0b\uff0c\u5b9a\u6027\u7814\u7a76\u81ea20\u4e16\u7eaa80\u5e74\u4ee3\u672b\u5f15\u5165\u4ee5\u6765\u9010\u6e10\u83b7\u5f97\u8ba4\u53ef\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e13\u5bb6\u8ba8\u8bba\uff0c\u53cd\u601d\u5b9a\u6027\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u7684\u73b0\u72b6\uff0c\u63a2\u8ba8\u5176\u91cd\u8981\u6027\u3001\u53d1\u5c55\u5386\u7a0b\u3001\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "method": "\u91c7\u7528\u4e13\u5bb6\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\u7684\u5f62\u5f0f\uff0c\u9080\u8bf7\u4e86\u4e09\u4f4d\u56fd\u9645\u4e13\u5bb6\uff08Rashina Hoda\u3001Carolyn Seaman\u3001Klaas Stol\uff09\u53c2\u4e0e\u5bf9\u8bdd\u3002\u8ba8\u8bba\u4e8e2025\u5e7410\u670825\u65e5\u8fdb\u884c\uff0c\u5185\u5bb9\u7ecf\u8fc7\u4e3b\u6301\u4eba\u548c\u7f16\u8f91\u6574\u7406\uff0c\u4f5c\u4e3aACM SIGSOFT SEN-ESE\u4e13\u680f\u6587\u7ae0\u53d1\u8868\u3002", "result": "\u4e13\u5bb6\u4eec\u8ba8\u8bba\u4e86\u5b9a\u6027\u7814\u7a76\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u91cd\u8981\u6027\u3001\u5386\u53f2\u6f14\u53d8\u8fc7\u7a0b\u3001\u5f53\u524d\u5b9e\u8df5\u4e2d\u7684\u5e38\u89c1\u969c\u788d\uff0c\u5e76\u5bf9\u5b9a\u6027\u7814\u7a76\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\u8fdb\u884c\u4e86\u5c55\u671b\u3002\u8ba8\u8bba\u5185\u5bb9\u53cd\u6620\u4e86\u5b9a\u6027\u7814\u7a76\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u73b0\u72b6\u548c\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u5b9a\u6027\u7814\u7a76\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u5df2\u7ecf\u5efa\u7acb\u4e86\u81ea\u5df1\u7684\u5730\u4f4d\uff0c\u4f46\u4ecd\u9762\u4e34\u4e00\u4e9b\u5b9e\u8df5\u969c\u788d\u3002\u901a\u8fc7\u4e13\u5bb6\u8ba8\u8bba\uff0c\u672c\u6587\u4e3a\u7406\u89e3\u5b9a\u6027\u7814\u7a76\u7684\u4ef7\u503c\u3001\u5386\u53f2\u53d1\u5c55\u548c\u672a\u6765\u8d8b\u52bf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.06589", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06589", "abs": "https://arxiv.org/abs/2512.06589", "authors": ["Xiaojun Jia", "Jie Liao", "Qi Guo", "Teng Ma", "Simeng Qin", "Ranjie Duan", "Tianlin Li", "Yihao Huang", "Zhitao Zeng", "Dongxian Wu", "Yiming Li", "Wenqi Ren", "Xiaochun Cao", "Yang Liu"], "title": "OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation", "comment": null, "summary": "Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.", "AI": {"tldr": "OmniSafeBench-MM\u662f\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb-\u9632\u5fa1\u8bc4\u4f30\u5de5\u5177\u7bb1\uff0c\u96c6\u6210\u4e8613\u79cd\u653b\u51fb\u65b9\u6cd5\u300115\u79cd\u9632\u5fa1\u7b56\u7565\u548c\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u4e86\u4e09\u7ef4\u8bc4\u4f30\u534f\u8bae\u6765\u8861\u91cf\u5371\u5bb3\u6027\u3001\u610f\u56fe\u5bf9\u9f50\u548c\u54cd\u5e94\u7ec6\u8282\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u867d\u7136\u5177\u5907\u7edf\u4e00\u7684\u611f\u77e5\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\uff0c\u7ed5\u8fc7\u5b89\u5168\u5bf9\u9f50\u5e76\u8bf1\u5bfc\u6709\u5bb3\u884c\u4e3a\u3002\u73b0\u6709\u57fa\u51c6\u5982JailBreakV-28K\u3001MM-SafetyBench\u548cHADES\u5b58\u5728\u5c40\u9650\u6027\uff1a\u653b\u51fb\u573a\u666f\u6709\u9650\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u9632\u5fa1\u8bc4\u4f30\u3001\u6ca1\u6709\u7edf\u4e00\u53ef\u590d\u73b0\u7684\u5de5\u5177\u7bb1\u3002", "method": "\u5f15\u5165OmniSafeBench-MM\u5de5\u5177\u7bb1\uff0c\u96c6\u621013\u79cd\u4ee3\u8868\u6027\u653b\u51fb\u65b9\u6cd5\u300115\u79cd\u9632\u5fa1\u7b56\u7565\uff0c\u4ee5\u53ca\u6db5\u76d69\u4e2a\u4e3b\u8981\u98ce\u9669\u9886\u57df\u548c50\u4e2a\u7ec6\u7c92\u5ea6\u7c7b\u522b\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\u3002\u6570\u636e\u96c6\u7ed3\u6784\u5305\u62ec\u54a8\u8be2\u6027\u3001\u547d\u4ee4\u6027\u548c\u9648\u8ff0\u6027\u67e5\u8be2\u7c7b\u578b\u3002\u5efa\u7acb\u4e09\u7ef4\u8bc4\u4f30\u534f\u8bae\uff1a1) \u5371\u5bb3\u6027\uff08\u4ece\u4f4e\u5f71\u54cd\u4e2a\u4f53\u5371\u5bb3\u5230\u707e\u96be\u6027\u793e\u4f1a\u5a01\u80c1\u7684\u591a\u7ea7\u5c3a\u5ea6\uff09\uff0c2) \u54cd\u5e94\u4e0e\u67e5\u8be2\u7684\u610f\u56fe\u5bf9\u9f50\uff0c3) \u54cd\u5e94\u7ec6\u8282\u6c34\u5e73\u3002", "result": "\u5bf910\u4e2a\u5f00\u6e90\u548c8\u4e2a\u95ed\u6e90MLLMs\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5bf9\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002\u901a\u8fc7\u5c06\u6570\u636e\u3001\u65b9\u6cd5\u548c\u8bc4\u4f30\u7edf\u4e00\u5230\u5f00\u6e90\u53ef\u590d\u73b0\u5e73\u53f0\u4e2d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u7840\u3002", "conclusion": "OmniSafeBench-MM\u901a\u8fc7\u7edf\u4e00\u6570\u636e\u3001\u65b9\u6cd5\u548c\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb-\u9632\u5fa1\u8bc4\u4f30\u5de5\u5177\u7bb1\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u591a\u6a21\u6001AI\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u3001\u53ef\u590d\u73b0\u7684\u57fa\u7840\u5e73\u53f0\u3002"}}
{"id": "2512.07404", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07404", "abs": "https://arxiv.org/abs/2512.07404", "authors": ["Francisco Ribeiro", "Claudio Spiess", "Prem Devanbu", "Sarah Nadi"], "title": "Do LLMs Trust the Code They Write?", "comment": null, "summary": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.", "AI": {"tldr": "\u8bba\u6587\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u662f\u5426\u7f16\u7801\u4e86\u4ee3\u7801\u6b63\u786e\u6027\u8868\u793a\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6b63\u786e\u548c\u9519\u8bef\u4ee3\u7801\u7684\u9690\u85cf\u72b6\u6001\u6765\u63d0\u53d6\u8fd9\u79cd\u8868\u793a\uff0c\u5e76\u5229\u7528\u5b83\u6539\u8fdb\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u8bc4\u4f30", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u5f88\u6709\u6548\uff0c\u4f46\u7ecf\u5e38\u8f93\u51fa\u9519\u8bef\u4ee3\u7801\u3002\u6a21\u578b\u8f93\u51fa\u6982\u7387\u4e0e\u6b63\u786e\u6027\u76f8\u5173\u6027\u4e0d\u5f3a\uff0c\u4e14\u53ea\u53cd\u6620\u751f\u6210\u8fc7\u7a0b\u7684\u6700\u7ec8\u8f93\u51fa\u3002\u53d7LLM\u5185\u90e8\u7f16\u7801\u771f\u5b9e\u6027\u6982\u5ff5\u7684\u542f\u53d1\uff0c\u63a2\u7d22LLM\u662f\u5426\u7c7b\u4f3c\u5730\u8868\u793a\u4ee3\u7801\u6b63\u786e\u6027", "method": "\u901a\u8fc7\u5bf9\u6bd4\u540c\u4e00\u7f16\u7a0b\u4efb\u52a1\u4e0b\u6b63\u786e\u548c\u9519\u8bef\u4ee3\u7801\u5bf9\u7684\u9690\u85cf\u72b6\u6001\uff0c\u5728\u56db\u4e2aLLM\u4e2d\u8bc6\u522b\u4ee3\u7801\u6b63\u786e\u6027\u8868\u793a\u3002\u5229\u7528\u63d0\u53d6\u7684\u6b63\u786e\u6027\u8868\u793a\u6539\u8fdb\u4ee3\u7801\u8d28\u91cf\u8bc4\u4f30\uff0c\u65e0\u9700\u6d4b\u8bd5\u6267\u884c", "result": "\u63d0\u53d6\u7684\u6b63\u786e\u6027\u8868\u793a\u4f18\u4e8e\u6807\u51c6\u5bf9\u6570\u4f3c\u7136\u6392\u540d\u548c\u6a21\u578b\u53e3\u5934\u7f6e\u4fe1\u5ea6\u3002\u5185\u90e8\u6b63\u786e\u6027\u4fe1\u53f7\u53ef\u7528\u4e8e\u9009\u62e9\u66f4\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\u6837\u672c\uff0c\u65e0\u9700\u6d4b\u8bd5\u6267\u884c", "conclusion": "\u5229\u7528\u5185\u90e8\u8868\u793a\u53ef\u4ee5\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\uff0c\u4f7fLLM\u66f4\u53ef\u9760\uff0c\u4ece\u800c\u63d0\u9ad8\u5bf9\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u7684\u4fe1\u5fc3\u3002\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u5185\u90e8\u8868\u5f81\u6539\u8fdb\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u8bc4\u4f30"}}
{"id": "2512.06660", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06660", "abs": "https://arxiv.org/abs/2512.06660", "authors": ["Saleha Muzammil", "Rahul Reddy", "Vishal Kamalakrishnan", "Hadi Ahmadi", "Wajih Ul Hassan"], "title": "Towards Small Language Models for Security Query Generation in SOC Workflows", "comment": null, "summary": "Analysts in Security Operations Centers routinely query massive telemetry streams using Kusto Query Language (KQL). Writing correct KQL requires specialized expertise, and this dependency creates a bottleneck as security teams scale. This paper investigates whether Small Language Models (SLMs) can enable accurate, cost-effective natural-language-to-KQL translation for enterprise security. We propose a three-knob framework targeting prompting, fine-tuning, and architecture design. First, we adapt existing NL2KQL framework for SLMs with lightweight retrieval and introduce error-aware prompting that addresses common parser failures without increasing token count. Second, we apply LoRA fine-tuning with rationale distillation, augmenting each NLQ-KQL pair with a brief chain-of-thought explanation to transfer reasoning from a teacher model while keeping the SLM compact. Third, we propose a two-stage architecture that uses an SLM for candidate generation and a low-cost LLM judge for schema-aware refinement and selection. We evaluate nine models (five SLMs and four LLMs) across syntax correctness, semantic accuracy, table selection, and filter precision, alongside latency and token cost. On Microsoft's NL2KQL Defender Evaluation dataset, our two-stage approach achieves 0.987 syntax and 0.906 semantic accuracy. We further demonstrate generalizability on Microsoft Sentinel data, reaching 0.964 syntax and 0.831 semantic accuracy. These results come at up to 10x lower token cost than GPT-5, establishing SLMs as a practical, scalable foundation for natural-language querying in security operations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLMs)\u5728\u4f01\u4e1a\u5b89\u5168\u9886\u57df\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5230Kusto\u67e5\u8be2\u8bed\u8a00(KQL)\u7684\u51c6\u786e\u3001\u7ecf\u6d4e\u9ad8\u6548\u7ffb\u8bd1\uff0c\u63d0\u51fa\u4e86\u4e09\u65cb\u94ae\u6846\u67b6\uff0c\u5728\u5fae\u8f6f\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.987\u8bed\u6cd5\u51c6\u786e\u7387\u548c0.906\u8bed\u4e49\u51c6\u786e\u7387\uff0c\u6bd4GPT-5\u964d\u4f4e10\u500d\u4ee4\u724c\u6210\u672c\u3002", "motivation": "\u5b89\u5168\u8fd0\u8425\u4e2d\u5fc3\u5206\u6790\u5e08\u9700\u8981\u67e5\u8be2\u5927\u91cf\u9065\u6d4b\u6570\u636e\uff0c\u4f46\u7f16\u5199\u6b63\u786e\u7684KQL\u67e5\u8be2\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8fd9\u79cd\u4f9d\u8d56\u5173\u7cfb\u5728\u5b89\u5168\u56e2\u961f\u6269\u5c55\u65f6\u6210\u4e3a\u74f6\u9888\u3002\u7814\u7a76SLMs\u80fd\u5426\u4e3a\u5b89\u5168\u8fd0\u8425\u63d0\u4f9b\u51c6\u786e\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u81ea\u7136\u8bed\u8a00\u5230KQL\u7ffb\u8bd1\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e09\u65cb\u94ae\u6846\u67b6\uff1a1) \u9488\u5bf9SLMs\u7684\u8f7b\u91cf\u7ea7\u68c0\u7d22\u548c\u9519\u8bef\u611f\u77e5\u63d0\u793a\uff0c\u89e3\u51b3\u5e38\u89c1\u89e3\u6790\u5931\u8d25\u95ee\u9898\uff1b2) \u5e94\u7528LoRA\u5fae\u8c03\u4e0e\u539f\u7406\u84b8\u998f\uff0c\u901a\u8fc7\u94fe\u5f0f\u601d\u8003\u89e3\u91ca\u4f20\u9012\u6559\u5e08\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff1b3) \u4e24\u9636\u6bb5\u67b6\u6784\uff1aSLM\u751f\u6210\u5019\u9009\u67e5\u8be2\uff0c\u4f4e\u6210\u672cLLM\u6cd5\u5b98\u8fdb\u884c\u6a21\u5f0f\u611f\u77e5\u4f18\u5316\u548c\u9009\u62e9\u3002", "result": "\u5728\u5fae\u8f6fNL2KQL Defender\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\uff0c\u4e24\u9636\u6bb5\u65b9\u6cd5\u8fbe\u52300.987\u8bed\u6cd5\u51c6\u786e\u7387\u548c0.906\u8bed\u4e49\u51c6\u786e\u7387\u3002\u5728Microsoft Sentinel\u6570\u636e\u4e0a\u8fbe\u52300.964\u8bed\u6cd5\u51c6\u786e\u7387\u548c0.831\u8bed\u4e49\u51c6\u786e\u7387\uff0c\u6bd4GPT-5\u964d\u4f4e\u9ad8\u8fbe10\u500d\u7684\u4ee4\u724c\u6210\u672c\u3002", "conclusion": "SLMs\u53ef\u4ee5\u4f5c\u4e3a\u5b89\u5168\u8fd0\u8425\u4e2d\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u57fa\u7840\uff0c\u63d0\u4f9b\u51c6\u786e\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e13\u4e1a\u67e5\u8be2\u7684\u4f9d\u8d56\u74f6\u9888\u3002"}}
{"id": "2512.07501", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07501", "abs": "https://arxiv.org/abs/2512.07501", "authors": ["Weilin Luo", "Xueyi Liang", "Haotian Deng", "Yanan Liu", "Hai Wan"], "title": "AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution", "comment": null, "summary": "Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\\% verification success rate, significantly surpassing the $65$\\% success rate of the SOTA approach.", "AI": {"tldr": "AutoICE\uff1a\u57fa\u4e8eLLM\u9a71\u52a8\u8fdb\u5316\u641c\u7d22\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4ee3\u7801\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6837\u5316\u521d\u59cb\u5316\u3001\u534f\u4f5c\u4ea4\u53c9\u548c\u81ea\u53cd\u601d\u53d8\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u9a8c\u8bc1\u6210\u529f\u7387\u81f390.36%", "motivation": "\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u81ea\u52a8\u5408\u6210\u53ef\u9a8c\u8bc1\u4ee3\u7801\u80fd\u786e\u4fdd\u8f6f\u4ef6\u6b63\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u964d\u4f4e\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u5e94\u7528\u95e8\u69db\u3002\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u800c\u5b58\u5728\u4e25\u91cd\u8bed\u6cd5\u548c\u8bed\u4e49\u9519\u8bef\uff0c\u4e14\u96be\u4ee5\u6709\u6548\u5f62\u5f0f\u5316\u9690\u5f0f\u77e5\u8bc6\u3002", "method": "\u63d0\u51faAutoICE\u65b9\u6cd5\uff0c\u91c7\u7528LLM\u9a71\u52a8\u7684\u8fdb\u5316\u641c\u7d22\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u591a\u6837\u5316\u4e2a\u4f53\u521d\u59cb\u5316\uff1b2\uff09\u534f\u4f5c\u4ea4\u53c9\u5b9e\u73b0\u591a\u6837\u5316\u8fed\u4ee3\u66f4\u65b0\uff1b3\uff09\u81ea\u53cd\u601d\u53d8\u5f02\u4fc3\u8fdb\u9690\u5f0f\u77e5\u8bc6\u53d1\u73b0\uff0c\u4ee5\u51cf\u8f7b\u5355\u667a\u80fd\u4f53\u8fed\u4ee3\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u3002", "result": "AutoICE\u6210\u529f\u9a8c\u8bc190.36%\u7684\u4ee3\u7801\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002\u5728\u5f00\u53d1\u8005\u53cb\u597d\u6570\u636e\u96c6\u53d8\u4f53\u4e0a\uff0c\u9a8c\u8bc1\u6210\u529f\u7387\u8fbe\u523088.33%\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u768465%\u6210\u529f\u7387\u3002", "conclusion": "AutoICE\u901a\u8fc7\u8fdb\u5316\u641c\u7d22\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e2d\u7684\u8bed\u6cd5\u8bed\u4e49\u9519\u8bef\u548c\u9690\u5f0f\u77e5\u8bc6\u5f62\u5f0f\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u9a8c\u8bc1\u6210\u529f\u7387\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u5230\u53ef\u9a8c\u8bc1\u4ee3\u7801\u7684\u81ea\u52a8\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06747", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06747", "abs": "https://arxiv.org/abs/2512.06747", "authors": ["Jifar Wakuma Ayana", "Huang Qiming"], "title": "PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance", "comment": null, "summary": "Large Language Models (LLMs) are emerging as powerful enablers for autonomous reasoning and natural-language coordination in unmanned aerial vehicle (UAV) swarms operating within Internet of Things (IoT) environments. However, existing LLM-driven UAV systems process sensitive operational data in plaintext, exposing them to privacy and security risks. This work introduces PrivLLMSwarm, a privacy-preserving framework that performs secure LLM inference for UAV swarm coordination through Secure Multi-Party Computation (MPC). The framework incorporates MPC-optimized transformer components with efficient approximations of nonlinear activations, enabling practical encrypted inference on resource-constrained aerial platforms. A fine-tuned GPT-based command generator, enhanced through reinforcement learning in simulation, provides reliable instructions while maintaining confidentiality. Experimental evaluation in urban-scale simulations demonstrates that PrivLLMSwarm achieves high semantic accuracy, low encrypted inference latency, and robust formation control under privacy constraints. Comparative analysis shows PrivLLMSwarm offers a superior privacy-utility balance compared to differential privacy, federated learning, and plaintext baselines. To support reproducibility, the full implementation including source code, MPC components, and a synthetic dataset is publicly available. PrivLLMSwarm establishes a practical foundation for secure, LLM-enabled UAV swarms in privacy-sensitive IoT applications including smart-city monitoring and emergency response.", "AI": {"tldr": "PrivLLMSwarm\u662f\u4e00\u4e2a\u4fdd\u62a4\u9690\u79c1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\uff08MPC\uff09\u4e3a\u65e0\u4eba\u673a\u7fa4\u534f\u8c03\u6267\u884c\u5b89\u5168\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u9a71\u52a8\u65e0\u4eba\u673a\u7cfb\u7edf\u5904\u7406\u654f\u611f\u64cd\u4f5c\u6570\u636e\u65f6\u7684\u9690\u79c1\u548c\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u65e0\u4eba\u673a\u7cfb\u7edf\u4ee5\u660e\u6587\u5f62\u5f0f\u5904\u7406\u654f\u611f\u64cd\u4f5c\u6570\u636e\uff0c\u66b4\u9732\u4e86\u9690\u79c1\u548c\u5b89\u5168\u98ce\u9669\u3002\u5728\u7269\u8054\u7f51\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\u7fa4\u9700\u8981\u534f\u8c03\u548c\u81ea\u4e3b\u63a8\u7406\uff0c\u4f46\u5fc5\u987b\u4fdd\u62a4\u654f\u611f\u6570\u636e\u4e0d\u88ab\u6cc4\u9732\u3002", "method": "\u63d0\u51fa\u4e86PrivLLMSwarm\u6846\u67b6\uff0c\u91c7\u7528\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\uff08MPC\uff09\u8fdb\u884c\u5b89\u5168\u7684LLM\u63a8\u7406\u3002\u6846\u67b6\u5305\u542bMPC\u4f18\u5316\u7684transformer\u7ec4\u4ef6\uff0c\u4f7f\u7528\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u7684\u9ad8\u6548\u8fd1\u4f3c\uff0c\u4f7f\u8d44\u6e90\u53d7\u9650\u7684\u7a7a\u4e2d\u5e73\u53f0\u80fd\u591f\u8fdb\u884c\u5b9e\u7528\u7684\u52a0\u5bc6\u63a8\u7406\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u6a21\u62df\u4e2d\u589e\u5f3a\u7684\u5fae\u8c03GPT\u547d\u4ee4\u751f\u6210\u5668\u63d0\u4f9b\u53ef\u9760\u6307\u4ee4\u5e76\u4fdd\u6301\u673a\u5bc6\u6027\u3002", "result": "\u5728\u57ce\u5e02\u89c4\u6a21\u6a21\u62df\u4e2d\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cPrivLLMSwarm\u5b9e\u73b0\u4e86\u9ad8\u8bed\u4e49\u51c6\u786e\u6027\u3001\u4f4e\u52a0\u5bc6\u63a8\u7406\u5ef6\u8fdf\uff0c\u4ee5\u53ca\u5728\u9690\u79c1\u7ea6\u675f\u4e0b\u7684\u9c81\u68d2\u7f16\u961f\u63a7\u5236\u3002\u6bd4\u8f83\u5206\u6790\u663e\u793a\uff0c\u4e0e\u5dee\u5206\u9690\u79c1\u3001\u8054\u90a6\u5b66\u4e60\u548c\u660e\u6587\u57fa\u7ebf\u76f8\u6bd4\uff0cPrivLLMSwarm\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u9690\u79c1-\u6548\u7528\u5e73\u8861\u3002", "conclusion": "PrivLLMSwarm\u4e3a\u9690\u79c1\u654f\u611f\u7684\u7269\u8054\u7f51\u5e94\u7528\uff08\u5305\u62ec\u667a\u6167\u57ce\u5e02\u76d1\u63a7\u548c\u5e94\u6025\u54cd\u5e94\uff09\u4e2d\u7684\u5b89\u5168\u3001LLM\u9a71\u52a8\u7684\u65e0\u4eba\u673a\u7fa4\u5efa\u7acb\u4e86\u5b9e\u7528\u57fa\u7840\u3002\u5b8c\u6574\u7684\u5b9e\u73b0\u5305\u62ec\u6e90\u4ee3\u7801\u3001MPC\u7ec4\u4ef6\u548c\u5408\u6210\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u652f\u6301\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2512.06835", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06835", "abs": "https://arxiv.org/abs/2512.06835", "authors": ["Tingyu Li", "Zheng Sun", "Jingxuan Wei", "Siyuan Li", "Conghui He", "Lijun Wu", "Cheng Tan"], "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning", "comment": "25 pages, 5 figures", "summary": "Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.", "AI": {"tldr": "DoGe\u662f\u4e00\u4e2a\u53cc\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5b66\u4e60\u8fc7\u7a0b\u5206\u89e3\u4e3a\u601d\u8003\u8005\u548c\u89e3\u51b3\u8005\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u9762\u4e34\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u81ea\u8fdb\u5316\u8def\u5f84\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u663e\u8457\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u4e13\u4e1a\u9886\u57df\uff08\u5982\u5316\u5b66\u3001\u5730\u7403\u79d1\u5b66\u3001\u591a\u6a21\u6001\u6570\u5b66\uff09\u9762\u4e34\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u5408\u6210\u6570\u636e\u548c\u81ea\u5956\u52b1\u673a\u5236\u5b58\u5728\u5206\u5e03\u6709\u9650\u548c\u5bf9\u9f50\u56f0\u96be\uff0c\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u6a21\u578b\u5229\u7528\u9ad8\u5956\u52b1\u6a21\u5f0f\u5bfc\u81f4\u7b56\u7565\u71b5\u5d29\u6e83\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faDoGe\uff08Decouple to Generalize\uff09\u53cc\u89e3\u8026\u6846\u67b6\uff1a1\uff09\u5c06\u5b66\u4e60\u8fc7\u7a0b\u89e3\u8026\u4e3a\u601d\u8003\u8005\u548c\u89e3\u51b3\u8005\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u5f15\u5bfc\u6a21\u578b\u9996\u5148\u4ece\u4e0a\u4e0b\u6587\u5b66\u4e60\u800c\u975e\u76f4\u63a5\u89e3\u51b3\u95ee\u9898\uff1b2\uff09\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ece\u81ea\u7531\u63a2\u7d22\u4e0a\u4e0b\u6587\u5230\u5b9e\u9645\u89e3\u51b3\u4efb\u52a1\uff1b3\uff09\u6784\u5efa\u6f14\u5316\u8bfe\u7a0b\u5b66\u4e60\u7ba1\u9053\uff0c\u5305\u62ec\u6269\u5c55\u7684\u672c\u5730\u9886\u57df\u77e5\u8bc6\u8bed\u6599\u5e93\u548c\u8fed\u4ee3\u6f14\u5316\u7684\u79cd\u5b50\u95ee\u9898\u6c60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e3a\u5b9e\u73b0\u81ea\u8fdb\u5316\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002", "conclusion": "DoGe\u901a\u8fc7\u53cc\u89e3\u8026\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u7406\u7684\u5956\u52b1\u4fe1\u53f7\u91cf\u5316\u548c\u6f14\u5316\u8bfe\u7a0b\u5b66\u4e60\uff0c\u4e3a\u4e13\u4e1a\u9886\u57df\u7684\u81ea\u8fdb\u5316\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07814", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.07814", "abs": "https://arxiv.org/abs/2512.07814", "authors": ["Hua Yang", "Alejandro Velasco", "Sen Fang", "Bowen Xu", "Denys Poshyvanyk"], "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach", "comment": "21 pages, 8 figures", "summary": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0d\u540cPII\u7c7b\u578b\u7684\u6cc4\u9732\u98ce\u9669\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u6cc4\u9732\u98ce\u9669\u4e0e\u8bad\u7ec3\u52a8\u6001\u76f8\u5173\uff1a\u6613\u5b66\u4e60\u7684PII\u7c7b\u578b\uff08\u5982IP\u5730\u5740\uff09\u6cc4\u9732\u98ce\u9669\u66f4\u9ad8\uff0c\u800c\u96be\u5b66\u4e60\u7684\u7c7b\u578b\uff08\u5982\u5bc6\u94a5\u548c\u5bc6\u7801\uff09\u6cc4\u9732\u8f83\u5c11\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u5c06PII\u89c6\u4e3a\u5355\u4e00\u7c7b\u522b\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u7c7b\u578bPII\u7684\u5f02\u8d28\u6027\u98ce\u9669\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u4e0d\u540cPII\u7c7b\u578b\u5728\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u88ab\u5b66\u4e60\u548c\u6cc4\u9732\u7684\u53ef\u80fd\u6027\u662f\u5426\u5b58\u5728\u5dee\u5f02\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5173\u7cfb\u662f\u5426\u5177\u6709\u56e0\u679c\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a\u6784\u5efa\u5305\u542b\u591a\u79cdPII\u7c7b\u578b\u7684\u6570\u636e\u96c6\uff1b\u5bf9\u4e0d\u540c\u89c4\u6a21\u7684\u4ee3\u8868\u6027\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff1b\u8ba1\u7b97\u771f\u5b9ePII\u6570\u636e\u7684\u8bad\u7ec3\u52a8\u6001\uff1b\u6784\u5efa\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u6765\u4f30\u8ba1\u53ef\u5b66\u4e60\u6027\u5bf9\u6cc4\u9732\u7684\u56e0\u679c\u6548\u5e94\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u4e0d\u540cPII\u7c7b\u578b\u7684\u6cc4\u9732\u98ce\u9669\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u4e0e\u8bad\u7ec3\u52a8\u6001\u76f8\u5173\u3002\u6613\u5b66\u4e60\u7684\u5b9e\u4f8b\uff08\u5982IP\u5730\u5740\uff09\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6cc4\u9732\u7387\uff0c\u800c\u8f83\u96be\u7684\u7c7b\u578b\uff08\u5982\u5bc6\u94a5\u548c\u5bc6\u7801\uff09\u6cc4\u9732\u9891\u7387\u8f83\u4f4e\u3002\u6a21\u7cca\u7c7b\u578b\u8868\u73b0\u51fa\u6df7\u5408\u884c\u4e3a\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u63d0\u4f9b\u4e86\u6cc4\u9732\u98ce\u9669\u5177\u6709\u7c7b\u578b\u4f9d\u8d56\u6027\u7684\u56e0\u679c\u8bc1\u636e\uff0c\u5e76\u4e3a\u5f00\u53d1\u7c7b\u578b\u611f\u77e5\u548c\u53ef\u5b66\u4e60\u6027\u611f\u77e5\u7684\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u9632\u5fa1\u673a\u5236\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2512.06781", "categories": ["cs.CR", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.06781", "abs": "https://arxiv.org/abs/2512.06781", "authors": ["Sima Jafarikhah", "Daniel Thompson", "Eva Deans", "Hossein Siadati", "Yi Liu"], "title": "From Description to Score: Can LLMs Quantify Vulnerabilities?", "comment": "10 pages", "summary": "Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study investigates the potential of general-purpose large language models (LLMs), namely ChatGPT, Llama, Grok, DeepSeek, and Gemini, to automate this process by analyzing over 31{,}000 recent Common Vulnerabilities and Exposures (CVE) entries. The results show that LLMs substantially outperform the baseline on certain metrics (e.g., \\textit{Availability Impact}), while offering more modest gains on others (e.g., \\textit{Attack Complexity}). Moreover, model performance varies across both LLM families and individual CVSS metrics, with ChatGPT-5 attaining the highest precision. Our analysis reveals that LLMs tend to misclassify many of the same CVEs, and ensemble-based meta-classifiers only marginally improve performance. Further examination shows that CVE descriptions often lack critical context or contain ambiguous phrasing, which contributes to systematic misclassifications. These findings underscore the importance of enhancing vulnerability descriptions and incorporating richer contextual details to support more reliable automated reasoning and alleviate the growing backlog of CVEs awaiting triage.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08ChatGPT\u3001Llama\u3001Grok\u3001DeepSeek\u3001Gemini\uff09\u5728\u81ea\u52a8\u5316\u6f0f\u6d1e\u8bc4\u5206\uff08CVSS\uff09\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLMs\u5728\u67d0\u4e9b\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4f46\u6027\u80fd\u56e0\u6a21\u578b\u548c\u6307\u6807\u800c\u5f02\uff0c\u4e14CVE\u63cf\u8ff0\u7684\u8d28\u91cf\u9650\u5236\u4e86\u81ea\u52a8\u5316\u8bc4\u5206\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u624b\u52a8\u6f0f\u6d1e\u8bc4\u5206\uff08\u5982CVSS\u8bc4\u5206\uff09\u662f\u4e00\u4e2a\u8d44\u6e90\u5bc6\u96c6\u578b\u8fc7\u7a0b\uff0c\u4e14\u5bb9\u6613\u53d7\u5230\u4e3b\u89c2\u89e3\u91ca\u7684\u5f71\u54cd\u3002\u968f\u7740CVE\u79ef\u538b\u4e0d\u65ad\u589e\u52a0\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u51cf\u8f7b\u4eba\u5de5\u8d1f\u62c5\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e8631,000\u591a\u4e2a\u6700\u8fd1\u7684CVE\u6761\u76ee\uff0c\u4f7f\u7528\u4e94\u79cd\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08ChatGPT\u3001Llama\u3001Grok\u3001DeepSeek\u3001Gemini\uff09\u8fdb\u884c\u81ea\u52a8\u5316CVSS\u8bc4\u5206\u3002\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u4e0d\u540cCVSS\u6307\u6807\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5c1d\u8bd5\u4e86\u96c6\u6210\u5143\u5206\u7c7b\u5668\u6765\u63d0\u5347\u6027\u80fd\u3002", "result": "LLMs\u5728\u67d0\u4e9b\u6307\u6807\uff08\u5982\u53ef\u7528\u6027\u5f71\u54cd\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4f46\u5728\u5176\u4ed6\u6307\u6807\uff08\u5982\u653b\u51fb\u590d\u6742\u5ea6\uff09\u4e0a\u63d0\u5347\u6709\u9650\u3002ChatGPT-5\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u7cbe\u786e\u5ea6\u3002\u6a21\u578b\u503e\u5411\u4e8e\u9519\u8bef\u5206\u7c7b\u76f8\u540c\u7684CVE\uff0c\u96c6\u6210\u65b9\u6cd5\u4ec5\u7565\u5fae\u6539\u5584\u6027\u80fd\u3002\u5206\u6790\u53d1\u73b0CVE\u63cf\u8ff0\u5e38\u7f3a\u4e4f\u5173\u952e\u4e0a\u4e0b\u6587\u6216\u5305\u542b\u6a21\u7cca\u63aa\u8f9e\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6027\u9519\u8bef\u5206\u7c7b\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6539\u8fdb\u6f0f\u6d1e\u63cf\u8ff0\u8d28\u91cf\u548c\u7eb3\u5165\u66f4\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u652f\u6301\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u5316\u63a8\u7406\uff0c\u51cf\u8f7bCVE\u79ef\u538b\u5904\u7406\u7684\u8d1f\u62c5\u3002\u867d\u7136LLMs\u5728\u81ea\u52a8\u5316\u6f0f\u6d1e\u8bc4\u5206\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u4ecd\u53d7\u9650\u4e8eCVE\u63cf\u8ff0\u7684\u8d28\u91cf\u95ee\u9898\u3002"}}
{"id": "2512.06859", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06859", "abs": "https://arxiv.org/abs/2512.06859", "authors": ["Ce Chi", "Xing Wang", "Zhendong Wang", "Xiaofan Liu", "Ce Li", "Zhiyan Song", "Chen Zhao", "Kexin Yang", "Boshen Shi", "Jingjing Yang", "Chao Deng", "Junlan Feng"], "title": "JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models", "comment": null, "summary": "In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.", "AI": {"tldr": "JT-DA-8B\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u590d\u6742\u8868\u683c\u63a8\u7406\u4efb\u52a1\u76848B\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b34\u4e2a\u8868\u683c\u63a8\u7406\u4efb\u52a1\u7684\u591a\u6837\u5316\u8bad\u7ec3\u8bed\u6599\uff0c\u7ed3\u5408SFT\u548cRL\u4f18\u5316\uff0c\u5728\u591a\u79cd\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u9488\u5bf9\u8868\u683c\u63a8\u7406\u573a\u666f\u4e2d\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u9700\u8981\u6784\u5efa\u4e13\u95e8\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6765\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u8868\u683c\u63a8\u7406\u4efb\u52a1\u3002", "method": "1) \u6784\u5efa\u5305\u542b29\u4e2a\u516c\u5f00\u8868\u683cQA\u6570\u636e\u96c6\u548c300\u4e07\u5f20\u8868\u683c\u7684\u591a\u6837\u5316\u8bad\u7ec3\u8bed\u6599\uff1b2) \u63d0\u51fa\u81ea\u52a8\u6d41\u6c34\u7ebf\u751f\u6210\u591a\u6b65\u5206\u6790\u4efb\u52a1\uff1b3) \u57fa\u4e8eJT-Coder-8B\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\uff1b4) \u4f7f\u7528LLM\u8bc4\u5206\u548c\u5de5\u4f5c\u6d41\u5bf9\u9f50\u8fc7\u6ee4\u6765\u63d0\u70bc\u9ad8\u8d28\u91cf\u8868\u683c\u6570\u636e\uff1b5) \u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u4f18\u5316\u6a21\u578b\uff1b6) \u63d0\u51fa\u56db\u9636\u6bb5\u8868\u683c\u63a8\u7406\u5de5\u4f5c\u6d41\uff08\u8868\u683c\u9884\u5904\u7406\u3001\u8868\u683c\u611f\u77e5\u3001\u5de5\u5177\u96c6\u6210\u63a8\u7406\u3001\u63d0\u793a\u5de5\u7a0b\uff09\u3002", "result": "JT-DA-8B\u5728\u5404\u79cd\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u4e2d\u5fc3\u751f\u6210\u548c\u5de5\u4f5c\u6d41\u9a71\u52a8\u4f18\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5168\u9762\u7684\u8bad\u7ec3\u8bed\u6599\u3001\u91c7\u7528\u6570\u636e\u8d28\u91cf\u7b5b\u9009\u673a\u5236\u3001\u7ed3\u5408SFT\u548cRL\u4f18\u5316\u4ee5\u53ca\u8bbe\u8ba1\u7cfb\u7edf\u5316\u63a8\u7406\u5de5\u4f5c\u6d41\uff0cJT-DA-8B\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u8868\u683c\u63a8\u7406\u4efb\u52a1\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u8868\u683c\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07824", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07824", "abs": "https://arxiv.org/abs/2512.07824", "authors": ["Rabe Abdalkareem"], "title": "Studying the Role of Reusing Crowdsourcing Knowledge in Software Development", "comment": null, "summary": "Crowdsourcing platforms, such as Stack Overflow, have changed and impacted the software development practice. In these platforms, developers share and reuse their software development and programming experience. Therefore, a plethora of research work focused on crowdsourcing in software engineering and showed that, among other things, crowdsourced development tends to increase developers' productivity and reduce time-to-market. However, in crowdsourcing, the empirical studies of software quality are lacking, and simple questions, such as what developers use the crowdsourcing knowledge for, are unanswered.\n  Therefore, our research focused on studying the impact of reusing crowdsourcing knowledge on software projects. To do so, we conduct several large-scale empirical studies on some of the well-known crowdsourcing platforms, including Stack Overflow and npm. Our results showed that reusing knowledge from these crowdsourcing platforms has the potential to assist software development practice, specifically in the form of reusing crowdsourced code. However, using such knowledge affects the quality of the software in several aspects, such as making the software projects suffer from dependency overhead and increasing the maintenance effort. Based on these findings, we use the gained knowledge to make sound data-driven decisions where we examine software quality assurance methods to mitigate the risk of relying on crowd sourcing knowledge in software development. We examine the use of continuous integration (CI). Our analysis showed how CI can be improved to increase developers' productivity and save their resources.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\uff0c\u91cd\u7528Stack Overflow\u548cnpm\u7b49\u4f17\u5305\u5e73\u53f0\u77e5\u8bc6\u80fd\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u4f1a\u589e\u52a0\u4f9d\u8d56\u5f00\u9500\u548c\u7ef4\u62a4\u6210\u672c\uff0c\u9700\u8981\u6539\u8fdb\u6301\u7eed\u96c6\u6210\u6765\u964d\u4f4e\u98ce\u9669\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4f17\u5305\u5bf9\u5f00\u53d1\u6548\u7387\u548c\u4e0a\u5e02\u65f6\u95f4\u7684\u5f71\u54cd\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u4e14\u4e0d\u6e05\u695a\u5f00\u53d1\u8005\u5982\u4f55\u4f7f\u7528\u4f17\u5305\u77e5\u8bc6\u3002", "method": "\u5bf9Stack Overflow\u548cnpm\u7b49\u77e5\u540d\u4f17\u5305\u5e73\u53f0\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u91cd\u7528\u4f17\u5305\u77e5\u8bc6\u5bf9\u8f6f\u4ef6\u9879\u76ee\u7684\u5f71\u54cd\u3002", "result": "\u91cd\u7528\u4f17\u5305\u77e5\u8bc6\uff08\u7279\u522b\u662f\u4ee3\u7801\uff09\u80fd\u8f85\u52a9\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\uff0c\u4f46\u4f1a\u5bfc\u81f4\u8f6f\u4ef6\u9879\u76ee\u9762\u4e34\u4f9d\u8d56\u5f00\u9500\u589e\u52a0\u3001\u7ef4\u62a4\u5de5\u4f5c\u91cf\u4e0a\u5347\u7b49\u8d28\u91cf\u95ee\u9898\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u6539\u8fdb\u6301\u7eed\u96c6\u6210\uff08CI\uff09\u7b49\u8f6f\u4ef6\u8d28\u91cf\u4fdd\u8bc1\u65b9\u6cd5\uff0c\u53ef\u4ee5\u964d\u4f4e\u4f9d\u8d56\u4f17\u5305\u77e5\u8bc6\u7684\u98ce\u9669\uff0c\u63d0\u9ad8\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u5e76\u8282\u7ea6\u8d44\u6e90\u3002"}}
{"id": "2512.06846", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.06846", "abs": "https://arxiv.org/abs/2512.06846", "authors": ["Xiaoqi Li", "Hailu Kuang", "Wenkai Li", "Zongwei Li", "Shipeng Ye"], "title": "CKG-LLM: LLM-Assisted Detection of Smart Contract Access Control Vulnerabilities Based on Knowledge Graphs", "comment": "6 pages, 4 figures", "summary": "Traditional approaches for smart contract analysis often rely on intermediate representations such as abstract syntax trees, control-flow graphs, or static single assignment form. However, these methods face limitations in capturing both semantic structures and control logic. Knowledge graphs, by contrast, offer a structured representation of entities and relations, enabling richer intermediate abstractions of contract code and supporting the use of graph query languages to identify rule-violating elements. This paper presents CKG-LLM, a framework for detecting access-control vulnerabilities in smart contracts. Leveraging the reasoning and code generation capabilities of large language models, CKG-LLM translates natural-language vulnerability patterns into executable queries over contract knowledge graphs to automatically locate vulnerable code elements. Experimental evaluation demonstrates that CKG-LLM achieves superior performance in detecting access-control vulnerabilities compared to existing tools. Finally, we discuss potential extensions of CKG-LLM as part of future research directions.", "AI": {"tldr": "CKG-LLM\uff1a\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u667a\u80fd\u5408\u7ea6\u8bbf\u95ee\u63a7\u5236\u6f0f\u6d1e\u7684\u6846\u67b6", "motivation": "\u4f20\u7edf\u667a\u80fd\u5408\u7ea6\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u4e2d\u95f4\u8868\u793a\uff08\u5982\u62bd\u8c61\u8bed\u6cd5\u6811\u3001\u63a7\u5236\u6d41\u56fe\u7b49\uff09\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6355\u6349\u8bed\u4e49\u7ed3\u6784\u548c\u63a7\u5236\u903b\u8f91\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u77e5\u8bc6\u56fe\u8c31\u80fd\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u5b9e\u4f53\u5173\u7cfb\u7ed3\u6784\u5316\u8868\u793a\uff0c\u652f\u6301\u4f7f\u7528\u56fe\u67e5\u8be2\u8bed\u8a00\u8bc6\u522b\u8fdd\u89c4\u5143\u7d20\u3002", "method": "\u63d0\u51faCKG-LLM\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6f0f\u6d1e\u6a21\u5f0f\u8f6c\u6362\u4e3a\u53ef\u5728\u5408\u7ea6\u77e5\u8bc6\u56fe\u8c31\u4e0a\u6267\u884c\u7684\u53ef\u6267\u884c\u67e5\u8be2\uff0c\u81ea\u52a8\u5b9a\u4f4d\u6613\u53d7\u653b\u51fb\u7684\u4ee3\u7801\u5143\u7d20\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cCKG-LLM\u5728\u68c0\u6d4b\u8bbf\u95ee\u63a7\u5236\u6f0f\u6d1e\u65b9\u9762\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "CKG-LLM\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u8868\u793a\u548cLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.06867", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06867", "abs": "https://arxiv.org/abs/2512.06867", "authors": ["John Licato", "Stephen Steinle", "Brayden Hollis"], "title": "Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?", "comment": "Accepted at IJCNLP-AACL 2025", "summary": "Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u683c\u63d0\u793a\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5bf9\u6218\u7565\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u4e16\u754c\u7edf\u6cbb\u68cb\u76d8\u6e38\u620fPERIL\u4e2d\uff0c\u67d0\u4e9b\u4e0e\u6218\u7565\u601d\u7ef4\u76f8\u5173\u7684\u4eba\u683c\u80fd\u63d0\u5347\u6e38\u620f\u8868\u73b0\uff0c\u4f46\u9700\u8981\u4e2d\u4ecb\u673a\u5236\u5c06\u4eba\u683c\u8f6c\u5316\u4e3a\u542f\u53d1\u5f0f\u503c\u3002", "motivation": "\u867d\u7136\u4eba\u683c\u63d0\u793a\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4f3c\u4e4e\u80fd\u89e6\u53d1\u4e0d\u540c\u7684\u6587\u672c\u751f\u6210\u98ce\u683c\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u98ce\u683c\u662f\u5426\u8f6c\u5316\u4e3a\u53ef\u6d4b\u91cf\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u5bf9\u6297\u6027\u6218\u7565\u73af\u5883\u4e2d\u5982\u4f55\u5f71\u54cd\u51b3\u7b56\u5236\u5b9a\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u7ffb\u8bd1\u8fc7\u7a0b\u4f5c\u4e3a\u4e2d\u4ecb\u673a\u5236\uff0c\u53d7\u63a2\u7d22\u6027\u56e0\u5b50\u5206\u6790\u542f\u53d1\uff0c\u5c06LLM\u751f\u6210\u7684\u5e93\u5b58\u54cd\u5e94\u6620\u5c04\u4e3a\u542f\u53d1\u5f0f\u503c\uff0c\u6bd4\u8f83\u4eba\u683c\u884d\u751f\u7684\u542f\u53d1\u5f0f\u7b56\u7565\u4e0e\u624b\u52a8\u9009\u62e9\u7684\u7b56\u7565\u3002", "result": "\u67d0\u4e9b\u4e0e\u6218\u7565\u601d\u7ef4\u76f8\u5173\u7684\u4eba\u683c\u786e\u5b9e\u80fd\u63d0\u9ad8\u6e38\u620f\u8868\u73b0\uff0c\u4f46\u4ec5\u5f53\u4f7f\u7528\u4e2d\u4ecb\u673a\u5236\u5c06\u4eba\u683c\u8f6c\u5316\u4e3a\u542f\u53d1\u5f0f\u503c\u65f6\uff1b\u7ed3\u6784\u5316\u7ffb\u8bd1\u65b9\u6cd5\u76f8\u6bd4\u76f4\u63a5\u63a8\u65ad\u7684\u542f\u53d1\u5f0f\u5177\u6709\u66f4\u9ad8\u7684\u53ef\u9760\u6027\u548c\u8868\u9762\u6548\u5ea6\u3002", "conclusion": "\u4eba\u683c\u63d0\u793a\u786e\u5b9e\u5f71\u54cdLLM\u7684\u51b3\u7b56\u5236\u5b9a\uff0c\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u751f\u6210\u65b9\u6cd5\u5c06\u5fc3\u7406\u6d4b\u91cf\u5b66\u539f\u7406\u5e94\u7528\u4e8eLLM\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7814\u7a76\u4eba\u683c\u7c7b\u578b\u5bf9\u51b3\u7b56\u7684\u5f71\u54cd\u3002"}}
{"id": "2512.06899", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.06899", "abs": "https://arxiv.org/abs/2512.06899", "authors": ["Tianhang Zhao", "Wei Du", "Haodong Zhao", "Sufeng Duan", "Gongshen Liu"], "title": "Patronus: Identifying and Mitigating Transferable Backdoors in Pre-trained Language Models", "comment": "Work in progress", "summary": "Transferable backdoors pose a severe threat to the Pre-trained Language Models (PLMs) supply chain, yet defensive research remains nascent, primarily relying on detecting anomalies in the output feature space. We identify a critical flaw that fine-tuning on downstream tasks inevitably modifies model parameters, shifting the output distribution and rendering pre-computed defense ineffective. To address this, we propose Patronus, a novel framework that use input-side invariance of triggers against parameter shifts. To overcome the convergence challenges of discrete text optimization, Patronus introduces a multi-trigger contrastive search algorithm that effectively bridges gradient-based optimization with contrastive learning objectives. Furthermore, we employ a dual-stage mitigation strategy combining real-time input monitoring with model purification via adversarial training. Extensive experiments across 15 PLMs and 10 tasks demonstrate that Patronus achieves $\\geq98.7\\%$ backdoor detection recall and reduce attack success rates to clean settings, significantly outperforming all state-of-the-art baselines in all settings. Code is available at https://github.com/zth855/Patronus.", "AI": {"tldr": "Patronus\u662f\u4e00\u4e2a\u9488\u5bf9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u540e\u95e8\u653b\u51fb\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u5165\u4fa7\u89e6\u53d1\u5668\u4e0d\u53d8\u6027\u5e94\u5bf9\u53c2\u6570\u53d8\u5316\uff0c\u4f7f\u7528\u591a\u89e6\u53d1\u5668\u5bf9\u6bd4\u641c\u7d22\u7b97\u6cd5\uff0c\u7ed3\u5408\u5b9e\u65f6\u8f93\u5165\u76d1\u63a7\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u572815\u4e2aPLM\u548c10\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u226598.7%\u7684\u540e\u95e8\u68c0\u6d4b\u53ec\u56de\u7387\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u9632\u5fa1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8f93\u51fa\u7279\u5f81\u7a7a\u95f4\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u4f46\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u4f1a\u6539\u53d8\u6a21\u578b\u53c2\u6570\uff0c\u5bfc\u81f4\u8f93\u51fa\u5206\u5e03\u504f\u79fb\uff0c\u4f7f\u9884\u8ba1\u7b97\u7684\u9632\u5fa1\u5931\u6548\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5e94\u5bf9\u53c2\u6570\u53d8\u5316\u7684\u9c81\u68d2\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPatronus\u6846\u67b6\uff1a1) \u5229\u7528\u89e6\u53d1\u5668\u5728\u8f93\u5165\u4fa7\u5bf9\u53c2\u6570\u53d8\u5316\u7684\u4e0d\u53d8\u6027\uff1b2) \u5f15\u5165\u591a\u89e6\u53d1\u5668\u5bf9\u6bd4\u641c\u7d22\u7b97\u6cd5\uff0c\u5c06\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u7ed3\u5408\uff0c\u89e3\u51b3\u79bb\u6563\u6587\u672c\u4f18\u5316\u6536\u655b\u95ee\u9898\uff1b3) \u91c7\u7528\u53cc\u9636\u6bb5\u7f13\u89e3\u7b56\u7565\uff0c\u7ed3\u5408\u5b9e\u65f6\u8f93\u5165\u76d1\u63a7\u548c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u7684\u6a21\u578b\u51c0\u5316\u3002", "result": "\u572815\u4e2a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c10\u4e2a\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPatronus\u8fbe\u5230\u226598.7%\u7684\u540e\u95e8\u68c0\u6d4b\u53ec\u56de\u7387\uff0c\u5e76\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u5230\u5e72\u51c0\u8bbe\u7f6e\u6c34\u5e73\uff0c\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u663e\u8457\u4f18\u4e8e\u6240\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Patronus\u901a\u8fc7\u8f93\u5165\u4fa7\u4e0d\u53d8\u6027\u548c\u521b\u65b0\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u540e\u95e8\u9632\u5fa1\u65b9\u6cd5\u5728\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u540e\u5931\u6548\u7684\u95ee\u9898\uff0c\u4e3aPLM\u4f9b\u5e94\u94fe\u5b89\u5168\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2512.06983", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06983", "abs": "https://arxiv.org/abs/2512.06983", "authors": ["Eli J. Laird", "Corey Clark"], "title": "On Memory: A comparison of memory mechanisms in world models", "comment": "10 pages, 1 figure", "summary": "World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u57fa\u4e8eTransformer\u7684\u4e16\u754c\u6a21\u578b\u7684\u6709\u6548\u8bb0\u5fc6\u8de8\u5ea6\uff0c\u901a\u8fc7\u5f15\u5165\u8bb0\u5fc6\u589e\u5f3a\u673a\u5236\u7684\u5206\u7c7b\u6cd5\uff0c\u63a2\u8ba8\u4e86\u8bb0\u5fc6\u7f16\u7801\u548c\u8bb0\u5fc6\u6ce8\u5165\u673a\u5236\u5982\u4f55\u6269\u5c55\u4e16\u754c\u6a21\u578b\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u673a\u5236\u5728\u89c6\u89c9Transformer\u4e2d\u6539\u5584\u957f\u671f\u89c4\u5212\u7684\u6548\u679c\u3002", "motivation": "\u4e16\u754c\u6a21\u578b\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u60f3\u8c61\u73af\u5883\u4e2d\u8fdb\u884c\u89c4\u5212\uff0c\u4f46\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u6709\u6548\u8bb0\u5fc6\u8de8\u5ea6\u6709\u9650\uff0c\u5bfc\u81f4\u957f\u5e8f\u5217\u63a8\u6f14\u4e2d\u51fa\u73b0\u611f\u77e5\u6f02\u79fb\uff0c\u963b\u788d\u4e86\u5728\u60f3\u8c61\u8f68\u8ff9\u4e2d\u5b8c\u6210\u73af\u8def\u95ed\u5408\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u591a\u79cd\u8bb0\u5fc6\u589e\u5f3a\u673a\u5236\uff0c\u5f15\u5165\u4e86\u533a\u5206\u8bb0\u5fc6\u7f16\u7801\u548c\u8bb0\u5fc6\u6ce8\u5165\u673a\u5236\u7684\u5206\u7c7b\u6cd5\uff0c\u4ece\u6b8b\u5dee\u6d41\u52a8\u6001\u7684\u89d2\u5ea6\u7406\u89e3\u5b83\u4eec\u6269\u5c55\u4e16\u754c\u6a21\u578b\u8bb0\u5fc6\u7684\u4f5c\u7528\uff0c\u5e76\u4f7f\u7528\u72b6\u6001\u56de\u5fc6\u8bc4\u4f30\u4efb\u52a1\u6d4b\u91cf\u6bcf\u79cd\u673a\u5236\u7684\u8bb0\u5fc6\u56de\u5fc6\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bb0\u5fc6\u673a\u5236\u80fd\u591f\u6539\u5584\u89c6\u89c9Transformer\u4e2d\u7684\u6709\u6548\u8bb0\u5fc6\u8de8\u5ea6\uff0c\u5e76\u4e3a\u5728\u4e16\u754c\u6a21\u578b\u7684\u60f3\u8c61\u4e2d\u5b8c\u6210\u73af\u8def\u95ed\u5408\u63d0\u4f9b\u4e86\u8def\u5f84\u3002", "conclusion": "\u8bb0\u5fc6\u589e\u5f3a\u673a\u5236\u5bf9\u4e8e\u6269\u5c55\u57fa\u4e8eTransformer\u7684\u4e16\u754c\u6a21\u578b\u7684\u957f\u671f\u89c4\u5212\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u5b9e\u73b0\u73af\u8def\u95ed\u5408\u548c\u51cf\u5c11\u611f\u77e5\u6f02\u79fb\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2512.07030", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07030", "abs": "https://arxiv.org/abs/2512.07030", "authors": ["Zahra Lotfi", "Mostafa Lotfi"], "title": "A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data", "comment": "13 pages, 5 figures", "summary": "Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e94\u79cd\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5728\u68c0\u6d4b\u96f6\u65e5\u653b\u51fb\u65b9\u9762\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u7f51\u683c\u641c\u7d22\u3001\u964d\u7ef4\u548c\u8fc7\u91c7\u6837\u6280\u672f\u63d0\u5347\u6a21\u578b\u6548\u679c\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u6027\u80fd\u6700\u4f73\u4f46\u5904\u7406\u65f6\u95f4\u8f83\u957f\uff0c\u6700\u7ec8\u9009\u62e9XGBoost\u4f5c\u4e3a\u6700\u4f73\u6a21\u578b\u3002", "motivation": "\u96f6\u65e5\u653b\u51fb\u96be\u4ee5\u68c0\u6d4b\uff0c\u56e0\u4e3a\u5176\u6a21\u5f0f\u548c\u7279\u5f81\u4e0e\u5df2\u77e5\u653b\u51fb\u4e0d\u5339\u914d\u3002\u73b0\u6709\u7684\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5728\u8bad\u7ec3\u9636\u6bb5\u5b66\u4e60\u5df2\u77e5\u653b\u51fb\u6a21\u5f0f\uff0c\u5bf9\u672a\u89c1\u8fc7\u7684\u96f6\u65e5\u653b\u51fb\u68c0\u6d4b\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u5728\u96f6\u65e5\u653b\u51fb\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "1. \u8bc4\u4f30\u4e94\u79cd\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5728\u96f6\u65e5\u653b\u51fb\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u548c\u6267\u884c\u65f6\u95f4\uff1b2. \u63d0\u51fa\u5305\u542b\u7f51\u683c\u641c\u7d22\u3001\u964d\u7ef4\u548c\u8fc7\u91c7\u6837\u65b9\u6cd5\u7684\u6846\u67b6\u4ee5\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff1b3. \u4f7f\u7528\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u6570\u636e\u96c6\u6a21\u62df\u771f\u5b9e\u653b\u51fb\u68c0\u6d4b\u573a\u666f\uff1b4. \u4ec5\u5728\u6d4b\u8bd5\u9636\u6bb5\u66b4\u9732\u96f6\u65e5\u653b\u51fb\uff0c\u6a21\u578b\u5728\u8bad\u7ec3\u9636\u6bb5\u4e0d\u63a5\u89e6\u8fd9\u4e9b\u653b\u51fb\u3002", "result": "1. \u968f\u673a\u68ee\u6797\u5728\u8fc7\u91c7\u6837\u548c\u975e\u8fc7\u91c7\u6837\u6761\u4ef6\u4e0b\u90fd\u8868\u73b0\u6700\u4f73\uff1b2. \u968f\u673a\u68ee\u6797\u6027\u80fd\u63d0\u5347\u7684\u4ee3\u4ef7\u662f\u66f4\u957f\u7684\u5904\u7406\u65f6\u95f4\uff1b3. XGBoost\u56e0\u5feb\u901f\u4e14\u9ad8\u7cbe\u5ea6\u7684\u96f6\u65e5\u653b\u51fb\u68c0\u6d4b\u6027\u80fd\u88ab\u9009\u4e3a\u6700\u4f73\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u53d1\u73b0\uff0c\u867d\u7136\u968f\u673a\u68ee\u6797\u5728\u96f6\u65e5\u653b\u51fb\u68c0\u6d4b\u4e2d\u6027\u80fd\u6700\u4f18\uff0c\u4f46XGBoost\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u68c0\u6d4b\u96f6\u65e5\u653b\u51fb\u7684\u4f18\u9009\u6a21\u578b\u3002\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5bf9\u672a\u77e5\u653b\u51fb\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2512.07081", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07081", "abs": "https://arxiv.org/abs/2512.07081", "authors": ["Rongjia Zhou", "Chengzhuo Li", "Carl Yang", "Jiaying Lu"], "title": "ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes", "comment": "10 pages, 2 figures. Submitted to AMIA 2026 Informatics Summit Student Paper Track", "summary": "Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.", "AI": {"tldr": "ClinNoteAgents\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u81ea\u7531\u6587\u672c\u4e34\u5e8a\u7b14\u8bb0\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u98ce\u9669\u56e0\u7d20\u8868\u793a\u548c\u4e34\u5e8a\u533b\u751f\u98ce\u683c\u62bd\u8c61\uff0c\u7528\u4e8e\u5fc3\u887030\u5929\u518d\u5165\u9662\u9884\u6d4b\uff0c\u5728\u6570\u636e\u6709\u9650\u7684\u533b\u7597\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u3002", "motivation": "\u5fc3\u8870\u662f\u7f8e\u56fd\u8001\u5e74\u4eba\u518d\u5165\u9662\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\uff0c\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u4e30\u5bcc\u7684\u60a3\u8005\u4fe1\u606f\u4f46\u672a\u5145\u5206\u5229\u7528\u3002\u4f20\u7edf\u6a21\u578b\u4f9d\u8d56\u4e13\u5bb6\u89c4\u5219\u3001\u533b\u5b66\u672f\u8bed\u548c\u672c\u4f53\u6765\u89e3\u91ca\u4e34\u5e8a\u7b14\u8bb0\uff0c\u4f46\u7b14\u8bb0\u4e2d\u5e38\u5305\u542b\u62fc\u5199\u9519\u8bef\u3001\u7f29\u5199\u548c\u9886\u57df\u7279\u5b9a\u672f\u8bed\u3002", "method": "\u63d0\u51faClinNoteAgents\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u81ea\u7531\u6587\u672c\u4e34\u5e8a\u7b14\u8bb0\u8f6c\u5316\u4e3a\uff1a(1) \u4e34\u5e8a\u548c\u793e\u4f1a\u98ce\u9669\u56e0\u7d20\u7684\u7ed3\u6784\u5316\u8868\u793a\u7528\u4e8e\u5173\u8054\u5206\u6790\uff1b(2) \u4e34\u5e8a\u533b\u751f\u98ce\u683c\u62bd\u8c61\u7528\u4e8e\u5fc3\u887030\u5929\u518d\u5165\u9662\u9884\u6d4b\u3002", "result": "\u57283,544\u4efd\u6765\u81ea2,065\u540d\u60a3\u8005\uff08\u518d\u5165\u9662\u738735.16%\uff09\u7684\u7b14\u8bb0\u4e0a\u8bc4\u4f30\uff0c\u5728\u4ece\u81ea\u7531\u6587\u672c\u63d0\u53d6\u98ce\u9669\u56e0\u7d20\u3001\u8bc6\u522b\u5173\u952e\u8d21\u732e\u56e0\u7d20\u548c\u9884\u6d4b\u518d\u5165\u9662\u98ce\u9669\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u901a\u8fc7\u51cf\u5c11\u5bf9\u7ed3\u6784\u5316\u5b57\u6bb5\u7684\u4f9d\u8d56\u5e76\u6700\u5c0f\u5316\u624b\u52a8\u6807\u6ce8\u548c\u6a21\u578b\u8bad\u7ec3\uff0cClinNoteAgents\u4e3a\u6570\u636e\u6709\u9650\u7684\u533b\u7597\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u57fa\u4e8e\u7b14\u8bb0\u7684\u5fc3\u8870\u518d\u5165\u9662\u98ce\u9669\u5efa\u6a21\u65b9\u6cd5\u3002"}}
{"id": "2512.07033", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.07033", "abs": "https://arxiv.org/abs/2512.07033", "authors": ["Daniyal Ganiuly", "Nurzhau Bolatbek", "Assel Smaiyl"], "title": "Managed TLS Under Migration: Authentication Authority Across CDN and Hosting Transitions", "comment": null, "summary": "Managed TLS has become a common approach for deploying HTTPS, with platforms generating and storing private keys and automating certificate issuance on behalf of domain operators. This model simplifies operational management but shifts control of authentication material from the domain owner to the platform. The implications of this shift during provider transitions remain insufficiently examined. This study investigates how managed TLS platforms behave when a domain is moved away from the platform that originally issued and stored its certificate. A controlled measurement environment was used to monitor multiple platforms after migration. Each platform was observed for the full remaining lifetime of the certificate that had been active during delegation. The measurements show that platforms continue to serve the same certificate until it expires, even after DNS resolvers direct traffic toward new infrastructure. No platform revoked, replaced, or retired the certificate, and no new certificate was issued after delegation ended. Direct connections to the previous platform continued to complete TLS handshakes with the stale certificate, which confirms that authentication capability persisted independently of DNS state. These findings indicate that authentication authority remains with the previous platform for the entire lifetime of certificates issued during the delegation period. The gap between DNS control and control of authentication material introduces a window in which multiple environments can authenticate the same domain. As managed TLS adoption grows, clearer mechanisms for key retirement and certificate invalidation are needed to ensure that the authentication authority follows operational authority during transitions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6258\u7ba1TLS\u5e73\u53f0\u5728\u57df\u540d\u8fc1\u79fb\u540e\u4ecd\u7ee7\u7eed\u4f7f\u7528\u539f\u8bc1\u4e66\u76f4\u5230\u8fc7\u671f\uff0c\u5bfc\u81f4\u8ba4\u8bc1\u6743\u9650\u4e0eDNS\u63a7\u5236\u5206\u79bb\uff0c\u5b58\u5728\u591a\u4e2a\u73af\u5883\u53ef\u8ba4\u8bc1\u540c\u4e00\u57df\u540d\u7684\u5b89\u5168\u7a97\u53e3\u3002", "motivation": "\u6258\u7ba1TLS\u7b80\u5316\u4e86HTTPS\u90e8\u7f72\uff0c\u4f46\u5c06\u79c1\u94a5\u548c\u8bc1\u4e66\u63a7\u5236\u6743\u4ece\u57df\u540d\u6240\u6709\u8005\u8f6c\u79fb\u5230\u5e73\u53f0\u3002\u8fd9\u79cd\u63a7\u5236\u6743\u8f6c\u79fb\u5728\u63d0\u4f9b\u5546\u8f6c\u6362\u671f\u95f4\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u9700\u8981\u4e86\u89e3\u6258\u7ba1TLS\u5e73\u53f0\u5728\u57df\u540d\u8fc1\u79fb\u540e\u7684\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u53d7\u63a7\u6d4b\u91cf\u73af\u5883\u76d1\u6d4b\u591a\u4e2a\u6258\u7ba1TLS\u5e73\u53f0\u5728\u57df\u540d\u8fc1\u79fb\u540e\u7684\u884c\u4e3a\u3002\u89c2\u5bdf\u6bcf\u4e2a\u5e73\u53f0\u5728\u59d4\u6258\u671f\u95f4\u9881\u53d1\u7684\u8bc1\u4e66\u7684\u6574\u4e2a\u5269\u4f59\u751f\u547d\u5468\u671f\uff0c\u76d1\u63a7DNS\u89e3\u6790\u5668\u5c06\u6d41\u91cf\u5bfc\u5411\u65b0\u57fa\u7840\u8bbe\u65bd\u540e\u7684\u5e73\u53f0\u53cd\u5e94\u3002", "result": "\u6240\u6709\u5e73\u53f0\u5728\u57df\u540d\u8fc1\u79fb\u540e\u7ee7\u7eed\u63d0\u4f9b\u76f8\u540c\u7684\u8bc1\u4e66\u76f4\u5230\u8fc7\u671f\uff0c\u6ca1\u6709\u5e73\u53f0\u64a4\u9500\u3001\u66ff\u6362\u6216\u505c\u7528\u8bc1\u4e66\uff0c\u59d4\u6258\u7ed3\u675f\u540e\u4e5f\u6ca1\u6709\u9881\u53d1\u65b0\u8bc1\u4e66\u3002\u76f4\u63a5\u8fde\u63a5\u5230\u539f\u5e73\u53f0\u4ecd\u80fd\u4f7f\u7528\u65e7\u8bc1\u4e66\u5b8c\u6210TLS\u63e1\u624b\uff0c\u8ba4\u8bc1\u80fd\u529b\u72ec\u7acb\u4e8eDNS\u72b6\u6001\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "\u8ba4\u8bc1\u6743\u9650\u5728\u6574\u4e2a\u59d4\u6258\u671f\u95f4\u9881\u53d1\u7684\u8bc1\u4e66\u751f\u547d\u5468\u671f\u5185\u4ecd\u4fdd\u7559\u5728\u539f\u5e73\u53f0\u3002DNS\u63a7\u5236\u4e0e\u8ba4\u8bc1\u6750\u6599\u63a7\u5236\u4e4b\u95f4\u7684\u5dee\u8ddd\u5bfc\u81f4\u5b58\u5728\u591a\u4e2a\u73af\u5883\u53ef\u8ba4\u8bc1\u540c\u4e00\u57df\u540d\u7684\u7a97\u53e3\u671f\u3002\u968f\u7740\u6258\u7ba1TLS\u91c7\u7528\u589e\u957f\uff0c\u9700\u8981\u66f4\u6e05\u6670\u7684\u5bc6\u94a5\u9000\u5f79\u548c\u8bc1\u4e66\u5931\u6548\u673a\u5236\uff0c\u786e\u4fdd\u8ba4\u8bc1\u6743\u9650\u5728\u8f6c\u6362\u671f\u95f4\u8ddf\u968f\u64cd\u4f5c\u6743\u9650\u3002"}}
{"id": "2512.07038", "categories": ["cs.CR", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.07038", "abs": "https://arxiv.org/abs/2512.07038", "authors": ["Min Jae Song", "Kameron Shahabi"], "title": "Ideal Attribution and Faithful Watermarks for Language Models", "comment": "30 pages", "summary": "We introduce ideal attribution mechanisms, a formal abstraction for reasoning about attribution decisions over strings. At the core of this abstraction lies the ledger, an append-only log of the prompt-response interaction history between a model and its user. Each mechanism produces deterministic decisions based on the ledger and an explicit selection criterion, making it well-suited to serve as a ground truth for attribution. We frame the design goal of watermarking schemes as faithful representation of ideal attribution mechanisms. This novel perspective brings conceptual clarity, replacing piecemeal probabilistic statements with a unified language for stating the guarantees of each scheme. It also enables precise reasoning about desiderata for future watermarking schemes, even when no current construction achieves them, since the ideal functionalities are specified first. In this way, the framework provides a roadmap that clarifies which guarantees are attainable in an idealized setting and worth pursuing in practice.", "AI": {"tldr": "\u63d0\u51fa\u7406\u60f3\u5f52\u56e0\u673a\u5236\u4f5c\u4e3a\u5b57\u7b26\u4e32\u5f52\u56e0\u51b3\u7b56\u7684\u5f62\u5f0f\u5316\u62bd\u8c61\uff0c\u4ee5\u8d26\u672c\u8bb0\u5f55\u6a21\u578b\u4e0e\u7528\u6237\u7684\u4ea4\u4e92\u5386\u53f2\uff0c\u4e3a\u6c34\u5370\u65b9\u6848\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u548c\u8bbe\u8ba1\u8def\u7ebf\u56fe\u3002", "motivation": "\u5f53\u524d\u6c34\u5370\u65b9\u6848\u7f3a\u4e4f\u7edf\u4e00\u7684\u5f52\u56e0\u4fdd\u8bc1\u6846\u67b6\uff0c\u9700\u8981\u5efa\u7acb\u5f62\u5f0f\u5316\u62bd\u8c61\u6765\u6f84\u6e05\u4e0d\u540c\u65b9\u6848\u7684\u53ef\u5b9e\u73b0\u4fdd\u8bc1\uff0c\u4e3a\u672a\u6765\u6c34\u5370\u65b9\u6848\u8bbe\u8ba1\u63d0\u4f9b\u660e\u786e\u76ee\u6807\u3002", "method": "\u5f15\u5165\u7406\u60f3\u5f52\u56e0\u673a\u5236\uff0c\u57fa\u4e8e\u8d26\u672c\uff08\u8bb0\u5f55\u6a21\u578b\u4e0e\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u7684\u53ea\u8ffd\u52a0\u65e5\u5fd7\uff09\u548c\u660e\u786e\u9009\u62e9\u6807\u51c6\u8fdb\u884c\u786e\u5b9a\u6027\u51b3\u7b56\uff0c\u5c06\u6c34\u5370\u65b9\u6848\u8bbe\u8ba1\u76ee\u6807\u6846\u67b6\u5316\u4e3a\u5bf9\u7406\u60f3\u5f52\u56e0\u673a\u5236\u7684\u5fe0\u5b9e\u8868\u793a\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u7528\u7406\u60f3\u5f52\u56e0\u673a\u5236\u66ff\u4ee3\u96f6\u6563\u7684\u6982\u7387\u9648\u8ff0\uff0c\u80fd\u591f\u7cbe\u786e\u63a8\u7406\u672a\u6765\u6c34\u5370\u65b9\u6848\u7684\u671f\u671b\u7279\u6027\uff0c\u5373\u4f7f\u5f53\u524d\u6784\u9020\u5c1a\u672a\u5b9e\u73b0\u8fd9\u4e9b\u7279\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u6982\u5ff5\u6e05\u6670\u5ea6\uff0c\u660e\u786e\u4e86\u5728\u7406\u60f3\u5316\u8bbe\u7f6e\u4e2d\u53ef\u5b9e\u73b0\u7684\u4fdd\u8bc1\uff0c\u4e3a\u5b9e\u8df5\u4e2d\u503c\u5f97\u8ffd\u6c42\u7684\u6c34\u5370\u65b9\u6848\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002"}}
{"id": "2512.07086", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07086", "abs": "https://arxiv.org/abs/2512.07086", "authors": ["Yunzhe Li", "Jianan Wang", "Hongzi Zhu", "James Lin", "Shan Chang", "Minyi Guo"], "title": "ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking", "comment": "This version includes the final camera-ready manuscript accepted by NDSS 2026", "summary": "Large Language Models (LLMs) have become foundational components in a wide range of applications, including natural language understanding and generation, embodied intelligence, and scientific discovery. As their computational requirements continue to grow, these models are increasingly deployed as cloud-based services, allowing users to access powerful LLMs via the Internet. However, this deployment model introduces a new class of threat: denial-of-service (DoS) attacks via unbounded reasoning, where adversaries craft specially designed inputs that cause the model to enter excessively long or infinite generation loops. These attacks can exhaust backend compute resources, degrading or denying service to legitimate users. To mitigate such risks, many LLM providers adopt a closed-source, black-box setting to obscure model internals. In this paper, we propose ThinkTrap, a novel input-space optimization framework for DoS attacks against LLM services even in black-box environments. The core idea of ThinkTrap is to first map discrete tokens into a continuous embedding space, then undertake efficient black-box optimization in a low-dimensional subspace exploiting input sparsity. The goal of this optimization is to identify adversarial prompts that induce extended or non-terminating generation across several state-of-the-art LLMs, achieving DoS with minimal token overhead. We evaluate the proposed attack across multiple commercial, closed-source LLM services. Our results demonstrate that, even far under the restrictive request frequency limits commonly enforced by these platforms, typically capped at ten requests per minute (10 RPM), the attack can degrade service throughput to as low as 1% of its original capacity, and in some cases, induce complete service failure.", "AI": {"tldr": "ThinkTrap\u662f\u4e00\u79cd\u9488\u5bf9\u9ed1\u76d2LLM\u670d\u52a1\u7684DoS\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u5165\u7a7a\u95f4\u4f18\u5316\u8bf1\u5bfc\u6a21\u578b\u8fdb\u5165\u8fc7\u957f\u6216\u65e0\u9650\u751f\u6210\u5faa\u73af\uff0c\u6d88\u8017\u8ba1\u7b97\u8d44\u6e90", "motivation": "\u968f\u7740LLM\u4f5c\u4e3a\u4e91\u670d\u52a1\u90e8\u7f72\uff0c\u51fa\u73b0\u4e86\u901a\u8fc7\u65e0\u754c\u63a8\u7406\u8fdb\u884cDoS\u653b\u51fb\u7684\u65b0\u5a01\u80c1\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u8bbe\u8ba1\u7279\u6b8a\u8f93\u5165\u4f7f\u6a21\u578b\u8fdb\u5165\u8fc7\u957f\u6216\u65e0\u9650\u751f\u6210\u5faa\u73af\uff0c\u8017\u5c3d\u540e\u7aef\u8ba1\u7b97\u8d44\u6e90", "method": "ThinkTrap\u6846\u67b6\u9996\u5148\u5c06\u79bb\u6563token\u6620\u5c04\u5230\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\uff0c\u7136\u540e\u5728\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9ad8\u6548\u7684\u9ed1\u76d2\u4f18\u5316\uff0c\u5229\u7528\u8f93\u5165\u7a00\u758f\u6027\u8bc6\u522b\u80fd\u8bf1\u5bfc\u6269\u5c55\u6216\u975e\u7ec8\u6b62\u751f\u6210\u7684\u5bf9\u6297\u63d0\u793a", "result": "\u653b\u51fb\u5728\u591a\u4e2a\u5546\u4e1a\u95ed\u6e90LLM\u670d\u52a1\u4e0a\u8bc4\u4f30\uff0c\u5373\u4f7f\u5728\u4e25\u683c\u8bf7\u6c42\u9891\u7387\u9650\u5236\u4e0b\uff08\u901a\u5e3810RPM\uff09\uff0c\u4e5f\u80fd\u5c06\u670d\u52a1\u541e\u5410\u91cf\u964d\u81f3\u539f\u5bb9\u91cf\u76841%\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5bfc\u81f4\u5b8c\u5168\u670d\u52a1\u6545\u969c", "conclusion": "ThinkTrap\u5c55\u793a\u4e86\u9ed1\u76d2\u73af\u5883\u4e0b\u5bf9LLM\u670d\u52a1\u8fdb\u884cDoS\u653b\u51fb\u7684\u53ef\u884c\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u90e8\u7f72\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u65b0\u7684\u9632\u5fa1\u673a\u5236"}}
{"id": "2512.07178", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07178", "abs": "https://arxiv.org/abs/2512.07178", "authors": ["Latifa Dwiyanti", "Sergio Ryan Wibisono", "Hidetaka Nambo"], "title": "ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation", "comment": "This paper was accepted and presented at the 7th World Symposium on Software Engineering (WSSE) 2025 on 25 October 2025 in Okayama, Japan, and is currently awaiting publication", "summary": "Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2aPython\u5305\uff0c\u5c06SHAP\u89e3\u91ca\u6846\u67b6\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT\uff09\u96c6\u6210\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u5316\u7684\u6587\u672c\u89e3\u91ca\uff0c\u4ee5\u589e\u5f3a\u975e\u6280\u672f\u7528\u6237\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u89e3\u91ca\u7684\u7406\u89e3\u3002", "motivation": "SHAP\u867d\u7136\u80fd\u6709\u6548\u53ef\u89c6\u5316\u7279\u5f81\u91cd\u8981\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u975e\u6280\u672f\u80cc\u666f\u7ec8\u7aef\u7528\u6237\u6709\u610f\u4e49\u7684\u4e0a\u4e0b\u6587\u89e3\u91ca\u3002\u5f53\u524dXAI\u65b9\u6cd5\u5728\u63d0\u4f9b\u7528\u6237\u53cb\u597d\u7684\u3001\u60c5\u5883\u5316\u7684\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aPython\u5305\uff0c\u5c06SHAP\u4e0eOpenAI\u7684GPT\u96c6\u6210\uff0c\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u7684\u53c2\u6570\uff08\u5982\u7279\u5f81\u522b\u540d\u3001\u63cf\u8ff0\u548c\u80cc\u666f\u4fe1\u606f\uff09\u6765\u751f\u6210\u4e0a\u4e0b\u6587\u5316\u7684\u6587\u672c\u89e3\u91ca\u3002\u5728\u533b\u7597\u76f8\u5173\u6848\u4f8b\u7814\u7a76\u4e2d\u5e94\u7528\uff0c\u5e76\u901a\u8fc7Likert\u91cf\u8868\u8c03\u67e5\u548c\u540e\u7eed\u8bbf\u8c08\u8fdb\u884c\u7528\u6237\u8bc4\u4f30\u3002", "result": "\u7528\u6237\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4ec5\u89c6\u89c9\u8f93\u51fa\u76f8\u6bd4\uff0c\u751f\u6210\u7684\u89e3\u91ca\u88ab\u8ba4\u4e3a\u66f4\u5bb9\u6613\u7406\u89e3\u548c\u66f4\u7b26\u5408\u4e0a\u4e0b\u6587\u3002\u521d\u6b65\u53d1\u73b0\u8868\u660e\uff0c\u53ef\u89c6\u5316\u4e0e\u4e0a\u4e0b\u6587\u5316\u6587\u672c\u7684\u7ed3\u5408\u53ef\u80fd\u652f\u6301\u66f4\u7528\u6237\u53cb\u597d\u548c\u53ef\u4fe1\u7684\u6a21\u578b\u89e3\u91ca\u3002", "conclusion": "\u5c06SHAP\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u53ef\u4ee5\u589e\u5f3a\u89e3\u91ca\u7684\u53ef\u7406\u89e3\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff0c\u7279\u522b\u662f\u5728\u975e\u6280\u672f\u7528\u6237\u7684\u5e94\u7528\u573a\u666f\u4e2d\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u5f00\u53d1\u66f4\u7528\u6237\u53cb\u597d\u7684XAI\u5de5\u5177\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2512.07292", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.07292", "abs": "https://arxiv.org/abs/2512.07292", "authors": ["Felix Oberhansl", "Marc Schink", "Nisha Jacob Kabakci", "Michael Gruber", "Dominik Klein", "Sven Freud", "Tobias Damm", "Michael Hartmeier", "Ivan Gavrilan", "Silvan Streit", "Jonas Stappenbeck", "Andreas Seelos Zankl"], "title": "Breaking ECDSA with Electromagnetic Side-Channel Attacks: Challenges and Practicality on Modern Smartphones", "comment": "This work has been submitted to Euro S&P 2026 for possible publication", "summary": "Smartphones handle sensitive tasks such as messaging and payment and may soon support critical electronic identification through initiatives such as the European Digital Identity (EUDI) wallet, currently under development. Yet the susceptibility of modern smartphones to physical side-channel analysis (SCA) is underexplored, with recent work limited to pre-2019 hardware. Since then, smartphone system on chip (SoC) platforms have grown more complex, with heterogeneous processor clusters, sub 10 nm nodes, and frequencies over 2 GHz, potentially complicating SCA. In this paper, we assess the feasibility of electromagnetic (EM) SCA on a Raspberry Pi 4, featuring a Broadcom BCM2711 SoC and a Fairphone 4 featuring a Snapdragon 750G 5G SoC. Using new attack methodologies tailored to modern SoCs, we recover ECDSA secrets from OpenSSL by mounting the Nonce@Once attack of Alam et al. (Euro S&P 2021) and show that the libgcrypt countermeasure does not fully mitigate it. We present case studies illustrating how hardware and software stacks impact EM SCA feasibility. Motivated by use cases such as the EUDI wallet, we survey Android cryptographic implementations and define representative threat models to assess the attack. Our findings show weaknesses in ECDSA software implementations and underscore the need for independently certified secure elements (SEs) in all smartphones.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u73b0\u4ee3\u667a\u80fd\u624b\u673a\u5bf9\u7535\u78c1\u4fa7\u4fe1\u9053\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u9488\u5bf9Raspberry Pi 4\u548cFairphone 4\u8bbe\u5907\uff0c\u6210\u529f\u6062\u590d\u4e86ECDSA\u5bc6\u94a5\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u8f6f\u4ef6\u9632\u62a4\u63aa\u65bd\u4e0d\u8db3\uff0c\u5f3a\u8c03\u9700\u8981\u72ec\u7acb\u8ba4\u8bc1\u7684\u5b89\u5168\u5143\u4ef6\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u624b\u673a\u5904\u7406\u654f\u611f\u4efb\u52a1\uff08\u5982\u652f\u4ed8\u3001\u6570\u5b57\u8eab\u4efd\u8ba4\u8bc1\uff09\u7684\u589e\u52a0\uff0c\u4ee5\u53ca\u786c\u4ef6\u5e73\u53f0\u53d8\u5f97\u66f4\u52a0\u590d\u6742\uff08\u5f02\u6784\u5904\u7406\u5668\u96c6\u7fa4\u300110\u7eb3\u7c73\u4ee5\u4e0b\u5236\u7a0b\u30012GHz\u4ee5\u4e0a\u9891\u7387\uff09\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u73b0\u4ee3\u667a\u80fd\u624b\u673a\u5bf9\u7269\u7406\u4fa7\u4fe1\u9053\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5373\u5c06\u63a8\u51fa\u7684\u6b27\u6d32\u6570\u5b57\u8eab\u4efd\u94b1\u5305\u7b49\u5173\u952e\u5e94\u7528\u3002", "method": "1. \u4f7f\u7528Raspberry Pi 4\uff08Broadcom BCM2711 SoC\uff09\u548cFairphone 4\uff08Snapdragon 750G 5G SoC\uff09\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff1b2. \u5f00\u53d1\u9488\u5bf9\u73b0\u4ee3SoC\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff1b3. \u5bf9OpenSSL\u5b9e\u65bdNonce@Once\u653b\u51fb\u4ee5\u6062\u590dECDSA\u5bc6\u94a5\uff1b4. \u8bc4\u4f30libgcrypt\u9632\u62a4\u63aa\u65bd\u7684\u6709\u6548\u6027\uff1b5. \u5206\u6790Android\u52a0\u5bc6\u5b9e\u73b0\u5e76\u5b9a\u4e49\u4ee3\u8868\u6027\u5a01\u80c1\u6a21\u578b\u3002", "result": "1. \u6210\u529f\u4eceOpenSSL\u4e2d\u6062\u590dECDSA\u5bc6\u94a5\uff1b2. \u8bc1\u660elibgcrypt\u7684\u9632\u62a4\u63aa\u65bd\u65e0\u6cd5\u5b8c\u5168\u7f13\u89e3\u653b\u51fb\uff1b3. \u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u786c\u4ef6\u548c\u8f6f\u4ef6\u5806\u6808\u5982\u4f55\u5f71\u54cd\u7535\u78c1\u4fa7\u4fe1\u9053\u653b\u51fb\u7684\u53ef\u884c\u6027\uff1b4. \u53d1\u73b0ECDSA\u8f6f\u4ef6\u5b9e\u73b0\u5b58\u5728\u5f31\u70b9\u3002", "conclusion": "\u73b0\u4ee3\u667a\u80fd\u624b\u673a\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u7535\u78c1\u4fa7\u4fe1\u9053\u653b\u51fb\uff0c\u5f53\u524d\u8f6f\u4ef6\u9632\u62a4\u63aa\u65bd\u4e0d\u8db3\u3002\u4e3a\u786e\u4fdd\u654f\u611f\u5e94\u7528\uff08\u5982\u6570\u5b57\u8eab\u4efd\u94b1\u5305\uff09\u7684\u5b89\u5168\uff0c\u6240\u6709\u667a\u80fd\u624b\u673a\u90fd\u9700\u8981\u914d\u5907\u72ec\u7acb\u8ba4\u8bc1\u7684\u5b89\u5168\u5143\u4ef6\u3002"}}
{"id": "2512.07179", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.07179", "abs": "https://arxiv.org/abs/2512.07179", "authors": ["Wonbeen Lee", "Channyoung Lee", "Junho Sohn", "Hansam Cho"], "title": "PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations", "comment": "15 pages, 5 figures, 17 tables. Preparing submission for EDM 2026 conference", "summary": "With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.", "AI": {"tldr": "\u63d0\u51faPICKT\u6a21\u578b\u89e3\u51b3\u77e5\u8bc6\u8ffd\u8e2a\u4e2d\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u5904\u7406\u591a\u79cd\u8f93\u5165\u6570\u636e\u683c\u5f0f\uff0c\u63d0\u5347\u5728\u771f\u5b9eITS\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u968f\u7740\u4e2a\u6027\u5316\u5b66\u4e60\u9700\u6c42\u7684\u589e\u957f\uff0c\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u9700\u8981\u51c6\u786e\u8ffd\u8e2a\u5b66\u751f\u77e5\u8bc6\u72b6\u6001\u5e76\u63d0\u4f9b\u4e2a\u6027\u5316\u5b66\u4e60\u8def\u5f84\u3002\u73b0\u6709\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u5b58\u5728\u8f93\u5165\u6570\u636e\u683c\u5f0f\u53d7\u9650\u3001\u65b0\u5b66\u751f/\u65b0\u95ee\u9898\u51b7\u542f\u52a8\u95ee\u9898\u3001\u4ee5\u53ca\u771f\u5b9e\u670d\u52a1\u73af\u5883\u4e2d\u7a33\u5b9a\u6027\u4e0d\u8db3\u7b49\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u5b9e\u7528\u7684\u4e92\u8fde\u6982\u5ff5\u77e5\u8bc6\u8ffd\u8e2a\uff08PICKT\uff09\u6a21\u578b\uff0c\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u5316\u6982\u5ff5\u95f4\u5173\u7cfb\uff0c\u8003\u8651\u95ee\u9898\u548c\u6982\u5ff5\u6587\u672c\u4fe1\u606f\uff0c\u6709\u6548\u5904\u7406\u591a\u79cd\u7c7b\u578b\u8f93\u5165\u6570\u636e\uff0c\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\u3002", "result": "\u5728\u53cd\u6620\u771f\u5b9e\u64cd\u4f5c\u73af\u5883\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002\u76f8\u6bd4\u73b0\u6709\u6a21\u578b\uff0c\u5728\u65b0\u5b66\u751f\u6ce8\u518c\u548c\u65b0\u95ee\u9898\u6dfb\u52a0\u4e24\u4e2a\u6838\u5fc3\u51b7\u542f\u52a8\u6311\u6218\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u901a\u8fc7\u7cbe\u7ec6\u5b9e\u9a8c\u8bbe\u8ba1\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "PICKT\u6a21\u578b\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u7684\u5b9e\u9645\u5b9e\u65bd\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u548c\u6280\u672f\u57fa\u7840\uff0c\u589e\u5f3a\u4e86\u5728\u771f\u5b9e\u4ea7\u54c1\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2512.07342", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07342", "abs": "https://arxiv.org/abs/2512.07342", "authors": ["Chen Gong", "Zheng Liu", "Kecen Li", "Tianhao Wang"], "title": "PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning", "comment": "Accepted at NDSS 2026; code available at https://github.com/2019ChenGong/PrivORL", "summary": "Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.\n  To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.", "AI": {"tldr": "PrivORL\u662f\u9996\u4e2a\u5dee\u5206\u9690\u79c1\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u96c6\u5408\u6210\u65b9\u6cd5\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u548c\u6269\u6563\u53d8\u6362\u5668\u5206\u522b\u5408\u6210\u8f6c\u79fb\u548c\u8f68\u8ff9\uff0c\u901a\u8fc7DP-SGD\u4fdd\u62a4\u9690\u79c1\uff0c\u5e76\u5f15\u5165\u597d\u5947\u5fc3\u9a71\u52a8\u9884\u8bad\u7ec3\u63d0\u5347\u6570\u636e\u591a\u6837\u6027\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u5171\u4eab\u9884\u6536\u96c6\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u907f\u514d\u4e86\u4e0e\u73af\u5883\u76f4\u63a5\u4ea4\u4e92\uff0c\u4f46\u5728\u5bfc\u822a\u7b49\u5173\u952e\u9886\u57df\u5e94\u7528\u65f6\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u9700\u8981\u4fdd\u62a4\u6570\u636e\u96c6\u4e2d\u7684\u9690\u79c1\u4fe1\u606f\u3002", "method": "\u63d0\u51faPrivORL\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5408\u6210\u8f6c\u79fb\uff0c\u6269\u6563\u53d8\u6362\u5668\u5408\u6210\u8f68\u8ff9\uff1b2\uff09\u91c7\u7528DP-SGD\u5728\u654f\u611f\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u9884\u8bad\u7ec3\u5408\u6210\u5668\uff1b3\uff09\u5f15\u5165\u597d\u5947\u5fc3\u9a71\u52a8\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u597d\u5947\u5fc3\u6a21\u5757\u53cd\u9988\u63d0\u5347\u5408\u6210\u6570\u636e\u591a\u6837\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u654f\u611f\u79bb\u7ebfRL\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728DP\u8f6c\u79fb\u548c\u8f68\u8ff9\u5408\u6210\u65b9\u9762\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6548\u7528\u548c\u4fdd\u771f\u5ea6\uff0c\u80fd\u751f\u6210\u4e0e\u654f\u611f\u6570\u636e\u96c6\u9ad8\u5ea6\u76f8\u4f3c\u7684\u591a\u6837\u5316\u5408\u6210\u6570\u636e\u3002", "conclusion": "PrivORL\u662f\u9996\u4e2a\u5dee\u5206\u9690\u79c1\u79bb\u7ebfRL\u6570\u636e\u96c6\u5408\u6210\u6846\u67b6\uff0c\u80fd\u6709\u6548\u4fdd\u62a4\u9690\u79c1\u540c\u65f6\u4fdd\u6301\u6570\u636e\u8d28\u91cf\uff0c\u4e3a\u5b89\u5168\u53d1\u5e03\u79bb\u7ebfRL\u6570\u636e\u96c6\u8fdb\u884c\u4e0b\u6e38\u5206\u6790\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.07533", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07533", "abs": "https://arxiv.org/abs/2512.07533", "authors": ["Yuzhou Nie", "Hongwei Li", "Chengquan Guo", "Ruizhe Jiang", "Zhun Wang", "Bo Li", "Dawn Song", "Wenbo Guo"], "title": "VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection", "comment": null, "summary": "We propose VulnLLM-R, the~\\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.", "AI": {"tldr": "VulnLLM-R\u662f\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u6f0f\u6d1e\u68c0\u6d4b\u7684\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7a0b\u5e8f\u72b6\u6001\u63a8\u7406\u800c\u975e\u7b80\u5355\u6a21\u5f0f\u5339\u914d\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5728Python\u3001C/C++\u3001Java\u7b49\u8bed\u8a00\u7684SOTA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u9759\u6001\u5206\u6790\u5de5\u5177\u548c\u5927\u578b\u63a8\u7406\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6700\u5148\u8fdb\u7684\u63a8\u7406LLM\u901a\u5e38\u89c4\u6a21\u8d85\u5927\u3001\u95ed\u6e90\u6216\u5728\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u9762\u6027\u80fd\u6709\u9650\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6a21\u5f0f\u5339\u914d\uff0c\u5bb9\u6613\u5b66\u4e60\u6377\u5f84\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u6f0f\u6d1e\u68c0\u6d4b\u7684\u63a8\u7406\u6a21\u578b\u6765\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u8bad\u7ec3\u65b9\u6cd5\uff1a\u4e13\u95e8\u7684\u6570\u636e\u9009\u62e9\u3001\u63a8\u7406\u6570\u636e\u751f\u6210\u3001\u63a8\u7406\u6570\u636e\u8fc7\u6ee4\u548c\u4fee\u6b63\u3001\u6d4b\u8bd5\u9636\u6bb5\u4f18\u5316\u3002\u4f7f\u7528\u8be5\u65b9\u6cd5\u8bad\u7ec3\u4e86\u4e00\u4e2a70\u4ebf\u53c2\u6570\u7684\u63a8\u7406\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e86\u56f4\u7ed5\u8be5\u6a21\u578b\u7684\u667a\u80fd\u4f53\u6846\u67b6\u3002", "result": "VulnLLM-R\u5728Python\u3001C/C++\u3001Java\u7684SOTA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u9759\u6001\u5206\u6790\u5de5\u5177\u548c\u5f00\u6e90/\u5546\u4e1a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u679c\u548c\u6548\u7387\u3002\u5728\u771f\u5b9e\u9879\u76ee\u4e2d\u8d85\u8d8a\u4e86CodeQL\u548cAFL++\uff0c\u5e76\u5728\u6d3b\u8dc3\u7ef4\u62a4\u7684\u4ed3\u5e93\u4e2d\u53d1\u73b0\u4e86\u4e00\u7cfb\u5217\u96f6\u65e5\u6f0f\u6d1e\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u901a\u8fc7\u4e13\u95e8\u63a8\u7406\u6a21\u578b\u9a71\u52a8\u7684AI\u667a\u80fd\u4f53\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u9879\u76ee\u7ea7\u6f0f\u6d1e\u68c0\u6d4b\u7684\u5f00\u521b\u6027\u5de5\u4f5c\uff0c\u5c55\u793a\u4e86\u4e13\u95e8\u5316\u63a8\u7406\u6a21\u578b\u5728\u6f0f\u6d1e\u68c0\u6d4b\u9886\u57df\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2512.07314", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07314", "abs": "https://arxiv.org/abs/2512.07314", "authors": ["Yuxiao Luo", "Songming Zhang", "Sijie Ruan", "Siran Chen", "Kang Liu", "Yang Xu", "Yu Zheng", "Ling Yin"], "title": "M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling", "comment": null, "summary": "Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.", "AI": {"tldr": "M-STAR\u6846\u67b6\u901a\u8fc7\u591a\u5c3a\u5ea6\u65f6\u7a7a\u81ea\u56de\u5f52\u65b9\u6cd5\u751f\u6210\u957f\u671f\u4eba\u7c7b\u79fb\u52a8\u8f68\u8ff9\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u901f\u5ea6\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u751f\u6210\u5355\u65e5\u8f68\u8ff9\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u957f\u671f\u8f68\u8ff9\u751f\u6210\uff08\u5982\u5468\u8f68\u8ff9\uff09\u65b9\u9762\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u7f3a\u4e4f\u663e\u5f0f\u7684\u65f6\u7a7a\u591a\u5c3a\u5ea6\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51faM-STAR\u6846\u67b6\uff0c\u91c7\u7528\u4ece\u7c97\u5230\u7ec6\u7684\u65f6\u7a7a\u9884\u6d4b\u8fc7\u7a0b\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u65f6\u7a7a\u6807\u8bb0\u5668\u7f16\u7801\u5c42\u6b21\u5316\u79fb\u52a8\u6a21\u5f0f\uff0c\u4ee5\u53ca\u57fa\u4e8eTransformer\u7684\u89e3\u7801\u5668\u8fdb\u884c\u4e0b\u4e00\u5c3a\u5ea6\u81ea\u56de\u5f52\u9884\u6d4b\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cM-STAR\u5728\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u901f\u5ea6\u3002", "conclusion": "M-STAR\u901a\u8fc7\u591a\u5c3a\u5ea6\u65f6\u7a7a\u81ea\u56de\u5f52\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u8f68\u8ff9\u751f\u6210\u7684\u6548\u7387\u548c\u5efa\u6a21\u95ee\u9898\uff0c\u4e3a\u4eba\u7c7b\u79fb\u52a8\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07355", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07355", "abs": "https://arxiv.org/abs/2512.07355", "authors": ["Alexandre Rocchi--Henry", "Thomas Fel", "Gianni Franchi"], "title": "A Geometric Unification of Concept Learning with Concept Cones", "comment": "22 pages", "summary": "Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\\footnote{We adopt the terminology of \\citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u76d1\u7763\u5f0f\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u548c\u65e0\u76d1\u7763\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u8054\u7cfb\u8d77\u6765\uff0c\u8ba4\u4e3a\u4e24\u8005\u90fd\u5b66\u4e60\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u65b9\u5411\uff0c\u5f62\u6210\u6982\u5ff5\u9525\u3002\u901a\u8fc7\u51e0\u4f55\u5305\u542b\u5173\u7cfb\u8bc4\u4f30SAEs\u53d1\u73b0\u7684\u6982\u5ff5\u4e0eCBM\u5b9a\u4e49\u6982\u5ff5\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "motivation": "\u4f20\u7edf\u4e0a\uff0c\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u8fd9\u4e24\u79cd\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5404\u81ea\u53d1\u5c55\uff0c\u7f3a\u4e4f\u4ea4\u6d41\u3002CBMs\u4f7f\u7528\u76d1\u7763\u5b66\u4e60\u5c06\u6fc0\u6d3b\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u6982\u5ff5\u5bf9\u9f50\uff0c\u800cSAEs\u901a\u8fc7\u7a00\u758f\u7f16\u7801\u53d1\u73b0\u6d8c\u73b0\u6982\u5ff5\u3002\u4f5c\u8005\u5e0c\u671b\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u8fde\u63a5\u8fd9\u4e24\u79cd\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u6846\u67b6\uff1a\u4e24\u79cd\u65b9\u6cd5\u90fd\u5b66\u4e60\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u65b9\u5411\uff0c\u5176\u975e\u8d1f\u7ec4\u5408\u5f62\u6210\u6982\u5ff5\u9525\u3002\u76d1\u7763\u548c\u975e\u76d1\u7763\u65b9\u6cd5\u7684\u533a\u522b\u4ec5\u5728\u4e8e\u5982\u4f55\u9009\u62e9\u8fd9\u4e2a\u9525\u3002\u57fa\u4e8e\u6b64\uff0c\u5efa\u7acb\u64cd\u4f5c\u6865\u6881\uff1aCBMs\u63d0\u4f9b\u4eba\u5de5\u5b9a\u4e49\u7684\u53c2\u8003\u51e0\u4f55\uff0cSAEs\u901a\u8fc7\u5176\u5b66\u4e60\u9525\u4e0eCBM\u9525\u7684\u8fd1\u4f3c\u6216\u5305\u542b\u7a0b\u5ea6\u6765\u8bc4\u4f30\u3002", "result": "\u5f00\u53d1\u4e86\u5b9a\u91cf\u6307\u6807\uff0c\u5c06SAE\u7684\u5f52\u7eb3\u504f\u7f6e\uff08\u5982\u7c7b\u578b\u3001\u7a00\u758f\u5ea6\u3001\u6269\u5c55\u6bd4\uff09\u4e0e\u5408\u7406\u6982\u5ff5\u7684\u51fa\u73b0\u8054\u7cfb\u8d77\u6765\u3002\u53d1\u73b0\u4e86\u7a00\u758f\u5ea6\u548c\u6269\u5c55\u56e0\u5b50\u7684\"\u6700\u4f73\u70b9\"\uff0c\u80fd\u6700\u5927\u5316\u4e0eCBM\u6982\u5ff5\u7684\u51e0\u4f55\u548c\u8bed\u4e49\u5bf9\u9f50\u3002", "conclusion": "\u901a\u8fc7\u5171\u4eab\u7684\u51e0\u4f55\u6846\u67b6\u7edf\u4e00\u4e86\u76d1\u7763\u548c\u975e\u76d1\u7763\u6982\u5ff5\u53d1\u73b0\uff0c\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6307\u6807\u6765\u8861\u91cfSAE\u8fdb\u5c55\uff0c\u5e76\u8bc4\u4f30\u53d1\u73b0\u7684\u6982\u5ff5\u4e0e\u5408\u7406\u4eba\u7c7b\u6982\u5ff5\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002"}}
{"id": "2512.07436", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07436", "abs": "https://arxiv.org/abs/2512.07436", "authors": ["Hang He", "Chuhuai Yue", "Chengqi Dong", "Mingxue Tian", "Zhenfeng Liu", "Jiajun Chai", "Xiaohan Wang", "Yufei Zhang", "Qun Liao", "Guojun Yin", "Wei Lin", "Chengcheng Wan", "Haiying Sun", "Ting Su"], "title": "LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services", "comment": null, "summary": "Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.", "AI": {"tldr": "LocalSearchBench\u662f\u9996\u4e2a\u9488\u5bf9\u672c\u5730\u751f\u6d3b\u670d\u52a1\u7684\u667a\u80fd\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u8d85\u8fc715\u4e07\u6761\u9ad8\u8d28\u91cf\u6570\u636e\u548c300\u4e2a\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\uff0c\u5b9e\u9a8c\u663e\u793a\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u5728\u8be5\u9886\u57df\u8868\u73b0\u4e5f\u8f83\u5dee\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u4fe1\u606f\u68c0\u7d22\uff0c\u5f88\u5c11\u63a2\u7d22\u5177\u6709\u72ec\u7279\u6311\u6218\u7684\u5782\u76f4\u9886\u57df\u3002\u672c\u5730\u751f\u6d3b\u670d\u52a1\u4e2d\u7684\u771f\u5b9e\u67e5\u8be2\u5f80\u5f80\u6a21\u7cca\u4e14\u9700\u8981\u8de8\u5546\u5bb6\u548c\u4ea7\u54c1\u7684\u591a\u8df3\u63a8\u7406\uff0c\u8fd9\u4e9b\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u89e3\u51b3\u3002", "method": "\u6784\u5efaLocalSearchBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u6765\u81ea\u4e0d\u540c\u57ce\u5e02\u548c\u4e1a\u52a1\u7c7b\u578b\u7684\u8d85\u8fc715\u4e07\u6761\u9ad8\u8d28\u91cf\u6761\u76ee\uff0c\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u6784\u5efa300\u4e2a\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u3002\u540c\u65f6\u5f00\u53d1LocalPlayground\u7edf\u4e00\u73af\u5883\uff0c\u96c6\u6210\u591a\u79cd\u5de5\u5177\u4f9b\u667a\u80fd\u4f53\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u5728LocalSearchBench\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1a\u6700\u4f73\u6a21\u578b\uff08DeepSeek-V3.1\uff09\u6b63\u786e\u7387\u4ec5\u4e3a34.34%\uff0c\u5927\u591a\u6570\u6a21\u578b\u5728\u5b8c\u6574\u6027\uff08\u5e73\u574777.33%\uff09\u548c\u5fe0\u5b9e\u6027\uff08\u5e73\u574761.99%\uff09\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002", "conclusion": "\u672c\u5730\u751f\u6d3b\u670d\u52a1\u9886\u57df\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u9886\u57df\u7279\u5b9a\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\uff0c\u73b0\u6709\u901a\u7528\u6a21\u578b\u5728\u8be5\u5782\u76f4\u9886\u57df\u9762\u4e34\u663e\u8457\u6311\u6218\u3002"}}
{"id": "2512.07611", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07611", "abs": "https://arxiv.org/abs/2512.07611", "authors": ["Yongsheng Lian"], "title": "Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement", "comment": null, "summary": "This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.\n  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e09\u79cd\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08PPO\u3001GRPO\u3001DAPO\uff09\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u590d\u6742\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u53d7\u63a7\u8fc1\u79fb\u5b66\u4e60\u8bc4\u4f30\u53d1\u73b0RL\u8bad\u7ec3\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff0c\u4f46\u6539\u8fdb\u7a0b\u5ea6\u56e0\u57fa\u51c6\u800c\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u4e0d\u540c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u590d\u6742\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6548\u679c\uff0c\u4e3aRL-based LLM\u8bad\u7ec3\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\uff0c\u7279\u522b\u662f\u6bd4\u8f83PPO\u3001GRPO\u548cDAPO\u4e09\u79cd\u7b97\u6cd5\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u53d7\u63a7\u8fc1\u79fb\u5b66\u4e60\u8bc4\u4f30\u65b9\u6cd5\uff1a\u9996\u5148\u5728\u4e13\u95e8\u7684Countdown Game\u4e0a\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u7136\u540e\u5728\u901a\u7528\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u8fdb\u884c\u53c2\u6570\u5206\u6790\uff0c\u5305\u62ecGRPO\u548cDAPO\u4e2d\u7ec4\u5927\u5c0f\u7684\u5f71\u54cd\u3001KL\u60e9\u7f5a\u7cfb\u6570\u7684\u975e\u5355\u8c03\u5f71\u54cd\uff0c\u4ee5\u53caDAPO\u4e2d\u52a8\u6001\u91c7\u6837\u7ec4\u4ef6\u7684\u6548\u679c\u8bc4\u4f30\u3002", "result": "\u5728\u6240\u6709\u4efb\u52a1\u4e2d\uff0cRL\u8bad\u7ec3\u6a21\u578b\u90fd\u4f18\u4e8e\u76f8\u5e94\u7684\u57fa\u51c6\u6a21\u578b\uff0c\u4f46\u6539\u8fdb\u7a0b\u5ea6\u56e0\u57fa\u51c6\u800c\u5f02\u3002\u589e\u52a0GRPO\u548cDAPO\u4e2d\u7684\u7ec4\u5927\u5c0f\u80fd\u5e26\u6765\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u52a8\u6001\u548c\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0cKL\u60e9\u7f5a\u7cfb\u6570\u7684\u5f71\u54cd\u662f\u975e\u5355\u8c03\u7684\u3002DAPO\u4e2d\u7684\u52a8\u6001\u91c7\u6837\u7ec4\u4ef6\u5e76\u672a\u63d0\u5347\u6027\u80fd\uff0c\u5b9e\u9645\u4e0a\u7981\u7528DS\u65f6DAPO\u53d6\u5f97\u4e86\u6700\u4f73\u6574\u4f53\u7ed3\u679c\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4e0d\u540c\u7b97\u6cd5\u7684\u6548\u679c\u5b58\u5728\u5dee\u5f02\u3002GRPO\u548cDAPO\u7684\u7ec4\u5927\u5c0f\u662f\u91cd\u8981\u53c2\u6570\uff0c\u800cDAPO\u7684\u52a8\u6001\u91c7\u6837\u7ec4\u4ef6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u4e0d\u9700\u8981\u3002\u7814\u7a76\u4e3aRL-based LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u53c2\u6570\u914d\u7f6e\u6307\u5bfc\u3002"}}
{"id": "2512.07631", "categories": ["cs.AI", "cs.CC", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07631", "abs": "https://arxiv.org/abs/2512.07631", "authors": ["Shahar Lutati"], "title": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds", "comment": null, "summary": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Agent Capability Problem (ACP)\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u667a\u80fd\u4f53\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u80fd\u5426\u89e3\u51b3\u95ee\u9898\uff0c\u901a\u8fc7\u4fe1\u606f\u83b7\u53d6\u89c6\u89d2\u5c06\u95ee\u9898\u89e3\u51b3\u5efa\u6a21\u4e3a\u4fe1\u606f\u83b7\u53d6\u8fc7\u7a0b\uff0c\u5e76\u63d0\u4f9b\u4e86\u6210\u672c\u9884\u6d4b\u7684\u7406\u8bba\u754c\u9650\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7ecf\u9a8c\u542f\u53d1\u5f0f\u65b9\u6cd5\u9884\u6d4b\u667a\u80fd\u4f53\u80fd\u5426\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u89e3\u51b3\u95ee\u9898\uff0c\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9884\u6d4b\u667a\u80fd\u4f53\u8d44\u6e90\u9700\u6c42\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4ee5\u4fbf\u5728\u5b9e\u9645\u641c\u7d22\u5f00\u59cb\u524d\u5c31\u80fd\u8bc4\u4f30\u4efb\u52a1\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51faACP\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u89e3\u51b3\u89c6\u4e3a\u4fe1\u606f\u83b7\u53d6\u8fc7\u7a0b\uff1a\u667a\u80fd\u4f53\u9700\u8981I_total\u6bd4\u7279\u4fe1\u606f\u6765\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\uff0c\u6bcf\u4e2a\u52a8\u4f5c\u83b7\u5f97I_step\u6bd4\u7279\u4fe1\u606f\uff0c\u6210\u672c\u4e3aC_step\u3002\u7531\u6b64\u63a8\u5bfc\u51fa\u6709\u6548\u6210\u672cC_eff = (I_total/I_step) * C_step\uff0c\u7528\u4e8e\u9884\u6d4b\u8d44\u6e90\u9700\u6c42\u3002\u63d0\u4f9b\u4e86C_eff\u4f5c\u4e3a\u671f\u671b\u6210\u672c\u4e0b\u754c\u7684\u7406\u8bba\u8bc1\u660e\uff0c\u5e76\u7ed9\u51fa\u4e86\u7d27\u5bc6\u7684\u6982\u7387\u4e0a\u754c\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660eACP\u9884\u6d4b\u4e0e\u5b9e\u9645\u667a\u80fd\u4f53\u6027\u80fd\u9ad8\u5ea6\u4e00\u81f4\uff0c\u80fd\u591f\u6301\u7eed\u9650\u5236\u641c\u7d22\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u6bd4\u8d2a\u5a6a\u548c\u968f\u673a\u7b56\u7565\u66f4\u9ad8\u6548\u3002\u8be5\u6846\u67b6\u53ef\u6cdb\u5316\u5230\u57fa\u4e8eLLM\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u4fe1\u606f\u8bba\u89c6\u89d2\u8fde\u63a5\u4e3b\u52a8\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u539f\u5219\u3002", "conclusion": "ACP\u6846\u67b6\u4e3a\u9884\u6d4b\u667a\u80fd\u4f53\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u4fe1\u606f\u83b7\u53d6\u89c6\u89d2\u7edf\u4e00\u4e86\u591a\u79cd\u5b66\u4e60\u8303\u5f0f\uff0c\u4e3a\u667a\u80fd\u4f53\u8d44\u6e90\u5206\u914d\u51b3\u7b56\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2512.07710", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07710", "abs": "https://arxiv.org/abs/2512.07710", "authors": ["Anxiang Zeng", "Haibo Zhang", "Hailing Zhang", "Kaixiang Mo", "Liang Yao", "Ling Hu", "Long Zhang", "Shuman Liu", "Shuyi Xie", "Yanshi Li", "Yizhang Chen", "Yuepeng Sheng", "Yuwei Huang", "Zhaochen Xu", "Zhiqiang Zhou", "Ziqin Liew"], "title": "Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE", "comment": null, "summary": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.", "AI": {"tldr": "CompassMax-V3-Thinking\u662f\u4e00\u4e2a\u5343\u4ebf\u89c4\u6a21\u7684MoE\u63a8\u7406\u6a21\u578b\uff0c\u91c7\u7528\u65b0\u7684RL\u8bad\u7ec3\u6846\u67b6\uff0c\u6838\u5fc3\u539f\u5219\u662f\"\u6bcf\u4e2a\u63d0\u793a\u90fd\u5fc5\u987b\u91cd\u8981\"\u3002\u901a\u8fc7\u591a\u9879\u521b\u65b0\u6280\u672f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21RL\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5c06\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u5230\u5343\u4ebf\u89c4\u6a21\u65f6\u66b4\u9732\u51fa\u5173\u952e\u6548\u7387\u95ee\u9898\uff1a\u96f6\u65b9\u5dee\u63d0\u793a\u6d6a\u8d39rollout\u8d44\u6e90\u3001\u957f\u89c6\u91ce\u91cd\u8981\u6027\u91c7\u6837\u4e0d\u7a33\u5b9a\u3001\u6807\u51c6\u5956\u52b1\u6a21\u578b\u5bfc\u81f4\u4f18\u52bf\u53cd\u8f6c\uff0c\u4ee5\u53carollout\u5904\u7406\u7684\u7cfb\u7edf\u6027\u74f6\u9888\u3002", "method": "\u5f15\u5165\u56db\u9879\u7edf\u4e00\u521b\u65b0\uff1a1) \u591a\u9636\u6bb5\u96f6\u65b9\u5dee\u6d88\u9664\uff0c\u8fc7\u6ee4\u975e\u4fe1\u606f\u6027\u63d0\u793a\u5e76\u7a33\u5b9a\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\uff1b2) ESPO\u71b5\u81ea\u9002\u5e94\u4f18\u5316\u65b9\u6cd5\uff0c\u5e73\u8861token\u7ea7\u548c\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u91c7\u6837\uff1b3) \u8def\u7531\u5668\u91cd\u653e\u7b56\u7565\uff0c\u5bf9\u9f50\u8bad\u7ec3\u65f6MoE\u8def\u7531\u5668\u51b3\u7b56\u4e0e\u63a8\u7406\u65f6\u884c\u4e3a\uff1b4) \u9ad8\u541e\u5410RL\u7cfb\u7edf\uff0c\u91c7\u7528FP8\u7cbe\u5ea6rollout\u3001\u91cd\u53e0\u5956\u52b1\u8ba1\u7b97\u548c\u957f\u5ea6\u611f\u77e5\u8c03\u5ea6\u3002", "result": "\u8fd9\u4e9b\u8d21\u732e\u5f62\u6210\u4e86\u4e00\u4e2a\u8fde\u8d2f\u7684\u7ba1\u9053\uff0c\u4f7f\u5343\u4ebf\u89c4\u6a21MoE\u6a21\u578b\u7684RL\u8bad\u7ec3\u53d8\u5f97\u7a33\u5b9a\u9ad8\u6548\u3002\u6700\u7ec8\u6a21\u578b\u5728\u5185\u90e8\u548c\u516c\u5171\u8bc4\u4f30\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u901a\u8fc7\u89e3\u51b3\u5927\u89c4\u6a21RL\u8bad\u7ec3\u4e2d\u7684\u5173\u952e\u6548\u7387\u74f6\u9888\uff0cCompassMax-V3-Thinking\u5c55\u793a\u4e86\u5728\u5343\u4ebf\u89c4\u6a21MoE\u6a21\u578b\u4e0a\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u3002"}}
{"id": "2512.07761", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07761", "abs": "https://arxiv.org/abs/2512.07761", "authors": ["Xiqiao Xiong", "Ouxiang Li", "Zhuo Liu", "Moxin Li", "Wentao Shi", "Fuli Feng", "Xiangnan He"], "title": "RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models", "comment": "19 pages, 15 figures", "summary": "Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6700\u7ec8\u8f93\u51fa\u7684\u5371\u5bb3\u6027\u4f5c\u4e3a\u5956\u52b1\uff0c\u5e76\u5f15\u5165\u8fc7\u7a0b\u5956\u52b1\u6765\u63d0\u5347\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u5f71\u54cd\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u8f6e\u4f18\u5316\uff0c\u4e0d\u8db3\u4ee5\u5b66\u4e60\u957f\u671f\u653b\u51fb\u7b56\u7565\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u8f6e\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u76f4\u63a5\u4f18\u5316\u6700\u7ec8\u8f6e\u8f93\u51fa\u7684\u5371\u5bb3\u6027\u4f5c\u4e3a\u7ed3\u679c\u5956\u52b1\u3002\u4e3a\u7f13\u89e3\u7a00\u758f\u76d1\u7763\u5e76\u4fc3\u8fdb\u957f\u671f\u653b\u51fb\u7b56\u7565\uff0c\u63d0\u51fa\u4e24\u79cd\u542f\u53d1\u5f0f\u8fc7\u7a0b\u5956\u52b1\uff1a1) \u63a7\u5236\u4e2d\u95f4\u8f93\u51fa\u7684\u5371\u5bb3\u6027\u4ee5\u907f\u514d\u89e6\u53d1\u9ed1\u76d2\u6a21\u578b\u7684\u62d2\u7edd\u673a\u5236\uff1b2) \u4fdd\u6301\u4e2d\u95f4\u8f93\u51fa\u7684\u8bed\u4e49\u76f8\u5173\u6027\u4ee5\u907f\u514d\u504f\u79bb\u4e3b\u9898\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u6301\u7eed\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u653b\u51fb\u6210\u529f\u7387\uff0c\u4e3a\u9ed1\u76d2\u6a21\u578b\u7684\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u624b\u6bb5\u3002"}}
{"id": "2512.07795", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07795", "abs": "https://arxiv.org/abs/2512.07795", "authors": ["Nearchos Potamitis", "Lars Klein", "Akhil Arora"], "title": "ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning", "comment": "11 pages, 3 tables, 4 figures", "summary": "Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .", "AI": {"tldr": "ReasonBENCH\u662f\u9996\u4e2a\u4e13\u95e8\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e0d\u7a33\u5b9a\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u591a\u8f6e\u8fd0\u884c\u534f\u8bae\u63d0\u4f9b\u7edf\u8ba1\u53ef\u9760\u7684\u8d28\u91cf\u548c\u6210\u672c\u6307\u6807\uff0c\u63ed\u793a\u5f53\u524d\u63a8\u7406\u65b9\u6cd5\u666e\u904d\u5b58\u5728\u7684\u9ad8\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u8bc4\u4f30\u4e3b\u8981\u62a5\u544a\u5355\u6b21\u8fd0\u884c\u7684\u51c6\u786e\u7387\uff0c\u5ffd\u7565\u4e86\u968f\u673a\u89e3\u7801\u5e26\u6765\u7684\u5185\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u65e0\u6cd5\u53ef\u9760\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u6210\u672c\u4e00\u81f4\u6027\uff0c\u5b58\u5728\u8bc4\u4f30\u76f2\u533a\u3002", "method": "\u5f00\u53d1ReasonBENCH\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\uff1a(1)\u6807\u51c6\u5316\u63a8\u7406\u6846\u67b6\u3001\u6a21\u578b\u548c\u4efb\u52a1\u7684\u6a21\u5757\u5316\u8bc4\u4f30\u5e93\uff1b(2)\u62a5\u544a\u7edf\u8ba1\u53ef\u9760\u8d28\u91cf\u548c\u6210\u672c\u6307\u6807\u7684\u591a\u8f6e\u8fd0\u884c\u534f\u8bae\uff1b(3)\u9f13\u52b1\u65b9\u5dee\u611f\u77e5\u62a5\u544a\u7684\u516c\u5f00\u6392\u884c\u699c\u3002", "result": "\u8de8\u4e0d\u540c\u9886\u57df\u4efb\u52a1\u53d1\u73b0\uff0c\u7edd\u5927\u591a\u6570\u63a8\u7406\u7b56\u7565\u548c\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u4e0d\u7a33\u5b9a\u6027\u3002\u5373\u4f7f\u5e73\u5747\u6027\u80fd\u76f8\u4f3c\u7684\u7b56\u7565\uff0c\u7f6e\u4fe1\u533a\u95f4\u5bbd\u5ea6\u53ef\u80fd\u76f8\u5dee\u56db\u500d\uff0c\u4e14\u6027\u80fd\u6700\u4f73\u7684\u65b9\u6cd5\u901a\u5e38\u6210\u672c\u66f4\u9ad8\u4e14\u66f4\u4e0d\u7a33\u5b9a\u3002", "conclusion": "\u63a8\u7406\u4e0d\u7a33\u5b9a\u6027\u4f1a\u635f\u5bb3\u8de8\u8fd0\u884c\u7684\u53ef\u91cd\u590d\u6027\uff0c\u8fdb\u800c\u5f71\u54cd\u62a5\u544a\u6027\u80fd\u7684\u53ef\u9760\u6027\u3002\u53ef\u91cd\u590d\u6027\u662f\u53ef\u9760LLM\u63a8\u7406\u7684\u5173\u952e\u7ef4\u5ea6\uff0cReasonBENCH\u4e3a\u672a\u6765\u63a8\u7406\u65b9\u6cd5\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
