<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Automated Insertion of Flushes and Fences for Persistency](https://arxiv.org/abs/2509.19459)
*Yutong Guo,Weiyu Luo,Brian Demsky*

Main category: cs.SE

TL;DR: PMRobust是一个编译器，能自动插入flush和fence操作，确保持久内存代码没有缺失flush和fence的bug。


<details>
  <summary>Details</summary>
Motivation: 持久内存和CXL共享内存需要在崩溃后保持内容，但正确使用flush和fence操作很困难。现有工具需要bug暴露的测试用例，且无法确保没有缺失flush的bug。

Method: PMRobust采用新颖的静态分析，针对新分配的对象进行优化，自动插入必要的flush和fence操作。

Result: 在持久内存库和多个持久内存数据结构上评估，相对于手动放置flush和fence操作的原始基准测试，几何平均开销为0.26%。

Conclusion: PMRobust能有效自动确保持久内存代码的正确性，且性能开销极小。

Abstract: CXL shared memory and persistent memory allow the contents of memory to
persist beyond crashes. Stores to persistent or CXL memory are typically not
immediately made persistent; developers must manually flush the corresponding
cache lines to force the data to be written to the underlying storage.
Correctly using flush and fence operations is known to be challenging. While
state-of-the-art tools can find missing flush instructions, they often require
bug-revealing test cases. No existing tools can ensure the absence of missing
flush bugs.
  In this paper, we present PMRobust, a compiler that automatically inserts
flush and fence operations to ensure that code using persistent memory is free
from missing flush and fence bugs. PMRobust employs a novel static analysis
with optimizations that target newly allocated objects. We have evaluated
PMRobust on persistent memory libraries and several persistent memory data
structures and measured a geometric mean overhead of 0.26% relative to the
original benchmarks with hand-placed flush and fence operations.

</details>


### [2] [Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation](https://arxiv.org/abs/2509.19533)
*Mengdi Lu,Steven Ding,Furkan Alaca,Philippe Charland*

Main category: cs.SE

TL;DR: 本文提出了一个将推理能力大语言模型（LLMs）与AFL++模糊测试框架集成的开源微服务框架，以解决传统模糊测试工具缺乏语义推理能力的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于变异的模糊测试工具（如AFL++）主要进行字节或比特级编辑，缺乏语义推理能力，无法处理复杂的协议逻辑和字段间依赖关系。而具备推理能力的LLMs可以利用预训练知识理解输入格式和约束条件，但缺乏监督微调的真实数据。

Method: 开发了一个开源微服务框架，将推理LLMs与AFL++集成在Google的FuzzBench上，解决了LLMs和模糊测试器在异步执行和硬件需求（GPU vs CPU）方面的差异。

Result: 实验评估了四个研究问题，发现Deepseek模型表现最佳，变异效果更多取决于提示复杂度和模型选择而非样本数量。响应延迟和吞吐量瓶颈是主要挑战。

Conclusion: 推理LLMs可以显著提升模糊测试的变异质量，但实际应用仍面临性能瓶颈，这为未来工作提供了方向。

Abstract: Security vulnerabilities in Internet-of-Things devices, mobile platforms, and
autonomous systems remain critical. Traditional mutation-based fuzzers -- while
effectively explore code paths -- primarily perform byte- or bit-level edits
without semantic reasoning. Coverage-guided tools such as AFL++ use
dictionaries, grammars, and splicing heuristics to impose shallow structural
constraints, leaving deeper protocol logic, inter-field dependencies, and
domain-specific semantics unaddressed. Conversely, reasoning-capable large
language models (LLMs) can leverage pretraining knowledge to understand input
formats, respect complex constraints, and propose targeted mutations, much like
an experienced reverse engineer or testing expert. However, lacking ground
truth for "correct" mutation reasoning makes supervised fine-tuning
impractical, motivating explorations of off-the-shelf LLMs via prompt-based
few-shot learning. To bridge this gap, we present an open-source microservices
framework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,
tackling asynchronous execution and divergent hardware demands (GPU- vs.
CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)
How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do
few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt
engineering with off-the-shelf models improve fuzzing directly? and (R4) Which
open-source reasoning LLMs perform best under prompt-only conditions?
Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3
highlight Deepseek as the most promising. Mutation effectiveness depends more
on prompt complexity and model choice than shot count. Response latency and
throughput bottlenecks remain key obstacles, offering directions for future
work.

</details>


### [3] [Reverse Engineering User Stories from Code using Large Language Models](https://arxiv.org/abs/2509.19587)
*Mohamed Ouf,Haoyu Li,Michael Zhang,Mariam Guizani*

Main category: cs.SE

TL;DR: 本文研究使用大型语言模型从源代码自动恢复用户故事，评估了5个最先进的LLM在6种提示策略下的表现，发现所有模型在200NLOC以内的代码上平均F1分数达到0.8，且小模型通过单个示例即可达到大模型性能。


<details>
  <summary>Details</summary>
Motivation: 用户故事在敏捷开发中至关重要，但在遗留系统和文档不完善的系统中经常缺失或过时，需要自动化的恢复方法。

Method: 使用1,750个标注的C++代码片段，评估5个最先进的LLM在6种提示策略下的表现，包括单示例提示和链式推理等方法。

Result: 所有模型在200NLOC以内的代码上平均F1分数达到0.8，小模型通过单个示例即可匹配大模型性能，而链式推理仅对较大模型有边际改进。

Conclusion: LLM能够有效从源代码恢复用户故事，提示设计对输出质量有重要影响，简单的单示例提示即可实现良好效果。

Abstract: User stories are essential in agile development, yet often missing or
outdated in legacy and poorly documented systems. We investigate whether large
language models (LLMs) can automatically recover user stories directly from
source code and how prompt design impacts output quality. Using 1,750 annotated
C++ snippets of varying complexity, we evaluate five state-of-the-art LLMs
across six prompting strategies. Results show that all models achieve, on
average, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a
single illustrative example enables the smallest model (8B) to match the
performance of a much larger 70B model. In contrast, structured reasoning via
Chain-of-Thought offers only marginal gains, primarily for larger models.

</details>


### [4] [Assertion Messages with Large Language Models (LLMs) for Code](https://arxiv.org/abs/2509.19673)
*Ahmed Aljohani,Anamul Haque Mollah,Hyunsook Do*

Main category: cs.SE

TL;DR: 本文评估了四种最先进的填充中间（FIM）大型语言模型在生成Java测试断言消息方面的能力，发现Codestral-22B表现最佳，但仍不及人工编写的质量。研究表明包含描述性测试注释可提升性能，并分析了模型在复制开发者语言模式方面的表现。


<details>
  <summary>Details</summary>
Motivation: 断言消息能显著提升单元测试的可理解性，但开发者和自动化测试生成工具经常忽略编写。尽管LLMs有潜力，但尚未系统评估其在生成信息性断言消息方面的能力。

Method: 使用包含216个Java测试方法的数据集，评估四种FIM LLMs（Qwen2.5-Coder-32B、Codestral-22B、CodeLlama-13B和StarCoder），采用类人评估方法进行质量评分，并进行消融研究分析测试注释的影响。

Result: Codestral-22B获得最高质量分2.76/5，而人工编写消息为3.24。包含描述性测试注释可将Codestral性能提升至2.97。所有模型都频繁复制开发者的语言模式。

Conclusion: 研究为推进测试代码中自动化、上下文感知的断言消息生成提供了重要基础，同时指出了所选模型和传统文本评估指标在捕捉多样化断言消息结构方面的局限性。

Abstract: Assertion messages significantly enhance unit tests by clearly explaining the
reasons behind test failures, yet they are frequently omitted by developers and
automated test-generation tools. Despite recent advancements, Large Language
Models (LLMs) have not been systematically evaluated for their ability to
generate informative assertion messages. In this paper, we introduce an
evaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -
Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset
of 216 Java test methods containing developer-written assertion messages. We
find that Codestral-22B achieves the highest quality score of 2.76 out of 5
using a human-like evaluation approach, compared to 3.24 for manually written
messages. Our ablation study shows that including descriptive test comments
further improves Codestral's performance to 2.97, highlighting the critical
role of context in generating clear assertion messages. Structural analysis
demonstrates that all models frequently replicate developers' preferred
linguistic patterns. We discuss the limitations of the selected models and
conventional text evaluation metrics in capturing diverse assertion message
structures. Our benchmark, evaluation results, and discussions provide an
essential foundation for advancing automated, context-aware generation of
assertion messages in test code. A replication package is available at
https://doi.org/10.5281/zenodo.15293133

</details>


### [5] [Intuition to Evidence: Measuring AI's True Impact on Developer Productivity](https://arxiv.org/abs/2509.19708)
*Anand Kumar,Vishal Khare,Deepak Sharma,Satyam Kumar,Vijay Saini,Anshul Yadav,Sachendra Jain,Ankit Rana,Pratham Verma,Vaibhav Meena,Avinash Edubilli*

Main category: cs.SE

TL;DR: 本文对在企业规模部署的AI辅助软件开发工具进行了为期一年的真实世界评估，涉及300名工程师使用内部AI平台(DeputyDev)，结果显示PR审查周期时间减少31.8%，开发者满意度达85%。


<details>
  <summary>Details</summary>
Motivation: 评估AI工具在企业级软件开发环境中的实际效果，弥补受控基准测试的不足，提供来自生产环境的实证证据。

Method: 采用纵向队列分析方法，让300名工程师在日常工作流程中集成内部AI平台(DeputyDev)，该平台结合了代码生成和自动审查功能。

Result: 统计显著的31.8% PR审查周期时间减少；85%开发者对代码审查功能满意；采用率从第1个月的4%增长到第6个月的83%；顶级采用者代码推送量增加61%。

Conclusion: 研究证明了AI在企业软件开发中的变革潜力，同时也揭示了实际部署挑战，为AI工具集成提供了重要的实证依据。

Abstract: We present a comprehensive real-world evaluation of AI-assisted software
development tools deployed at enterprise scale. Over one year, 300 engineers
across multiple teams integrated an in-house AI platform (DeputyDev) that
combines code generation and automated review capabilities into their daily
workflows. Through rigorous cohort analysis, our study demonstrates
statistically significant productivity improvements, including an overall 31.8%
reduction in PR review cycle time.
  Developer adoption was strong, with 85% satisfaction for code review features
and 93% expressing a desire to continue using the platform. Adoption patterns
showed systematic scaling from 4% engagement in month 1 to 83% peak usage by
month 6, stabilizing at 60% active engagement. Top adopters achieved a 61%
increase in code volume pushed to production, contributing to approximately 30
to 40% of code shipped to production through this tool, accounting for an
overall 28% increase in code shipment volume.
  Unlike controlled benchmark evaluations, our longitudinal analysis provides
empirical evidence from production environments, revealing both the
transformative potential and practical deployment challenges of integrating AI
into enterprise software development workflows.

</details>


### [6] [Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation](https://arxiv.org/abs/2509.19918)
*Micheline Bénédicte Moumoula,Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: XL-CoGen是一个协调多智能体架构，通过数据驱动的桥接语言选择机制，显著提升了多编程语言代码生成的质量，特别是在训练数据有限的语言上表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前软件系统使用多种编程语言构建，但大语言模型在不同语言上的代码生成能力差异很大，特别是对于训练数据有限的语言。现有方法往往孤立处理每种语言，未能充分利用跨语言的知识共享和模式复用。

Method: 采用协调多智能体架构，集成中间表示、代码生成、翻译和自动修复功能。核心创新是基于经验推导的转移矩阵选择桥接语言，通过早期输出验证、迭代错误纠正和中间产物复用作为后续翻译的上下文支架。

Result: 实验显示XL-CoGen相比最强微调基线提升13个百分点，比现有单语言多智能体方法提升高达30个百分点。消融研究证实兼容性引导的桥接显著优于基于LLM的启发式方法。

Conclusion: XL-CoGen通过累积的跨语言知识转移机制，有效解决了多编程语言代码生成的质量不均衡问题，证明了跨语言协作的价值。

Abstract: Producing high-quality code across multiple programming languages is
increasingly important as today's software systems are built on heterogeneous
stacks. Large language models (LLMs) have advanced the state of automated
programming, yet their proficiency varies sharply between languages, especially
those with limited training data such as Rust, Perl, OCaml, and Erlang. Many
current solutions including language-specific fine-tuning, multi-agent
orchestration, transfer learning, and intermediate-representation pipelines
still approach each target language in isolation, missing opportunities to
share knowledge or exploit recurring cross-language patterns.
  XL-CoGen tackles this challenge with a coordinated multi-agent architecture
that integrates intermediate representation, code generation, translation, and
automated repair. Its distinguishing feature is a data-driven mechanism for
selecting bridging languages: empirically derived transfer matrices identify
the best intermediate languages based on demonstrated translation success
rather than raw generation accuracy. The system performs early output
validation, iteratively corrects errors, and reuses intermediate artifacts as
contextual scaffolds for subsequent translations.
  Extensive experiments show that XL-CoGen yields notable improvements with 13
percentage-point gains over the strongest fine-tuned baseline and as much as 30
percentage points over existing single-language multi-agent methods. Ablation
studies further demonstrate that compatibility-guided bridging significantly
outperforms LLM-based heuristics, confirming the value of cumulative
cross-language knowledge transfer.

</details>


### [7] [Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories](https://arxiv.org/abs/2509.20010)
*Xiaoning Ren,Yuhang Ye,Xiongfei Wu,Yueming Wu,Yinxing Xue*

Main category: cs.SE

TL;DR: 该论文提出了神经网络物料清单（NNBOM）的概念，用于分析神经网络软件的演化趋势，并基于55,997个PyTorch GitHub仓库构建了大规模数据集，进行了实证研究。


<details>
  <summary>Details</summary>
Motivation: 传统软件物料清单（SBOM）不适合分析神经网络软件，因为神经网络依赖预定义模块和预训练模型，具有独特的组件结构和重用模式。现有的AI物料清单（AIBOM）也缺乏大规模演化分析的实际实现。

Method: 创建了NNBOM数据集构造方法，从55,997个精选的PyTorch GitHub仓库中收集数据，分类记录其第三方库、预训练模型和模块。基于此数据库进行神经网络软件演化的实证分析。

Result: 构建了大规模NNBOM数据库，分析了神经网络软件在软件规模、组件重用和跨领域依赖等方面的演化趋势，并开发了两个原型应用来展示分析的实际价值。

Conclusion: NNBOM为神经网络软件的演化分析提供了有效工具，有助于开发者和维护者了解长期趋势，指导开发工作。

Abstract: Neural networks have become integral to many fields due to their exceptional
performance. The open-source community has witnessed a rapid influx of neural
network (NN) repositories with fast-paced iterations, making it crucial for
practitioners to analyze their evolution to guide development and stay ahead of
trends. While extensive research has explored traditional software evolution
using Software Bill of Materials (SBOMs), these are ill-suited for NN software,
which relies on pre-defined modules and pre-trained models (PTMs) with distinct
component structures and reuse patterns. Conceptual AI Bills of Materials
(AIBOMs) also lack practical implementations for large-scale evolutionary
analysis. To fill this gap, we introduce the Neural Network Bill of Material
(NNBOM), a comprehensive dataset construct tailored for NN software. We create
a large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,
cataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct
a comprehensive empirical study of neural network software evolution across
software scale, component reuse, and inter-domain dependency, providing
maintainers and developers with a holistic view of its long-term trends.
Building on these findings, we develop two prototype applications,
\textit{Multi repository Evolution Analyzer} and \textit{Single repository
Component Assessor and Recommender}, to demonstrate the practical value of our
analysis.

</details>


### [8] [V-GameGym: Visual Game Generation for Code Large Language Models](https://arxiv.org/abs/2509.20136)
*Wei Zhang,Jack Yang,Renshuai Tao,Lingzheng Chai,Shawn Guo,Jiajun Wu,Xiaoming Chen,Ganqu Cui,Ning Ding,Xander Xu,Hu Wei,Bowen Zhou*

Main category: cs.SE

TL;DR: V-GameGym是一个针对视觉游戏开发的多模态代码生成基准测试，包含2,219个高质量样本，旨在填补当前代码LLM在算法问题解决与实用游戏开发需求之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型基准主要关注单模态编程任务，忽视了游戏开发中关键的视觉美学、可玩性和用户参与度等指标，无法满足实际游戏部署的全面需求。

Method: 采用基于聚类的筛选方法从真实仓库中提取100个主题集群的样本，构建多模态评估框架，使用自动化LLM驱动管道在完整UI沙盒环境中进行视觉代码合成。

Result: V-GameGym有效弥合了代码生成准确性与实际游戏开发工作流之间的差距，为视觉编程和交互元素生成提供了可量化的质量指标。

Conclusion: 该基准测试为评估代码LLM在视觉游戏开发方面的能力提供了全面框架，推动了从单纯语法正确性到实际应用质量的评估转变。

Abstract: Code large language models have demonstrated remarkable capabilities in
programming tasks, yet current benchmarks primarily focus on single modality
rather than visual game development. Most existing code-related benchmarks
evaluate syntax correctness and execution accuracy, overlooking critical
game-specific metrics such as playability, visual aesthetics, and user
engagement that are essential for real-world deployment. To address the gap
between current LLM capabilities in algorithmic problem-solving and competitive
programming versus the comprehensive requirements of practical game
development, we present V-GameGym, a comprehensive benchmark comprising 2,219
high-quality samples across 100 thematic clusters derived from real-world
repositories, adopting a novel clustering-based curation methodology to ensure
both diversity and structural completeness. Further, we introduce a multimodal
evaluation framework with an automated LLM-driven pipeline for visual code
synthesis using complete UI sandbox environments. Our extensive analysis
reveals that V-GameGym effectively bridges the gap between code generation
accuracy and practical game development workflows, providing quantifiable
quality metrics for visual programming and interactive element generation.

</details>


### [9] [Enhancing Requirement Traceability through Data Augmentation Using Large Language Models](https://arxiv.org/abs/2509.20149)
*Jianzhang Zhang,Jialong Zhou,Nan Niu,Chuang Liu*

Main category: cs.SE

TL;DR: 本文提出了一种利用大型语言模型进行数据增强的方法来解决需求追踪中的数据稀缺问题，通过提示技术生成增强的需求-代码追踪链接，显著提升了追踪模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化需求追踪方法受到训练数据稀缺和需求与代码之间语义鸿沟的限制，本研究旨在通过LLM数据增强来解决这些问题。

Method: 使用四种LLM（Gemini 1.5 Pro、Claude 3、GPT-3.5、GPT-4）和零样本/少样本提示模板生成追踪链接，并优化追踪模型的编码器组件以适应增强数据。

Result: 实验结果表明该方法显著提升了模型性能，F1分数最高提升了28.59%。

Conclusion: 该方法证明了利用LLM进行数据增强在需求追踪中的有效性和实际应用潜力。

Abstract: Requirements traceability is crucial in software engineering to ensure
consistency between requirements and code. However, existing automated
traceability methods are constrained by the scarcity of training data and
challenges in bridging the semantic gap between artifacts. This study aims to
address the data scarcity problem in requirements traceability by employing
large language models (LLMs) for data augmentation. We propose a novel approach
that utilizes prompt-based techniques with LLMs to generate augmented
requirement-to-code trace links, thereby enhancing the training dataset. Four
LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both
zero-shot and few-shot templates. Moreover, we optimized the encoder component
of the tracing model to improve its efficiency and adaptability to augmented
data. The key contributions of this paper are: (1) proposing and evaluating
four prompt templates for data augmentation; (2) providing a comparative
analysis of four LLMs for generating trace links; (3) enhancing the model's
encoder for improved adaptability to augmented datasets. Experimental results
show that our approach significantly enhances model performance, achieving an
F1 score improvement of up to 28.59%, thus demonstrating its effectiveness and
potential for practical application.

</details>


### [10] [Benchmarking Web API Integration Code Generation](https://arxiv.org/abs/2509.20172)
*Daniel Maninger,Leon Chemnitz,Amir Molzam Sharifloo,Jannis Brugger,Mira Mezini*

Main category: cs.SE

TL;DR: 该论文评估了大型语言模型在生成Web API集成代码方面的能力，发现现有开源模型表现不佳，无法解决超过40%的任务。


<details>
  <summary>Details</summary>
Motivation: API集成是数字基础设施的核心，但编写正确的API调用代码具有挑战性。虽然LLMs在软件开发中很受欢迎，但它们在自动化生成Web API集成代码方面的有效性尚未被探索。

Method: 提出了一个数据集和评估管道，用于评估LLMs生成Web API调用代码的能力，并对多个开源LLMs进行了实验。

Result: 实验显示生成API调用存在重大挑战，导致幻觉端点、参数使用错误等问题。评估的开源模型都无法解决超过40%的任务。

Conclusion: 当前的开源LLMs在生成Web API集成代码方面能力有限，需要进一步改进。

Abstract: API integration is a cornerstone of our digital infrastructure, enabling
software systems to connect and interact. However, as shown by many studies,
writing or generating correct code to invoke APIs, particularly web APIs, is
challenging. Although large language models~(LLMs) have become popular in
software development, their effectiveness in automating the generation of web
API integration code remains unexplored. In order to address this, we present a
dataset and evaluation pipeline designed to assess the ability of LLMs to
generate web API invocation code. Our experiments with several open-source LLMs
reveal that generating API invocations poses a significant challenge, resulting
in hallucinated endpoints, incorrect argument usage, and other errors. None of
the evaluated open-source models were able to solve more than 40% of the tasks.

</details>


### [11] [The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation](https://arxiv.org/abs/2509.20215)
*Guang Yang,Wei Zheng,Xiang Chen,Yifan Sun,Fengji Zhang,Terry Yue Zhuo*

Main category: cs.SE

TL;DR: 提出VCD-RNK模型解决LLMs在Verilog生成中的语义对齐问题，通过Verilog特定推理进行代码重排序


<details>
  <summary>Details</summary>
Motivation: LLMs在Verilog生成中面临领域知识不足的挑战，硬件工程师需要可信赖的解决方案而非不确定的候选方案

Method: VCD-RNK模型通过三个维度提炼专家知识：代码语义分析、测试用例生成和功能正确性评估，在推理时模拟这些过程避免计算密集型测试执行

Result: VCD-RNK能够有效进行Verilog代码重排序，提高生成代码的可靠性

Conclusion: VCD-RNK为Verilog代码生成提供了高效的语义对齐解决方案，解决了现有方法中计算密集的问题

Abstract: LLMs face significant challenges in Verilog generation due to limited
domain-specific knowledge. While sampling techniques improve pass@k metrics,
hardware engineers need one trustworthy solution rather than uncertain
candidates. To bridge this gap, we formulate it as a semantic alignment problem
between requirements and Verilog implementations, and propose VCD-RNK, a
discriminator model tailored for efficient Verilog code reranking.
Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling
expert knowledge across three dimensions: code semantic analysis, test case
generation, and functional correctness assessment. By explicitly simulating the
above reasoning processes during inference, VCD-RNK effectively avoids
computationally intensive test execution in existing methods.

</details>


### [12] [Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs](https://arxiv.org/abs/2509.20300)
*Jannis Kiesel,Jonathan Heiss*

Main category: cs.SE

TL;DR: 本文提出了一种基于零知识证明（ZKP）的方法，用于在保护商业机密的同时实现业务流程的可验证执行。通过将ZK虚拟机集成到业务流程管理引擎中，支持链式可验证计算，并以产品碳足迹为例展示了该方法的应用。


<details>
  <summary>Details</summary>
Motivation: 解决跨组织业务流程中确保流程完整性而不泄露商业机密信息的挑战。传统方法难以在验证流程正确性的同时保护敏感的商业数据。

Method: 将ZK虚拟机（zkVMs）集成到业务流程管理引擎中，构建系统架构和原型实现。支持通过证明组合实现链式可验证计算，并在产品碳足迹模型中评估不同ZKP证明变体的效率。

Result: 实验评估表明，该方法能够在给定机密性约束下自动化流程验证，组织可以证明和验证可验证流程的完整性而无需暴露敏感信息。

Conclusion: ZKP技术可以有效地集成到业务流程管理生命周期中，为跨组织协作提供既保护机密性又确保流程完整性的解决方案。

Abstract: Ensuring the integrity of business processes without disclosing confidential
business information is a major challenge in inter-organizational processes.
This paper introduces a zero-knowledge proof (ZKP)-based approach for the
verifiable execution of business processes while preserving confidentiality. We
integrate ZK virtual machines (zkVMs) into business process management engines
through a comprehensive system architecture and a prototypical implementation.
Our approach supports chained verifiable computations through proof
compositions. On the example of product carbon footprinting, we model
sequential footprinting activities and demonstrate how organizations can prove
and verify the integrity of verifiable processes without exposing sensitive
information. We assess different ZKP proving variants within process models for
their efficiency in proving and verifying, and discuss the practical
integration of ZKPs throughout the Business Process Management (BPM) lifecycle.
Our experiment-driven evaluation demonstrates the automation of process
verification under given confidentiality constraints.

</details>


### [13] [Protocol Testing with I/O Grammars](https://arxiv.org/abs/2509.20308)
*Alexander Liggesmeyer,José Antonio Zamudio Amaya,Andreas Zeller*

Main category: cs.SE

TL;DR: 本文提出了一种结合输入生成和输出检查的协议测试新方法，使用I/O语法完整指定协议语法和语义，并通过FANDANGO框架实现测试生成器、模拟对象和预言机的多功能工具。


<details>
  <summary>Details</summary>
Motivation: 协议测试面临两个基本问题：需要生成语法语义正确且多样化的输入，以及需要预言机来检查输出是否正确。现有工具无法同时解决这两个问题。

Method: 引入I/O语法完整指定协议的消息、状态和交互，基于FANDANGO框架实现多功能测试工具，支持用户定义约束和k路径指导系统覆盖协议特征。

Result: 在DNS、FTP、SMTP等协议上的评估表明，I/O语法能正确完整地指定高级协议特征，系统化覆盖I/O语法比随机方法能更快覆盖输入和响应空间。

Conclusion: I/O语法为协议测试提供了统一的规范框架，结合系统化覆盖方法，显著提高了测试效率和效果。

Abstract: Generating software tests faces two fundamental problems. First, one needs to
_generate inputs_ that are syntactically and semantically correct, yet
sufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check
outputs_ whether a test case is correct or not. Both problems become apparent
in _protocol testing_, where inputs are messages exchanged between parties, and
outputs are the responses of these parties.
  In this paper, we propose a novel approach to protocol testing that combines
input generation and output checking in a single framework. We introduce _I/O
grammars_ as the first means to _completely_ specify the syntax and semantics
of protocols, including messages, states, and interactions. Our implementation,
based on the FANDANGO framework, takes a single I/O grammar, and can act as a
_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a
_server_, or both (or actually any number of parties), a versatility not found
in any existing tool or formalism. User-defined _constraints}_can have the
generator focus on arbitrary protocol features; $k$-path guidance
systematically covers states, messages, responses, and value alternatives in a
unified fashion.
  We evaluate the effectiveness of our approach by applying it to several
protocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can
specify advanced protocol features correctly and completely, while also
enabling output validation of the programs under test. In its evaluation, we
find that systematic coverage of the I/O grammar results in much quicker
coverage of the input and response spaces (and thus functionality) compared to
the random-based state-of-the-art approaches.

</details>


### [14] [Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study](https://arxiv.org/abs/2509.20353)
*Viktoria Stray,Elias Goldmann Brandtzæg,Viggo Tellefsen Wivestad,Astri Barbala,Nils Brede Moe*

Main category: cs.SE

TL;DR: 本研究调查了生成式AI工具GitHub Copilot对开发者活动和感知生产力的实际影响，通过混合方法案例研究发现Copilot用户在使用工具前后在提交活动指标上没有显著变化，但主观生产力感知存在差异。


<details>
  <summary>Details</summary>
Motivation: 了解生成式AI工具在实际工作环境中的真实影响，特别是对开发者活动和生产力感知的影响，以填补当前研究中缺乏实证数据的空白。

Method: 采用混合方法案例研究，分析NAV IT组织中26,317个非合并提交数据，对比25名Copilot用户和14名非用户的活动指标，并结合问卷调查和13次访谈。

Result: Copilot用户在使用工具前后在提交活动指标上没有统计学显著变化，但用户群体本身比非用户更活跃。主观调查显示生产力感知与实际活动指标存在差异。

Conclusion: 生成式AI工具的实际影响可能无法通过传统的提交活动指标完全捕捉，需要结合主观体验来全面评估其生产力影响。

Abstract: This study investigates the real-world impact of the generative AI (GenAI)
tool GitHub Copilot on developer activity and perceived productivity. We
conducted a mixed-methods case study in NAV IT, a large public sector agile
organization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's
GitHub repositories over a two-year period, focusing on commit-based activity
metrics from 25 Copilot users and 14 non-users. The analysis was complemented
by survey responses on their roles and perceived productivity, as well as 13
interviews. Our analysis of activity metrics revealed that individuals who used
Copilot were consistently more active than non-users, even prior to Copilot's
introduction. We did not find any statistically significant changes in
commit-based activity for Copilot users after they adopted the tool, although
minor increases were observed. This suggests a discrepancy between changes in
commit-based metrics and the subjective experience of productivity.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [15] [Identifying and Addressing User-level Security Concerns in Smart Homes Using "Smaller" LLMs](https://arxiv.org/abs/2509.19485)
*Hafijul Hoque Chowdhury,Riad Ahmed Anonto,Sourov Jajodia,Suryadipta Majumdar,Md. Shohrab Hossain*

Main category: cs.CR

TL;DR: 本文开发了一个针对智能家居安全的问答系统，通过分析论坛数据识别主要安全关切，并利用小型Transformer模型构建资源友好的解决方案。


<details>
  <summary>Details</summary>
Motivation: 智能家居设备快速增长带来了安全风险，但用户通常只能通过复杂的技术资料获取安全信息，这与普通用户的使用习惯不符，进一步威胁智能家居安全。

Method: 从公共论坛收集问答数据构建数据集，使用LDA主题建模提取主要安全关切，并微调T5和Flan-T5等小型Transformer模型构建专门的问答系统。

Result: 实验表明，该方法显著提升了基础模型的性能，能够为智能家居用户提供准确相关的安全答案。

Conclusion: 小型Transformer模型在资源受限的智能家居环境中具有部署优势，能够有效帮助用户解决常见的安全关切问题。

Abstract: With the rapid growth of smart home IoT devices, users are increasingly
exposed to various security risks, as evident from recent studies. While
seeking answers to know more on those security concerns, users are mostly left
with their own discretion while going through various sources, such as online
blogs and technical manuals, which may render higher complexity to regular
users trying to extract the necessary information. This requirement does not go
along with the common mindsets of smart home users and hence threatens the
security of smart homes furthermore. In this paper, we aim to identify and
address the major user-level security concerns in smart homes. Specifically, we
develop a novel dataset of Q&A from public forums, capturing practical security
challenges faced by smart home users. We extract major security concerns in
smart homes from our dataset by leveraging the Latent Dirichlet Allocation
(LDA). We fine-tune relatively "smaller" transformer models, such as T5 and
Flan-T5, on this dataset to build a QA system tailored for smart home security.
Unlike larger models like GPT and Gemini, which are powerful but often resource
hungry and require data sharing, smaller models are more feasible for
deployment in resource-constrained or privacy-sensitive environments like smart
homes. The dataset is manually curated and supplemented with synthetic data to
explore its potential impact on model performance. This approach significantly
improves the system's ability to deliver accurate and relevant answers, helping
users address common security concerns with smart home IoT devices. Our
experiments on real-world user concerns show that our work improves the
performance of the base models.

</details>


### [16] [Knock-Knock: Black-Box, Platform-Agnostic DRAM Address-Mapping Reverse Engineering](https://arxiv.org/abs/2509.19568)
*Antoine Plin,Lorenzo Casalino,Thomas Rokicki,Ruben Salvador*

Main category: cs.CR

TL;DR: 本文提出了一种基于线性代数理论的高效算法，用于完全恢复物理地址到DRAM地址的映射关系，解决了现有启发式方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代SoC使用未文档化的线性地址加扰函数来混淆DRAM寻址，这阻碍了DRAM感知的性能优化和安全分析（特别是Rowhammer攻击）。现有逆向工程方法不完整、成本高且不实用。

Method: 在GF(2)有限域上建立线性代数模型，通过分析行缓冲冲突的时间指纹，开发了噪声鲁棒的平台无关算法，能在多项式时间内恢复完整的bank掩码基，并扩展到复杂行映射的自动恢复。

Result: 在嵌入式和服务级架构上的评估显示，该方法成功重建了已知映射并发现了新的加扰函数，在所有测试平台上达到99%的召回率和准确率，即使在500GB以上DRAM的系统上也只需几分钟。

Conclusion: 该方法为DRAM逆向工程提供了自动化、理论化的准确路径，显著提升了现有方法的可扩展性和效率。

Abstract: Modern Systems-on-Chip (SoCs) employ undocumented linear address-scrambling
functions to obfuscate DRAM addressing, which complicates DRAM-aware
performance optimizations and hinders proactive security analysis of DRAM-based
attacks; most notably, Rowhammer. Although previous work tackled the issue of
reversing physical-to-DRAM mapping, existing heuristic-based
reverse-engineering approaches are partial, costly, and impractical for
comprehensive recovery. This paper establishes a rigorous theoretical
foundation and provides efficient practical algorithms for black-box, complete
physical-to-DRAM address-mapping recovery.
  We first formulate the reverse-engineering problem within a linear algebraic
model over the finite field GF(2). We characterize the timing fingerprints of
row-buffer conflicts, proving a relationship between a bank addressing matrix
and an empirically constructed matrix of physical addresses. Based on this
characterization, we develop an efficient, noise-robust, and fully
platform-agnostic algorithm to recover the full bank-mask basis in polynomial
time, a significant improvement over the exponential search from previous
works. We further generalize our model to complex row mappings, introducing new
hardware-based hypotheses that enable the automatic recovery of a row basis
instead of previous human-guided contributions.
  Evaluations across embedded and server-class architectures confirm our
method's effectiveness, successfully reconstructing known mappings and
uncovering previously unknown scrambling functions. Our method provides a 99%
recall and accuracy on all tested platforms. Most notably, Knock-Knock runs in
under a few minutes, even on systems with more than 500GB of DRAM, showcasing
the scalability of our method. Our approach provides an automated, principled
pathway to accurate DRAM reverse engineering.

</details>


### [17] [SoK: A Systematic Review of Malware Ontologies and Taxonomies and Implications for the Quantum Era](https://arxiv.org/abs/2509.19650)
*Dehinde Molade,Dave Ormrod,Mamello Thinyane,Nalin Arachchilage,Jill Slay*

Main category: cs.CR

TL;DR: 本文通过系统文献综述和知识框架分析，探讨量子恶意软件的基本性质和影响，为未来开发适当的缓解措施和防御手段提供基础。


<details>
  <summary>Details</summary>
Motivation: 量子恶意软件是一个真实且日益增长的安全威胁，如果不尽早解决，将对科学和技术产生灾难性影响。如果被武器化或利用，特别是在错误的手中，恶意软件将破坏由下一代量子架构支持的高度复杂的关键系统。

Method: 通过系统文献综述（SLR），利用本体论和分类法等知识框架来探索恶意软件，分析恶意行为如何转化为对量子技术的攻击。研究采用欧洲量子技术能力框架（CFQT）作为指导，将恶意软件行为映射到多个能力层。

Result: 研究提供了分析恶意软件对量子技术严重性的视角，并在这个新兴领域创建了基础。

Conclusion: 该研究为理解量子恶意软件的威胁提供了系统框架，并为开发针对量子技术的防御措施奠定了基础。

Abstract: The threat of quantum malware is real and a growing security concern that
will have catastrophic scientific and technological impacts, if not addressed
early. If weaponised or exploited especially by the wrong hands, malware will
undermine highly sophisticated critical systems supported by next-generation
quantum architectures, for example, in defence, communications, energy, and
space. This paper explores the fundamental nature and implications of quantum
malware to enable the future development of appropriate mitigations and
defences, thereby protecting critical infrastructure. By conducting a
systematic literature review (SLR) that draws on knowledge frameworks such as
ontologies and taxonomies to explore malware, this provides insights into how
malicious behaviours can be translated into attacks on quantum technologies,
thereby providing a lens to analyse the severity of malware against quantum
technologies. This study employs the European Competency Framework for Quantum
Technologies (CFQT) as a guide to map malware behaviour to several competency
layers, creating a foundation in this emerging field.

</details>


### [18] [Unmasking Fake Careers: Detecting Machine-Generated Career Trajectories via Multi-layer Heterogeneous Graphs](https://arxiv.org/abs/2509.19677)
*Michiharu Yamashita,Thanh Tran,Delvin Ce Zhang,Dongwon Lee*

Main category: cs.CR

TL;DR: 该论文提出了一种新的检测方法CareerScape，用于识别LLM生成的虚假职业轨迹，通过构建异构分层多图层框架来有效检测结构化职业数据中的合成内容。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，能够生成高度逼真的合成数据，特别是虚假简历中的职业轨迹，这带来了新的安全漏洞，需要有效的检测方法。

Method: 提出CareerScape框架，构建一个基于真实简历的全局图，建模职业实体及其关系，采用结构感知框架，通过增强用户特定子图与全局图中可信邻域信息来捕获全局结构模式和局部不一致性。

Result: 实验结果表明，CareerScape相对最先进的基线方法性能提升5.8-85.0%，证明了结构感知检测在识别机器生成内容中的重要性。

Conclusion: 传统基于文本的检测器在处理结构化职业数据时表现不佳，而结构感知的CareerScape框架能有效检测LLM生成的虚假职业轨迹，为机器生成内容检测提供了新思路。

Abstract: The rapid advancement of Large Language Models (LLMs) has enabled the
generation of highly realistic synthetic data. We identify a new vulnerability,
LLMs generating convincing career trajectories in fake resumes and explore
effective detection methods. To address this challenge, we construct a dataset
of machine-generated career trajectories using LLMs and various methods, and
demonstrate that conventional text-based detectors perform poorly on structured
career data. We propose CareerScape, a novel heterogeneous, hierarchical
multi-layer graph framework that models career entities and their relations in
a unified global graph built from genuine resumes. Unlike conventional
classifiers that treat each instance independently, CareerScape employs a
structure-aware framework that augments user-specific subgraphs with trusted
neighborhood information from a global graph, enabling the model to capture
both global structural patterns and local inconsistencies indicative of
synthetic career paths. Experimental results show that CareerScape outperforms
state-of-the-art baselines by 5.8-85.0% relatively, highlighting the importance
of structure-aware detection for machine-generated content.

</details>


### [19] [A Set of Generalized Components to Achieve Effective Poison-only Clean-label Backdoor Attacks with Collaborative Sample Selection and Triggers](https://arxiv.org/abs/2509.19947)
*Zhixiao Wu,Yao Lu,Jie Wen,Hao Sun,Qi Zhou,Guangming Lu*

Main category: cs.CR

TL;DR: 该论文提出了一种双向协作框架，通过组件A、B、C协同优化样本选择和触发器设计，显著提升仅毒化无标签后门攻击的攻击成功率和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将样本选择和触发器设计孤立处理，导致攻击成功率和隐蔽性提升有限。论文旨在探索两者之间的双向协作关系，解决这一困境。

Method: 提出三个协作组件：组件A基于触发器规模优化样本选择；组件B通过相似性选择提升隐蔽性；组件C基于人类视觉系统敏感性调整RGB毒化强度。

Result: 该方法能够显著提高攻击成功率和隐蔽性，同时保持在不同攻击中的泛化能力。

Conclusion: 通过双向协作框架，实现了样本选择与触发器设计的协同优化，为仅毒化无标签后门攻击提供了有效的解决方案。

Abstract: Poison-only Clean-label Backdoor Attacks aim to covertly inject
attacker-desired behavior into DNNs by merely poisoning the dataset without
changing the labels. To effectively implant a backdoor, multiple
\textbf{triggers} are proposed for various attack requirements of Attack
Success Rate (ASR) and stealthiness. Additionally, sample selection enhances
clean-label backdoor attacks' ASR by meticulously selecting ``hard'' samples
instead of random samples to poison. Current methods 1) usually handle the
sample selection and triggers in isolation, leading to severely limited
improvements on both ASR and stealthiness. Consequently, attacks exhibit
unsatisfactory performance on evaluation metrics when converted to PCBAs via a
mere stacking of methods. Therefore, we seek to explore the bidirectional
collaborative relations between the sample selection and triggers to address
the above dilemma. 2) Since the strong specificity within triggers, the simple
combination of sample selection and triggers fails to substantially enhance
both evaluation metrics, with generalization preserved among various attacks.
Therefore, we seek to propose a set of components to significantly improve both
stealthiness and ASR based on the commonalities of attacks. Specifically,
Component A ascertains two critical selection factors, and then makes them an
appropriate combination based on the trigger scale to select more reasonable
``hard'' samples for improving ASR. Component B is proposed to select samples
with similarities to relevant trigger implanted samples to promote
stealthiness. Component C reassigns trigger poisoning intensity on RGB colors
through distinct sensitivity of the human visual system to RGB for higher ASR,
with stealthiness ensured by sample selection, including Component B.
Furthermore, all components can be strategically integrated into diverse PCBAs.

</details>


### [20] [CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning](https://arxiv.org/abs/2509.20166)
*Lauren Deason,Adam Bali,Ciprian Bejean,Diana Bolocan,James Crnkovich,Ioana Croitoru,Krishna Durai,Chase Midler,Calin Miron,David Molnar,Brad Moon,Bruno Ostarcevic,Alberto Peltea,Matt Rosenberg,Catalin Sandu,Arthur Saputkin,Sagar Shah,Daniel Stan,Ernest Szocs,Shengye Wan,Spencer Whitman,Sven Krasser,Joshua Saxe*

Main category: cs.CR

TL;DR: 本文介绍了CyberSOCEval，一个针对网络安全运营中心（SOC）操作的新开源基准测试套件，旨在评估大型语言模型在恶意软件分析和威胁情报推理等核心防御任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全防御者面临大量安全警报和威胁情报的挑战，而现有的LLM评估无法充分反映真实世界防御场景的需求。恶意行为者正在使用AI扩大网络攻击规模，因此需要开源基准来推动防御者和模型开发者的采用和社区驱动的改进。

Method: 在CyberSecEval 4框架内开发了CyberSOCEval基准测试套件，专门针对恶意软件分析和威胁情报推理两个核心防御领域进行评估。

Result: 评估显示更大、更现代的LLM表现更好，证实了训练扩展定律。但推理模型在测试时扩展方面未能获得与编码和数学相同的提升，表明这些模型尚未针对网络安全分析进行推理训练。当前LLM远未达到评估饱和点。

Conclusion: CyberSOCEval为AI开发者提供了显著挑战，以改进网络防御能力，指出了模型在网络安全推理训练方面的关键改进机会。

Abstract: Today's cyber defenders are overwhelmed by a deluge of security alerts,
threat intelligence signals, and shifting business context, creating an urgent
need for AI systems to enhance operational security work. While Large Language
Models (LLMs) have the potential to automate and scale Security Operations
Center (SOC) operations, existing evaluations do not fully assess the scenarios
most relevant to real-world defenders. This lack of informed evaluation impacts
both AI developers and those applying LLMs to SOC automation. Without clear
insight into LLM performance in real-world security scenarios, developers lack
a north star for development, and users cannot reliably select the most
effective models. Meanwhile, malicious actors are using AI to scale cyber
attacks, highlighting the need for open source benchmarks to drive adoption and
community-driven improvement among defenders and model developers. To address
this, we introduce CyberSOCEval, a new suite of open source benchmarks within
CyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in
two tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive
domains with inadequate coverage in current benchmarks. Our evaluations show
that larger, more modern LLMs tend to perform better, confirming the training
scaling laws paradigm. We also find that reasoning models leveraging test time
scaling do not achieve the same boost as in coding and math, suggesting these
models have not been trained to reason about cybersecurity analysis, and
pointing to a key opportunity for improvement. Finally, current LLMs are far
from saturating our evaluations, showing that CyberSOCEval presents a
significant challenge for AI developers to improve cyber defense capabilities.

</details>


### [21] [STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation](https://arxiv.org/abs/2509.20190)
*Tanmay Khule,Stefan Marksteiner,Jose Alguindigue,Hannes Fuchs,Sebastian Fischmeister,Apurva Narayan*

Main category: cs.CR

TL;DR: STAF框架利用LLM和自校正RAG技术，从攻击树自动生成可执行的安全测试用例，显著提升汽车安全测试的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前汽车安全测试中，从攻击树生成全面测试用例仍是劳动密集型、易出错的任务，缺乏自动化解决方案。

Method: 采用四步自校正RAG框架，结合LLM（GPT-4.1和DeepSeek）自动生成可执行的汽车安全测试套件，并与自动化测试框架集成。

Result: 相比通用LLM，定制化方法在效率、准确性、可扩展性和工作流集成方面均有显著提升。

Conclusion: STAF框架通过连接安全开发过程中的关键要素，实现了汽车安全测试方法的重大进步。

Abstract: In modern automotive development, security testing is critical for
safeguarding systems against increasingly advanced threats. Attack trees are
widely used to systematically represent potential attack vectors, but
generating comprehensive test cases from these trees remains a labor-intensive,
error-prone task that has seen limited automation in the context of testing
vehicular systems. This paper introduces STAF (Security Test Automation
Framework), a novel approach to automating security test case generation.
Leveraging Large Language Models (LLMs) and a four-step self-corrective
Retrieval-Augmented Generation (RAG) framework, STAF automates the generation
of executable security test cases from attack trees, providing an end-to-end
solution that encompasses the entire attack surface. We particularly show the
elements and processes needed to provide an LLM to actually produce sensible
and executable automotive security test suites, along with the integration with
an automated testing framework. We further compare our tailored approach with
general purpose (vanilla) LLMs and the performance of different LLMs (namely
GPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our
operation step-by-step in a concrete case study. Our results show significant
improvements in efficiency, accuracy, scalability, and easy integration in any
workflow, marking a substantial advancement in automating automotive security
testing methodologies. Using TARAs as an input for verfication tests, we create
synergies by connecting two vital elements of a secure automotive development
process.

</details>


### [22] [Investigating Security Implications of Automatically Generated Code on the Software Supply Chain](https://arxiv.org/abs/2509.20277)
*Xiaofan Li,Xing Gao*

Main category: cs.CR

TL;DR: 本文研究了大型语言模型（LLMs）生成代码时存在的安全问题对软件供应链（SSC）造成的威胁，识别了11种潜在威胁，并开发了SSCGuard工具进行分析，最后提出了两种防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在代码生成中的广泛应用，其固有的问题（如虚构、错误信息和过时数据）可能导致严重的软件供应链安全威胁，需要系统性地研究这些威胁。

Method: 设计了SSCGuard工具，基于在线收集的SSC相关问题生成439,138个提示，分析GPT和Llama等四种流行LLMs的响应，识别了11种SSC相关威胁。

Result: 研究发现所有识别的SSC相关威胁持续存在，部分威胁可能导致软件被劫持，而其他威胁可能随时间推移潜在地破坏软件安全。

Conclusion: 提出了基于提示的防御机制Chain-of-Confirmation以减少虚构问题，以及基于中间件的防御机制来提醒用户各种SSC威胁，以减轻这些风险。

Abstract: In recent years, various software supply chain (SSC) attacks have posed
significant risks to the global community. Severe consequences may arise if
developers integrate insecure code snippets that are vulnerable to SSC attacks
into their products. Particularly, code generation techniques, such as large
language models (LLMs), have been widely utilized in the developer community.
However, LLMs are known to suffer from inherent issues when generating code,
including fabrication, misinformation, and reliance on outdated training data,
all of which can result in serious software supply chain threats. In this
paper, we investigate the security threats to the SSC that arise from these
inherent issues. We examine three categories of threats, including eleven
potential SSC-related threats, related to external components in source code,
and continuous integration configuration files. We find some threats in
LLM-generated code could enable attackers to hijack software and workflows,
while some others might cause potential hidden threats that compromise the
security of the software over time. To understand these security impacts and
severity, we design a tool, SSCGuard, to generate 439,138 prompts based on
SSC-related questions collected online, and analyze the responses of four
popular LLMs from GPT and Llama. Our results show that all identified
SSC-related threats persistently exist. To mitigate these risks, we propose a
novel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce
fabrication, and a middleware-based defense that informs users of various SSC
threats.

</details>


### [23] [Monitoring Violations of Differential Privacy over Time](https://arxiv.org/abs/2509.20283)
*Önder Askin,Tim Kutta,Holger Dette*

Main category: cs.CR

TL;DR: 本文提出了一种新的差分隐私持续审计方法，用于监控在开发和部署过程中可能发生变化的机制，通过利用整个部署历史信息来减少采样需求并维持审计可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有隐私审计方法主要针对静态机制，无法有效应对在开发或部署过程中可能发生变化的机制的持续监控需求。重复运行现有审计方法需要过多采样，且可靠性会随时间下降。

Method: 提出一种新的监控程序，从算法的整个部署历史中提取信息，通过利用历史数据来减少采样需求，同时维持审计结果的可靠性。

Result: 理论分析和实验评估表明，该方法对文献中的重要机制具有有效性能，能够显著减少采样需求并维持审计可靠性。

Conclusion: 该方法为差分隐私机制的持续监控提供了可行的解决方案，具有理论保证和实际应用价值。

Abstract: Auditing differential privacy has emerged as an important area of research
that supports the design of privacy-preserving mechanisms. Privacy audits help
to obtain empirical estimates of the privacy parameter, to expose flawed
implementations of algorithms and to compare practical with theoretical privacy
guarantees. In this work, we investigate an unexplored facet of privacy
auditing: the sustained auditing of a mechanism that can go through changes
during its development or deployment. Monitoring the privacy of algorithms over
time comes with specific challenges. Running state-of-the-art (static) auditors
repeatedly requires excessive sampling efforts, while the reliability of such
methods deteriorates over time without proper adjustments. To overcome these
obstacles, we present a new monitoring procedure that extracts information from
the entire deployment history of the algorithm. This allows us to reduce
sampling efforts, while sustaining reliable outcomes of our auditor. We derive
formal guarantees with regard to the soundness of our methods and evaluate
their performance for important mechanisms from the literature. Our theoretical
findings and experiments demonstrate the efficacy of our approach.

</details>


### [24] [RAG Security and Privacy: Formalizing the Threat Model and Attack Surface](https://arxiv.org/abs/2509.20324)
*Atousa Arzanipour,Rouzbeh Behnia,Reza Ebrahimi,Kaushik Dutta*

Main category: cs.CR

TL;DR: 本文提出了首个检索增强生成（RAG）系统的正式威胁模型，定义了基于对手访问权限的分类法和关键威胁向量，为RAG系统的隐私和安全研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: RAG系统虽然能减少幻觉和提高事实一致性，但引入了不同于传统LLM的隐私和安全挑战，目前缺乏正式的威胁模型框架来系统分析这些风险。

Method: 提出了基于对手对模型组件和数据访问权限的结构化分类法，正式定义了文档级成员推断和数据投毒等关键威胁向量。

Result: 建立了RAG系统的第一个正式威胁模型，为理解RAG系统中的隐私和安全风险提供了理论基础。

Conclusion: 通过建立正式定义和攻击模型，为RAG系统的隐私和安全研究提供了更加严谨和原则性的理解基础。

Abstract: Retrieval-Augmented Generation (RAG) is an emerging approach in natural
language processing that combines large language models (LLMs) with external
document retrieval to produce more accurate and grounded responses. While RAG
has shown strong potential in reducing hallucinations and improving factual
consistency, it also introduces new privacy and security challenges that differ
from those faced by traditional LLMs. Existing research has demonstrated that
LLMs can leak sensitive information through training data memorization or
adversarial prompts, and RAG systems inherit many of these vulnerabilities. At
the same time, reliance of RAG on an external knowledge base opens new attack
surfaces, including the potential for leaking information about the presence or
content of retrieved documents, or for injecting malicious content to
manipulate model behavior. Despite these risks, there is currently no formal
framework that defines the threat landscape for RAG systems. In this paper, we
address a critical gap in the literature by proposing, to the best of our
knowledge, the first formal threat model for retrieval-RAG systems. We
introduce a structured taxonomy of adversary types based on their access to
model components and data, and we formally define key threat vectors such as
document-level membership inference and data poisoning, which pose serious
privacy and integrity risks in real-world deployments. By establishing formal
definitions and attack models, our work lays the foundation for a more rigorous
and principled understanding of privacy and security in RAG systems.

</details>


### [25] [chainScale: Secure Functionality-oriented Scalability for Decentralized Resource Markets](https://arxiv.org/abs/2509.20356)
*Mohamed E. Najd,Ghada Almashaqbeh*

Main category: cs.CR

TL;DR: chainScale是一个安全的混合侧链分片解决方案，旨在提升去中心化资源市场的吞吐量并降低延迟和存储占用。通过依赖侧链和功能导向的工作负载分割来并行处理流量，相比现有方案在吞吐量和延迟方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 去中心化资源市场虽然承诺降低成本、提高透明度和灵活性，但由于需要及时处理大量工作负载，面临严重的可扩展性问题。现有的区块链可扩展性解决方案无法有效应对这些市场的工作模型和流量模式。

Method: chainScale采用依赖侧链和功能导向的工作负载分割，将每个市场模块分配给一个侧链进行并行处理。引入分层工作负载共享进一步细分过载模块，使用加权矿工分配将有利害关系的矿工分配到关键模块的侧链。通过侧链同步保持主链作为系统状态的唯一真相，并采用剪枝技术丢弃过时记录。

Result: 实验表明，与基于单侧链的现有解决方案相比，chainScale将吞吐量提升4倍，确认延迟降低5倍。与分片技术相比，吞吐量提升2.5倍，延迟降低3.5倍。

Conclusion: chainScale通过创新的混合侧链分片架构有效解决了去中心化资源市场的可扩展性问题，在保持安全性的同时显著提升了系统性能。

Abstract: Decentralized resource markets are Web 3.0 applications that build
open-access platforms for trading digital resources among users without any
central management. They promise cost reduction, transparency, and flexible
service provision. However, these markets usually have large workload that must
be processed in a timely manner, leading to serious scalability problems.
Despite the large amount of work on blockchain scalability, existing solutions
are ineffective as they do not account for these markets' work models and
traffic patterns.
  We introduce chainScale, a secure hybrid sidechain-sharding solution that
aims to boost throughput of decentralized resource markets and reduce their
latency and storage footprint. At its core, chainScale leverages dependent
sidechains and functionality-oriented workload splitting to parallelize traffic
processing by having each market module assigned to a sidechain. Different from
sharding, chainScale does not incur any cross-sidechain transactions that tend
to be costly. chainScale introduces several techniques, including hierarchical
workload sharing that further sub-divides overloaded modules, and weighted
miner assignment that assigns miners with vested interest in the system to
critical modules' sidechains. Furthermore, chainScale employs sidechain syncing
to maintain the mainchain as the single truth of system state, and pruning to
discard stale records. Beside analyzing security, we build a proof-of-concept
implementation for a distributed file storage market as a use case. Our
experiments show that, compared to a single sidechain-based prior solution,
chainScale boosts throughput by 4x and reduces confirmation latency by 5x.
Also, they show that chainScale outperforms sharding by 2.5x in throughput and
3.5x in latency.

</details>


### [26] [FlyTrap: Physical Distance-Pulling Attack Towards Camera-based Autonomous Target Tracking Systems](https://arxiv.org/abs/2509.20362)
*Shaoyuan Xie,Mohamad Habib Fakih,Junchi Lu,Fayzah Alshammari,Ningfei Wang,Takami Sato,Halima Bouzidi,Mohammad Abdullah Al Faruque,Qi Alfred Chen*

Main category: cs.CR

TL;DR: 本文提出了一种新型的距离牵引攻击（DPA）和FlyTrap攻击框架，利用对抗性雨伞作为攻击向量，能够危险地减少自主目标跟踪无人机的跟踪距离，导致无人机被捕获、易受传感器攻击甚至物理碰撞。


<details>
  <summary>Details</summary>
Motivation: 自主目标跟踪系统在监控、边境控制等应用中广泛使用，但也可能被滥用于跟踪和破坏行为。ATT系统的安全性对实际应用至关重要，因此需要研究其安全漏洞。

Method: 提出FlyTrap攻击框架，采用对抗性雨伞作为可部署的领域特定攻击向量，通过渐进式距离牵引策略和可控的时空一致性设计，在真实世界设置中操纵ATT无人机。

Result: 评估结果表明FlyTrap能够将跟踪距离减少到可被捕获、传感器攻击甚至直接坠毁的范围，突显了ATT系统的严重安全风险。

Conclusion: 该研究揭示了ATT系统存在的紧急安全风险，对ATT系统的安全部署具有重要实际意义。

Abstract: Autonomous Target Tracking (ATT) systems, especially ATT drones, are widely
used in applications such as surveillance, border control, and law enforcement,
while also being misused in stalking and destructive actions. Thus, the
security of ATT is highly critical for real-world applications. Under the
scope, we present a new type of attack: distance-pulling attacks (DPA) and a
systematic study of it, which exploits vulnerabilities in ATT systems to
dangerously reduce tracking distances, leading to drone capturing, increased
susceptibility to sensor attacks, or even physical collisions. To achieve these
goals, we present FlyTrap, a novel physical-world attack framework that employs
an adversarial umbrella as a deployable and domain-specific attack vector.
FlyTrap is specifically designed to meet key desired objectives in attacking
ATT drones: physical deployability, closed-loop effectiveness, and
spatial-temporal consistency. Through novel progressive distance-pulling
strategy and controllable spatial-temporal consistency designs, FlyTrap
manipulates ATT drones in real-world setups to achieve significant system-level
impacts. Our evaluations include new datasets, metrics, and closed-loop
experiments on real-world white-box and even commercial ATT drones, including
DJI and HoverAir. Results demonstrate FlyTrap's ability to reduce tracking
distances within the range to be captured, sensor attacked, or even directly
crashed, highlighting urgent security risks and practical implications for the
safe deployment of ATT systems.

</details>


### [27] [LLMs as verification oracles for Solidity](https://arxiv.org/abs/2509.19153)
*Massimo Bartoletti,Enrico Lipparini,Livio Pompianu*

Main category: cs.CR

TL;DR: 本文评估了GPT-5作为智能合约验证工具的能力，发现推理导向的大型语言模型可以有效地作为验证预言机，为AI与形式化方法的融合开辟了新前沿。


<details>
  <summary>Details</summary>
Motivation: 智能合约的正确性至关重要，但现有形式化验证工具学习曲线陡峭且规范语言受限。LLMs在安全相关任务中显示出潜力，但能否作为验证预言机处理任意合约特定属性仍是开放性问题。

Method: 对GPT-5在大量验证任务数据集上进行系统性评估，将其输出与已建立的形式化验证工具进行比较，并在真实审计场景中评估其实用有效性。结合定量指标和定性分析。

Result: 研究表明，近期推理导向的LLMs作为验证预言机出人意料地有效。

Conclusion: LLMs可以作为智能合约验证的有效工具，这标志着AI与形式化方法在安全智能合约开发和审计领域融合的新前沿。

Abstract: Ensuring the correctness of smart contracts is critical, as even subtle flaws
can lead to severe financial losses. While bug detection tools able to spot
common vulnerability patterns can serve as a first line of defense, most
real-world exploits and losses stem from errors in the contract business logic.
Formal verification tools such as SolCMC and the Certora Prover address this
challenge, but their impact remains limited by steep learning curves and
restricted specification languages. Recent works have begun to explore the use
of large language models (LLMs) for security-related tasks such as
vulnerability detection and test generation. Yet, a fundamental question
remains open: can LLMs serve as verification oracles, capable of reasoning
about arbitrary contract-specific properties? In this paper, we provide the
first systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this
role. We benchmark its performance on a large dataset of verification tasks,
compare its outputs against those of established formal verification tools, and
assess its practical effectiveness in real-world auditing scenarios. Our study
combines quantitative metrics with qualitative analysis, and shows that recent
reasoning-oriented LLMs can be surprisingly effective as verification oracles,
suggesting a new frontier in the convergence of AI and formal methods for
secure smart contract development and auditing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [28] [The Indispensable Role of User Simulation in the Pursuit of AGI](https://arxiv.org/abs/2509.19456)
*Krisztian Balog,ChengXiang Zhai*

Main category: cs.AI

TL;DR: 用户模拟是加速通用人工智能发展的关键催化剂，通过创建模拟人类交互的计算代理来解决AGI在复杂系统评估和交互数据获取方面的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 通用人工智能发展面临重大瓶颈，特别是在严格评估复杂交互系统和获取训练自适应代理所需的大量交互数据方面。

Method: 提出用户模拟技术，创建计算代理来模拟人类与AI系统的交互，为可扩展评估、交互学习数据生成和培养AGI核心适应能力提供必要环境。

Result: 用户模拟与智能任务代理研究具有深度协同效应，需要同步推进。文章阐述了用户模拟对AGI的关键作用，探讨了构建真实模拟器的跨学科性质。

Conclusion: 用户模拟技术是克服AGI发展瓶颈、加速AGI进步的关键推动力，需要制定未来研究议程来解决包括大语言模型带来的挑战。

Abstract: Progress toward Artificial General Intelligence (AGI) faces significant
bottlenecks, particularly in rigorously evaluating complex interactive systems
and acquiring the vast interaction data needed for training adaptive agents.
This paper posits that user simulation -- creating computational agents that
mimic human interaction with AI systems -- is not merely a useful tool, but is
a critical catalyst required to overcome these bottlenecks and accelerate AGI
development. We argue that realistic simulators provide the necessary
environments for scalable evaluation, data generation for interactive learning,
and fostering the adaptive capabilities central to AGI. Therefore, research
into user simulation technology and intelligent task agents are deeply
synergistic and must advance hand-in-hand. This article elaborates on the
critical role of user simulation for AGI, explores the interdisciplinary nature
of building realistic simulators, identifies key challenges including those
posed by large language models, and proposes a future research agenda.

</details>


### [29] [Evaluation-Aware Reinforcement Learning](https://arxiv.org/abs/2509.19464)
*Shripad Vilasrao Deshmukh,Will Schwarzer,Scott Niekum*

Main category: cs.AI

TL;DR: 该论文提出了评估感知强化学习（EvA-RL）框架，通过在训练策略时同时最小化评估误差，使策略既高效又易于评估。


<details>
  <summary>Details</summary>
Motivation: 现有策略评估方法存在高方差（数据有限、长时程任务）或高偏差（支持度不均、环境模型不准确）的问题，这些挑战源于传统RL范式未将评估作为显式考虑因素。

Method: 设计EvA-RL框架，训练策略同时最大化期望回报和最小化给定价值预测方案下的评估误差；进一步扩展方法，联合学习评估条件状态价值预测器。

Result: 在离散和连续动作领域的实验表明，EvA-RL能显著降低评估误差，同时保持有竞争力的回报表现。

Conclusion: EvA-RL为RL方法开辟了新方向，将可靠评估作为训练过程中的首要原则，为安全关键系统的部署提供了更可靠的评估基础。

Abstract: Policy evaluation is often a prerequisite for deploying safety- and
performance-critical systems. Existing evaluation approaches frequently suffer
from high variance due to limited data and long-horizon tasks, or high bias due
to unequal support or inaccurate environmental models. We posit that these
challenges arise, in part, from the standard reinforcement learning (RL)
paradigm of policy learning without explicit consideration of evaluation. As an
alternative, we propose evaluation-aware reinforcement learning (EvA-RL), in
which a policy is trained to maximize expected return while simultaneously
minimizing expected evaluation error under a given value prediction scheme --
in other words, being "easy" to evaluate. We formalize a framework for EvA-RL
and design an instantiation that enables accurate policy evaluation,
conditioned on a small number of rollouts in an assessment environment that can
be different than the deployment environment. However, our theoretical analysis
and empirical results show that there is often a tradeoff between evaluation
accuracy and policy performance when using a fixed value-prediction scheme
within EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an
assessment-conditioned state-value predictor alongside the policy. Empirical
results across diverse discrete and continuous action domains demonstrate that
EvA-RL can substantially reduce evaluation error while maintaining competitive
returns. This work lays the foundation for a broad new class of RL methods that
treat reliable evaluation as a first-class principle during training.

</details>


### [30] [Estimating the Self-Consistency of LLMs](https://arxiv.org/abs/2509.19489)
*Robert Nowak*

Main category: cs.AI

TL;DR: 本文分析了在固定计算预算下，LLM自一致性估计器的权衡，建议将预算大致按平方根比例分配给提示采样和重复调用。


<details>
  <summary>Details</summary>
Motivation: 系统通常重复相同的提示给大语言模型并聚合响应以提高可靠性，需要分析在固定计算预算下的最优分配策略。

Method: 分析自一致性估计器，在固定计算预算B=mn（m为提示采样数，n为每个提示的重复调用数）下研究权衡关系。

Result: 分析结果表明最优分配策略是m和n大致与B的平方根成比例，即m,n∝√B。

Conclusion: 在固定计算预算下，将预算大致按平方根比例分配给提示采样和重复调用是最优策略。

Abstract: Systems often repeat the same prompt to large language models (LLMs) and
aggregate responses to improve reliability. This short note analyzes an
estimator of the self-consistency of LLMs and the tradeoffs it induces under a
fixed compute budget $B=mn$, where $m$ is the number of prompts sampled from
the task distribution and $n$ is the number of repeated LLM calls per prompt;
the resulting analysis favors a rough split $m,n\propto\sqrt{B}$.

</details>


### [31] [Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning](https://arxiv.org/abs/2509.19517)
*Sai Teja Reddy Adapala*

Main category: cs.AI

TL;DR: 该研究提出了计算认知负荷理论，设计了ICE基准测试来评估大语言模型在认知负荷下的推理能力，发现认知负荷是导致推理失败的关键因素。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在静态基准测试中表现出色，但在动态、信息丰富的环境中表现脆弱。研究旨在理解模型在认知负荷下的计算限制机制。

Method: 引入计算认知负荷理论，设计ICE基准测试系统操纵上下文饱和和注意力残留两个负荷因素，在200个多跳推理问题上进行10次重复测试。

Result: 小型开源模型在所有条件下准确率为0%，Gemini-2.0-Flash-001在控制条件下达到85%准确率，但在上下文饱和条件下显著下降。

Conclusion: 认知负荷是推理失败的关键因素，动态的认知感知压力测试对于评估AI系统的真实韧性和安全性至关重要。

Abstract: The scaling of Large Language Models (LLMs) has exposed a critical gap
between their performance on static benchmarks and their fragility in dynamic,
information-rich environments. While models excel at isolated tasks, the
computational limits that govern their reasoning under cognitive load remain
poorly understood. In this work, we introduce a formal theory of computational
cognitive load, positing that extraneous, task-irrelevant information (Context
Saturation) and interference from task-switching (Attentional Residue) are key
mechanisms that degrade performance. We designed the Interleaved Cognitive
Evaluation (ICE), a deconfounded benchmark to systematically manipulate these
load factors on challenging multi-hop reasoning tasks. A comprehensive study (N
= 10 replications per item across 200 questions) revealed significant
performance variations across five instruction-tuned models. Smaller
open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)
exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all
conditions, including clean controls, on this high-intrinsic-load task. In
contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%
accuracy in control conditions, with a statistically significant degradation
under context saturation ($\beta = -0.003$ per % load, $p < 0.001$). These
findings provide preliminary evidence that cognitive load is a key contributor
to reasoning failures, supporting theories of hallucination-as-guessing under
uncertainty. We conclude that dynamic, cognitive-aware stress testing, as
exemplified by the ICE benchmark, is essential for evaluating the true
resilience and safety of advanced AI systems.

</details>


### [32] [Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation](https://arxiv.org/abs/2509.19524)
*Ramy ElMallah,Krish Chhajer,Chi-Guhn Lee*

Main category: cs.AI

TL;DR: 提出StepEval评估框架，使用视觉语言模型自动评估机器人操作任务中的子目标完成情况，替代单一的成功率报告


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习论文仅报告单一二进制成功率，无法显示多步骤操作任务中各子目标的完成情况，需要更细粒度的评估方法

Method: 设计StepEval框架，利用VLMs作为自动评判器，从记录图像或视频中评估子目标结果，生成每个子目标的成功率向量

Result: 提出可扩展、社区驱动的开源项目设计原则，支持模型无关、单/多视图输入，轻量级易部署

Conclusion: StepEval旨在建立标准化的子目标评估实践，使评估步骤而不仅仅是最终目标成为可复现的标准

Abstract: Robot learning papers typically report a single binary success rate (SR),
which obscures where a policy succeeds or fails along a multi-step manipulation
task. We argue that subgoal-level reporting should become routine: for each
trajectory, a vector of per-subgoal SRs that makes partial competence visible
(e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware
plug-in evaluation framework that utilizes vision-language models (VLMs) as
automated judges of subgoal outcomes from recorded images or videos. Rather
than proposing new benchmarks or APIs, our contribution is to outline design
principles for a scalable, community-driven open-source project. In StepEval,
the primary artifact for policy evaluation is the per-subgoal SR vector;
however, other quantities (e.g., latency or cost estimates) are also considered
for framework-optimization diagnostics to help the community tune evaluation
efficiency and accuracy when ground-truth subgoal success labels are available.
We discuss how such a framework can remain model-agnostic, support single- or
multi-view inputs, and be lightweight enough to adopt across labs. The intended
contribution is a shared direction: a minimal, extensible seed that invites
open-source contributions, so that scoring the steps, not just the final goal,
becomes a standard and reproducible practice.

</details>


### [33] [Nano Bio-Agents (NBA): Small Language Model Agents for Genomics](https://arxiv.org/abs/2509.19566)
*George Hong,Daniel Trejo Banos*

Main category: cs.AI

TL;DR: 该研究开发了Nano Bio-Agent框架，将小型语言模型（<100亿参数）与任务分解、工具编排和API访问相结合，用于基因组学问答，在保持高准确率的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在基因组学问答中的幻觉问题和计算成本挑战，探索小型语言模型在专业领域的应用潜力。

Method: 实现Nano Bio-Agent框架，整合任务分解、工具编排和NCBI、AlphaGenome等系统API访问，使用3-100亿参数的小型语言模型。

Result: 在GeneTuring基准测试中达到98%准确率，3-100亿参数模型实现85-97%准确率，计算资源需求远低于传统方法。

Conclusion: 小型语言模型结合智能体框架在基因组学领域具有巨大潜力，可实现效率提升、成本节约和工具民主化，同时保持高性能。

Abstract: We investigate the application of Small Language Models (<10 billion
parameters) for genomics question answering via agentic framework to address
hallucination issues and computational cost challenges. The Nano Bio-Agent
(NBA) framework we implemented incorporates task decomposition, tool
orchestration, and API access into well-established systems such as NCBI and
AlphaGenome. Results show that SLMs combined with such agentic framework can
achieve comparable and in many cases superior performance versus existing
approaches utilising larger models, with our best model-agent combination
achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B
parameter models consistently achieve 85-97% accuracy while requiring much
lower computational resources than conventional approaches. This demonstrates
promising potential for efficiency gains, cost savings, and democratization of
ML-powered genomics tools while retaining highly robust and accurate
performance.

</details>


### [34] [What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities](https://arxiv.org/abs/2509.19590)
*Nathanael Jo,Ashia Wilson*

Main category: cs.AI

TL;DR: 本文提出了一个将AI评估视为推理的框架，强调基准测试分数不应被视为简单测量，而应作为基于能力理论的能力推断。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在基准数据上的评估普遍存在可靠性问题，基准分数被当作简单测量而非推断，这导致对AI能力的误解。

Method: 从能力理论出发，推导估计能力的方法，特别针对敏感性扰动问题，引入了考虑不确定性的方法，包括显著降低样本复杂度的自适应算法。

Result: 该框架为通过基准测试获得更可靠和可信的AI能力估计奠定了基础。

Conclusion: 评估应明确基于能力理论，采用推理方法而非简单测量，这有助于提高AI能力评估的可靠性和可信度。

Abstract: Evaluations of generative models on benchmark data are now ubiquitous, and
their outcomes critically shape public and scientific expectations of AI's
capabilities. Yet growing skepticism surrounds their reliability. How can we
know that a reported accuracy genuinely reflects a model's true performance?
Evaluations are often presented as simple measurements, but in reality they are
inferences: to treat benchmark scores as evidence of capability is already to
assume a theory of what capability is and how it manifests in a test. We make
this step explicit by proposing a principled framework for evaluation as
inference: begin from a theory of capability, and then derive methods for
estimating it. This perspective, familiar in fields such as psychometrics, has
not yet become commonplace in AI evaluation. As a proof of concept, we address
a central challenge that undermines reliability: sensitivity to perturbations.
After formulating a model of ability, we introduce methods that infer ability
while accounting for uncertainty from sensitivity and finite samples, including
an adaptive algorithm that significantly reduces sample complexity. Together,
these contributions lay the groundwork for more reliable and trustworthy
estimates of AI capabilities as measured through benchmarks.

</details>


### [35] [SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation](https://arxiv.org/abs/2509.19623)
*Xutao Mao,Tao Liu,Hongying Zan*

Main category: cs.AI

TL;DR: SteinerSQL是一个统一框架，将复杂的Text-to-SQL查询中的数学推理和模式导航挑战整合为图优化问题，在LogicCat和Spider2.0-Lite基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法孤立处理复杂Text-to-SQL查询中的数学推理和模式导航挑战，导致推理过程碎片化，影响逻辑和结构正确性。

Method: SteinerSQL采用三阶段方法：数学分解识别所需表格（终端节点），通过Steiner树问题构建最优推理支架，以及多级验证确保正确性。

Result: 在LogicCat和Spider2.0-Lite基准测试中，使用Gemini-2.5-Pro分别达到36.10%和40.04%的执行准确率，创下新纪录。

Conclusion: SteinerSQL为Text-to-SQL提出了新的统一范式，为复杂推理任务提供了更稳健和原则性的解决方案。

Abstract: Large Language Models (LLMs) struggle with complex Text-to-SQL queries that
demand both sophisticated mathematical reasoning and intricate schema
navigation. Existing methods often tackle these challenges in isolation,
creating a fractured reasoning process that compromises logical and structural
correctness. To resolve this, we introduce SteinerSQL, a framework that unifies
these dual challenges into a single, graph-centric optimization problem.
SteinerSQL operates in three stages: mathematical decomposition to identify
required tables (terminals), optimal reasoning scaffold construction via a
Steiner tree problem, and multi-level validation to ensure correctness. On the
challenging LogicCat and Spider2.0-Lite benchmarks, SteinerSQL establishes a
new state-of-the-art with 36.10% and 40.04% execution accuracy, respectively,
using Gemini-2.5-Pro. Beyond accuracy, SteinerSQL presents a new, unified
paradigm for Text-to-SQL, paving the way for more robust and principled
solutions to complex reasoning tasks.

</details>


### [36] [Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving](https://arxiv.org/abs/2509.19681)
*Anisha Garg,Engin Tekin,Yash More,David Bick,Nishit Neema,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的成对解释验证器，通过生成校准的置信度分数和自然语言推理来改进推理模型的自评估能力，提升测试时策略的效果。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型的自评估能力较差，限制了测试时计算策略的有效性，需要更好的验证机制来识别错误模式。

Method: 使用强化学习（GRPO）训练成对解释验证器，生成校准的置信度分数和相关的自然语言推理。

Result: 验证器提高了best-of-n和自反思等测试时策略的准确性和效率，特别擅长识别具有挑战性的失败模式。

Conclusion: 该方法在标准方法（如多数投票）失败的情况下仍能成功识别错误，为推理模型的测试时策略提供了有效改进。

Abstract: Advanced test-time computing strategies are essential for scaling reasoning
models, but their effectiveness is capped by the models' poor self-evaluation.
We propose a pairwise Explanatory Verifier, trained via reinforcement learning
(GRPO), that produces calibrated confidence scores and associated natural
language reasoning for generated solutions. Our verifier improves the accuracy
and efficiency of test-time strategies like best-of-n and self-reflection.
Crucially, it excels at identifying challenging failure modes, such as when
both candidate solutions are identically incorrect, succeeding where standard
methods like majority voting fail.

</details>


### [37] [UserRL: Training Interactive User-Centric Agent via Reinforcement Learning](https://arxiv.org/abs/2509.19736)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Jielin Qiu,Zhiwei Liu,Haolin Chen,Shirley Kokane,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: UserRL是一个统一的框架，通过标准化gym环境和模拟用户来训练和评估以用户为中心的智能体能力，研究发现奖励塑造和用户模拟选择对开发稳健的用户中心智能体模型至关重要。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练智能体模型方面显示出潜力，但智能体的最终价值在于协助用户的能力，而用户交互的多样性和动态性带来了挑战。

Method: 提出UserRL框架，系统变化回合级奖励分配和轨迹级分数计算，分析不同公式在GRPO算法下如何影响学习，在Qwen3模型上进行实验。

Result: 三个关键发现：(i)SFT冷启动对解锁初始交互能力和实现持续RL改进至关重要；(ii)有意的轨迹评分能产生更高效的多轮交互；(iii)虽然更强的模拟用户有助于训练，但开源模拟器仍是成本效益高且可转移的选择。

Conclusion: 精心设计奖励塑造和用户模拟选择与模型规模同等重要，UserRL为开发稳健的用户中心智能体模型提供了实用途径。

Abstract: Reinforcement learning (RL) has shown promise in training agentic models that
move beyond static benchmarks to engage in dynamic, multi-turn interactions.
Yet, the ultimate value of such agents lies in their ability to assist users, a
setting where diversity and dynamics of user interaction pose challenges. In
this work, we propose UserRL, a unified framework for training and evaluating
user-centric abilities through standardized gym environments paired with
simulated users. We systematically vary turn-level reward assignment and
trajectory-level score calculation to analyze how different formulations affect
learning under the GRPO algorithm. Our experiments across Qwen3 models reveal
three key findings: (i) SFT cold start is critical for unlocking initial
interaction ability and enabling sustained RL improvements; (ii) deliberate
trajectory scoring yields more efficient and effective multi-turn interactions;
and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,
open-source simulators (e.g., Qwen3-32B) remain a cost-effective and
transferable option. Together, these results highlight that careful design of
reward shaping and user simulation choice is as crucial as model scale, and
establish UserRL as a practical pathway for developing robust user-centric
agentic models. All codes and data are public for future research.

</details>


### [38] [The Conductor and the Engine: A Path Towards Co-Designed Reasoning](https://arxiv.org/abs/2509.19762)
*Yuanxin Wang,Pawel Filipczuk,Anisha Garg,Amaan Dhada,Mohammad Hassanpour,David Bick,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 提出了一种优化的推理工作流（cepo），使较小的开源模型能够超越比它们大数倍的模型，通过减少模型冗余和改善指令遵循来提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现代LLM推理依赖于大量的测试时计算，但模型冗余和指令遵循不佳导致计算浪费，需要优化能力与成本之间的权衡。

Method: 引入cepo优化推理工作流，通过协同设计编排框架与底层模型能力，减少模型冗余和改善指令遵循。

Result: 较小的开源模型能够超越比它们大数倍的模型，展示了在中小型模型中解锁强大推理能力的清晰路径。

Conclusion: 工作将开源以促进进一步研究，证明了协同设计编排框架与模型能力的重要性，为中小型模型实现强大推理提供了可行方案。

Abstract: Modern LLM reasoning relies on extensive test-time computation, driven by
internal model training and external agentic orchestration. However, this
synergy is often inefficient, as model verbosity and poor instruction following
lead to wasted compute. We analyze this capability-cost trade-off and introduce
an optimized reasoning workflow (\cepo) that empowers smaller open-source
models to outperform models multiple times their size. We will open-source this
workflow to enable further research. Our work demonstrates a clear path toward
co-designing orchestration frameworks with the underlying model capabilities to
unlock powerful reasoning in small-to-medium sized models.

</details>


### [39] [Agentic Metacognition: Designing a "Self-Aware" Low-Code Agent for Failure Prediction and Human Handoff](https://arxiv.org/abs/2509.19783)
*Jiexi Xu*

Main category: cs.AI

TL;DR: 提出了一种在低代码/无代码环境中集成元认知层的架构模式，通过主动监控和预测任务失败来增强自主代理的可靠性，并在预测到失败时主动启动人工交接。


<details>
  <summary>Details</summary>
Motivation: 自主代理的非确定性特性在LCNC环境中带来了可靠性挑战，如陷入循环、生成错误输出或遇到不可恢复的故障，导致用户沮丧和信任崩溃。

Method: 集成一个受人类内省启发的元认知层，该层基于定义的触发器（如过度延迟或重复操作）主动监控主要LCNC代理并预测即将发生的任务失败。预测到失败时，元认知代理会启动人工交接，向用户提供代理的"思考过程"摘要和无法继续的详细解释。

Result: 原型系统的实证分析表明，该方法显著提高了整体任务成功率，但性能提升伴随着计算开销的显著增加。

Conclusion: 将人工交接重新定义为增强系统韧性、改善用户体验和通过提供代理内部状态透明度来建立信任的核心设计特征，而非失败的承认。报告讨论了该方法的实践和伦理影响，并确定了未来研究的关键方向。

Abstract: The inherent non-deterministic nature of autonomous agents, particularly
within low-code/no-code (LCNC) environments, presents significant reliability
challenges. Agents can become trapped in unforeseen loops, generate inaccurate
outputs, or encounter unrecoverable failures, leading to user frustration and a
breakdown of trust. This report proposes a novel architectural pattern to
address these issues: the integration of a secondary, "metacognitive" layer
that actively monitors the primary LCNC agent. Inspired by human introspection,
this layer is designed to predict impending task failures based on a defined
set of triggers, such as excessive latency or repetitive actions. Upon
predicting a failure, the metacognitive agent proactively initiates a human
handoff, providing the user with a clear summary of the agent's "thought
process" and a detailed explanation of why it could not proceed. An empirical
analysis of a prototype system demonstrates that this approach significantly
increases the overall task success rate. However, this performance gain comes
with a notable increase in computational overhead. The findings reframe human
handoffs not as an admission of defeat but as a core design feature that
enhances system resilience, improves user experience, and builds trust by
providing transparency into the agent's internal state. The report discusses
the practical and ethical implications of this approach and identifies key
directions for future research.

</details>


### [40] [Analysis of approximate linear programming solution to Markov decision problem with log barrier function](https://arxiv.org/abs/2509.19800)
*Donghwan Lee,Hyukjun Yang,Bum Geun Park*

Main category: cs.AI

TL;DR: 本文提出了一种使用对数障碍函数将基于线性规划的马尔可夫决策过程转化为无约束优化问题的新方法，通过梯度下降获得近似解，并建立了该方法的理论基础。


<details>
  <summary>Details</summary>
Motivation: 基于线性规划的MDP方法虽然在某些场景（如离线强化学习）中受到关注，但由于其导致不等式约束优化问题，相比基于贝尔曼方程的方法更难有效求解。本文旨在为更有效实用的LP-based MDP求解建立理论基础。

Method: 利用不等式约束优化中广泛使用的对数障碍函数，将MDP的LP表述转化为无约束优化问题，使梯度下降能够轻松获得近似解。

Result: 该方法虽然看似简单，但作者首次对其进行了系统的理论解释和分析。

Conclusion: 本文填补了对数障碍函数在LP-based MDP求解中理论基础的空白，为该方法提供了坚实的理论支撑。

Abstract: There are two primary approaches to solving Markov decision problems (MDPs):
dynamic programming based on the Bellman equation and linear programming (LP).
Dynamic programming methods are the most widely used and form the foundation of
both classical and modern reinforcement learning (RL). By contrast, LP-based
methods have been less commonly employed, although they have recently gained
attention in contexts such as offline RL. The relative underuse of the LP-based
methods stems from the fact that it leads to an inequality-constrained
optimization problem, which is generally more challenging to solve effectively
compared with Bellman-equation-based methods. The purpose of this paper is to
establish a theoretical foundation for solving LP-based MDPs in a more
effective and practical manner. Our key idea is to leverage the log-barrier
function, widely used in inequality-constrained optimization, to transform the
LP formulation of the MDP into an unconstrained optimization problem. This
reformulation enables approximate solutions to be obtained easily via gradient
descent. While the method may appear simple, to the best of our knowledge, a
thorough theoretical interpretation of this approach has not yet been
developed. This paper aims to bridge this gap.

</details>


### [41] [LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation](https://arxiv.org/abs/2509.19839)
*Huizhen Shu,Xuying Li,Zhuo Li*

Main category: cs.AI

TL;DR: LATENTGUARD是一个三阶段框架，通过结合行为对齐和监督潜在空间控制，在保持语言模型实用性的同时实现可解释的安全控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在表示层面平衡全面安全性和细粒度可控性，需要一种既能确保安全又不损害实用性的新方法。

Method: 三阶段方法：1）在包含对抗性提示和良性查询的数据集上微调LLM；2）训练结构化变分自编码器（VAE）学习解耦的潜在表示；3）通过操纵潜在维度实现选择性拒绝行为。

Result: 在Qwen3-8B上实验显示安全可控性和响应可解释性显著提升，在Mistral-7B上的跨架构验证证实了方法的通用性。

Conclusion: 结构化表示级干预为构建更安全实用的LLM系统提供了有前景的途径。

Abstract: Achieving robust safety alignment in large language models (LLMs) while
preserving their utility remains a fundamental challenge. Existing approaches
often struggle to balance comprehensive safety with fine-grained
controllability at the representation level. We introduce LATENTGUARD, a novel
three-stage framework that combines behavioral alignment with supervised latent
space control for interpretable and precise safety steering. Our approach
begins by fine-tuning an LLM on rationalized datasets containing both
reasoning-enhanced refusal responses to adversarial prompts and
reasoning-enhanced normal responses to benign queries, establishing robust
behavioral priors across both safety-critical and utility-preserving scenarios.
We then train a structured variational autoencoder (VAE) on intermediate MLP
activations, supervised by multi-label annotations including attack types,
attack methods, and benign indicators. This supervision enables the VAE to
learn disentangled latent representations that capture distinct adversarial
characteristics while maintaining semantic interpretability. Through targeted
manipulation of learned latent dimensions, LATENTGUARD achieves selective
refusal behavior, effectively blocking harmful requests while preserving
helpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate
significant improvements in both safety controllability and response
interpretability without compromising utility. Cross-architecture validation on
Mistral-7B confirms the generalizability of our latent steering approach,
showing consistent effectiveness across different model families. Our results
suggest that structured representation-level intervention offers a promising
pathway toward building safer yet practical LLM systems.

</details>


### [42] [CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain](https://arxiv.org/abs/2509.19925)
*Ajeet Kumar Singh,Rajsabi Surya,Anurag Tripathi,Santanu Choudhury,Sudhir Bisane*

Main category: cs.AI

TL;DR: CON-QA是一个混合隐私保护框架，专门用于企业合同的安全问答，结合本地和云端LLM来保护敏感信息。


<details>
  <summary>Details</summary>
Motivation: 随着企业将云端LLM集成到法律文档工作流中，保护敏感合同信息（如PII和商业敏感条款）成为关键挑战。

Method: CON-QA采用三阶段方法：1）语义查询分解和文档块检索；2）通过结构化映射方案匿名化敏感实体；3）云端LLM生成匿名响应，本地准确重建原始答案。

Result: 基于CUAD-QA语料库的评估显示，CON-QA在保持隐私和实用性的同时，有效维护答案质量、法律条款语义保真度，并显著降低隐私风险。

Conclusion: CON-QA框架适用于企业级合同文档的安全处理，证明了其在保护敏感信息的同时保持问答质量的实用性。

Abstract: As enterprises increasingly integrate cloud-based large language models
(LLMs) such as ChatGPT and Gemini into their legal document workflows,
protecting sensitive contractual information - including Personally
Identifiable Information (PII) and commercially sensitive clauses - has emerged
as a critical challenge. In this work, we propose CON-QA, a hybrid
privacy-preserving framework designed specifically for secure question
answering over enterprise contracts, effectively combining local and
cloud-hosted LLMs. The CON-QA framework operates through three stages: (i)
semantic query decomposition and query-aware document chunk retrieval using a
locally deployed LLM analysis, (ii) anonymization of detected sensitive
entities via a structured one-to-many mapping scheme, ensuring semantic
coherence while preventing cross-session entity inference attacks, and (iii)
anonymized response generation by a cloud-based LLM, with accurate
reconstruction of the original answer locally using a session-consistent
many-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce
CUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world
CUAD contract documents, encompassing simple, complex, and summarization-style
queries. Empirical evaluations, complemented by detailed human assessments,
confirm that CON-QA effectively maintains both privacy and utility, preserves
answer quality, maintains fidelity to legal clause semantics, and significantly
mitigates privacy risks, demonstrating its practical suitability for secure,
enterprise-level contract documents.

</details>


### [43] [Embodied AI: From LLMs to World Models](https://arxiv.org/abs/2509.20021)
*Tongtong Feng,Xin Wang,Yu-Gang Jiang,Wenwu Zhu*

Main category: cs.AI

TL;DR: 本文综述了具身人工智能的发展现状，重点分析了大型语言模型和世界模型在具身AI中的作用，提出了联合MLLM-WM架构的重要性，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 具身AI是实现通用人工智能的关键范式，近年来LLMs和WMs的突破性进展为具身AI带来了新的发展机遇，需要系统梳理该领域的研究进展。

Method: 采用文献综述方法，从基础到前沿全面梳理具身AI的发展历程，重点分析LLM驱动和WM驱动的具身AI工作，并提出联合架构思路。

Result: 系统阐述了具身AI的关键技术、硬件系统和应用场景，论证了LLMs在语义推理和WMs在物理交互中的互补作用，提出了MLLM-WM联合架构的必要性。

Conclusion: 具身AI是AGI发展的重要方向，LLMs和WMs的融合将推动具身AI在复杂物理世界任务中的应用，未来需要进一步探索多模态融合、实时交互等研究方向。

Abstract: Embodied Artificial Intelligence (AI) is an intelligent system paradigm for
achieving Artificial General Intelligence (AGI), serving as the cornerstone for
various applications and driving the evolution from cyberspace to physical
systems. Recent breakthroughs in Large Language Models (LLMs) and World Models
(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs
empower embodied AI via semantic reasoning and task decomposition, bringing
high-level natural language instructions and low-level natural language actions
into embodied cognition. On the other hand, WMs empower embodied AI by building
internal representations and future predictions of the external world,
facilitating physical law-compliant embodied interactions. As such, this paper
comprehensively explores the literature in embodied AI from basics to advances,
covering both LLM driven and WM driven works. In particular, we first present
the history, key technologies, key components, and hardware systems of embodied
AI, as well as discuss its development via looking from unimodal to multimodal
angle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,
embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,
meticulously delineating their indispensable roles in end-to-end embodied
cognition and physical laws-driven embodied interactions. Building upon the
above advances, we further share our insights on the necessity of the joint
MLLM-WM driven embodied AI architecture, shedding light on its profound
significance in enabling complex tasks within physical worlds. In addition, we
examine representative applications of embodied AI, demonstrating its wide
applicability in real-world scenarios. Last but not least, we point out future
research directions of embodied AI that deserve further investigation.

</details>


### [44] [MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM](https://arxiv.org/abs/2509.20067)
*Wenliang Li,Rui Yan,Xu Zhang,Li Chen,Hongji Zhu,Jing Zhao,Junjun Li,Mengru Li,Wei Cao,Zihang Jiang,Wei Wei,Kun Zhang,Shaohua Kevin Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体临床诊断框架（MACD），通过让LLM在多智能体管道中总结、提炼和应用诊断见解来自学临床知识，显著提高了临床诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 传统提示方法在处理复杂真实世界临床诊断时面临挑战，现有方法通常优化孤立推理而忽略了可重用临床经验的积累。

Method: 提出MACD框架，采用多智能体管道让LLM自学临床知识，并扩展到MACD-人类协作工作流程，包括多个诊断智能体迭代咨询、评估智能体和人类监督。

Result: 在4,390个真实患者病例上评估，MACD显著提高主要诊断准确性，比临床指南提升达22.3%，在某些子集上达到或超过人类医生水平（比纯医生诊断提升16%）。

Conclusion: 该工作为LLM辅助诊断提供了一个可扩展的自学范式，弥合了LLM内在知识与真实世界临床实践之间的差距。

Abstract: Large language models (LLMs) have demonstrated notable potential in medical
applications, yet they face substantial challenges in handling complex
real-world clinical diagnoses using conventional prompting methods. Current
prompt engineering and multi-agent approaches typically optimize isolated
inferences, neglecting the accumulation of reusable clinical experience. To
address this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)
framework, which allows LLMs to self-learn clinical knowledge via a multi-agent
pipeline that summarizes, refines, and applies diagnostic insights. It mirrors
how physicians develop expertise through experience, enabling more focused and
accurate diagnosis on key disease-specific cues. We further extend it to a
MACD-human collaborative workflow, where multiple LLM-based diagnostician
agents engage in iterative consultations, supported by an evaluator agent and
human oversight for cases where agreement is not reached. Evaluated on 4,390
real-world patient cases across seven diseases using diverse open-source LLMs
(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves
primary diagnostic accuracy, outperforming established clinical guidelines with
gains up to 22.3% (MACD). On the subset of the data, it achieves performance on
par with or exceeding that of human physicians (up to 16% improvement over
physicians-only diagnosis). Additionally, on the MACD-human workflow, it
achieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,
self-learned knowledge exhibits strong cross-model stability, transferability,
and model-specific personalization, while the system can generate traceable
rationales, enhancing explainability. Consequently, this work presents a
scalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap
between the intrinsic knowledge of LLMs and real-world clinical practice.

</details>


### [45] [From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms](https://arxiv.org/abs/2509.20095)
*Aymeric Vellinger,Nemanja Antonic,Elio Tuci*

Main category: cs.AI

TL;DR: 该研究建立了秀丽隐杆线虫信息素介导的聚集行为与强化学习之间的理论等价性，揭示了信息素动态如何数学上反映强化学习算法，并展示了通过引入探索性个体可以恢复群体适应性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解群体智能如何通过分散的简单个体交互实现集体问题解决，特别是将生物系统中的信息素通信机制与强化学习理论联系起来，为可编程生物系统和群体机器人提供理论基础。

Method: 方法包括建立工程化线虫群体模型进行觅食任务，将信息素动态建模为强化学习更新过程，并通过计算实验在多臂赌博机场景中验证模型，引入对信息素不敏感的探索性个体来研究群体适应性。

Result: 结果显示模型能准确复制静态环境下的线虫觅食模式，在动态环境中信息素轨迹会阻碍适应性，但引入探索性个体可以恢复集体可塑性，实现快速任务切换和过时策略的群体级消除。

Conclusion: 结论表明信息素系统固有地编码了分布式强化学习过程，环境信号作为集体信用分配的外部记忆，这项工作通过结合合成生物学和群体机器人学，推进了在多变环境中具有弹性决策能力的可编程生命系统的发展。

Abstract: Swarm intelligence emerges from decentralised interactions among simple
agents, enabling collective problem-solving. This study establishes a
theoretical equivalence between pheromone-mediated aggregation in \celeg\ and
reinforcement learning (RL), demonstrating how stigmergic signals function as
distributed reward mechanisms. We model engineered nematode swarms performing
foraging tasks, showing that pheromone dynamics mathematically mirror
cross-learning updates, a fundamental RL algorithm. Experimental validation
with data from literature confirms that our model accurately replicates
empirical \celeg\ foraging patterns under static conditions. In dynamic
environments, persistent pheromone trails create positive feedback loops that
hinder adaptation by locking swarms into obsolete choices. Through
computational experiments in multi-armed bandit scenarios, we reveal that
introducing a minority of exploratory agents insensitive to pheromones restores
collective plasticity, enabling rapid task switching. This behavioural
heterogeneity balances exploration-exploitation trade-offs, implementing
swarm-level extinction of outdated strategies. Our results demonstrate that
stigmergic systems inherently encode distributed RL processes, where
environmental signals act as external memory for collective credit assignment.
By bridging synthetic biology with swarm robotics, this work advances
programmable living systems capable of resilient decision-making in volatile
environments.

</details>


### [46] [Steerable Adversarial Scenario Generation through Test-Time Preference Alignment](https://arxiv.org/abs/2509.20102)
*Tong Nie,Yuewen Mei,Yihong Tang,Junlin He,Jie Sun,Haotian Shi,Wei Ma,Jian Sun*

Main category: cs.AI

TL;DR: 本文提出了SAGE框架，将对抗性场景生成重新定义为多目标偏好对齐问题，实现了在推理时对对抗性和真实性权衡的细粒度控制，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常局限于单一固定的目标权衡，缺乏在推理时调整的灵活性和效率，无法为多样化的训练和测试需求生成定制化场景。

Method: 提出分层组偏好优化方法，将硬可行性约束与软偏好解耦，通过微调两个偏好相反的专家模型，在推理时通过线性插值其权重构建连续策略谱。

Result: 实验表明SAGE不仅能生成具有更优对抗性和真实性平衡的场景，还能更有效地进行驾驶策略的闭环训练。

Conclusion: SAGE框架为对抗性场景生成提供了灵活高效的解决方案，通过理论证明和实验验证了其有效性。

Abstract: Adversarial scenario generation is a cost-effective approach for safety
assessment of autonomous driving systems. However, existing methods are often
constrained to a single, fixed trade-off between competing objectives such as
adversariality and realism. This yields behavior-specific models that cannot be
steered at inference time, lacking the efficiency and flexibility to generate
tailored scenarios for diverse training and testing requirements. In view of
this, we reframe the task of adversarial scenario generation as a
multi-objective preference alignment problem and introduce a new framework
named \textbf{S}teerable \textbf{A}dversarial scenario \textbf{GE}nerator
(SAGE). SAGE enables fine-grained test-time control over the trade-off between
adversariality and realism without any retraining. We first propose
hierarchical group-based preference optimization, a data-efficient offline
alignment method that learns to balance competing objectives by decoupling hard
feasibility constraints from soft preferences. Instead of training a fixed
model, SAGE fine-tunes two experts on opposing preferences and constructs a
continuous spectrum of policies at inference time by linearly interpolating
their weights. We provide theoretical justification for this framework through
the lens of linear mode connectivity. Extensive experiments demonstrate that
SAGE not only generates scenarios with a superior balance of adversariality and
realism but also enables more effective closed-loop training of driving
policies. Project page: https://tongnie.github.io/SAGE/.

</details>


### [47] [PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs](https://arxiv.org/abs/2509.20105)
*Venkat Margapuri,Garik Kazanjian,Naren Kosaraju*

Main category: cs.AI

TL;DR: 本文提出了一种量子启发的PEPS保真度奖励方法，通过PPO优化来提高LLM在多步推理中的连贯性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多步推理任务中难以保持连贯的逻辑流程，需要一种新的方法来增强推理轨迹的全局一致性

Method: 将基于投影纠缠对态(PEPS)的保真度奖励整合到近端策略优化(PPO)中，通过结构一致性指导学习过程

Result: 在GSM8K、StrategyQA和EntailmentBank等多个数据集上，该方法在连贯性指标上显著优于监督学习、对比学习和预训练基线方法

Conclusion: 量子启发的保真度方法为提升LLM推理轨迹连贯性提供了有效基础，证明了结构一致性指导的重要性

Abstract: Large Language Models (LLMs) often struggle with maintaining coherent
multi-step reasoning traces, particularly in tasks that require a structured
logical flow. This work introduces a quantum-inspired approach to address the
challenge by incorporating a fidelity-based reward derived from Projected
Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior
approaches that use direct supervision or contrastive objectives, the proposed
method guides learning through structural consistency, offering a novel
approach to enforce global coherence in generated reasoning traces. The
proposed framework is evaluated using multiple coherence-determining metrics on
diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning
arithmetic, intuitive, and entailment-based reasoning. Results show that the
proposed quantum-inspired approach offers significant improvements over
supervised, contrastive, and pretrained baseline approaches, highlighting the
effectiveness of quantum-inspired fidelity as a foundation to improve reasoning
trace coherence in LLMs.

</details>


### [48] [Formal Verification of Minimax Algorithms](https://arxiv.org/abs/2509.20138)
*Wieger Wesselink,Kees Huizing,Huub van de Wetering*

Main category: cs.AI

TL;DR: 使用Dafny验证系统对多种极小极大搜索算法进行形式化验证，包括带alpha-beta剪枝和置换表的变体，为深度受限搜索引入基于见证的正确性标准。


<details>
  <summary>Details</summary>
Motivation: 确保极小极大搜索算法的正确性，特别是在使用优化技术如alpha-beta剪枝和置换表时，这些优化可能引入错误。

Method: 使用Dafny验证系统进行形式化验证，为深度受限搜索定义基于见证的正确性标准，并应用于两个代表性算法。

Result: 成功验证了包括带alpha-beta剪枝和置换表的极小极大搜索算法，所有验证工件（包括证明和Python实现）已公开。

Conclusion: 形式化验证可以有效确保搜索算法的正确性，基于见证的标准为深度受限搜索提供了可靠的验证方法。

Abstract: Using the Dafny verification system, we formally verify a range of minimax
search algorithms, including variations with alpha-beta pruning and
transposition tables. For depth-limited search with transposition tables, we
introduce a witness-based correctness criterion and apply it to two
representative algorithms. All verification artifacts, including proofs and
Python implementations, are publicly available.

</details>


### [49] [Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI](https://arxiv.org/abs/2509.20175)
*Lorenzo Giusti,Ole Anton Werner,Riccardo Taiello,Matilde Carvalho Costa,Emre Tosun,Andrea Protani,Marc Molina,Rodrigo Lopes de Almeida,Paolo Cacace,Diogo Reis Santos,Luigi Serio*

Main category: cs.AI

TL;DR: FoA是一个分布式编排框架，通过版本化能力向量实现动态的多智能体协作，在HealthBench上比单模型基线提升13倍性能


<details>
  <summary>Details</summary>
Motivation: 将静态的多智能体协调转变为动态的、基于能力的协作，释放异构AI智能体联邦的集体智能

Method: 结合语义路由、动态任务分解和智能聚类三大创新：1）语义路由通过HNSW索引匹配任务与智能体；2）动态任务分解让兼容智能体协作将复杂任务分解为子任务DAG；3）智能聚类将处理相似子任务的智能体分组进行多轮精炼

Result: 在HealthBench评估中比单模型基线提升13倍性能，聚类增强的协作对需要多视角的复杂推理任务特别有效，系统可水平扩展且保持稳定性能

Conclusion: 基于语义编排的结构化协作能够解锁异构AI智能体联邦的集体智能

Abstract: We present Federation of Agents (FoA), a distributed orchestration framework
that transforms static multi-agent coordination into dynamic, capability-driven
collaboration. FoA introduces Versioned Capability Vectors (VCVs):
machine-readable profiles that make agent capabilities searchable through
semantic embeddings, enabling agents to advertise their capabilities, cost, and
limitations. Our aarchitecturecombines three key innovations: (1) semantic
routing that matches tasks to agents over sharded HNSW indices while enforcing
operational constraints through cost-biased optimization, (2) dynamic task
decomposition where compatible agents collaboratively break down complex tasks
into DAGs of subtasks through consensus-based merging, and (3) smart clustering
that groups agents working on similar subtasks into collaborative channels for
k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe
semantics for scalable message passing, FoA achieves sub-linear complexity
through hierarchical capability matching and efficient index maintenance.
Evaluation on HealthBench shows 13x improvements over single-model baselines,
with clustering-enhanced laboration particularly effective for complex
reasoning tasks requiring multiple perspectives. The system scales horizontally
while maintaining consistent performance, demonstrating that semantic
orchestration with structured collaboration can unlock the collective
intelligence of heterogeneous federations of AI agents.

</details>


### [50] [Design Insights and Comparative Evaluation of a Hardware-Based Cooperative Perception Architecture for Lane Change Prediction](https://arxiv.org/abs/2509.20218)
*Mohamed Manzour,Catherine M. Elias,Omar M. Shehata,Rubén Izquierdo,Miguel Ángel Sotelo*

Main category: cs.AI

TL;DR: 本研究通过真实硬件部署探索协同变道预测，分享实施和测试中的实践经验，突出实际挑战和局限性。


<details>
  <summary>Details</summary>
Motivation: 现有变道预测研究多在仿真环境或预录数据集进行，依赖简化假设，实际部署较少且实践经验不足。

Method: 在混合交通中进行真实硬件部署，实施协同变道预测系统。

Result: 识别了系统瓶颈、可靠性问题和操作约束等实际挑战。

Conclusion: 通过记录这些经验，为类似系统开发提供指导。

Abstract: Research on lane change prediction has gained attention in the last few
years. Most existing works in this area have been conducted in simulation
environments or with pre-recorded datasets, these works often rely on
simplified assumptions about sensing, communication, and traffic behavior that
do not always hold in practice. Real-world deployments of lane-change
prediction systems are relatively rare, and when they are reported, the
practical challenges, limitations, and lessons learned are often
under-documented. This study explores cooperative lane-change prediction
through a real hardware deployment in mixed traffic and shares the insights
that emerged during implementation and testing. We highlight the practical
challenges we faced, including bottlenecks, reliability issues, and operational
constraints that shaped the behavior of the system. By documenting these
experiences, the study provides guidance for others working on similar
pipelines.

</details>


### [51] [Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent](https://arxiv.org/abs/2509.20270)
*Xingjian Kang,Linda Vorberg,Andreas Maier,Alexander Katzmann,Oliver Taubmann*

Main category: cs.AI

TL;DR: 提出基于大语言模型的智能代理框架，用于辅助CT扫描协议的自然语言配置管理，提高工作流程效率


<details>
  <summary>Details</summary>
Motivation: CT扫描协议管理耗时且需要专业知识，同时放射科技术人才短缺，需要自动化解决方案

Method: 结合上下文学习、指令跟随和结构化工具调用能力的LLM代理框架，识别协议元素并应用精确修改

Result: 实验表明代理能有效检索协议组件、生成设备兼容协议文件并忠实执行用户请求

Conclusion: 该方法展示了LLM代理在CT扫描协议管理中的可行性，但仍面临设备API不统一和复杂请求处理等挑战

Abstract: Managing scan protocols in Computed Tomography (CT), which includes adjusting
acquisition parameters or configuring reconstructions, as well as selecting
postprocessing tools in a patient-specific manner, is time-consuming and
requires clinical as well as technical expertise. At the same time, we observe
an increasing shortage of skilled workforce in radiology. To address this
issue, a Large Language Model (LLM)-based agent framework is proposed to assist
with the interpretation and execution of protocol configuration requests given
in natural language or a structured, device-independent format, aiming to
improve the workflow efficiency and reduce technologists' workload. The agent
combines in-context-learning, instruction-following, and structured toolcalling
abilities to identify relevant protocol elements and apply accurate
modifications. In a systematic evaluation, experimental results indicate that
the agent can effectively retrieve protocol components, generate device
compatible protocol definition files, and faithfully implement user requests.
Despite demonstrating feasibility in principle, the approach faces limitations
regarding syntactic and semantic validity due to lack of a unified device API,
and challenges with ambiguous or complex requests. In summary, the findings
show a clear path towards LLM-based agents for supporting scan protocol
management in CT imaging.

</details>
