<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.CR](#cs.CR) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning](https://arxiv.org/abs/2602.06107)
*Zhuoming Chen,Hongyi Liu,Yang Zhou,Haizhong Zheng,Beidi Chen*

Main category: cs.AI

TL;DR: Jackpot框架通过最优预算拒绝采样(OBRS)解决RL中rollout模型与策略分布不匹配问题，显著提升训练稳定性，使Qwen3-8B-Base在300步更新内达到接近on-policy RL的性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的强化学习成本高昂，特别是rollout阶段。将rollout生成与策略优化解耦（如使用更高效的模型进行rollout）可显著提升效率，但这会引入严重的分布不匹配问题，导致学习不稳定

Method: 提出Jackpot框架，采用最优预算拒绝采样(OBRS)直接减少rollout模型与演化策略之间的分布差异。包括：原则性的OBRS程序、联合更新策略和rollout模型的统一训练目标、通过top-k概率估计和批次级偏差校正实现的高效系统实现

Result: 理论分析表明OBRS在可控接受预算下持续使rollout分布更接近目标分布。实证显示，相比重要性采样基线，Jackpot显著提升训练稳定性，在训练Qwen3-8B-Base时，经过最多300步批次大小为64的更新，性能达到与on-policy RL相当的水平

Conclusion: OBRS-based alignment使RL for LLMs中rollout生成与策略优化的解耦更接近实用和有效，为解决分布不匹配问题提供了有前景的解决方案

Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.

</details>


### [2] [Large Language Model Reasoning Failures](https://arxiv.org/abs/2602.06176)
*Peiyang Song,Pengrui Han,Noah Goodman*

Main category: cs.AI

TL;DR: 该论文首次对大型语言模型（LLMs）的推理失败进行了全面调查，提出了新的分类框架，将推理分为具身与非具身类型，并将推理失败分为基础性、应用特定和鲁棒性问题三类，为理解和改进LLM推理能力提供了系统化视角。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在各种任务中展现出卓越的推理能力，但在看似简单的场景中仍存在显著的推理失败。目前缺乏对这些失败的系统性理解和分类，研究较为分散。本文旨在填补这一空白，为理解LLM推理的系统性弱点提供结构化视角。

Method: 提出了一个新颖的分类框架：首先将推理分为具身推理和非具身推理，后者进一步细分为非正式（直觉）推理和正式（逻辑）推理。同时，将推理失败分为三类：影响下游任务的基础性架构失败、特定领域表现的应用特定限制、以及跨微小变化表现不一致的鲁棒性问题。对每种失败类型提供了明确定义、现有研究分析、根本原因探讨和缓解策略。

Result: 通过整合分散的研究工作，该调查为LLM推理的系统性弱点提供了结构化视角。发布了GitHub存储库（https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures），收集了关于LLM推理失败的全面研究资料，为该领域提供了便捷的入门途径。

Conclusion: 该论文首次对LLM推理失败进行了全面调查，提出的分类框架有助于系统理解推理弱点，为未来研究提供了有价值的见解和指导方向，有助于构建更强大、可靠和鲁棒的LLM推理能力。

Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

</details>


### [3] [Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)](https://arxiv.org/abs/2602.06227)
*Pierriccardo Olivieri,Fausto Lasca,Alessandro Gianola,Matteo Papini*

Main category: cs.AI

TL;DR: 提出基于LTLfMT逻辑框架的非马尔可夫奖励规范方法，解决大状态空间MDP中复杂任务的表达问题，通过理论分析和HER实践方法实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理非结构化异构数据域中的复杂任务时表达能力有限，需要手动编码谓词，缺乏统一可重用框架。LTLfMT提供了更强大的表达能力，但面临理论和计算挑战。

Method: 1) 理论层面：识别LTLfMT的可处理片段，适用于无限状态空间奖励规范；2) 实践层面：基于奖励机器和事后经验回放(HER)的方法，将一阶逻辑规范转化为可学习形式，解决奖励稀疏问题。

Result: 在连续控制环境中使用非线性算术理论进行评估，证明该方法能够自然地规范复杂任务。实验结果表明，定制的HER实现对于解决具有复杂目标的任务至关重要。

Conclusion: 提出的LTLfMT框架为MDP中的非马尔可夫奖励规范提供了统一、可重用的解决方案，平衡了表达能力和计算可行性，通过理论分析和实践方法成功解决了复杂任务的学习问题。

Abstract: In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal logic in which predicates are first-order formulas of arbitrary first-order theories rather than simple Boolean variables. This enhanced expressiveness enables the specification of complex tasks over unstructured and heterogeneous data domains, promoting a unified and reusable framework that eliminates the need for manual predicate encoding. However, the increased expressive power of LTLfMT introduces additional theoretical and computational challenges compared to standard LTLf specifications. We address these challenges from a theoretical standpoint, identifying a fragment of LTLfMT that is tractable but sufficiently expressive for reward specification in an infinite-state-space context. From a practical perspective, we introduce a method based on reward machines and Hindsight Experience Replay (HER) to translate first-order logic specifications and address reward sparsity. We evaluate this approach to a continuous-control setting using Non-Linear Arithmetic Theory, showing that it enables natural specification of complex tasks. Experimental results show how a tailored implementation of HER is fundamental in solving tasks with complex goals.

</details>


### [4] [Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems](https://arxiv.org/abs/2602.06319)
*Qifan Zhang,Jianhao Ruan,Aochuan Chen,Kang Zeng,Nuo Chen,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: GrAlgoBench是一个基于图算法问题的基准测试，用于评估大型推理模型，解决了现有基准在长上下文评估、挑战性和可编程验证方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有数学、代码和常识推理基准存在局限性：缺乏长上下文评估、挑战性不足、答案难以通过程序验证。需要一个新的基准来更好地评估大型推理模型的推理能力。

Method: 引入GrAlgoBench基准，通过图算法问题评估大型推理模型。图算法问题特别适合测试推理能力：需要长上下文推理、允许细粒度难度控制、支持标准化程序化评估。

Result: 在9个任务上的系统实验揭示了当前大型推理模型的两个主要弱点：1）随着上下文长度增加，准确率急剧下降（图节点超过120个时低于50%），主要由执行错误、弱记忆和冗余推理导致；2）存在过度思考现象，主要由大量无效的自我验证引起，增加了推理痕迹但没有提高正确性。

Conclusion: GrAlgoBench通过暴露这些局限性，确立了图算法问题作为严格、多维且实际相关的测试平台，用于推进大型推理模型推理能力的研究。

Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.

</details>


### [5] [Difficulty-Estimated Policy Optimization](https://arxiv.org/abs/2602.06375)
*Yu Zhao,Fan Jiang,Tianle Liu,Bo Zeng,Yu Liu,Longyue Wang,Weihua Luo*

Main category: cs.AI

TL;DR: 提出DEPO框架，通过动态难度估计筛选训练样本，减少低效rollout计算，在保持性能的同时降低2倍计算成本


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在处理过于简单或复杂问题时存在梯度信号衰减问题，而DAPO等变体虽然缓解梯度消失但计算开销大，需要更高效鲁棒的推理对齐方法

Method: 提出Difficulty-Estimated Policy Optimization (DEPO)框架，集成在线难度估计器，在rollout阶段前动态评估和筛选训练数据，优先分配计算资源给高学习潜力的样本

Result: 实验结果显示DEPO在保持模型性能的同时，将rollout成本降低高达2倍，显著降低了训练高性能推理模型的计算门槛

Conclusion: DEPO为推理缩放提供了更可持续的路径，通过智能资源分配优化推理对齐的效率和鲁棒性

Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.

</details>


### [6] [Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization](https://arxiv.org/abs/2602.06394)
*Arvid E. Gollwitzer,Paridhi Latawa,David de Gruijl,Deepak A. Subramanian,Adrián Noriega de la Colina*

Main category: cs.AI

TL;DR: QA-Token是一种质量感知的分词方法，通过双层优化和强化学习，将数据可靠性直接融入词汇表构建，在基因组学和金融领域显著提升性能，减少15%的token数量。


<details>
  <summary>Details</summary>
Motivation: 当前的分词方法处理序列数据时没有考虑信号质量，限制了其在嘈杂现实世界语料库上的有效性。需要一种能够直接整合数据可靠性的分词方法。

Method: 提出了QA-Token（质量感知分词）方法，包含三个关键贡献：(1) 双层优化公式，联合优化词汇表构建和下游性能；(2) 强化学习方法，通过质量感知奖励学习合并策略并保证收敛；(3) 通过Gumbel-Softmax松弛的自适应参数学习机制，实现端到端优化。

Result: 实验评估显示一致改进：基因组学（变异检测F1分数比BPE提高6.7个百分点）、金融（夏普比率提高30%）。在基础模型规模上，对1.7万亿碱基对的预训练语料进行分词，实现了最先进的病原体检测（94.53 MCC），同时减少了15%的token数量。

Conclusion: QA-Token解锁了嘈杂的现实世界语料库（包括数petabases的基因组序列和数TB的金融时间序列），用于基础模型训练，且推理时无额外开销。

Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.

</details>


### [7] [Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution](https://arxiv.org/abs/2602.06413)
*Hsien-Jyh Liao*

Main category: cs.AI

TL;DR: 论文指出，大语言模型在长时程推理任务中性能急剧下降的根本原因不是任务复杂度，而是自回归生成的过程级不稳定性，这导致了推理链的指数级衰减，需要离散分段和图结构来解决。


<details>
  <summary>Details</summary>
Motivation: 传统解释将大语言模型在长时程任务中的性能下降归因于任务复杂度（如组合搜索爆炸或长期信用分配），但作者认为这些解释不完整。即使在无分支的线性任务中，自回归执行也存在固有的稳定性限制，需要从过程级不稳定性角度重新理解长时程推理问题。

Method: 提出理论框架，推导定理A，证明单路径自回归推理中的决策优势随执行长度呈指数衰减，从而对可维护的推理链施加了基本限制。通过合成环境和真实TextWorld任务的实证研究，验证理论预测的可观测性能悬崖。

Result: 实证研究表明，在合成环境和真实TextWorld任务中观察到的性能悬崖与理论预测一致。这证实了自回归架构在维持长期连贯性方面存在根本性限制，短时程评估协议可能掩盖了结构不稳定性。

Conclusion: 长时程推理失败的根本原因是自回归生成的过程级不稳定性，而非单纯的任务复杂度。稳定长时程推理需要离散分段，自然诱导出有向无环图等图状执行结构。未来推理系统可能需要从单纯扩展转向结构化治理。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit.
  We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs).
  Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoretical predictions. Our findings provide a dynamical perspective on long-horizon reasoning failure and suggest new limitations on maintaining long-term coherence under purely autoregressive architectures. Furthermore, we highlight that short-horizon evaluation protocols may obscure structural instability, indicating a potential shift from scaling toward structured governance in future reasoning systems.

</details>


### [8] [AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents](https://arxiv.org/abs/2602.06485)
*Haotian Chen,Xin Cong,Shengda Fan,Yuyang Fu,Ziqin Gong,Yaxi Lu,Yishan Li,Boye Niu,Chengjun Pan,Zijun Song,Huadong Wang,Yesai Wu,Yueying Wu,Zihao Xie,Yukun Yan,Zhong Zhang,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文提出了AgentCPM-Explore，一个4B参数的边缘规模智能体模型，通过参数空间模型融合、奖励信号去噪和上下文信息精炼等创新方法，在多项基准测试中超越了8B级模型甚至更大规模模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体系统过度依赖大规模模型，而边缘规模模型（4B参数级别）的潜力尚未被充分探索。本文旨在系统研究如何训练高性能的边缘规模智能体模型。

Method: 提出了AgentCPM-Explore训练框架，包含三个关键技术：1）参数空间模型融合解决SFT中的灾难性遗忘问题；2）奖励信号去噪缓解RL训练中的噪声敏感性；3）上下文信息精炼处理长上下文场景中的推理退化问题。

Result: AgentCPM-Explore在4B级模型中达到SOTA性能，在四个基准测试中匹配或超越8B级SOTA模型，在五个基准测试中甚至超越了Claude-4.5-Sonnet或DeepSeek-v3.2等更大规模模型。在GAIA文本任务上达到97.09%的准确率（pass@64）。

Conclusion: 边缘规模模型的瓶颈并非其固有能力上限，而是推理稳定性问题。通过本文建立的训练框架，AgentCPM-Explore有效释放了边缘规模模型被低估的潜力，证明了小规模模型也能实现高性能智能体能力。

Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.

</details>


### [9] [JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks](https://arxiv.org/abs/2602.06486)
*Lanbo Lin,Jiayao Liu,Tianyuan Yang,Li Cai,Yuanwu Xu,Lei Wei,Sicong Xie,Guannan Zhang*

Main category: cs.AI

TL;DR: JADE是一个双层评估框架，结合了预定义评估技能和动态声明级评估，解决了开放专业任务中评估的严谨性与灵活性之间的困境。


<details>
  <summary>Details</summary>
Motivation: 评估智能体AI在开放专业任务上面临基本困境：静态评估标准虽然严谨可重复但无法适应多样有效的响应策略，而LLM作为评估者虽然能适应个体响应但存在不稳定性和偏见。人类专家通过结合领域基础原则和动态声明级评估来解决这一困境。

Method: JADE采用双层框架：第一层将专家知识编码为预定义的评估技能集，提供稳定的评估标准；第二层执行报告特定的声明级评估，灵活评估多样推理策略，并通过证据依赖性门控来使基于被反驳声明的结论无效。

Result: 在BizBench上的实验显示，JADE提高了评估稳定性，并揭示了整体LLM评估器遗漏的关键智能体失败模式。与专家制定的评估标准有很强的一致性，并能有效迁移到医疗领域基准测试，验证了JADE在不同专业领域的有效性。

Conclusion: JADE通过结合预定义评估技能和动态声明级评估，成功解决了开放专业任务评估中的严谨性与灵活性困境，提供了一种稳定、灵活且可迁移的评估框架。

Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.

</details>


### [10] [HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction](https://arxiv.org/abs/2602.06527)
*Shengxuan Qiu,Haochen Huang,Shuzhang Zhong,Pengfei Zuo,Meng Li*

Main category: cs.AI

TL;DR: HyPER是一种针对专家混合模型的训练免费在线控制策略，通过动态扩展-缩减控制重新分配计算资源，在固定预算下优化多路径推理的探索-利用权衡，显著提高准确性同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索-利用权衡上存在局限性：树状搜索通过脆弱的扩展规则硬编码探索，干扰了训练后的推理；并行推理则过度探索冗余假设路径且依赖弱答案选择。研究发现最优平衡是阶段依赖的，正确与错误推理路径往往在后期才分叉。

Method: 将测试时扩展重新表述为假设池上的动态扩展-缩减控制问题。提出HyPER：1) 在线控制器根据假设池演化从探索转向利用；2) 令牌级细化机制实现高效生成时利用而无需全路径重采样；3) 长度和置信度感知的聚合策略实现可靠答案时利用。

Result: 在四个专家混合语言模型和多样化推理基准测试中，HyPER始终实现更优的准确性-计算权衡，将准确性提高8-10%，同时将令牌使用量减少25-40%。

Conclusion: HyPER通过动态控制策略有效解决了多路径推理中的探索-利用权衡问题，在固定计算预算下显著提升了专家混合模型的推理性能，为测试时计算扩展提供了更灵活高效的解决方案。

Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consistently achieves a superior accuracy-compute trade-off, improving accuracy by 8 to 10 percent while reducing token usage by 25 to 40 percent.

</details>


### [11] [SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees](https://arxiv.org/abs/2602.06554)
*Tianyi Hu,Qingxu Fu,Yanxi Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: 本文提出SeeUPO算法，解决现有RL算法在多轮交互场景中缺乏收敛保证的问题，通过序列级顺序更新策略优化实现无critic且收敛的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的AI智能体主要使用强化学习训练，但现有骨干RL算法在多轮场景中缺乏经过验证的收敛保证，导致训练不稳定和无法收敛到最优策略。

Method: 提出SeeUPO（序列级顺序更新策略优化），将多轮交互建模为顺序执行的多智能体赌博机问题，通过反向执行顺序的逐轮顺序策略更新，确保单调改进并通过后向归纳收敛到全局最优解。

Result: 在AppWorld和BFCL v4上的实验显示，SeeUPO相比现有骨干算法有显著提升：在Qwen3-14B上相对增益43.3%-54.6%，在Qwen2.5-14B上相对增益24.1%-41.9%（跨基准平均），并表现出优越的训练稳定性。

Conclusion: SeeUPO解决了现有RL算法在多轮场景中无法同时实现无critic和收敛保证的问题，为基于LLM的AI智能体训练提供了具有理论保证的稳定解决方案。

Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.
  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.
  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.
  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.

</details>


### [12] [Same Answer, Different Representations: Hidden instability in VLMs](https://arxiv.org/abs/2602.06652)
*Farooq Ahmad Wani,Alessandro Suglia,Rohit Saxena,Aryo Pradipta Gema,Wai-Chung Kwan,Fazl Barez,Maria Sofia Bucarelli,Fabrizio Silvestri,Pasquale Minervini*

Main category: cs.AI

TL;DR: 该研究挑战了视觉语言模型稳健性评估的传统假设，提出了一个结合表示层和频率分析的新评估框架，揭示了三种失败模式：内部表示漂移、规模不带来稳健性提升、以及扰动对不同任务的差异化影响。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs稳健性评估主要基于输出层面的不变性假设，即稳定预测反映稳定的多模态处理。本文认为这一假设不足，需要更深入评估模型内部表示的变化。

Method: 提出了一个表示感知和频率感知的评估框架，测量内部嵌入漂移、频谱敏感性和结构平滑性（视觉token的空间一致性），同时结合标准基于标签的指标。在SEEDBench、MMMU和POPE数据集上对现代VLMs进行评估。

Result: 揭示了三种失败模式：1）模型经常保持预测答案不变但经历显著的内部表示漂移；2）稳健性不随模型规模提升，更大模型虽然准确率更高但敏感性相等或更大；3）扰动对不同任务影响不同：在推理任务中破坏粗粒度与细粒度视觉线索的结合，但在幻觉基准测试中可能减少误报。

Conclusion: 仅依赖输出层面的不变性评估VLMs稳健性是不充分的，需要结合内部表示分析。模型可能在预测不变的情况下经历显著表示漂移，且规模扩大不必然提升稳健性，这对VLMs的可靠部署提出了新挑战。

Abstract: The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.

</details>


### [13] [Autoregressive Models for Knowledge Graph Generation](https://arxiv.org/abs/2602.06707)
*Thiviyan Thanapalasingam,Antonis Vozikis,Peter Bloem,Paul Groth*

Main category: cs.AI

TL;DR: ARK是一种自回归知识图谱生成模型，将图视为三元组序列，无需显式规则监督即可学习语义约束，在IntelliGraphs基准上达到89.2%-100%语义有效性，并能生成训练中未见的新图。


<details>
  <summary>Details</summary>
Motivation: 知识图谱生成需要模型学习三元组间的复杂语义依赖，同时保持领域有效性约束。与独立评分三元组的链接预测不同，生成模型必须捕捉整个子图的相互依赖关系以产生语义连贯的结构。

Method: 提出ARK（自回归知识图谱生成）模型家族，将图视为(head, relation, tail)三元组序列进行自回归生成。模型直接从数据中学习隐式语义约束（类型一致性、时间有效性、关系模式），无需显式规则监督。还引入了SAIL，ARK的变分扩展，通过学习的潜在表示实现可控生成。

Result: 在IntelliGraphs基准上，模型在多样化数据集上达到89.2%到100.0%的语义有效性，并能生成训练中未见的新图。分析显示模型容量（隐藏维度≥64）比架构深度对KG生成更重要，循环架构在保持可比有效性的同时提供显著计算效率。

Conclusion: 自回归模型为知识图谱生成提供了有效框架，在知识库补全和查询回答中具有实际应用价值。模型容量比架构深度更关键，循环架构在效率和有效性间取得了良好平衡。

Abstract: Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantically coherent structures. We present ARK (Auto-Regressive Knowledge Graph Generation), a family of autoregressive models that generate KGs by treating graphs as sequences of (head, relation, tail) triples. ARK learns implicit semantic constraints directly from data, including type consistency, temporal validity, and relational patterns, without explicit rule supervision. On the IntelliGraphs benchmark, our models achieve 89.2% to 100.0% semantic validity across diverse datasets while generating novel graphs not seen during training. We also introduce SAIL, a variational extension of ARK that enables controlled generation through learned latent representations, supporting both unconditional sampling and conditional completion from partial graphs. Our analysis reveals that model capacity (hidden dimensionality >= 64) is more critical than architectural depth for KG generation, with recurrent architectures achieving comparable validity to transformer-based alternatives while offering substantial computational efficiency. These results demonstrate that autoregressive models provide an effective framework for KG generation, with practical applications in knowledge base completion and query answering.

</details>


### [14] [Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions](https://arxiv.org/abs/2602.06746)
*Alessandro Abate,Giuseppe De Giacomo,Mathias Jackermeier,Jan Kretínský,Maximilian Prokop,Christoph Weinhuber*

Main category: cs.AI

TL;DR: 该论文提出了一种基于语义LTL到自动机转换的多任务强化学习方法，能够处理复杂的线性时序逻辑规范，实现通用策略学习。


<details>
  <summary>Details</summary>
Motivation: 研究多任务强化学习，目标是学习一个能够泛化到任意（包括未见过的）任务的通用策略。现有方法在处理复杂的线性时序逻辑规范时存在局限性。

Method: 提出新颖的任务嵌入技术，利用新一代语义LTL到自动机的转换方法（最初为时序综合开发）。生成的语义标记自动机包含丰富的结构化状态信息，支持：1）在线高效计算自动机；2）提取表达性强的任务嵌入用于条件化策略；3）天然支持完整的LTL规范。

Result: 在多个领域的实验结果表明，该方法实现了最先进的性能，能够扩展到复杂规范，而现有方法在这些复杂规范上会失败。

Conclusion: 通过结合语义LTL到自动机的转换技术，提出的多任务强化学习方法能够有效处理复杂的时序逻辑规范，实现高性能的通用策略学习，并在复杂场景中超越现有方法。

Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.

</details>


### [15] [Wild Guesses and Mild Guesses in Active Concept Learning](https://arxiv.org/abs/2602.06818)
*Anirudh Chari,Neil Pattanaik*

Main category: cs.AI

TL;DR: 研究比较了两种主动概念学习策略：最大化期望信息增益的理性主动学习者和类似人类的积极测试策略，发现在简单概念上积极测试策略表现更好，这可能是因为它能维持假设生成器的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究人类概念学习中主动查询策略的权衡问题，特别是如何平衡查询的信息量与学习者生成和评分假设的稳定性。探讨为什么人类常采用的积极测试策略（看似有确认偏误）在实际学习中可能比完全理性的信息最大化策略更有效。

Method: 采用神经符号贝叶斯学习框架，其中假设是由大型语言模型生成的可执行程序，通过贝叶斯更新重新加权。比较两种策略：1）理性主动学习者（最大化近似期望信息增益EIG）；2）积极测试策略（PTS），查询当前最佳假设预测为正的实例。在经典数字游戏任务中进行实验。

Result: 在需要证伪的复杂概念（如复合规则或包含例外的规则）上，EIG策略有效；但在简单概念上表现不佳。EIG策略失败的原因是：高度诊断性的边界查询会将后验推向LLM生成器产生无效或过于具体程序的区域，导致粒子近似中的支持不匹配陷阱。PTS虽然信息次优，但通过选择"安全"查询维持了生成器的有效性，在简单规则上收敛更快。

Conclusion: 人类的"确认偏误"可能不是认知错误，而是在人类思维特有的稀疏、开放式假设空间中维持可处理推理的理性适应。积极测试策略通过避免支持不匹配陷阱，在简单概念学习中表现出优势。

Abstract: Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting "safe" queries, leading to faster convergence on simple rules. Our results suggest that "confirmation bias" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.

</details>


### [16] [ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training](https://arxiv.org/abs/2602.06820)
*Dunwei Tu,Hongyan Hao,Hansi Yang,Yihao Chen,Yi-Kai Zhang,Zhikang Xia,Yu Yang,Yueqing Sun,Xingchen Liu,Furao Shen,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.AI

TL;DR: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性，显著提升智能体在未见多轮工具使用基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 训练能够适应多样化场景的通用智能体需要交互式环境进行自探索，但现有交互环境严重不足，现有合成方法在环境多样性和可扩展性方面存在显著限制。

Method: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性。

Result: 在未见的多轮工具使用基准（如τ²-Bench和VitaBench）上表现出显著性能提升，展示了强大的泛化能力；研究了领域数量增加与模型泛化性能的关系，提供了环境多样性扩展对稳健智能体学习至关重要的经验证据。

Conclusion: ScaleEnv框架解决了交互环境稀缺和现有方法多样性不足的问题，通过扩展环境多样性显著提升了智能体的泛化能力，为稳健的智能体学习提供了有效解决方案。

Abstract: Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $τ^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.

</details>


### [17] [POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models](https://arxiv.org/abs/2602.06822)
*Yi Chen,Wonjin Shin,Shuhong Liu,Tho Mai,Jeongmo Lee,Chuanbo Hua,Kun Wang,Jun Liu,Joo-Young Kim*

Main category: cs.AI

TL;DR: POP是一种轻量级在线结构化剪枝框架，通过分区引导的动态剪枝在自回归生成过程中实现上下文条件化的剪枝决策，无需预处理或重训练。


<details>
  <summary>Details</summary>
Motivation: 当前结构化剪枝方法在推理时采用固定的剪枝决策，忽略了自回归token生成过程中出现的稀疏性模式。大型基础模型通过扩展获得强大性能，但需要更高效的推理方法。

Method: POP将模型通道划分为保留区、候选区和剪枝区。预填充阶段定义粗粒度剪枝分区，解码阶段在候选区内生成细粒度掩码，避免全通道重新评估。粗粒度分区保留始终重要的权重，细粒度掩码提供解码时的上下文条件化变化。

Result: 在多种大型基础模型（包括大语言模型、专家混合模型和视觉语言模型）上的广泛评估表明，POP始终比现有剪枝方法提供更高的准确性，同时产生更小的计算开销并最小化推理延迟。

Conclusion: POP是一种轻量级即插即用方法，无需预处理（包括离线校准、重训练或学习预测器），能够在自回归生成过程中实现高效的上下文条件化动态剪枝。

Abstract: Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.

</details>


### [18] [An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization](https://arxiv.org/abs/2602.06838)
*Jin Wang,Hui Ma,Fei Xing,Ming Yan*

Main category: cs.AI

TL;DR: 本文提出了一种自适应差分隐私联邦学习框架，通过客户端轻量级压缩模块、服务器端自适应梯度裁剪和约束感知聚合机制，解决异构数据和隐私约束下的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在实际部署中面临设备异构性、非独立同分布数据导致的梯度更新不稳定和偏差问题。当加入差分隐私保护时，传统的固定梯度裁剪和高斯噪声注入会进一步放大梯度扰动，导致训练震荡和性能下降。

Method: 1. 客户端引入轻量级本地压缩模块，正则化中间表示并约束梯度变异性，减轻本地优化过程中的噪声放大；2. 服务器端采用自适应梯度裁剪策略，基于历史更新统计动态调整裁剪阈值，避免过度裁剪和噪声主导；3. 设计约束感知聚合机制，抑制不可靠或噪声主导的客户端更新，稳定全局优化。

Result: 在CIFAR-10和SVHN数据集上的大量实验表明，该框架提高了收敛稳定性和分类准确率。

Conclusion: 提出的自适应差分隐私联邦学习框架有效解决了异构和隐私约束环境下的模型效率问题，通过客户端压缩、自适应裁剪和智能聚合机制，实现了更稳定和高效的联邦学习训练。

Abstract: Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.

</details>


### [19] [From Features to Actions: Explainability in Traditional and Agentic AI Systems](https://arxiv.org/abs/2602.06841)
*Sindhuja Chaduvula,Jessee Ho,Kina Kim,Aravind Narayanan,Mahshid Alinoori,Muskan Garg,Dhanesh Ramachandram,Shaina Raza*

Main category: cs.AI

TL;DR: 该研究比较了静态预测解释与智能体系统解释方法，发现传统特征归因方法适用于静态分类但不适用于诊断智能体执行失败，而基于轨迹的评估方法能有效定位智能体行为故障。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，AI系统从静态预测转向多步决策的智能体系统。传统解释方法主要针对单次预测，而智能体系统的成功失败由决策序列决定，需要研究如何将解释方法从静态设置扩展到智能体设置。

Method: 通过实证比较静态分类任务中的归因解释方法与智能体基准测试（TAU-bench Airline和AssistantBench）中的轨迹诊断方法，区分两种解释方法的适用性。

Result: 归因方法在静态设置中特征排序稳定（Spearman ρ=0.86），但无法可靠诊断智能体轨迹中的执行级故障；基于轨迹的评估能一致定位行为故障，发现状态跟踪不一致在失败运行中高出2.7倍，使成功概率降低49%。

Conclusion: 研究结果表明需要从静态解释转向轨迹级解释，以更好地评估和诊断自主AI智能体系统的行为，为智能体系统的可解释性提供了新的方向。

Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\times$ more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.
  Resources:
  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [Scaling Mobile Chaos Testing with AI-Driven Test Execution](https://arxiv.org/abs/2602.06223)
*Juan Marcano,Ashish Samant,Kai Song,Lingchao Chen,Kaelan Mikowicz,Tim Smyth,Mengdie Zhang,Ali Zamani,Arturo Bravo Rovirosa,Sowjanya Puligadda,Srikanth Prodduturi,Mayank Bansal*

Main category: cs.SE

TL;DR: 论文提出了一种自动化移动混沌测试系统，结合LLM驱动的移动测试平台和服务级故障注入系统，解决了大规模分布式系统中移动应用后端服务故障测试的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式系统中的移动应用容易受到后端服务故障的影响，但传统的混沌工程方法无法扩展到移动测试，因为需要验证的用户流程、地理位置和故障场景的组合呈爆炸式增长。

Method: 开发了一个自动化移动混沌测试系统，将基于LLM的移动测试平台DragonCrawl与服务级故障注入系统uHavoc集成。关键洞察是自适应AI驱动的测试执行可以在后端降级条件下导航移动应用，无需为每个用户流程、城市和故障类型的组合手动编写测试用例。

Result: 自2024年第一季度以来，系统在Uber的Rider、Driver和Eats应用的47个关键流程中执行了超过18万次自动化混沌测试，相当于约3.9万小时的手动测试工作量。识别了23个弹性风险，其中70%是架构依赖违规问题。自动根因分析将调试时间从数小时减少到数分钟，在将移动故障归因于特定后端服务方面实现了88%的precision@5。

Conclusion: 该系统设计展示了在故障注入下保持99%的测试可靠性，运营经验证明连续移动弹性验证可以在生产规模上实现，解决了传统方法无法扩展的移动混沌测试挑战。

Abstract: Mobile applications in large-scale distributed systems are susceptible to backend service failures, yet traditional chaos engineering approaches cannot scale mobile testing due to the combinatorial explosion of flows, locations, and failure scenarios that need validation. We present an automated mobile chaos testing system that integrates DragonCrawl, an LLM-based mobile testing platform, with uHavoc, a service-level fault injection system. The key insight is that adaptive AI-driven test execution can navigate mobile applications under degraded backend conditions, eliminating the need to manually write test cases for each combination of user flow, city, and failure type. Since Q1 2024, our system has executed over 180,000 automated chaos tests across 47 critical flows in Uber's Rider, Driver, and Eats applications, representing approximately 39,000 hours of manual testing effort that would be impractical at this scale. We identified 23 resilience risks, with 70% being architectural dependency violations where non-critical service failures degraded core user flows. Twelve issues were severe enough to prevent trip requests or food orders. Two caused application crashes detectable only through mobile chaos testing, not backend testing alone. Automated root cause analysis reduced debugging time from hours to minutes, achieving 88% precision@5 in attributing mobile failures to specific backend services. This paper presents the system design, evaluates its performance under fault injection (maintaining 99% test reliability), and reports operational experience demonstrating that continuous mobile resilience validation is achievable at production scale.

</details>


### [21] [Code vs Serialized AST Inputs for LLM-Based Code Summarization: An Empirical Study](https://arxiv.org/abs/2602.06671)
*Shijia Dong,Haoruo Zhao,Paul Harvey*

Main category: cs.SE

TL;DR: AST(NIT)是一种AST增强和序列化方法，将完整的AST结构信息编码为LLM兼容序列，在减少输入长度和训练时间的同时，达到与现有方法相当的代码摘要质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码摘要方法主要依赖原始代码或仅包含部分AST信号，未能充分利用完整AST表示的潜力。传统编码器-解码器模型已证明AST能提升摘要质量，但LLM方法尚未充分探索完整AST表示。

Method: 提出AST(NIT)方法，包括AST增强和序列化技术，保留词汇细节并将结构信息编码为LLM兼容的序列表示，使LLM能够处理完整的AST信息。

Result: 在CodeXGLUE Python数据集上使用LLaMA-3.1-8B模型实验表明，序列化AST能减少LLM输入长度、缩短训练时间，同时达到与现有方法相当的代码摘要质量。

Conclusion: AST(NIT)方法成功将完整AST表示引入LLM代码摘要，在保持摘要质量的同时提高了效率，为LLM代码理解任务提供了新的AST表示方法。

Abstract: Summarizing source code into natural language descriptions (code summarization) helps developers better understand program functionality and reduce the burden of software maintenance. Abstract Syntax Trees (ASTs), as opposed to source code, have been shown to improve summarization quality in traditional encoder-decoder-based code summarization models. However, most large language model (LLM)-based code summarization methods rely on raw code or only incorporate partial AST signals, meaning that the potential of complete AST representation has not been fully explored for LLMs. This paper presents AST(NIT), an AST augmentation and serialization method that preserves lexical details and encodes structural information into LLM-compatible sequences. Experiments with the LLaMA-3.1-8B model on the CodeXGLUE Python dataset show that the proposed serialized ASTs reduce the length of LLM inputs, require shorter training times, and achieve summarization quality comparable to existing approaches.

</details>


### [22] [Using Large Language Models to Support Automation of Failure Management in CI/CD Pipelines: A Case Study in SAP HANA](https://arxiv.org/abs/2602.06709)
*Duong Bui,Stefan Grintz,Alexander Berndt,Thomas Bach*

Main category: cs.SE

TL;DR: LLM结合历史故障数据可实现92.1%的CI/CD管道故障管理自动化准确率


<details>
  <summary>Details</summary>
Motivation: CI/CD管道故障管理手动操作耗时，传统程序难以处理非结构化信息，而LLM在处理非结构化数据方面表现出潜力

Method: 在SAP HANA大型工业项目中评估基于LLM的系统，提供管道信息、故障管理指令和历史故障数据等不同领域知识，通过消融研究确定各类知识对解决方案准确性的贡献

Result: 历史故障数据对系统准确性贡献最大，使系统在92.1%的情况下生成精确解决方案；提供领域知识时错误定位准确率达97.4%，无领域知识时为84.2%

Conclusion: LLM结合历史故障数据是实现CI/CD管道故障管理自动化的有前景方法

Abstract: CI/CD pipeline failure management is time-consuming when performed manually. Automating this process is non-trivial because the information required for effective failure management is unstructured and cannot be automatically processed by traditional programs. With their ability to process unstructured data, large language models (LLMs) have shown promising results for automated failure management by previous work. Following these studies, we evaluated whether an LLM-based system could automate failure management in a CI/CD pipeline in the context of a large industrial software project, namely SAP HANA. We evaluated the ability of the LLM-based system to identify the error location and to propose exact solutions that contain no unnecessary actions. To support the LLM in generating exact solutions, we provided it with different types of domain knowledge, including pipeline information, failure management instructions, and data from historical failures. We conducted an ablation study to determine which type of domain knowledge contributed most to solution accuracy. The results show that data from historical failures contributed the most to the system's accuracy, enabling it to produce exact solutions in 92.1% of cases in our dataset. The system correctly identified the error location with 97.4% accuracy when provided with domain knowledge, compared to 84.2% accuracy without it. In conclusion, our findings indicate that LLMs, when provided with data from historical failures, represent a promising approach for automating CI/CD pipeline failure management.

</details>


### [23] [Statistical-Based Metric Threshold Setting Method for Software Fault Prediction in Firmware Projects: An Industrial Experience](https://arxiv.org/abs/2602.06831)
*Marco De Luca,Domenico Amalfitano,Anna Rita Fasolino,Porfirio Tramontana*

Main category: cs.SE

TL;DR: 本文提出了一种为嵌入式固件定义上下文特定软件度量阈值的结构化方法，用于工业环境中的故障预测，替代黑盒AI模型，提供可解释的解决方案。


<details>
  <summary>Details</summary>
Motivation: 嵌入式固件软件质量至关重要，特别是在安全关键领域需要符合ISO 26262标准。虽然基于机器学习的故障预测模型精度高，但缺乏可解释性限制了其在工业环境中的应用。开发人员需要可直接用于软件质量保证过程和指导缺陷缓解策略的可操作见解。

Method: 提出结构化流程定义上下文特定软件度量阈值，支持跨项目故障预测：从一个项目集推导阈值，应用于独立开发的固件。分析三个真实世界的C嵌入式固件项目，使用Coverity和Understand静态分析工具提取软件度量。通过统计分析和假设检验，识别区分性度量并推导经验阈值。

Result: 推导的阈值通过实验评估验证，证明能够以高精度识别易错函数。结果确认推导的阈值可作为故障预测的可解释解决方案，符合行业标准和SQA实践。

Conclusion: 该方法为黑盒AI模型提供了实用替代方案，允许开发人员系统评估软件质量，采取预防措施，并将基于度量的故障预测集成到工业开发工作流中以缓解软件故障。

Abstract: Ensuring software quality in embedded firmware is critical, especially in safety-critical domains where compliance with functional safety standards (ISO 26262) requires strong guarantees of software reliability. While machine learning-based fault prediction models have demonstrated high accuracy, their lack of interpretability limits their adoption in industrial settings. Developers need actionable insights that can be directly employed in software quality assurance processes and guide defect mitigation strategies. In this paper, we present a structured process for defining context-specific software metric thresholds suitable for integration into fault detection workflows in industrial settings. Our approach supports cross-project fault prediction by deriving thresholds from one set of projects and applying them to independently developed firmware, thereby enabling reuse across similar software systems without retraining or domain-specific tuning. We analyze three real-world C-embedded firmware projects provided by an industrial partner, using Coverity and Understand static analysis tools to extract software metrics. Through statistical analysis and hypothesis testing, we identify discriminative metrics and derived empirical threshold values capable of distinguishing faulty from non-faulty functions. The derived thresholds are validated through an experimental evaluation, demonstrating their effectiveness in identifying fault-prone functions with high precision. The results confirm that the derived thresholds can serve as an interpretable solution for fault prediction, aligning with industry standards and SQA practices. This approach provides a practical alternative to black-box AI models, allowing developers to systematically assess software quality, take preventive actions, and integrate metric-based fault prediction into industrial development workflows to mitigate software faults.

</details>


### [24] [TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code](https://arxiv.org/abs/2602.06875)
*Jiangping Huang,Wenguang Ye,Weisong Sun,Jian Zhang,Mingyue Zhang,Yang Liu*

Main category: cs.SE

TL;DR: TraceCoder是一个多智能体代码修复框架，通过运行时追踪和因果分析精确定位错误，结合历史教训学习和回滚机制，显著提升代码修复准确率。


<details>
  <summary>Details</summary>
Motivation: 现有代码修复方法依赖简单的通过/失败信号，对程序行为理解有限，难以精确定位错误，且缺乏从历史失败中学习的能力，导致修复过程低效重复。

Method: TraceCoder采用多智能体协作框架，模拟人类专家的观察-分析-修复过程：1) 通过诊断探针捕获细粒度运行时追踪；2) 对追踪进行因果分析定位根本原因；3) 引入历史教训学习机制从先前失败中提取经验；4) 使用回滚机制确保每次迭代都是严格改进。

Result: 在多个基准测试中，TraceCoder相比现有先进基线实现了最高34.43%的相对Pass@1准确率提升。消融研究表明迭代修复过程单独贡献了65.61%的相对准确率增益，且在准确率和成本效率方面均显著优于领先的迭代方法。

Conclusion: TraceCoder通过深度运行时分析、历史教训学习和严格改进保证，有效解决了LLM生成代码的细微但关键的错误修复问题，显著提升了代码修复的准确性和效率。

Abstract: Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [25] [Know Your Scientist: KYC as Biosecurity Infrastructure](https://arxiv.org/abs/2602.06172)
*Jonathan Feldman,Tal Feldman,Annie I Anton*

Main category: cs.CR

TL;DR: 论文提出基于金融反洗钱KYC原则的三层生物AI安全框架，从内容审查转向用户验证和监控，以应对蛋白质设计工具的双重用途风险


<details>
  <summary>Details</summary>
Motivation: 生物AI工具（蛋白质设计和结构预测）快速发展带来双重用途风险，现有基于内容审查的安全措施（关键词过滤、输出筛查、访问限制）不适用于生物学领域，因为可靠功能预测难以实现，新型威胁可能被设计绕过检测

Method: 提出三层KYC框架：第一层利用研究机构作为信任锚点，为关联研究人员提供担保并负责审查；第二层通过序列同源性搜索和功能注释进行输出筛查；第三层监控行为模式以检测与声明研究目的不一致的异常

Result: 该分层方法为合法研究人员保留访问权限，同时通过机构问责和可追溯性提高滥用成本，可以立即使用现有机构基础设施实施，无需新立法或监管授权

Conclusion: 基于KYC的生物AI治理框架比当前内容审查方法更有效，将重点从内容检查转向用户验证和监控，能够更好地应对蛋白质设计工具的双重用途风险

Abstract: Biological AI tools for protein design and structure prediction are advancing rapidly, creating dual-use risks that existing safeguards cannot adequately address. Current model-level restrictions, including keyword filtering, output screening, and content-based access denials, are fundamentally ill-suited to biology, where reliable function prediction remains beyond reach and novel threats evade detection by design. We propose a three-tier Know Your Customer (KYC) framework, inspired by anti-money laundering (AML) practices in the financial sector, that shifts governance from content inspection to user verification and monitoring. Tier I leverages research institutions as trust anchors to vouch for affiliated researchers and assume responsibility for vetting. Tier II applies output screening through sequence homology searches and functional annotation. Tier III monitors behavioral patterns to detect anomalies inconsistent with declared research purposes. This layered approach preserves access for legitimate researchers while raising the cost of misuse through institutional accountability and traceability. The framework can be implemented immediately using existing institutional infrastructure, requiring no new legislation or regulatory mandates.

</details>


### [26] [AdFL: In-Browser Federated Learning for Online Advertisement](https://arxiv.org/abs/2602.06336)
*Ahmad Alemari,Pritam Sen,Cristian Borcea*

Main category: cs.CR

TL;DR: AdFL是一个在浏览器中运行的联邦学习框架，用于学习用户广告偏好，以平衡定向广告收入与用户隐私保护，支持差分隐私保护且无需客户端安装软件。


<details>
  <summary>Details</summary>
Motivation: 随着GDPR等在线隐私法规的出台，在线出版商需要在定向广告收入和用户隐私之间找到平衡。传统定向广告需要收集用户数据，而联邦学习可以在不共享原始数据的情况下进行分布式学习。

Method: 提出了AdFL框架，在浏览器中运行联邦学习，利用浏览器可用特征（如广告可见性、点击率、页面停留时间等）训练本地模型，在发布商服务器上聚合全局模型，无需客户端安装额外软件。

Result: 在日访问量4万用户的网站上测试，AdFL能在几毫秒内捕获浏览器训练信息，广告可见性预测达到92.59% AUC，差分隐私保护版本性能下降有限。

Conclusion: AdFL证明了在浏览器中实施联邦学习进行广告偏好预测的可行性，既能保护用户隐私，又能保持广告定向效果，为在线出版商提供了GDPR合规的解决方案。

Abstract: Since most countries are coming up with online privacy regulations, such as GDPR in the EU, online publishers need to find a balance between revenue from targeted advertisement and user privacy. One way to be able to still show targeted ads, based on user personal and behavioral information, is to employ Federated Learning (FL), which performs distributed learning across users without sharing user raw data with other stakeholders in the publishing ecosystem. This paper presents AdFL, an FL framework that works in the browsers to learn user ad preferences. These preferences are aggregated in a global FL model, which is then used in the browsers to show more relevant ads to users. AdFL can work with any model that uses features available in the browser such as ad viewability, ad click-through, user dwell time on pages, and page content. The AdFL server runs at the publisher and coordinates the learning process for the users who browse pages on the publisher's website. The AdFL prototype does not require the client to install any software, as it is built utilizing standard APIs available on most modern browsers. We built a proof-of-concept model for ad viewability prediction that runs on top of AdFL. We tested AdFL and the model with two non-overlapping datasets from a website with 40K visitors per day. The experiments demonstrate AdFL's feasibility to capture the training information in the browser in a few milliseconds, show that the ad viewability prediction achieves up to 92.59% AUC, and indicate that utilizing differential privacy (DP) to safeguard local model parameters yields adequate performance, with only modest declines in comparison to the non-DP variant.

</details>


### [27] [VENOMREC: Cross-Modal Interactive Poisoning for Targeted Promotion in Multimodal LLM Recommender Systems](https://arxiv.org/abs/2602.06409)
*Guowei Guan,Yurong Hao,Jiaming Zhang,Tiantong Wu,Fuyao Zhang,Tianxiang Chen,Longtao Huang,Cyril Leung,Wei Yang Bryan Lim*

Main category: cs.CR

TL;DR: 本文提出VENOMREC攻击方法，针对多模态大语言模型推荐系统，通过跨模态协同投毒操纵融合表示，在保持推荐效用的同时显著提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型推荐系统通过跨模态融合增强了鲁棒性，但也引入了新的攻击面——协同多模态投毒可以稳定地操纵融合表示。本文旨在形式化这种威胁并开发有效的攻击方法。

Method: 提出VENOMREC攻击框架：1) 曝光对齐：识别联合嵌入空间中的高曝光区域；2) 跨模态交互扰动：通过注意力引导的耦合token-patch编辑来制作协同投毒样本。

Result: 在三个真实世界多模态数据集上的实验表明，VENOMREC始终优于强基线，平均ER@20达到0.73，比最强基线平均提升+0.52绝对ER点，同时保持可比的推荐效用。

Conclusion: 跨模态协同投毒是多模态推荐系统的新安全威胁，VENOMREC方法有效展示了这种攻击的可行性，为多模态推荐系统的安全设计提供了重要启示。

Abstract: Multimodal large language models (MLLMs) are pushing recommender systems (RecSys) toward content-grounded retrieval and ranking via cross-modal fusion. We find that while cross-modal consensus often mitigates conventional poisoning that manipulates interaction logs or perturbs a single modality, it also introduces a new attack surface where synchronised multimodal poisoning can reliably steer fused representations along stable semantic directions during fine-tuning. To characterise this threat, we formalise cross-modal interactive poisoning and propose VENOMREC, which performs Exposure Alignment to identify high-exposure regions in the joint embedding space and Cross-modal Interactive Perturbation to craft attention-guided coupled token-patch edits. Experiments on three real-world multimodal datasets demonstrate that VENOMREC consistently outperforms strong baselines, achieving 0.73 mean ER@20 and improving over the strongest baseline by +0.52 absolute ER points on average, while maintaining comparable recommendation utility.

</details>


### [28] [The Avatar Cache: Enabling On-Demand Security with Morphable Cache Architecture](https://arxiv.org/abs/2602.06433)
*Anubhav Bhatla,Navneet Navneet,Moinuddin Qureshi,Biswabandan Panda*

Main category: cs.CR

TL;DR: Avatar是一个安全且可变形的末级缓存设计，支持三种模式：非安全模式、随机化安全模式和分区安全模式，能够动态切换，在保证安全性的同时最小化性能、功耗和面积开销。


<details>
  <summary>Details</summary>
Motivation: 现代处理器仍使用不安全的组相联LLC，容易受到跨核冲突和占用攻击。现有安全LLC设计要么有显著面积开销和设计复杂性（随机化方案），要么有较大性能开销或需要操作系统支持（分区方案），阻碍了工业采用。本文旨在探索能否通过对传统组相联LLC进行最小改动来实现强LLC安全性。

Method: 提出Avatar，一个安全且可变形的LLC，支持三种模式：非安全模式（Avatar-N）、随机化安全模式（Avatar-R）和分区安全模式（Avatar-P），能够动态切换。Avatar-R通过引入额外无效条目并利用高相联度提供强安全保证；Avatar-P通过分区设计同时缓解冲突和占用攻击。

Result: Avatar-R仅需1.5%存储开销、静态功耗增加2.7%、性能仅降低0.2%，同时提供极强安全性（每10^30年才发生一次组相联驱逐）。Avatar-P仅带来3%性能开销，显著优于先前基于路的分区LLC。当不需要安全时，可切换到Avatar-N以最大化性能和能效。

Conclusion: Avatar通过最小化对传统组相联LLC的改动，实现了强LLC安全性，同时保持了低性能、功耗和面积开销，有望促进安全LLC的工业采用。

Abstract: The sharing of the last-level cache (LLC) among multiple cores makes it vulnerable to cross-core conflict- and occupancy-based attacks. Despite extensive prior work, modern processors still employ non-secure set-associative LLCs. Existing secure LLC designs broadly fall into two categories: (i) randomized and (ii) partitioned. The state-of-the-art randomized design, Mirage, mitigates conflict-based attacks but incurs significant area overhead (20% additional storage) and design complexity. Partitioned LLCs mitigate both conflict- and occupancy-based attacks, but often suffer from large performance overheads (on average over 5% and up to 49%), require OS support in set-based schemes, or face scalability issues in way-based schemes. These factors pose major obstacles to the industrial adoption of secure LLCs. This paper asks whether strong LLC security can be achieved with minimal changes to a conventional set-associative LLC, enabling security only when needed while preserving low performance, power, and area overheads. We propose Avatar, a secure and morphable LLC that supports three modes: non-secure (Avatar-N), randomized secure (Avatar-R), and partitioned secure (Avatar-P), and can switch dynamically between them. Avatar closely resembles a conventional set-associative LLC, facilitating industrial adoption. Avatar-R introduces extra invalid entries and leverages high associativity to provide a strong security guarantee with little capacity loss, achieving only one set-associative eviction per $10^{30}$ years, while incurring 1.5% storage overhead, a 2.7% increase in static power, and a 0.2% slowdown over a 16~MB baseline. Avatar-P mitigates both conflict- and occupancy-based attacks with only a 3% performance overhead, substantially outperforming prior way-based partitioned LLCs. When security is unnecessary, Avatar switches to Avatar-N to maximize performance and energy efficiency.

</details>


### [29] [Subgraph Reconstruction Attacks on Graph RAG Deployments with Practical Defenses](https://arxiv.org/abs/2602.06495)
*Minkyoo Song,Jaehan Kim,Myungchul Kang,Hanna Kim,Seungwon Shin,Sooel Son*

Main category: cs.CR

TL;DR: 论文提出GRASP攻击方法，能够从Graph RAG系统中有效提取知识图谱子图，克服现有攻击方法在安全提示下的局限性，并提出了防御措施。


<details>
  <summary>Details</summary>
Motivation: Graph RAG系统虽然增强了关系推理能力，但存在隐私泄露风险：攻击者可能从目标RAG系统的知识图谱中重建子图，从而窃取隐私和复制知识资产。现有攻击方法在简单的提示安全措施下效果有限，因此需要研究更有效的攻击方法来揭示实际威胁。

Method: 提出GRASP攻击方法：1) 将提取重构为上下文处理任务；2) 通过每条记录的唯一标识符强制格式合规、实例接地的输出，减少幻觉并保留关系细节；3) 使用动量感知调度器多样化目标驱动的攻击查询，在严格查询预算内操作。

Result: 在两个真实世界知识图谱、四个安全对齐的LLM和多个Graph RAG框架上，GRASP实现了最强的类型忠实重建，F1分数最高达82.9%，而先前方法完全失败。

Conclusion: Graph RAG系统存在严重的子图重建威胁，GRASP攻击方法有效揭示了这一风险。论文进一步评估了防御措施，并提出了两种轻量级缓解方法，能在不损失实用性的情况下显著降低重建保真度。

Abstract: Graph-based retrieval-augmented generation (Graph RAG) is increasingly deployed to support LLM applications by augmenting user queries with structured knowledge retrieved from a knowledge graph. While Graph RAG improves relational reasoning, it introduces a largely understudied threat: adversaries can reconstruct subgraphs from a target RAG system's knowledge graph, enabling privacy inference and replication of curated knowledge assets. We show that existing attacks are largely ineffective against Graph RAG even with simple prompt-based safeguards, because these attacks expose explicit exfiltration intent and are therefore easily suppressed by lightweight safe prompts. We identify three technical challenges for practical Graph RAG extraction under realistic safeguards and introduce GRASP, a closed-box, multi-turn subgraph reconstruction attack. GRASP (i) reframes extraction as a context-processing task, (ii) enforces format-compliant, instance-grounded outputs via per-record identifiers to reduce hallucinations and preserve relational details, and (iii) diversifies goal-driven attack queries using a momentum-aware scheduler to operate within strict query budgets. Across two real-world knowledge graphs, four safety-aligned LLMs, and multiple Graph RAG frameworks, GRASP attains the strongest type-faithful reconstruction where prior methods fail, reaching up to 82.9 F1. We further evaluate defenses and propose two lightweight mitigations that substantially reduce reconstruction fidelity without utility loss.

</details>


### [30] [Sequential Auditing for f-Differential Privacy](https://arxiv.org/abs/2602.06518)
*Tim Kutta,Martin Dunsche,Yu Wei,Vassilis Zikas*

Main category: cs.CR

TL;DR: 提出基于f-DP的新型差分隐私审计器，能够自适应确定所需样本量，无需预先指定样本大小，显著降低审计成本


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私审计器多为批量处理或针对传统(ε,δ)-DP，通常需要预先指定样本量，导致样本量过大，审计成本高，特别是对于DP-SGD等昂贵的训练过程

Method: 基于f-DP隐私概念开发新型审计器，支持白盒和黑盒设置，能够自适应确定最优样本量，无需用户指定样本大小，具有统计显著性保证

Result: 新审计器能够检测整个隐私谱系中的违规行为，通过理论和仿真支持统计显著性保证，显著减少所需样本量，特别适用于DP-SGD等昂贵训练过程

Conclusion: 提出的f-DP审计器在自适应确定样本量方面具有优势，避免了现有方法中常见的过大样本需求，降低了审计成本，支持多种审计框架

Abstract: We present new auditors to assess Differential Privacy (DP) of an algorithm based on output samples. Such empirical auditors are common to check for algorithmic correctness and implementation bugs. Most existing auditors are batch-based or targeted toward the traditional notion of $(\varepsilon,δ)$-DP; typically both. In this work, we shift the focus to the highly expressive privacy concept of $f$-DP, in which the entire privacy behavior is captured by a single tradeoff curve. Our auditors detect violations across the full privacy spectrum with statistical significance guarantees, which are supported by theory and simulations. Most importantly, and in contrast to prior work, our auditors do not require a user-specified sample size as an input. Rather, they adaptively determine a near-optimal number of samples needed to reach a decision, thereby avoiding the excessively large sample sizes common in many auditing studies. This reduction in sampling cost becomes especially beneficial for expensive training procedures such as DP-SGD. Our method supports both whitebox and blackbox settings and can also be executed in single-run frameworks.

</details>


### [31] [Dependable Artificial Intelligence with Reliability and Security (DAIReS): A Unified Syndrome Decoding Approach for Hallucination and Backdoor Trigger Detection](https://arxiv.org/abs/2602.06532)
*Hema Karnam Surendrababu,Nithin Nagaraj*

Main category: cs.CR

TL;DR: 该论文提出了一种基于Syndrome Decoding的统一方法，用于检测机器学习系统中的安全性和可靠性违规，包括检测后门数据中毒攻击和LLM幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型（包括大语言模型）存在多种系统级属性问题，如安全性和可靠性。研究表明ML模型容易受到多种安全违规，特别是后门数据中毒攻击，同时模型可靠性不足会导致LLM产生幻觉，给终端用户带来重大风险。

Method: 提出基于Syndrome Decoding的统一方法，将综合征解码方法适配到NLP句子嵌入空间，能够区分ML训练数据中的中毒和非中毒样本，同时也能有效检测LLM中因自引用元解释任务产生的幻觉内容。

Result: 该方法能够同时检测安全违规（后门数据中毒攻击）和可靠性问题（LLM幻觉），为学习型系统提供了一个统一的检测框架。

Conclusion: DAIReS框架通过Syndrome Decoding方法为机器学习系统的可靠性和安全性提供了一个统一的检测解决方案，能够有效应对后门攻击和幻觉问题这两类重要威胁。

Abstract: Machine Learning (ML) models, including Large Language Models (LLMs), are characterized by a range of system-level attributes such as security and reliability. Recent studies have demonstrated that ML models are vulnerable to multiple forms of security violations, among which backdoor data-poisoning attacks represent a particularly insidious threat, enabling unauthorized model behavior and systematic misclassification. In parallel, deficiencies in model reliability can manifest as hallucinations in LLMs, leading to unpredictable outputs and substantial risks for end users. In this work on Dependable Artificial Intelligence with Reliability and Security (DAIReS), we propose a novel unified approach based on Syndrome Decoding for the detection of both security and reliability violations in learning-based systems. Specifically, we adapt the syndrome decoding approach to the NLP sentence-embedding space, enabling the discrimination of poisoned and non-poisoned samples within ML training datasets. Additionally, the same methodology can effectively detect hallucinated content due to self referential meta explanation tasks in LLMs.

</details>


### [32] [Beyond Function-Level Analysis: Context-Aware Reasoning for Inter-Procedural Vulnerability Detection](https://arxiv.org/abs/2602.06751)
*Yikun Li,Ting Zhang,Jieke Shi,Chengran Yang,Junda He,Xin Zhou,Jinfeng Jiang,Huihui Huang,Wen Bin Leow,Yide Yin,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.CR

TL;DR: CPRVul是一个上下文感知的漏洞检测框架，通过上下文分析和结构化推理，在三个高质量漏洞数据集上显著优于仅基于函数的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法大多在函数级别操作，缺乏跨过程上下文信息，而实际漏洞的存在和根本原因往往依赖于上下文信息。简单附加上下文会因长度、冗余和噪声问题导致性能下降。

Method: CPRVul框架包含两个阶段：1) 上下文分析和选择 - 构建代码属性图提取候选上下文，使用LLM生成安全相关配置文件并分配相关性分数，选择高影响力的上下文元素；2) 结构化推理 - 整合目标函数、选定上下文和漏洞元数据生成推理轨迹，用于微调LLMs进行基于推理的漏洞检测。

Result: 在PrimeVul、TitanVul和CleanVul三个数据集上，CPRVul准确率从64.94%到73.76%，显著优于UniXcoder的56.65%到63.68%。在PrimeVul基准测试中达到67.78%准确率，相比之前最佳方法55.17%提升了22.9%。

Conclusion: 仅使用原始上下文或处理后的上下文单独都不能提升强代码模型的性能，只有当处理后的上下文与结构化推理结合时才能获得显著改进。CPRVul证明了上下文感知和结构化推理在漏洞检测中的重要性。

Abstract: Recent progress in ML and LLMs has improved vulnerability detection, and recent datasets have reduced label noise and unrelated code changes. However, most existing approaches still operate at the function level, where models are asked to predict whether a single function is vulnerable without inter-procedural context. In practice, vulnerability presence and root cause often depend on contextual information. Naively appending such context is not a reliable solution: real-world context is long, redundant, and noisy, and we find that unstructured context frequently degrades the performance of strong fine-tuned code models.
  We present CPRVul, a context-aware vulnerability detection framework that couples Context Profiling and Selection with Structured Reasoning. CPRVul constructs a code property graph, and extracts candidate context. It then uses an LLM to generate security-focused profiles and assign relevance scores, selecting only high-impact contextual elements that fit within the model's context window. In the second phase, CPRVul integrates the target function, the selected context, and auxiliary vulnerability metadata to generate reasoning traces, which are used to fine-tune LLMs for reasoning-based vulnerability detection.
  We evaluate CPRVul on three high-quality vulnerability datasets: PrimeVul, TitanVul, and CleanVul. Across all datasets, CPRVul consistently outperforms function-only baselines, achieving accuracies ranging from 64.94% to 73.76%, compared to 56.65% to 63.68% for UniXcoder. Specifically, on the challenging PrimeVul benchmark, CPRVul achieves 67.78% accuracy, outperforming prior state-of-the-art approaches, improving accuracy from 55.17% to 67.78% (22.9% improvement). Our ablations further show that neither raw context nor processed context alone benefits strong code models; gains emerge only when processed context is paired with structured reasoning.

</details>


### [33] [TrapSuffix: Proactive Defense Against Adversarial Suffixes in Jailbreaking](https://arxiv.org/abs/2602.06630)
*Mengyao Du,Han Fang,Haokai Ma,Gang Yang,Quanjun Yin,Shouling Ji,Ee-Chien Chang*

Main category: cs.CR

TL;DR: TrapSuffix是一种主动防御方法，通过轻量级微调在LLM中植入陷阱行为，将越狱攻击引导至两个无胜算的结果：要么攻击失败，要么生成带有可追踪指纹的对抗后缀。


<details>
  <summary>Details</summary>
Motivation: 现有基于后缀的越狱攻击防御大多是被动检测，没有充分利用防御者能够主动植入秘密和隐藏漏洞的不对称优势。攻击者可以生成无数种表面形式的对抗后缀，使得被动防御变得困难。

Method: 提出TrapSuffix方法，采用轻量级微调在不改变推理流程的情况下向基础模型注入陷阱对齐行为。该方法通过重塑模型对对抗后缀的响应景观，将越狱尝试引导至两个结果：要么落入防御者设计的优化陷阱而失败，要么只能生成带有独特可追踪指纹的对抗后缀才能成功。

Result: 在各种基于后缀的越狱攻击设置中，TrapSuffix将平均攻击成功率降低到0.01%以下，平均追踪成功率达到87.9%。该方法不引入推理时开销，平均仅增加15.87MB内存，而现有基于LLM的检测防御通常需要1e4MB级别的内存开销。

Conclusion: TrapSuffix提供了一种有效的主动防御策略，通过可控制性导向的方法将攻击者置于无胜算的困境，同时提供强大的防御和可靠的追踪能力，并能与现有的基于过滤的防御自然组合形成互补保护。

Abstract: Suffix-based jailbreak attacks append an adversarial suffix, i.e., a short token sequence, to steer aligned LLMs into unsafe outputs. Since suffixes are free-form text, they admit endlessly many surface forms, making jailbreak mitigation difficult. Most existing defenses depend on passive detection of suspicious suffixes, without leveraging the defender's inherent asymmetric ability to inject secrets and proactively conceal gaps. Motivated by this, we take a controllability-oriented perspective and develop a proactive defense that nudges attackers into a no-win dilemma: either they fall into defender-designed optimization traps and fail to produce an effective adversarial suffix, or they can succeed only by generating adversarial suffixes that carry distinctive, traceable fingerprints. We propose TrapSuffix, a lightweight fine-tuning approach that injects trap-aligned behaviors into the base model without changing the inference pipeline. TrapSuffix channels jailbreak attempts into these two outcomes by reshaping the model's response landscape to adversarial suffixes. Across diverse suffix-based jailbreak settings, TrapSuffix reduces the average attack success rate to below 0.01 percent and achieves an average tracing success rate of 87.9 percent, providing both strong defense and reliable traceability. It introduces no inference-time overhead and incurs negligible memory cost, requiring only 15.87 MB of additional memory on average, whereas state-of-the-art LLM-based detection defenses typically incur memory overheads at the 1e4 MB level, while composing naturally with existing filtering-based defenses for complementary protection.

</details>


### [34] [Evaluating and Enhancing the Vulnerability Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2602.06687)
*Li Lu,Yanjie Zhao,Hongzhou Rao,Kechi Zhang,Haoyu Wang*

Main category: cs.CR

TL;DR: 论文提出DAGVul框架，通过有向无环图建模漏洞推理过程，结合可验证奖励的强化学习，显著提升大语言模型在漏洞检测中的逻辑一致性，在8B参数规模下达到与Claude-Sonnet-4.5竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在漏洞检测中存在可靠性问题：模型经常基于幻觉逻辑或表面模式得出正确检测结果，但实际推理过程与根本原因存在偏差。现有基准主要关注粗粒度分类指标，缺乏评估底层推理过程的细粒度真实标签。

Method: 1. 构建包含两个数据集的基准：真实漏洞（专家标注因果推理作为真实标签）和语义等价代码扰动（评估推理鲁棒性）；2. 提出DAGVul框架，将漏洞推理建模为有向无环图生成任务，明确映射因果依赖关系以增强结构一致性；3. 引入可验证奖励的强化学习，使模型推理轨迹与程序内在逻辑对齐。

Result: 大规模实证研究揭示最先进模型在语义代码理解中难以保持逻辑一致性，表现出12种系统性失败模式。DAGVul框架相比所有基线平均提升推理F1分数18.9%，8B参数实现不仅超越同规模模型，还超过专门的大规模推理模型，甚至与Claude-Sonnet-4.5竞争（75.47% vs. 76.11%）。

Conclusion: DAGVul框架通过有向无环图建模和可验证奖励的强化学习，有效解决了大语言模型在漏洞检测中的逻辑一致性问题，在不同模型规模上建立了新的漏洞推理效率标准。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in vulnerability detection. However, a critical reliability gap persists: models frequently yield correct detection verdicts based on hallucinated logic or superficial patterns that deviate from the actual root cause. This misalignment remains largely obscured because contemporary benchmarks predominantly prioritize coarse-grained classification metrics, lacking the granular ground truth required to evaluate the underlying reasoning process. To bridge this gap, we first construct a benchmark consisting of two datasets: (1) real-world vulnerabilities with expert-curated causal reasoning as ground truth, and (2) semantically equivalent code perturbations for assessing reasoning robustness. Our large-scale empirical study reveals that even state-of-the-art models struggle to maintain logical consistency during semantic code comprehension, exhibiting 12 systematic failure patterns. Addressing these limitations, we propose DAGVul, a novel framework that models vulnerability reasoning as a Directed Acyclic Graph (DAG) generation task. Unlike linear chain-of-thought (CoT), our approach explicitly maps causal dependencies to enforce structural consistency. By further introducing Reinforcement Learning with Verifiable Rewards (RLVR), we align model reasoning trace with program-intrinsic logic. Experimental results demonstrate that our framework improves the reasoning F1-score by an average of 18.9% over all the baselines. Remarkably, our 8B-parameter implementation not only outperforms existing models of comparable scale but also surpasses specialized large-scale reasoning models, including Qwen3-30B-Reasoning and GPT-OSS-20B-High. It is even competitive with state-of-the-art models like Claude-Sonnet-4.5 (75.47% vs. 76.11%), establishing new efficiency in vulnerability reasoning across model scales.

</details>


### [35] [Taipan: A Query-free Transfer-based Multiple Sensitive Attribute Inference Attack Solely from Publicly Released Graphs](https://arxiv.org/abs/2602.06700)
*Ying Song,Balaji Palanisamy*

Main category: cs.CR

TL;DR: Taipan是首个无需查询的基于迁移的图多敏感属性推断攻击框架，利用图结构本身的内在信息泄露漏洞，无需模型查询即可推断多个敏感属性。


<details>
  <summary>Details</summary>
Motivation: 现有属性推断攻击(AIAs)大多假设攻击者可通过重复模型查询探测敏感属性，这在现实场景中不切实际。更重要的是，现有方法忽略了图数据发布本身固有的多敏感信息泄露漏洞。

Method: 提出Taipan框架，包含：1)分层攻击知识路由，捕捉属性间复杂关联；2)提示引导的攻击原型精炼，缓解负迁移和性能下降。还提出了针对G-MSAIAs的系统评估框架。

Result: 在多种真实图数据集上的实验表明，Taipan在相同分布、异构相似分布和异分布设置下均表现优异，即使在严格的差分隐私保证下仍保持有效。

Conclusion: 研究揭示了图数据发布中固有的多敏感属性泄露风险，强调了需要更鲁棒的多属性隐私保护图发布方法和数据共享实践。

Abstract: Graph-structured data underpin a wide spectrum of modern applications. However, complex graph topologies and homophilic patterns can facilitate attribute inference attacks (AIAs) by enabling sensitive information leakage to propagate across local neighborhoods. Existing AIAs predominantly assume that adversaries can probe sensitive attributes through repeated model queries. Such assumptions are often impractical in real-world settings due to stringent data protection regulations, prohibitive query budgets, and heightened detection risks, especially when inferring multiple sensitive attributes. More critically, this model-centric perspective obscures a pervasive blind spot: \textbf{intrinsic multiple sensitive information leakage arising solely from publicly released graphs.} To exploit this unexplored vulnerability, we introduce a new attack paradigm and propose \textbf{Taipan, the first query-free transfer-based attack framework for multiple sensitive attribute inference attacks on graphs (G-MSAIAs).} Taipan integrates \emph{Hierarchical Attack Knowledge Routing} to capture intricate inter-attribute correlations, and \emph{Prompt-guided Attack Prototype Refinement} to mitigate negative transfer and performance degradation. We further present a systematic evaluation framework tailored to G-MSAIAs. Extensive experiments on diverse real-world graph datasets demonstrate that Taipan consistently achieves strong attack performance across same-distribution settings and heterogeneous similar- and out-of-distribution settings with mismatched feature dimensionalities, and remains effective even under rigorous differential privacy guarantees. Our findings underscore the urgent need for more robust multi-attribute privacy-preserving graph publishing methods and data-sharing practices.

</details>


### [36] [GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models](https://arxiv.org/abs/2602.06718)
*Zuyao Xu,Yuqi Qiu,Lu Sun,FaSheng Miao,Fubin Wu,Xinyi Wang,Xiang Li,Haozhe Lu,ZhengZe Zhang,Yuxin Hu,Jialu Li,Jin Luo,Feng Zhang,Rui Luo,Xinran Liu,Yingxian Li,Jiaji Liu*

Main category: cs.CR

TL;DR: LLM生成的"幽灵引用"威胁科学引用可信度，CiteVerifier框架验证显示所有主流LLM都会伪造引用，AI/ML和安全领域论文中1.07%包含无效引用，研究人员和审稿人存在严重验证缺口。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在学术写作中的广泛应用，其伪造引用（"幽灵引用"）的趋势对引用有效性构成系统性威胁，需要量化这种威胁并制定缓解措施。

Method: 开发CiteVerifier开源框架进行大规模引用验证，通过三个实验：1) 在40个研究领域基准测试13个最先进LLM的引用生成；2) 分析2020-2025年顶级AI/ML和安全会议56,381篇论文的220万条引用；3) 调查97名研究人员并分析94份有效问卷。

Result: 所有LLM的引用伪造率在14.23%到94.93%之间，不同研究领域差异显著；1.07%的论文包含无效或伪造引用（604篇），2025年单独增长80.9%；41.5%的研究人员复制粘贴BibTeX而不检查，44.4%对可疑引用采取无行动，76.7%的审稿人不彻底检查引用，80%从未怀疑伪造引用。

Conclusion: 不可靠的AI工具与研究人员不足的人类验证以及同行评审监督不足相结合，导致伪造引用污染科学记录的危机正在加速。需要为研究人员、会议和工具开发者提出干预措施来保护引用完整性。

Abstract: Citations provide the basis for trusting scientific claims; when they are invalid or fabricated, this trust collapses. With the advent of Large Language Models (LLMs), this risk has intensified: LLMs are increasingly used for academic writing, yet their tendency to fabricate citations (``ghost citations'') poses a systemic threat to citation validity.
  To quantify this threat and inform mitigation, we develop CiteVerifier, an open-source framework for large-scale citation verification, and conduct the first comprehensive study of citation validity in the LLM era through three experiments built on it. We benchmark 13 state-of-the-art LLMs on citation generation across 40 research domains, finding that all models hallucinate citations at rates from 14.23\% to 94.93\%, with significant variation across research domains. Moreover, we analyze 2.2 million citations from 56,381 papers published at top-tier AI/ML and Security venues (2020--2025), confirming that 1.07\% of papers contain invalid or fabricated citations (604 papers), with an 80.9\% increase in 2025 alone. Furthermore, we survey 97 researchers and analyze 94 valid responses after removing 3 conflicting samples, revealing a critical ``verification gap'': 41.5\% of researchers copy-paste BibTeX without checking and 44.4\% choose no-action responses when encountering suspicious references; meanwhile, 76.7\% of reviewers do not thoroughly check references and 80.0\% never suspect fake citations. Our findings reveal an accelerating crisis where unreliable AI tools, combined with inadequate human verification by researchers and insufficient peer review scrutiny, enable fabricated citations to contaminate the scientific record. We propose interventions for researchers, venues, and tool developers to protect citation integrity.

</details>


### [37] [$f$-Differential Privacy Filters: Validity and Approximate Solutions](https://arxiv.org/abs/2602.06756)
*Long Tran,Antti Koskela,Ossi Räisä,Antti Honkela*

Main category: cs.CR

TL;DR: 该论文研究了完全自适应组合下的隐私损失计算问题，重点分析了f-DP隐私过滤器的有效性，发现了自然过滤方法的根本缺陷，并建立了其有效性的充要条件。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私中，完全自适应组合下的隐私损失计算是一个核心挑战。隐私过滤器作为确保全局隐私预算不被超过的停止规则，其有效性在基于最优权衡函数的f-DP框架下尚不明确。

Method: 分析了自然f-DP过滤器的有效性，建立了其有效性的充要条件。证明了完全自适应下的f-DP中心极限定理，并为子采样高斯机制构造了近似高斯DP过滤器。

Result: 发现自然f-DP过滤器方法存在根本性无效问题，并确定了其有效性的条件。针对子采样高斯机制，在采样率q<0.2和q>0.8时构造了比基于Rényi DP过滤器更紧的隐私保证。

Conclusion: 完全自适应组合下的f-DP隐私过滤器设计需要谨慎，自然方法可能无效。论文提供了理论分析和实用构造方法，为自适应差分隐私提供了新的理论工具。

Abstract: Accounting for privacy loss under fully adaptive composition -- where both the choice of mechanisms and their privacy parameters may depend on the entire history of prior outputs -- is a central challenge in differential privacy (DP). In this setting, privacy filters are stopping rules for compositions that ensure a prescribed global privacy budget is not exceeded. It remains unclear whether optimal trade-off-function-based notions, such as $f$-DP, admit valid privacy filters under fully adaptive interaction. We show that the natural approach to defining an $f$-DP filter -- composing individual trade-off curves and stopping when the prescribed $f$-DP curve is crossed -- is fundamentally invalid. We characterise when and why this failure occurs, and establish necessary and sufficient conditions under which the natural filter is valid. Furthermore, we prove a fully adaptive central limit theorem for $f$-DP and construct an approximate Gaussian DP filter for subsampled Gaussian mechanisms at small sampling rates $q<0.2$ and large sampling rates $q>0.8$, yielding tighter privacy guarantees than filters based on Rényi DP in the same setting.

</details>


### [38] ["Tab, Tab, Bug'': Security Pitfalls of Next Edit Suggestions in AI-Integrated IDEs](https://arxiv.org/abs/2602.06759)
*Yunlong Lyu,Yixuan Tang,Peng Chen,Tian Dong,Xinyu Wang,Zhiqiang Dong,Hao Chen*

Main category: cs.CR

TL;DR: 首次对现代AI集成IDE中的Next Edit Suggestions（NES）系统进行系统性安全研究，揭示了NES机制带来的新威胁向量、安全漏洞以及开发者对其风险认知不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现代AI集成IDE正从被动代码补全转向主动的Next Edit Suggestions（NES），这种演进引入了更复杂的上下文检索机制和交互模式。然而，现有研究主要关注独立LLM代码生成的安全问题，忽视了NES在现代AI集成IDE中可能带来的攻击向量。NES的底层机制尚未充分探索，其安全影响也未被完全理解。

Method: 1. 深入剖析NES机制以理解新引入的威胁向量；2. 通过实验室研究评估NES的安全影响；3. 对200多名专业开发者进行大规模在线调查，评估真实开发工作流中NES安全风险的认知情况。

Result: 研究发现：1. NES检索显著扩展的上下文（包括不可感知的用户操作和全局代码库检索），增加了攻击面；2. NES易受上下文污染攻击，对事务性编辑和人机交互敏感；3. 开发者普遍缺乏对NES相关安全风险的认知，需要加强教育和改进安全防护措施。

Conclusion: 这是首个对NES系统进行的系统性安全研究，揭示了NES机制带来的新安全挑战。研究结果表明NES存在显著的安全漏洞，而开发者对其风险认知不足，强调了在AI集成IDE中加强安全教育和改进防护措施的必要性。

Abstract: Modern AI-integrated IDEs are shifting from passive code completion to proactive Next Edit Suggestions (NES). Unlike traditional autocompletion, NES is designed to construct a richer context from both recent user interactions and the broader codebase to suggest multi-line, cross-line, or even cross-file modifications. This evolution significantly streamlines the programming workflow into a tab-by-tab interaction and enhances developer productivity. Consequently, NES introduces a more complex context retrieval mechanism and sophisticated interaction patterns. However, existing studies focus almost exclusively on the security implications of standalone LLM-based code generation, ignoring the potential attack vectors posed by NES in modern AI-integrated IDEs. The underlying mechanisms of NES remain under-explored, and their security implications are not yet fully understood.
  In this paper, we conduct the first systematic security study of NES systems. First, we perform an in-depth dissection of the NES mechanisms to understand the newly introduced threat vectors. It is found that NES retrieves a significantly expanded context, including inputs from imperceptible user actions and global codebase retrieval, which increases the attack surfaces. Second, we conduct a comprehensive in-lab study to evaluate the security implications of NES. The evaluation results reveal that NES is susceptible to context poisoning and is sensitive to transactional edits and human-IDE interactions. Third, we perform a large-scale online survey involving over 200 professional developers to assess the perceptions of NES security risks in real-world development workflows. The survey results indicate a general lack of awareness regarding the potential security pitfalls associated with NES, highlighting the need for increased education and improved security countermeasures in AI-integrated IDEs.

</details>


### [39] [Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs](https://arxiv.org/abs/2602.06777)
*Yassine Chagna,Antal Goldschmidt*

Main category: cs.CR

TL;DR: 该研究探索使用大语言模型进行异构日志源的异常检测，解决了传统入侵检测系统的高误报率、语义盲点和数据稀缺问题，提出了平衡数据集、改进评估指标和两阶段训练框架的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统入侵检测系统存在高误报率、语义盲点（无法理解日志语义）和数据稀缺（日志数据敏感，干净数据集稀少）三大问题，需要更有效的异常检测方法。

Method: 提出三方面贡献：1) LogAtlas-Foundation-Sessions和LogAtlas-Defense-Set平衡异构日志数据集，包含明确攻击标注和隐私保护；2) 实证基准测试揭示传统指标如F1和准确率在安全应用中的误导性；3) 两阶段训练框架：日志理解阶段（Base-AMAN，30亿参数）和实时检测阶段（AMAN，5亿参数，通过知识蒸馏）。

Result: 展示了实际可行性：推理时间每会话0.3-0.5秒，运营成本低于每天50美元，证明了大语言模型在日志异常检测中的实用价值。

Conclusion: 该研究通过创新的数据集、评估指标和训练框架，成功将大语言模型应用于异构日志源的异常检测，解决了传统方法的局限性，并证明了其在现实环境中的可行性和经济性。

Abstract: This project explores large language models (LLMs) for anomaly detection across heterogeneous log sources. Traditional intrusion detection systems suffer from high false positive rates, semantic blindness, and data scarcity, as logs are inherently sensitive, making clean datasets rare. We address these challenges through three contributions: (1) LogAtlas-Foundation-Sessions and LogAtlas-Defense-Set, balanced and heterogeneous log datasets with explicit attack annotations and privacy preservation; (2) empirical benchmarking revealing why standard metrics such as F1 and accuracy are misleading for security applications; and (3) a two phase training framework combining log understanding (Base-AMAN, 3B parameters) with real time detection (AMAN, 0.5B parameters via knowledge distillation). Results demonstrate practical feasibility, with inference times of 0.3-0.5 seconds per session and operational costs below 50 USD per day.

</details>


### [40] [Plato's Form: Toward Backdoor Defense-as-a-Service for LLMs with Prototype Representations](https://arxiv.org/abs/2602.06887)
*Chen Chen,Yuchen Sun,Jiaxin Gao,Yanwen Jia,Xueluan Gong,Qian Wang,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: PROTOPURIFY是一个针对大语言模型后门攻击的防御框架，通过参数编辑实现后门净化，在最小假设下工作，支持后门防御即服务（BDaaS）


<details>
  <summary>Details</summary>
Motivation: 现有后门防御方法难以实现后门防御即服务（BDaaS），因为它们需要不切实际的辅助信息（如下游干净数据、已知触发器/目标或任务领域特定信息），并且缺乏跨不同后门模型的可重用、可扩展的净化方法

Method: PROTOPURIFY首先从干净模型和后门模型对构建后门向量池，将向量聚合成候选原型，通过相似性匹配选择与目标模型最对齐的候选原型。然后通过层间原型对齐识别边界层，通过抑制受影响层中与原型对齐的组件进行有针对性的净化，实现细粒度缓解

Result: 在各种大语言模型上的分类和生成任务实验中，PROTOPURIFY在6种不同攻击（包括单触发器、多触发器和无触发器后门设置）中始终优于6种代表性防御方法。PROTOPURIFY将攻击成功率（ASR）降低到10%以下，在某些情况下甚至低至1.6%，同时干净效用下降不到3%

Conclusion: PROTOPURIFY是一个BDaaS就绪的基础框架，支持可重用性、可定制性、可解释性和运行时效率，对自适应后门变体具有鲁棒性，在非后门模型上保持稳定

Abstract: Large language models (LLMs) are increasingly deployed in security-sensitive applications, yet remain vulnerable to backdoor attacks. However, existing backdoor defenses are difficult to operationalize for Backdoor Defense-as-a-Service (BDaaS), as they require unrealistic side information (e.g., downstream clean data, known triggers/targets, or task domain specifics), and lack reusable, scalable purification across diverse backdoored models. In this paper, we present PROTOPURIFY, a backdoor purification framework via parameter edits under minimal assumptions. PROTOPURIFY first builds a backdoor vector pool from clean and backdoored model pairs, aggregates vectors into candidate prototypes, and selects the most aligned candidate for the target model via similarity matching. PROTOPURIFY then identifies a boundary layer through layer-wise prototype alignment and performs targeted purification by suppressing prototype-aligned components in the affected layers, achieving fine-grained mitigation with minimal impact on benign utility. Designed as a BDaaS-ready primitive, PROTOPURIFY supports reusability, customizability, interpretability, and runtime efficiency. Experiments across various LLMs on both classification and generation tasks show that PROTOPURIFY consistently outperforms 6 representative defenses against 6 diverse attacks, including single-trigger, multi-trigger, and triggerless backdoor settings. PROTOPURIFY reduces ASR to below 10%, and even as low as 1.6% in some cases, while incurring less than a 3% drop in clean utility. PROTOPURIFY further demonstrates robustness against adaptive backdoor variants and stability on non-backdoored models.

</details>


### [41] [TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering](https://arxiv.org/abs/2602.06911)
*Saad Hossain,Tom Tseng,Punya Syon Pandey,Samanvay Vajpayee,Matthew Kowal,Nayeema Nonta,Samuel Simko,Stephen Casper,Zhijing Jin,Kellin Pelrine,Sirisha Rambhatla*

Main category: cs.CR

TL;DR: TamperBench是一个用于系统评估大语言模型抗篡改能力的统一框架，包含攻击方法库、超参数扫描和安全性/实用性评估，对21个开源模型进行了标准化测试。


<details>
  <summary>Details</summary>
Motivation: 随着开源大语言模型的部署，提高其抗篡改能力以防止意外或故意的危险修改变得至关重要，但目前缺乏标准化的评估方法，不同数据集、指标和篡改配置使得模型间的安全性、实用性和鲁棒性难以比较。

Method: TamperBench框架包含三个核心组件：(1) 收集最先进的权重空间微调攻击和潜在空间表示攻击方法库；(2) 通过每个攻击-模型对的系统化超参数扫描实现真实对抗评估；(3) 提供安全性和实用性评估。框架只需最少额外代码即可指定任何微调配置、对齐阶段防御方法和指标套件，确保端到端可复现性。

Result: 使用TamperBench评估了21个开源大语言模型（包括防御增强变体），在9种篡改威胁下使用标准化安全性和能力指标进行测试。研究发现：后训练对模型抗篡改能力有影响；越狱调优通常是最严重的攻击；Triplet成为领先的对齐阶段防御方法。

Conclusion: TamperBench为系统评估大语言模型的抗篡改能力提供了首个统一框架，通过标准化测试揭示了模型安全性的重要见解，并识别出有效的防御方法，有助于推动更安全的开源模型部署。

Abstract: As increasingly capable open-weight large language models (LLMs) are deployed, improving their tamper resistance against unsafe modifications, whether accidental or intentional, becomes critical to minimize risks. However, there is no standard approach to evaluate tamper resistance. Varied data sets, metrics, and tampering configurations make it difficult to compare safety, utility, and robustness across different models and defenses. To this end, we introduce TamperBench, the first unified framework to systematically evaluate the tamper resistance of LLMs. TamperBench (i) curates a repository of state-of-the-art weight-space fine-tuning attacks and latent-space representation attacks; (ii) enables realistic adversarial evaluation through systematic hyperparameter sweeps per attack-model pair; and (iii) provides both safety and utility evaluations. TamperBench requires minimal additional code to specify any fine-tuning configuration, alignment-stage defense method, and metric suite while ensuring end-to-end reproducibility. We use TamperBench to evaluate 21 open-weight LLMs, including defense-augmented variants, across nine tampering threats using standardized safety and capability metrics with hyperparameter sweeps per model-attack pair. This yields novel insights, including effects of post-training on tamper resistance, that jailbreak-tuning is typically the most severe attack, and that Triplet emerges as a leading alignment-stage defense. Code is available at: https://github.com/criticalml-uw/TamperBench

</details>
