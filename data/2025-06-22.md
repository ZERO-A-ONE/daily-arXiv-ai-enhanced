<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.CR](#cs.CR) [Total: 24]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/abs/2506.14866)
*Thomas Kuntz,Agatha Duzan,Hao Zhao,Francesco Croce,Zico Kolter,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.SE

TL;DR: OS-Harm是一个新基准，用于评估基于LLM的计算机使用代理的安全性，覆盖用户滥用、提示注入攻击和模型行为不当三类危害。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理的安全性研究不足，阻碍其广泛应用，需系统性评估其潜在危害。

Method: 基于OSWorld环境构建OS-Harm基准，包含150个任务，测试代理在多种安全违规场景下的表现，并提出自动化评估方法。

Result: 评估显示，前沿模型易受滥用和提示注入攻击，且可能执行不安全操作。自动化评估与人工标注一致性高（F1分数0.76和0.79）。

Conclusion: OS-Harm为计算机使用代理的安全性提供了标准化评估工具，揭示当前模型的潜在风险，需进一步改进安全性。

Abstract: Computer use agents are LLM-based agents that can directly interact with a
graphical user interface, by processing screenshots or accessibility trees.
While these systems are gaining popularity, their safety has been largely
overlooked, despite the fact that evaluating and understanding their potential
for harmful behavior is essential for widespread adoption. To address this gap,
we introduce OS-Harm, a new benchmark for measuring safety of computer use
agents. OS-Harm is built on top of the OSWorld environment and aims to test
models across three categories of harm: deliberate user misuse, prompt
injection attacks, and model misbehavior. To cover these cases, we create 150
tasks that span several types of safety violations (harassment, copyright
infringement, disinformation, data exfiltration, etc.) and require the agent to
interact with a variety of OS applications (email client, code editor, browser,
etc.). Moreover, we propose an automated judge to evaluate both accuracy and
safety of agents that achieves high agreement with human annotations (0.76 and
0.79 F1 score). We evaluate computer use agents based on a range of frontier
models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide
insights into their safety. In particular, all models tend to directly comply
with many deliberate misuse queries, are relatively vulnerable to static prompt
injections, and occasionally perform unsafe actions. The OS-Harm benchmark is
available at https://github.com/tml-epfl/os-harm.

</details>


### [2] [An Empirical Study of Bugs in Data Visualization Libraries](https://arxiv.org/abs/2506.15084)
*Weiqi Lu,Yongqiang Tian,Xiaohan Zhong,Haoyang Ma,Zhenyang Xu,Shing-Chi Cheung,Chengnian Sun*

Main category: cs.SE

TL;DR: 该研究首次全面分析了数据可视化库中的错误，收集了五个常用库的564个错误，系统分析了症状和根源，并提出分类法。研究发现错误图形计算是主要根源，并探索了视觉语言模型在检测错误中的可行性。


<details>
  <summary>Details</summary>
Motivation: 数据可视化库的错误可能导致用户误解和决策失误，因此需要深入了解这些错误的特性以改进检测和修复方法。

Method: 研究收集了五个广泛使用的数据可视化库中的564个错误，系统分析了其症状和根源，并提出了分类法。同时探索了视觉语言模型在检测错误中的可行性。

Result: 研究发现错误图形计算是主要根源，并提出了触发错误的八个关键步骤和两个测试预言。视觉语言模型的检测效果在29%到57%之间，取决于提示内容。

Conclusion: 该研究为数据可视化库的错误检测提供了系统分析和方法，并指出了视觉语言模型在检测中的潜力与局限性。

Abstract: Data visualization (DataViz) libraries play a crucial role in presentation,
data analysis, and application development, underscoring the importance of
their accuracy in transforming data into visual representations. Incorrect
visualizations can adversely impact user experience, distort information
conveyance, and influence user perception and decision-making processes. Visual
bugs in these libraries can be particularly insidious as they may not cause
obvious errors like crashes, but instead mislead users of the underlying data
graphically, resulting in wrong decision making. Consequently, a good
understanding of the unique characteristics of bugs in DataViz libraries is
essential for researchers and developers to detect and fix bugs in DataViz
libraries.
  This study presents the first comprehensive analysis of bugs in DataViz
libraries, examining 564 bugs collected from five widely-used libraries. Our
study systematically analyzes their symptoms and root causes, and provides a
detailed taxonomy. We found that incorrect/inaccurate plots are pervasive in
DataViz libraries and incorrect graphic computation is the major root cause,
which necessitates further automated testing methods for DataViz libraries.
Moreover, we identified eight key steps to trigger such bugs and two test
oracles specific to DataViz libraries, which may inspire future research in
designing effective automated testing techniques. Furthermore, with the recent
advancements in Vision Language Models (VLMs), we explored the feasibility of
applying these models to detect incorrect/inaccurate plots. The results show
that the effectiveness of VLMs in bug detection varies from 29% to 57%,
depending on the prompts, and adding more information in prompts does not
necessarily increase the effectiveness. More findings can be found in our
manuscript.

</details>


### [3] [Program Feature-based Fuzzing Benchmarking](https://arxiv.org/abs/2506.15088)
*Miao Miao*

Main category: cs.SE

TL;DR: 本文提出了一种新的基准测试方法，通过可配置的细粒度程序特征来评估模糊测试的效果，填补了传统模糊测试评估的不足。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试评估通常关注整体性能，而忽略了细粒度程序特征对测试效果的影响。本文旨在填补这一空白。

Method: 通过分析25项灰盒模糊测试研究，提取了7个与控制和数据流相关的程序特征，生成了包含153个程序的基准测试集，并评估了11种流行的模糊测试工具。

Result: 结果显示，模糊测试工具的性能受程序特征及其强度影响显著，强调了在评估中考虑程序特性的重要性。

Conclusion: 本文提出的基准测试方法能够更全面地评估模糊测试工具的性能，为未来的模糊测试研究提供了新的方向。

Abstract: Fuzzing is a powerful software testing technique renowned for its
effectiveness in identifying software vulnerabilities. Traditional fuzzing
evaluations typically focus on overall fuzzer performance across a set of
target programs, yet few benchmarks consider how fine-grained program features
influence fuzzing effectiveness. To bridge this gap, we introduce a novel
benchmark designed to generate programs with configurable, fine-grained program
features to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing
studies, extracting 7 program features related to control-flow and data-flow
that can impact fuzzer performance. Using these features, we generated a
benchmark consisting of 153 programs controlled by 10 fine-grained configurable
parameters. We evaluated 11 popular fuzzers using this benchmark. The results
indicate that fuzzer performance varies significantly based on the program
features and their strengths, highlighting the importance of incorporating
program characteristics into fuzzing evaluations.

</details>


### [4] [Enhancement Report Approval Prediction: A Comparative Study of Large Language Models](https://arxiv.org/abs/2506.15098)
*Haosheng Zuo,Feifei Niu,Chuanyi Li*

Main category: cs.SE

TL;DR: 该论文研究了利用大语言模型（LLM）自动化处理软件改进报告（ERs）的方法，发现LLM在预测准确率和召回率上优于传统方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 手动处理软件改进报告效率低下且资源密集，因此需要自动化工具来提升决策效率。

Method: 系统评估了18种LLM变体（包括编码器和解码器模型）与传统方法（如CNN/LSTM-BERT/GloVe）的性能。

Result: LLM在预测准确率上显著优于传统方法，尤其是LoRA微调的Llama 3.1 8B Instruct模型达到79%的准确率。

Conclusion: LLM是解决软件改进报告自动化处理的优越方案，未来研究可进一步优化模型性能。

Abstract: Enhancement reports (ERs) serve as a critical communication channel between
users and developers, capturing valuable suggestions for software improvement.
However, manually processing these reports is resource-intensive, leading to
delays and potential loss of valuable insights. To address this challenge,
enhancement report approval prediction (ERAP) has emerged as a research focus,
leveraging machine learning techniques to automate decision-making. While
traditional approaches have employed feature-based classifiers and deep
learning models, recent advancements in large language models (LLM) present new
opportunities for enhancing prediction accuracy. This study systematically
evaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and
XLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1
8B Instruct and DeepSeek-V3 for decoder models) against traditional methods
(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)
Incorporating creator profiles increases unfine-tuned decoder-only models'
overall accuracy by 10.8 percent though it may introduce bias; (2) LoRA
fine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79
percent accuracy and significantly enhancing recall for approved reports (76.1
percent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5
percent under strict chronological evaluation and effectively addressing class
imbalance issues. These findings establish LLM as a superior solution for ERAP,
demonstrating their potential to streamline software maintenance workflows and
improve decision-making in real-world development environments. We also
investigated and summarized the ER cases where the large models underperformed,
providing valuable directions for future research.

</details>


### [5] [Towards Bug-Free Distributed Go Programs](https://arxiv.org/abs/2506.15135)
*Zhengqun Koo*

Main category: cs.SE

TL;DR: 本文提出了一种静态验证框架，用于证明使用Go语言子集的分布式程序中通信竞争的缺失。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的并发问题（如通信竞争）可能导致接收错误消息或无消息接收，因此需要一种方法来避免此类问题。

Method: 通过扩展happens-before顺序到缓冲和非缓冲通道，静态分析分布式程序的执行行为。

Result: 该框架能够证明程序在特定条件下不存在通信竞争。

Conclusion: 该静态验证方法为分布式程序的通信竞争问题提供了有效的解决方案。

Abstract: Programmers of distributed systems need to reason about concurrency to avoid
races. However, reasoning about concurrency is difficult, and unexpected races
show up as bugs. Data race detection in shared memory systems is well-studied
(dynamic data race detection [13], behavioral types [15], dynamic race
detection [31]). Similar to how a data race consists of reads and writes not
related by happens-before at a shared memory location, a communication race
consists of receives and sends not related by happens-before on a shared
channel. Communication races are problematic: a receiver expects a specific
message from a specific sender, but with a communication race, the receiver can
receive a message meant for another receiver, or not receive anything at all.
In this work, we describe a verification framework that can prove the absence
of communication races for distributed programs that use a subset of the Go
programming language, where synchronization is mainly achieved via message
passing. We statically reason about how a distributed program executes, using a
happens-before order, extended to buffered and unbuffered channels.

</details>


### [6] [Advanced approach for Agile/Scrum Process: RetroAI++](https://arxiv.org/abs/2506.15172)
*Maria Spichkova,Kevin Iwan,Madeleine Zwart,Hina Lee,Yuwon Yoon,Xiaohan Qin*

Main category: cs.SE

TL;DR: RetroAI++是一个基于智能技术的原型工具，旨在自动化并优化Agile/Scrum开发中的冲刺计划和回顾分析。


<details>
  <summary>Details</summary>
Motivation: 支持软件开发者在冲刺计划和回顾分析中更高效地应用Agile/Scrum流程。

Method: 利用AI技术开发RetroAI++原型，自动化冲刺计划、开发和回顾阶段的多项流程，并提供智能建议和深入分析。

Result: RetroAI++能够为冲刺组织和回顾反思提供智能化的支持。

Conclusion: RetroAI++通过AI技术提升了Agile/Scrum流程的自动化水平和实用性。

Abstract: In Agile/Scrum software development, sprint planning and retrospective
analysis are the key elements of project management. The aim of our work is to
support software developers in these activities. In this paper, we present our
prototype tool RetroAI++, based on emerging intelligent technologies. In our
RetroAI++ prototype, we aim to automate and refine the practical application of
Agile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI
insights, our prototype aims to automate and refine the many processes involved
in the Sprint Planning, Development and Retrospective stages of Agile/Scrum
development projects, offering intelligent suggestions for sprint organisation
as well as meaningful insights for retrospective reflection.

</details>


### [7] [Large Language Models for Unit Testing: A Systematic Literature Review](https://arxiv.org/abs/2506.15227)
*Quanjun Zhang,Chunrong Fang,Siqi Gu,Ye Shang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: 本文对截至2025年3月关于LLMs在单元测试中应用的文献进行了首次系统性综述，总结了现有成果、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，其在单元测试自动化中的应用迅速增长，但缺乏系统性总结，研究者难以全面了解该领域的进展和挑战。

Method: 通过分析相关文献，从单元测试和LLMs的角度分类现有任务（如测试生成和预言生成），并讨论模型使用、适应策略和混合方法等关键方面。

Result: 总结了LLMs在单元测试中的应用现状，提出了未解决的关键挑战，并指出了未来研究方向。

Conclusion: 本文为单元测试社区提供了系统性概述，帮助研究者全面理解成果并推动未来研究。

Abstract: Unit testing is a fundamental practice in modern software engineering, with
the aim of ensuring the correctness, maintainability, and reliability of
individual software components. Very recently, with the advances in Large
Language Models (LLMs), a rapidly growing body of research has leveraged LLMs
to automate various unit testing tasks, demonstrating remarkable performance
and significantly reducing manual effort. However, due to ongoing explorations
in the LLM-based unit testing field, it is challenging for researchers to
understand existing achievements, open challenges, and future opportunities.
This paper presents the first systematic literature review on the application
of LLMs in unit testing until March 2025. We analyze \numpaper{} relevant
papers from the perspectives of both unit testing and LLMs. We first categorize
existing unit testing tasks that benefit from LLMs, e.g., test generation and
oracle generation. We then discuss several critical aspects of integrating LLMs
into unit testing research, including model usage, adaptation strategies, and
hybrid approaches. We further summarize key challenges that remain unresolved
and outline promising directions to guide future research in this area.
Overall, our paper provides a systematic overview of the research landscape to
the unit testing community, helping researchers gain a comprehensive
understanding of achievements and promote future research. Our artifacts are
publicly available at the GitHub repository:
https://github.com/iSEngLab/AwesomeLLM4UT.

</details>


### [8] [Uncovering Intention through LLM-Driven Code Snippet Description Generation](https://arxiv.org/abs/2506.15453)
*Yusuf Sulistyo Nugroho,Farah Danisha Salam,Brittany Reid,Raula Gaikovina Kula,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 研究探讨了开发者常用的代码片段描述类型，并评估了Llama模型在生成描述时的表现。研究发现，大多数原始描述（55.5%）基于示例，LLM能准确识别79.75%的示例描述，生成的描述相似度为0.7173，显示相关性但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 代码片段的文档化对开发者和用户至关重要，尤其是第三方库的API和示例。研究旨在分析开发者常用的描述类型，并评估LLM（如Llama）在生成描述时的能力。

Method: 使用NPM代码片段数据集（185,412个包，1,024,579个片段），选取400个片段及其描述作为样本。手动分类原始描述，并评估LLM生成的描述与原始描述的相似度。

Result: 55.5%的原始描述为示例用途；LLM正确识别79.75%的示例描述；生成的描述平均相似度为0.7173，显示相关性但存在不相关情况。

Conclusion: 代码片段的文档意图因任务而异，可能是使用说明、安装指南或学习示例。LLM在生成描述时表现良好，但仍有改进空间。

Abstract: Documenting code snippets is essential to pinpoint key areas where both
developers and users should pay attention. Examples include usage examples and
other Application Programming Interfaces (APIs), which are especially important
for third-party libraries. With the rise of Large Language Models (LLMs), the
key goal is to investigate the kinds of description developers commonly use and
evaluate how well an LLM, in this case Llama, can support description
generation. We use NPM Code Snippets, consisting of 185,412 packages with
1,024,579 code snippets. From there, we use 400 code snippets (and their
descriptions) as samples. First, our manual classification found that the
majority of original descriptions (55.5%) highlight example-based usage. This
finding emphasizes the importance of clear documentation, as some descriptions
lacked sufficient detail to convey intent. Second, the LLM correctly identified
the majority of original descriptions as "Example" (79.75%), which is identical
to our manual finding, showing a propensity for generalization. Third, compared
to the originals, the produced description had an average similarity score of
0.7173, suggesting relevance but room for improvement. Scores below 0.9
indicate some irrelevance. Our results show that depending on the task of the
code snippet, the intention of the document may differ from being instructions
for usage, installations, or descriptive learning examples for any user of a
library.

</details>


### [9] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: 论文提出了一种基于抽象语法树（AST）的代码分块方法（\ourwork），以解决现有行分块方法破坏语义结构的问题，显著提升了代码生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的行分块方法在代码检索增强生成（RAG）中常破坏语义结构（如拆分函数或合并无关代码），影响生成质量。

Method: 提出基于AST的结构感知分块方法，递归分解大AST节点并合并兄弟节点，生成语义连贯的代码块。

Result: 在多种代码生成任务中表现优异，如RepoEval检索的Recall@5提升4.3点，SWE-bench生成的Pass@1提升2.67点。

Conclusion: 结构感知分块对提升检索增强代码智能的扩展性至关重要。

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [10] [Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning](https://arxiv.org/abs/2506.14913)
*Wassim Bouaziz,Mathurin Videau,Nicolas Usunier,El-Mahdi El-Mhamdi*

Main category: cs.CR

TL;DR: 论文提出了一种间接数据中毒方法，通过梯度优化提示调整，使语言模型学习秘密序列，从而保护数据集并追踪其使用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的训练依赖于难以管理的大规模文本数据，现有方法依赖于训练数据的记忆，而模型提供者试图限制这种记忆。

Method: 使用基于梯度的优化提示调整技术，使模型学习训练数据中不存在的秘密序列（秘密提示和秘密响应）。

Result: 实验表明，仅需少于0.005%的中毒标记即可让模型学习秘密，并以极高置信度（p < 10^{-55}）检测到，且不影响模型性能。

Conclusion: 间接数据中毒是可行的，能够在不影响模型性能的情况下保护数据集并追踪其使用。

Abstract: The pre-training of large language models (LLMs) relies on massive text
datasets sourced from diverse and difficult-to-curate origins. Although
membership inference attacks and hidden canaries have been explored to trace
data usage, such methods rely on memorization of training data, which LM
providers try to limit. In this work, we demonstrate that indirect data
poisoning (where the targeted behavior is absent from training data) is not
only feasible but also allow to effectively protect a dataset and trace its
use. Using gradient-based optimization prompt-tuning, we make a model learn
arbitrary secret sequences: secret responses to secret prompts that are absent
from the training corpus. We validate our approach on language models
pre-trained from scratch and show that less than 0.005% of poisoned tokens are
sufficient to covertly make a LM learn a secret and detect it with extremely
high confidence ($p < 10^{-55}$) with a theoretically certifiable scheme.
Crucially, this occurs without performance degradation (on LM benchmarks) and
despite secrets never appearing in the training set.

</details>


### [11] [Fair Data Exchange with Constant-Time Proofs](https://arxiv.org/abs/2506.14944)
*Majid Khabbazian*

Main category: cs.CR

TL;DR: 本文提出了一种改进的Fair Data Exchange (FDE)协议，通过将文件视为Reed-Solomon码字并加密，显著降低了证明和验证的运行时间，同时保持了公平性。


<details>
  <summary>Details</summary>
Motivation: 现有FDE协议的证明和验证时间随文件长度线性增长，效率较低。

Method: 将文件视为Reed-Solomon码字，扩展为低速率码并加密，仅对少量随机密文子集进行正确性证明，利用RS解码修复损坏符号。

Result: 改进后的协议将运行时间降至接近常数，同时保持公平性，并增加了可调的通信冗余开销。

Conclusion: 该协议显著提升了FDE的效率，并通过zk-SNARK修复了比特币实现中的椭圆曲线不匹配问题，支持链下运行。

Abstract: The Fair Data Exchange (FDE) protocol introduced at CCS 2024 offers atomic
pay-per-file transfers with constant-size proofs, but its prover and verifier
runtimes still scale linearly with the file length n. We collapse these costs
to essentially constant by viewing the file as a rate-1 Reed-Solomon (RS)
codeword, extending it to a lower-rate RS code with constant redundancy,
encrypting this extended vector, and then proving correctness for only a small
random subset of the resulting ciphertexts; RS decoding repairs any corrupted
symbols with negligible failure probability. Our protocol preserves full
client- and server-fairness, and adds only a tunable communication redundancy
overhead.
  Finally, we patch the elliptic-curve mismatch in the Bitcoin instantiation of
FDE with a compact zk-SNARK, enabling the entire exchange to run off-chain and
falling back to just two on-chain transactions when channels are unavailable.

</details>


### [12] [Narrowing the Gap between TEEs Threat Model and Deployment Strategies](https://arxiv.org/abs/2506.14964)
*Filip Rezabek,Jonathan Passerat-Palmbach,Moe Mahhouk,Frieder Erdmann,Andrew Miller*

Main category: cs.CR

TL;DR: 论文探讨了机密虚拟机（CVM）在物理保护和侧信道攻击方面的不足，提出通过扩展TEE认证绑定CVM与云提供商以增强安全性。


<details>
  <summary>Details</summary>
Motivation: 当前CVM的威胁模型未涵盖物理攻击和侧信道攻击，依赖可信云提供商，但TEE认证未提供运营者信息，用户无法准确评估风险。

Method: 提出利用受保护平台标识符（PPID）绑定CVM与提供商，并讨论TEE认证的强化与扩展。

Result: 现有TEE实现和认证流程的多样性导致验证、迁移和应用开发困难，需解决这一关键限制以推广CVM。

Conclusion: 需扩展TEE认证以提供端到端安全保障，减少对可信第三方的依赖。

Abstract: Confidential Virtual Machines (CVMs) provide isolation guarantees for data in
use, but their threat model does not include physical level protection and
side-channel attacks. Therefore, current deployments rely on trusted cloud
providers to host the CVMs' underlying infrastructure. However, TEE
attestations do not provide information about the operator hosting a CVM.
Without knowing whether a Trusted Execution Environment (TEE) runs within a
provider's infrastructure, a user cannot accurately assess the risks of
physical attacks. We observe a misalignment in the threat model where the
workloads are protected against other tenants but do not offer end-to-end
security assurances to external users without relying on cloud providers. The
attestation should be extended to bind the CVM with the provider. A possible
solution can rely on the Protected Platform Identifier (PPID), a unique CPU
identifier. However, the implementation details of various TEE manufacturers,
attestation flows, and providers vary. This makes verification of attestations,
ease of migration, and building applications without relying on a trusted party
challenging, highlighting a key limitation that must be addressed for the
adoption of CVMs. We discuss two points focusing on hardening and extensions of
TEEs' attestation.

</details>


### [13] [Private Continual Counting of Unbounded Streams](https://arxiv.org/abs/2506.15018)
*Ben Jacobsen,Kassem Fawaz*

Main category: cs.CR

TL;DR: 论文研究了无界设置下的差分隐私持续计数问题，提出了一种基于对数扰动的新矩阵分解方法，解决了现有算法需要预知输入大小的限制，实现了平滑误差和高效性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于矩阵机制的最优算法在无界设置下无法直接应用，因为其隐私保证依赖于预知输入大小。使用“倍增技巧”会导致次优和非平滑误差。

Method: 引入基于对数扰动的新矩阵分解方法，利用函数$\frac{1}{\sqrt{1-z}}$的变体，设计了一种高效算法。

Result: 算法实现了平滑误差，对任意$t \leq n$，能以$O(\log^{2+2\alpha}(t))$方差估计前$t$个数据点的和，空间和时间复杂度分别为$O(t)$和$O(\log t)$。

Conclusion: 新算法在无界设置下表现优异，误差平滑且性能高效，实际测试中方差表现接近现有最优有界输入算法。

Abstract: We study the problem of differentially private continual counting in the
unbounded setting where the input size $n$ is not known in advance. Current
state-of-the-art algorithms based on optimal instantiations of the matrix
mechanism cannot be directly applied here because their privacy guarantees only
hold when key parameters are tuned to $n$. Using the common `doubling trick'
avoids knowledge of $n$ but leads to suboptimal and non-smooth error. We solve
this problem by introducing novel matrix factorizations based on logarithmic
perturbations of the function $\frac{1}{\sqrt{1-z}}$ studied in prior works,
which may be of independent interest. The resulting algorithm has smooth error,
and for any $\alpha > 0$ and $t\leq n$ it is able to privately estimate the sum
of the first $t$ data points with $O(\log^{2+2\alpha}(t))$ variance. It
requires $O(t)$ space and amortized $O(\log t)$ time per round, compared to
$O(\log(n)\log(t))$ variance, $O(n)$ space and $O(n \log n)$ pre-processing
time for the nearly-optimal bounded-input algorithm of Henzinger et al. (SODA
2023). Empirically, we find that our algorithm's performance is also comparable
to theirs in absolute terms: our variance is less than $1.5\times$ theirs for
$t$ as large as $2^{24}$.

</details>


### [14] [Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices](https://arxiv.org/abs/2506.15028)
*Gargi Mitra,Mohammadreza Hallajiyan,Inji Kim,Athish Pranav Dharmalingam,Mohammed Elnawawy,Shahrear Iqbal,Karthik Pattabiraman,Homa Alemzadeh*

Main category: cs.CR

TL;DR: 论文探讨了AI/ML医疗设备的网络安全风险，提出了一套工具和技术以支持上市前风险评估，旨在将安全性融入设计核心。


<details>
  <summary>Details</summary>
Motivation: AI/ML医疗设备的快速发展带来了显著的网络安全风险，可能危及患者安全，因此需在上市前阶段解决这些挑战。

Method: 通过分析设备召回、不良事件和已知漏洞数据，开发了一套工具和技术以支持全面的上市前风险评估。

Result: 提出了一套工具和技术，帮助制造商将网络安全作为AI/ML医疗设备的核心设计原则。

Conclusion: 强调在AI/ML医疗设备设计中嵌入网络安全的重要性，以确保患者安全。

Abstract: The integration of AI/ML into medical devices is rapidly transforming
healthcare by enhancing diagnostic and treatment facilities. However, this
advancement also introduces serious cybersecurity risks due to the use of
complex and often opaque models, extensive interconnectivity, interoperability
with third-party peripheral devices, Internet connectivity, and vulnerabilities
in the underlying technologies. These factors contribute to a broad attack
surface and make threat prevention, detection, and mitigation challenging.
Given the highly safety-critical nature of these devices, a cyberattack on
these devices can cause the ML models to mispredict, thereby posing significant
safety risks to patients. Therefore, ensuring the security of these devices
from the time of design is essential. This paper underscores the urgency of
addressing the cybersecurity challenges in ML-enabled medical devices at the
pre-market phase. We begin by analyzing publicly available data on device
recalls and adverse events, and known vulnerabilities, to understand the threat
landscape of AI/ML-enabled medical devices and their repercussions on patient
safety. Building on this analysis, we introduce a suite of tools and techniques
designed by us to assist security analysts in conducting comprehensive
premarket risk assessments. Our work aims to empower manufacturers to embed
cybersecurity as a core design principle in AI/ML-enabled medical devices,
thereby making them safe for patients.

</details>


### [15] [MECHA: Multithreaded and Efficient Cryptographic Hardware Access](https://arxiv.org/abs/2506.15034)
*Pratama Derry,Laksmono Agus Mahardika Ari,Iqbal Muhammad,Howon Kim*

Main category: cs.CR

TL;DR: MECHA是一种高效的多线程加密硬件访问方案，通过UNIX域套接字管理多应用请求，提升加密操作速度83%，适用于云计算和物联网。


<details>
  <summary>Details</summary>
Motivation: 传统加密接口设计需要上下文切换，效率低下，MECHA旨在解决这一问题，提供更高效的并发加密操作管理。

Method: MECHA包含服务器线程、客户端线程、收发线程及发送接收队列，支持多种通信协议，设计便携。

Result: 实验结果显示，MECHA比传统接口设计快83%，能高效处理并发加密请求。

Conclusion: MECHA在安全通信领域潜力巨大，为云计算和物联网提供了更高效的加密解决方案。

Abstract: This paper presents a multithread and efficient cryptographic hardware access
(MECHA) for efficient and fast cryptographic operations that eliminates the
need for context switching. Utilizing a UNIX domain socket, MECHA manages
multiple requests from multiple applications simultaneously, resulting in
faster processing and improved efficiency. We comprise several key components,
including the Server thread, Client thread, Transceiver thread, and a pair of
Sender and Receiver queues. MECHA design is portable and can be used with any
communication protocol, with experimental results demonstrating a 83% increase
in the speed of concurrent cryptographic requests compared to conventional
interface design. MECHA architecture has significant potential in the field of
secure communication applications ranging from cloud computing to the IoT,
offering a faster and more efficient solution for managing multiple
cryptographic operation requests concurrently.

</details>


### [16] [Advanced Prediction of Hypersonic Missile Trajectories with CNN-LSTM-GRU Architectures](https://arxiv.org/abs/2506.15043)
*Amir Hossein Baradaran*

Main category: cs.CR

TL;DR: 本文提出了一种结合CNN、LSTM和GRU的混合深度学习模型，用于高精度预测高超音速导弹的复杂轨迹，为防御策略和拦截技术提供了重要支持。


<details>
  <summary>Details</summary>
Motivation: 高超音速导弹因其高速和机动性对防御系统构成重大挑战，准确预测其轨迹是有效应对的关键。

Method: 采用CNN、LSTM和GRU结合的混合深度学习模型。

Result: 模型能够高精度预测高超音速导弹的复杂轨迹。

Conclusion: 研究表明，先进机器学习技术能显著提升防御系统的预测能力。

Abstract: Advancements in the defense industry are paramount for ensuring the safety
and security of nations, providing robust protection against emerging threats.
Among these threats, hypersonic missiles pose a significant challenge due to
their extreme speeds and maneuverability, making accurate trajectory prediction
a critical necessity for effective countermeasures. This paper addresses this
challenge by employing a novel hybrid deep learning approach, integrating
Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks,
and Gated Recurrent Units (GRUs). By leveraging the strengths of these
architectures, the proposed method successfully predicts the complex
trajectories of hypersonic missiles with high accuracy, offering a significant
contribution to defense strategies and missile interception technologies. This
research demonstrates the potential of advanced machine learning techniques in
enhancing the predictive capabilities of defense systems.

</details>


### [17] [Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine](https://arxiv.org/abs/2506.15070)
*Rasha Karakchi,Rye Stahle-Smith,Nishant Chinnasami,Tiffany Yu*

Main category: cs.CR

TL;DR: SPiME是一种轻量级、可扩展的FPGA兼容安全处理器内存加密架构，集成AES-128到内存处理框架中，显著提升边缘计算的加密效率。


<details>
  <summary>Details</summary>
Motivation: 物联网应用的快速增长需要高效、高吞吐和节能的边缘数据处理，传统CPU加密方法在资源受限环境中性能不足。

Method: SPiME采用并行内存处理单元阵列，每个单元结合AES核心和最小控制单元，实现分布式就地加密，并通过Verilog实现和FPGA测试。

Result: SPiME可扩展至4000多个并行单元，资源利用率低于5%，提供超过25Gbps的持续加密吞吐量，延迟低且可预测。

Conclusion: SPiME因其便携性、可配置性和资源效率，成为安全边缘计算和嵌入式加密系统的理想解决方案。

Abstract: The exponential growth of Internet of Things (IoT) applications has
intensified the demand for efficient, high-throughput, and energy-efficient
data processing at the edge. Conventional CPU-centric encryption methods suffer
from performance bottlenecks and excessive data movement, especially in
latency-sensitive and resource-constrained environments. In this paper, we
present SPiME, a lightweight, scalable, and FPGA-compatible Secure
Processor-in-Memory Encryption architecture that integrates the Advanced
Encryption Standard (AES-128) directly into a Processing-in-Memory (PiM)
framework. SPiME is designed as a modular array of parallel PiM units, each
combining an AES core with a minimal control unit to enable distributed
in-place encryption with minimal overhead. The architecture is fully
implemented in Verilog and tested on multiple AMD UltraScale and UltraScale+
FPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units
while maintaining less than 5\% utilization of key FPGA resources on high-end
devices. It delivers over 25~Gbps in sustained encryption throughput with
predictable, low-latency performance. The design's portability,
configurability, and resource efficiency make it a compelling solution for
secure edge computing, embedded cryptographic systems, and customizable
hardware accelerators.

</details>


### [18] [CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID Datasets](https://arxiv.org/abs/2506.15075)
*Samhita Kuili,Mohammadreza Amini,Burak Kantarci*

Main category: cs.CR

TL;DR: 论文提出了一种基于卷积自编码器（CAE）的5G-NR无线网络中的干扰检测方法，通过生成对抗网络（CWGAN-GP）平衡数据集，并在复杂数据环境下表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 5G-NR网络中普遍存在的空中干扰攻击影响了信号接收质量，需要一种有效的检测方法。

Method: 利用卷积自编码器（CAE）检测干扰，并通过CWGAN-GP平衡数据集，比较了CDAE和CSAE的性能。

Result: CAE在干扰信号检测中表现出色，平均精确率97.33%，召回率91.33%，F1分数94.08%，准确率94.35%。

Conclusion: CAE在复杂数据环境下具有鲁棒性，优于其他基准模型，适用于5G-NR网络的干扰检测。

Abstract: In the ever-expanding domain of 5G-NR wireless cellular networks,
over-the-air jamming attacks are prevalent as security attacks, compromising
the quality of the received signal. We simulate a jamming environment by
incorporating additive white Gaussian noise (AWGN) into the real-world In-phase
and Quadrature (I/Q) OFDM datasets. A Convolutional Autoencoder (CAE) is
exploited to implement a jamming detection over various characteristics such as
heterogenous I/Q datasets; extracting relevant information on Synchronization
Signal Blocks (SSBs), and fewer SSB observations with notable class imbalance.
Given the characteristics of datasets, balanced datasets are acquired by
employing a Conv1D conditional Wasserstein Generative Adversarial
Network-Gradient Penalty(CWGAN-GP) on both majority and minority SSB
observations. Additionally, we compare the performance and detection ability of
the proposed CAE model on augmented datasets with benchmark models:
Convolutional Denoising Autoencoder (CDAE) and Convolutional Sparse Autoencoder
(CSAE). Despite the complexity of data heterogeneity involved across all
datasets, CAE depicts the robustness in detection performance of jammed signal
by achieving average values of 97.33% precision, 91.33% recall, 94.08%
F1-score, and 94.35% accuracy over CDAE and CSAE.

</details>


### [19] [Flexible Hardware-Enabled Guarantees for AI Compute](https://arxiv.org/abs/2506.15093)
*James Petrie,Onni Aarne,Nora Ammann,David Dalrymple*

Main category: cs.CR

TL;DR: 论文提出了一种名为flexHEGs的硬件保障系统，用于解决AI发展中的国际安全风险，支持隐私保护的验证和执行。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益强大，其对国际安全的威胁增加，现有治理方法难以在不泄露敏感信息或国家安全的前提下解决问题。

Method: flexHEGs由可审计的保障处理器和安全外壳组成，监控AI加速器使用并提供物理防篡改保护，系统开源且支持灵活更新。

Result: flexHEGs支持多种治理机制，如隐私保护模型评估、受控部署、训练计算限制和自动安全协议执行。

Conclusion: 尽管技术挑战大，flexHEGs为解决前沿AI发展中的监管和国际安全问题提供了一种可行方案。

Abstract: As artificial intelligence systems become increasingly powerful, they pose
growing risks to international security, creating urgent coordination
challenges that current governance approaches struggle to address without
compromising sensitive information or national security. We propose flexible
hardware-enabled guarantees (flexHEGs), that could be integrated with AI
accelerators to enable trustworthy, privacy-preserving verification and
enforcement of claims about AI development. FlexHEGs consist of an auditable
guarantee processor that monitors accelerator usage and a secure enclosure
providing physical tamper protection. The system would be fully open source
with flexible, updateable verification capabilities. FlexHEGs could enable
diverse governance mechanisms including privacy-preserving model evaluations,
controlled deployment, compute limits for training, and automated safety
protocol enforcement. In this first part of a three part series, we provide a
comprehensive introduction of the flexHEG system, including an overview of the
governance and security capabilities it offers, its potential development and
adoption paths, and the remaining challenges and limitations it faces. While
technically challenging, flexHEGs offer an approach to address emerging
regulatory and international security challenges in frontier AI development.

</details>


### [20] [International Security Applications of Flexible Hardware-Enabled Guarantees](https://arxiv.org/abs/2506.15100)
*Onni Aarne,James Petrie*

Main category: cs.CR

TL;DR: flexHEGs（灵活硬件保障）通过标准化设计和国际治理框架，为AI安全提供技术基础，解决恶意使用、失控风险等国际安全问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力快速发展，国际安全面临新挑战，需通过硬件保障和治理框架管理AI风险。

Method: 分析flexHEGs在限制扩散、实施安全规范、管理军事AI风险和支持战略稳定中的应用，探讨验证型和规则型两种治理模型。

Result: 通过博弈论分析，全面flexHEG协议在合理假设下可保持稳定，但需解决技术门槛和治理权力滥用等挑战。

Conclusion: flexHEGs为国际AI治理提供技术基础，需国际合作以应对新兴威胁。

Abstract: As AI capabilities advance rapidly, flexible hardware-enabled guarantees
(flexHEGs) offer opportunities to address international security challenges
through comprehensive governance frameworks. This report examines how flexHEGs
could enable internationally trustworthy AI governance by establishing
standardized designs, robust ecosystem defenses, and clear operational
parameters for AI-relevant chips. We analyze four critical international
security applications: limiting proliferation to address malicious use,
implementing safety norms to prevent loss of control, managing risks from
military AI systems, and supporting strategic stability through
balance-of-power mechanisms while respecting national sovereignty. The report
explores both targeted deployments for specific high-risk facilities and
comprehensive deployments covering all AI-relevant compute. We examine two
primary governance models: verification-based agreements that enable
transparent compliance monitoring, and ruleset-based agreements that
automatically enforce international rules through cryptographically-signed
updates. Through game-theoretic analysis, we demonstrate that comprehensive
flexHEG agreements could remain stable under reasonable assumptions about state
preferences and catastrophic risks. The report addresses critical
implementation challenges including technical thresholds for AI-relevant chips,
management of existing non-flexHEG hardware, and safeguards against abuse of
governance power. While requiring significant international coordination,
flexHEGs could provide a technical foundation for managing AI risks at the
scale and speed necessary to address emerging threats to international security
and stability.

</details>


### [21] [EVA-S2PMLP: Secure and Scalable Two-Party MLP via Spatial Transformation](https://arxiv.org/abs/2506.15102)
*Shizhao Peng,Shoumo Li,Tianle Tao*

Main category: cs.CR

TL;DR: EVA-S2PMLP是一个高效、可验证且准确的安全两方多层感知机框架，用于垂直分区场景下的隐私保护神经网络训练，通过空间尺度优化提升隐私和性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨机构协作建模中的隐私保护问题，确保数据机密性。

Method: 提出安全转换管道，将标量输入映射到向量和矩阵空间，支持线性与非线性安全计算的原子协议。

Result: 实验表明，EVA-S2PMLP在保持模型效用的同时，显著降低通信开销，最高提升12.3倍。

Conclusion: 该框架适用于金融、医疗等领域的隐私保护神经网络训练，具有实际应用价值。

Abstract: Privacy-preserving neural network training in vertically partitioned
scenarios is vital for secure collaborative modeling across institutions. This
paper presents \textbf{EVA-S2PMLP}, an Efficient, Verifiable, and Accurate
Secure Two-Party Multi-Layer Perceptron framework that introduces spatial-scale
optimization for enhanced privacy and performance. To enable reliable
computation under real-number domain, EVA-S2PMLP proposes a secure
transformation pipeline that maps scalar inputs to vector and matrix spaces
while preserving correctness. The framework includes a suite of atomic
protocols for linear and non-linear secure computations, with modular support
for secure activation, matrix-vector operations, and loss evaluation.
Theoretical analysis confirms the reliability, security, and asymptotic
complexity of each protocol. Extensive experiments show that EVA-S2PMLP
achieves high inference accuracy and significantly reduced communication
overhead, with up to $12.3\times$ improvement over baselines. Evaluation on
benchmark datasets demonstrates that the framework maintains model utility
while ensuring strict data confidentiality, making it a practical solution for
privacy-preserving neural network training in finance, healthcare, and
cross-organizational AI applications.

</details>


### [22] [PDLRecover: Privacy-preserving Decentralized Model Recovery with Machine Unlearning](https://arxiv.org/abs/2506.15112)
*Xiangman Li,Xiaodong Wu,Jianbing Ni,Mohamed Mahmoud,Maazen Alsabaan*

Main category: cs.CR

TL;DR: PDLRecover是一种高效恢复被攻击的全局模型的方法，利用历史模型信息并保护隐私，避免了重新训练的高成本。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法难以恢复已被破坏的全局模型，重新训练成本高且可能影响隐私和一致性。

Method: 通过近似Hessian矩阵计算的线性性和秘密共享技术保护历史更新，结合客户端准备、周期性恢复更新和最终精确更新。

Result: 恢复的全局模型性能接近完全重新训练的模型，计算和时间成本显著降低，同时保护了本地模型参数的隐私。

Conclusion: PDLRecover在高效恢复模型的同时确保了隐私和准确性，为去中心化学习中的毒化攻击提供了实用解决方案。

Abstract: Decentralized learning is vulnerable to poison attacks, where malicious
clients manipulate local updates to degrade global model performance. Existing
defenses mainly detect and filter malicious models, aiming to prevent a limited
number of attackers from corrupting the global model. However, restoring an
already compromised global model remains a challenge. A direct approach is to
remove malicious clients and retrain the model using only the benign clients.
Yet, retraining is time-consuming, computationally expensive, and may
compromise model consistency and privacy.
  We propose PDLRecover, a novel method to recover a poisoned global model
efficiently by leveraging historical model information while preserving
privacy. The main challenge lies in protecting shared historical models while
enabling parameter estimation for model recovery. By exploiting the linearity
of approximate Hessian matrix computation, we apply secret sharing to protect
historical updates, ensuring local models are not leaked during transmission or
reconstruction. PDLRecover introduces client-side preparation, periodic
recovery updates, and a final exact update to ensure robustness and convergence
of the recovered model. Periodic updates maintain accurate curvature
information, and the final step ensures high-quality convergence. Experiments
show that the recovered global model achieves performance comparable to a fully
retrained model but with significantly reduced computation and time cost.
Moreover, PDLRecover effectively prevents leakage of local model parameters,
ensuring both accuracy and privacy in recovery.

</details>


### [23] [deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses](https://arxiv.org/abs/2506.15648)
*Georgios Androutsopoulos,Antonio Bianchi*

Main category: cs.CR

TL;DR: deepSURF结合静态分析和LLM引导的模糊测试生成，有效检测Rust库中的内存安全漏洞，尤其在处理泛型时表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有工具在检测Rust内存漏洞时能力有限，无法充分处理Rust特有类型或依赖人工干预。

Method: deepSURF通过替换泛型为自定义类型并生成特质实现，结合LLM动态增强模糊测试工具链，探索复杂API交互。

Result: 在27个Rust库中，deepSURF成功复现20个已知漏洞并发现6个新漏洞，优于现有工具。

Conclusion: deepSURF显著提升了Rust内存漏洞检测能力，尤其在泛型和复杂API处理方面表现优异。

Abstract: Although Rust ensures memory safety by default, it also permits the use of
unsafe code, which can introduce memory safety vulnerabilities if misused.
Unfortunately, existing tools for detecting memory bugs in Rust typically
exhibit limited detection capabilities, inadequately handle Rust-specific
types, or rely heavily on manual intervention.
  To address these limitations, we present deepSURF, a tool that integrates
static analysis with Large Language Model (LLM)-guided fuzzing harness
generation to effectively identify memory safety vulnerabilities in Rust
libraries, specifically targeting unsafe code. deepSURF introduces a novel
approach for handling generics by substituting them with custom types and
generating tailored implementations for the required traits, enabling the
fuzzer to simulate user-defined behaviors within the fuzzed library.
Additionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,
facilitating exploration of complex API interactions and significantly
increasing the likelihood of exposing memory safety vulnerabilities. We
evaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20
known memory safety bugs and uncovering 6 previously unknown vulnerabilities,
demonstrating clear improvements over state-of-the-art tools.

</details>


### [24] [CipherMind: The Longest Codebook in the World](https://arxiv.org/abs/2506.15117)
*Ming Nie,Zhixiong Yang,Bingsheng Wei*

Main category: cs.CR

TL;DR: 提出CipherMind，利用大模型推理的中间结果作为加密传输内容，适用于内网传输等场景。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的广泛应用启发了利用其推理过程进行通信加密的思路。

Method: 通过确定性微调大模型的推理中间结果作为传输内容，利用其语义参数的不可解释性实现加密。

Result: 该方法理论上适用于任何大模型，并能用于内网传输等场景。

Conclusion: CipherMind为通信加密提供了一种新思路，具有广泛的应用潜力。

Abstract: In recent years, the widespread application of large language models has
inspired us to consider using inference for communication encryption. We
therefore propose CipherMind, which utilizes intermediate results from
deterministic fine-tuning of large model inferences as transmission content.
The semantic parameters of large models exhibit characteristics like opaque
underlying implementations and weak interpretability, thus enabling their use
as an encryption method for data transmission. This communication paradigm can
be applied in scenarios like intra-gateway transmission, and theoretically, it
can be implemented using any large model as its foundation.

</details>


### [25] [From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem](https://arxiv.org/abs/2506.15170)
*Yanxu Mao,Tiehan Cui,Peipei Liu,Datao You,Hongsong Zhu*

Main category: cs.CR

TL;DR: 本文系统综述了大型语言模型（LLMs）发展中越狱攻击与防御机制的复杂性，强调了多模态LLMs和智能代理带来的安全挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs从单模态发展为多模态和智能代理，其安全风险日益严重，需要系统梳理越狱攻击与防御机制以提升模型安全性。

Method: 通过分类主流越狱技术（攻击影响与可见性视角），分析代表性攻击方法、数据集和评估指标，并基于响应时间和技术方法组织防御策略。

Result: 总结了现有研究的局限性（如代理安全问题关注不足、混合越狱方法分类不清等），并提供了最新研究的综合分析与未来方向。

Conclusion: 研究旨在加深对越狱机制的理解，推动更鲁棒和自适应的防御策略发展，以应对能力不断增强的LLMs。

Abstract: Large language models (LLMs) are rapidly evolving from single-modal systems
to multimodal LLMs and intelligent agents, significantly expanding their
capabilities while introducing increasingly severe security risks. This paper
presents a systematic survey of the growing complexity of jailbreak attacks and
corresponding defense mechanisms within the expanding LLM ecosystem. We first
trace the developmental trajectory from LLMs to MLLMs and Agents, highlighting
the core security challenges emerging at each stage. Next, we categorize
mainstream jailbreak techniques from both the attack impact and visibility
perspectives, and provide a comprehensive analysis of representative attack
methods, related datasets, and evaluation metrics. On the defense side, we
organize existing strategies based on response timing and technical approach,
offering a structured understanding of their applicability and implementation.
Furthermore, we identify key limitations in existing surveys, such as
insufficient attention to agent-specific security issues, the absence of a
clear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of
experimental setups, and outdated coverage of recent advancements. To address
these limitations, we provide an updated synthesis of recent work and outline
future research directions in areas such as dataset construction, evaluation
framework optimization, and strategy generalization. Our study seeks to enhance
the understanding of jailbreak mechanisms and facilitate the advancement of
more resilient and adaptive defense strategies in the context of ever more
capable LLMs.

</details>


### [26] [LLM vs. SAST: A Technical Analysis on Detecting Coding Bugs of GPT4-Advanced Data Analysis](https://arxiv.org/abs/2506.15212)
*Madjid G. Tehrani,Eldar Sultanow,William J. Buchanan,Mahkame Houmani,Christel H. Djaha Fodja*

Main category: cs.CR

TL;DR: GPT-4在漏洞扫描中表现优于传统SAST工具，准确率达94%。


<details>
  <summary>Details</summary>
Motivation: 研究GPT-4在识别软件漏洞方面的有效性，并与传统SAST工具对比。

Method: 通过分析多种安全错误，评估GPT-4在漏洞扫描中的表现。

Result: GPT-4（高级数据分析）在检测32种可利用漏洞时准确率达94%，优于SAST工具。

Conclusion: 研究强调了LLM在安全领域的潜力，同时呼吁重视AI的安全设计和最佳实践。

Abstract: With the rapid advancements in Natural Language Processing (NLP), large
language models (LLMs) like GPT-4 have gained significant traction in diverse
applications, including security vulnerability scanning. This paper
investigates the efficacy of GPT-4 in identifying software vulnerabilities
compared to traditional Static Application Security Testing (SAST) tools.
Drawing from an array of security mistakes, our analysis underscores the potent
capabilities of GPT-4 in LLM-enhanced vulnerability scanning. We unveiled that
GPT-4 (Advanced Data Analysis) outperforms SAST by an accuracy of 94% in
detecting 32 types of exploitable vulnerabilities. This study also addresses
the potential security concerns surrounding LLMs, emphasising the imperative of
security by design/default and other security best practices for AI.

</details>


### [27] [Facility Location Problem under Local Differential Privacy without Super-set Assumption](https://arxiv.org/abs/2506.15224)
*Kevin Pfisterer,Quentin Hillebrand,Vorapong Suppakitpaisarn*

Main category: cs.CR

TL;DR: 本文提出了一种设施位置问题的变体，并在本地差分隐私（LDP）框架下进行分析，证明其能保护用户在特定位置的存在隐私。


<details>
  <summary>Details</summary>
Motivation: 传统设施位置问题在差分隐私下存在近似比下限，而现有方法可能牺牲用户隐私。本文旨在解决这一问题。

Method: 提出一种LDP算法，实现常数近似比和较小加性因子，并通过实验验证其优于直接方法。

Result: 算法在合成和真实数据集上表现优于直接方法，且不适用传统下限。

Conclusion: 本文的LDP算法在保护隐私的同时，显著提升了设施位置问题的解决效率。

Abstract: In this paper, we introduce an adaptation of the facility location problem
and analyze it within the framework of local differential privacy (LDP). Under
this model, we ensure the privacy of client presence at specific locations.
When n is the number of points, Gupta et al. established a lower bound of
$\Omega(\sqrt{n})$ on the approximation ratio for any differentially private
algorithm applied to the original facility location problem. As a result,
subsequent works have adopted the super-set assumption, which may, however,
compromise user privacy. We show that this lower bound does not apply to our
adaptation by presenting an LDP algorithm that achieves a constant
approximation ratio with a relatively small additive factor. Additionally, we
provide experimental results demonstrating that our algorithm outperforms the
straightforward approach on both synthetically generated and real-world
datasets.

</details>


### [28] [RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments](https://arxiv.org/abs/2506.15253)
*Yuchuan Fu,Xiaohan Yuan,Dongxia Wang*

Main category: cs.CR

TL;DR: RAS-Eval是一个针对大型语言模型（LLM）代理的安全评估基准，包含80个测试用例和3,802个攻击任务，覆盖11种常见弱点类别。评估显示攻击显著降低了代理的任务完成率，并揭示了模型规模对安全能力的影响。


<details>
  <summary>Details</summary>
Motivation: 由于LLM代理在医疗和金融等关键领域的快速部署，缺乏标准化安全评估基准，因此需要开发一个全面的安全框架。

Method: 引入RAS-Eval基准，支持模拟和真实工具执行，使用JSON、LangGraph和MCP格式实现工具，评估6种先进LLM在多样化场景中的表现。

Result: 攻击平均降低代理任务完成率36.78%，学术环境中攻击成功率高达85.65%，且模型规模与安全能力呈正相关。

Conclusion: 研究揭示了LLM代理在实际部署中的重大安全风险，为未来安全研究提供了基础框架。

Abstract: The rapid deployment of Large language model (LLM) agents in critical domains
like healthcare and finance necessitates robust security frameworks. To address
the absence of standardized evaluation benchmarks for these agents in dynamic
environments, we introduce RAS-Eval, a comprehensive security benchmark
supporting both simulated and real-world tool execution. RAS-Eval comprises 80
test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration
(CWE) categories, with tools implemented in JSON, LangGraph, and Model Context
Protocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse
scenarios, revealing significant vulnerabilities: attacks reduced agent task
completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate
in academic settings. Notably, scaling laws held for security capabilities,
with larger models outperforming smaller counterparts. Our findings expose
critical risks in real-world agent deployments and provide a foundational
framework for future security research. Code and data are available at
https://github.com/lanzer-tree/RAS-Eval.

</details>


### [29] [Evaluation Pipeline for systematically searching for Anomaly Detection Systems](https://arxiv.org/abs/2506.15388)
*Florian Rokohl,Alexander Lehnert,Marc Reichenbach*

Main category: cs.CR

TL;DR: 提出了一种基于FPGA的硬件异常检测系统，用于实时检测医疗数字化环境中的恶意客户端。


<details>
  <summary>Details</summary>
Motivation: 医疗数字化带来便利的同时也面临网络安全威胁，需要实时检测恶意行为。

Method: 利用FPGA满足实时性和功耗限制，通过整体系统评估优化性能。

Result: 系统能够在实时性和功耗限制下有效检测恶意客户端。

Conclusion: 基于FPGA的硬件异常检测系统是解决医疗数字化安全问题的可行方案。

Abstract: Digitalization in the medical world provides major benefits while making it a
target for attackers and thus hard to secure. To deal with network intruders we
propose an anomaly detection system on hardware to detect malicious clients in
real-time. We meet real-time and power restrictions using FPGAs. Overall system
performance is achieved via the presented holistic system evaluation.

</details>


### [30] [Detecting Hardware Trojans in Microprocessors via Hardware Error Correction Code-based Modules](https://arxiv.org/abs/2506.15417)
*Alessandro Palumbo,Ruben Salvador*

Main category: cs.CR

TL;DR: 提出了一种基于硬件的方法，利用ECC在RISC-V微处理器上检测运行时硬件木马激活，实现了100%检测率且无额外开销。


<details>
  <summary>Details</summary>
Motivation: 硬件木马（HTs）可能导致未授权软件执行或特权操作访问，需要一种高效检测方法。

Method: 采用Hamming单错误校正（HSEC）架构的硬件安全检查器（HSC）检测恶意指令注入。

Result: 实验显示100%检测率，无假阳性或漏检，仅需72个LUT、24个FF和0.5个BRAM，无性能损失。

Conclusion: 该方法高效且低开销，适用于实时检测硬件木马激活。

Abstract: Software-exploitable Hardware Trojans (HTs) enable attackers to execute
unauthorized software or gain illicit access to privileged operations. This
manuscript introduces a hardware-based methodology for detecting runtime HT
activations using Error Correction Codes (ECCs) on a RISC-V microprocessor.
Specifically, it focuses on HTs that inject malicious instructions, disrupting
the normal execution flow by triggering unauthorized programs. To counter this
threat, the manuscript introduces a Hardware Security Checker (HSC) leveraging
Hamming Single Error Correction (HSEC) architectures for effective HT
detection. Experimental results demonstrate that the proposed solution achieves
a 100% detection rate for potential HT activations, with no false positives or
undetected attacks. The implementation incurs minimal overhead, requiring only
72 #LUTs, 24 #FFs, and 0.5 #BRAM while maintaining the microprocessor's
original operating frequency and introducing no additional time delay.

</details>


### [31] [Side-Channel Extraction of Dataflow AI Accelerator Hardware Parameters](https://arxiv.org/abs/2506.15432)
*Guillaume Lomet,Ruben Salvador,Brice Colombier,Vincent Grosso,Olivier Sentieys,Cedric Killian*

Main category: cs.CR

TL;DR: 本文提出了一种通过侧信道攻击（SCA）恢复FINN框架生成的数据流加速器硬件配置的方法，显著降低了计算开销，并在短时间内高精度恢复参数。


<details>
  <summary>Details</summary>
Motivation: 数据流神经网络加速器在FPGA上的部署简化了AI任务处理，但也使其易受恶意攻击者通过侧信道攻击窃取知识产权（IP）。

Method: 采用无监督降维技术减少计算开销，结合轻量级分类器恢复折叠和量化参数。

Result: 攻击阶段仅需337毫秒恢复硬件参数（准确率>95%），421毫秒完全恢复参数（平均4次跟踪），比现有方法快940倍和110倍。

Conclusion: 该方法提供了更现实的攻击场景，且在无需平均跟踪的情况下优于现有技术。

Abstract: Dataflow neural network accelerators efficiently process AI tasks on FPGAs,
with deployment simplified by ready-to-use frameworks and pre-trained models.
However, this convenience makes them vulnerable to malicious actors seeking to
reverse engineer valuable Intellectual Property (IP) through Side-Channel
Attacks (SCA). This paper proposes a methodology to recover the hardware
configuration of dataflow accelerators generated with the FINN framework.
Through unsupervised dimensionality reduction, we reduce the computational
overhead compared to the state-of-the-art, enabling lightweight classifiers to
recover both folding and quantization parameters. We demonstrate an attack
phase requiring only 337 ms to recover the hardware parameters with an accuracy
of more than 95% and 421 ms to fully recover these parameters with an averaging
of 4 traces for a FINN-based accelerator running a CNN, both using a random
forest classifier on side-channel traces, even with the accelerator dataflow
fully loaded. This approach offers a more realistic attack scenario than
existing methods, and compared to SoA attacks based on tsfresh, our method
requires 940x and 110x less time for preparation and attack phases,
respectively, and gives better results even without averaging traces.

</details>


### [32] [An efficient construction of Raz's two-source randomness extractor with improved parameters](https://arxiv.org/abs/2506.15547)
*Cameron Foreman,Lewis Wooltorton,Kevin Milner,Florian J. Curchod*

Main category: cs.CR

TL;DR: 本文改进了Raz的提取器，使其计算时间从多项式降至准线性，并降低了熵要求，同时提供了开源实现和数值参数计算模块。


<details>
  <summary>Details</summary>
Motivation: 解决Raz提取器因高计算复杂度而不实用的问题。

Method: 提出改进版的Raz提取器，优化计算时间和熵要求，并进行理论和数值分析。

Result: 实现了准线性计算时间的提取器，并提供了开源代码和参数计算工具。

Conclusion: 改进后的提取器更高效实用，适用于实际应用。

Abstract: Randomness extractors are algorithms that distill weak random sources into
near-perfect random numbers. Two-source extractors enable this distillation
process by combining two independent weak random sources. Raz's extractor (STOC
'05) was the first to achieve this in a setting where one source has linear
min-entropy (i.e., proportional to its length), while the other has only
logarithmic min-entropy in its length. However, Raz's original construction is
impractical due to a polynomial computation time of at least degree 4. Our work
solves this problem by presenting an improved version of Raz's extractor with
quasi-linear computation time, as well as a new analytic theorem with reduced
entropy requirements. We provide comprehensive analytical and numerical
comparisons of our construction with others in the literature, and we derive
strong and quantum-proof versions of our efficient Raz extractor. Additionally,
we offer an easy-to-use, open-source code implementation of the extractor and a
numerical parameter calculation module.

</details>


### [33] [PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website Detection](https://arxiv.org/abs/2506.15656)
*Wenhao Li,Selvakumar Manickam,Yung-wey Chong,Shankar Karuppayah*

Main category: cs.CR

TL;DR: 论文提出PhishDebate，一种基于多代理LLM的辩论框架，用于钓鱼网站检测，通过多角度分析和协调提升准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有钓鱼检测方法依赖单代理分类，存在幻觉风险和缺乏可解释性或鲁棒性。

Method: PhishDebate采用四个专门代理分别分析网页的不同文本方面（URL结构、HTML组成、语义内容和品牌冒充），由协调者和最终法官协调。

Result: 在真实钓鱼数据集上，PhishDebate召回率和真阳性率均达98.2%，优于单代理和CoT基线。

Conclusion: PhishDebate通过模块化设计和多代理辩论，显著提升了钓鱼检测的性能和适应性。

Abstract: Phishing websites continue to pose a significant cybersecurity threat, often
leveraging deceptive structures, brand impersonation, and social engineering
tactics to evade detection. While recent advances in large language models
(LLMs) have enabled improved phishing detection through contextual
understanding, most existing approaches rely on single-agent classification
facing the risks of hallucination and lack interpretability or robustness. To
address these limitations, we propose PhishDebate, a modular multi-agent
LLM-based debate framework for phishing website detection. PhishDebate employs
four specialized agents to independently analyze different textual aspects of a
webpage--URL structure, HTML composition, semantic content, and brand
impersonation--under the coordination of a Moderator and a final Judge. Through
structured debate and divergent thinking, the framework delivers more accurate
and interpretable decisions. Extensive evaluations on commercial LLMs
demonstrate that PhishDebate achieves 98.2% recall and 98.2% True Positive Rate
(TPR) on a real-world phishing dataset, and outperforms single-agent and Chain
of Thought (CoT) baselines. Additionally, its modular design allows agent-level
configurability, enabling adaptation to varying resource and application
requirements.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study](https://arxiv.org/abs/2506.15207)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Jimmy Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 本文探讨了利用强化学习（RL）和多智能体强化学习（MARL）解决低地球轨道（LEO）卫星在动态地球观测（EO）任务中的自主协调问题，并验证了MARL算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星数量的激增，传统优化方法难以满足动态EO任务的实时决策需求，因此需要探索RL和MARL在自主任务规划中的应用。

Method: 通过建模单卫星操作并扩展到多卫星星座，使用MARL框架（如PPO、IPPO、MAPPO和HAPPO）解决能源、数据存储限制及部分可观测性下的分散协调问题。

Result: 实验表明，MARL能有效平衡成像与资源管理，并解决多卫星协调中的非平稳性和奖励依赖性问题。

Conclusion: 研究为自主卫星操作提供了基础，并为分散EO任务中的策略学习改进提供了实用指南。

Abstract: The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised
Earth Observation (EO) missions, addressing challenges in climate monitoring,
disaster management, and more. However, autonomous coordination in
multi-satellite systems remains a fundamental challenge. Traditional
optimisation approaches struggle to handle the real-time decision-making
demands of dynamic EO missions, necessitating the use of Reinforcement Learning
(RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we
investigate RL-based autonomous EO mission planning by modelling
single-satellite operations and extending to multi-satellite constellations
using MARL frameworks. We address key challenges, including energy and data
storage limitations, uncertainties in satellite observations, and the
complexities of decentralised coordination under partial observability. By
leveraging a near-realistic satellite simulation environment, we evaluate the
training stability and performance of state-of-the-art MARL algorithms,
including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can
effectively balance imaging and resource management while addressing
non-stationarity and reward interdependency in multi-satellite coordination.
The insights gained from this study provide a foundation for autonomous
satellite operations, offering practical guidelines for improving policy
learning in decentralised EO missions.

</details>


### [35] [CALM: Contextual Analog Logic with Multimodality](https://arxiv.org/abs/2506.14936)
*Maxwell J. Jacobson,Corey J. Maley,Yexiang Xue*

Main category: cs.AI

TL;DR: CALM结合符号推理与神经生成，通过多模态数据实现上下文敏感决策，填补逻辑与神经感知之间的空白。


<details>
  <summary>Details</summary>
Motivation: 经典二值逻辑系统无法捕捉人类决策的细微差别，且在多模态环境中需要人工标注，而神经网络虽能提取多模态数据的丰富信息，但缺乏可解释的推理结构。CALM旨在解决这一问题。

Method: CALM通过领域树表示谓词，神经网络预测迭代细化其模拟真值，并通过符号推理模块确保约束满足。

Result: 在填空物体放置任务中，CALM准确率达92.2%，优于经典逻辑（86.3%）和LLM（59.4%），并能生成符合逻辑约束和人类偏好的空间热图。

Conclusion: CALM展示了在多模态环境中结合逻辑结构与神经网络的潜力，为下一代AI系统奠定了基础。

Abstract: In this work, we introduce Contextual Analog Logic with Multimodality (CALM).
CALM unites symbolic reasoning with neural generation, enabling systems to make
context-sensitive decisions grounded in real-world multi-modal data.
  Background: Classic bivalent logic systems cannot capture the nuance of human
decision-making. They also require human grounding in multi-modal environments,
which can be ad-hoc, rigid, and brittle. Neural networks are good at extracting
rich contextual information from multi-modal data, but lack interpretable
structures for reasoning.
  Objectives: CALM aims to bridge the gap between logic and neural perception,
creating an analog logic that can reason over multi-modal inputs. Without this
integration, AI systems remain either brittle or unstructured, unable to
generalize robustly to real-world tasks. In CALM, symbolic predicates evaluate
to analog truth values computed by neural networks and constrained search.
  Methods: CALM represents each predicate using a domain tree, which
iteratively refines its analog truth value when the contextual groundings of
its entities are determined. The iterative refinement is predicted by neural
networks capable of capturing multi-modal information and is filtered through a
symbolic reasoning module to ensure constraint satisfaction.
  Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2%
accuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It
also demonstrated spatial heatmap generation aligned with logical constraints
and delicate human preferences, as shown by a human study.
  Conclusions: CALM demonstrates the potential to reason with logic structure
while aligning with preferences in multi-modal environments. It lays the
foundation for next-gen AI systems that require the precision and
interpretation of logic and the multimodal information processing of neural
networks.

</details>


### [36] [SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence](https://arxiv.org/abs/2506.15672)
*Yao Zhang,Chenyang Lin,Shijie Tang,Haokun Chen,Shijie Zhou,Yunpu Ma,Volker Tresp*

Main category: cs.AI

TL;DR: SwarmAgentic是一个全自动代理系统生成框架，通过语言驱动探索和群体智能优化代理功能与协作，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统生成框架缺乏完全自主性，限制了适应性和可扩展性。

Method: SwarmAgentic通过群体智能（如PSO）优化代理功能和协作，从零构建代理系统。

Result: 在六个真实任务中表现优异，如TravelPlanner基准上相对ADAS提升261.8%。

Conclusion: SwarmAgentic为可扩展和自主的代理系统设计迈出重要一步，结合了群体智能与全自动多代理生成。

Abstract: The rapid progress of Large Language Models has advanced agentic systems in
decision-making, coordination, and task execution. Yet, existing agentic system
generation frameworks lack full autonomy, missing from-scratch agent
generation, self-optimizing agent functionality, and collaboration, limiting
adaptability and scalability. We propose SwarmAgentic, a framework for fully
automated agentic system generation that constructs agentic systems from
scratch and jointly optimizes agent functionality and collaboration as
interdependent components through language-driven exploration. To enable
efficient search over system-level structures, SwarmAgentic maintains a
population of candidate systems and evolves them via feedback-guided updates,
drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our
method on six real-world, open-ended, and exploratory tasks involving
high-level planning, system-level coordination, and creative reasoning. Given
only a task description and an objective function, SwarmAgentic outperforms all
baselines, achieving a +261.8% relative improvement over ADAS on the
TravelPlanner benchmark, highlighting the effectiveness of full automation in
structurally unconstrained tasks. This framework marks a significant step
toward scalable and autonomous agentic system design, bridging swarm
intelligence with fully automated system multi-agent generation. Our code is
publicly released at https://yaoz720.github.io/SwarmAgentic/.

</details>


### [37] [MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.14990)
*Tristan Tomilin,Luka van den Boogaard,Samuel Garcin,Bram Grooten,Meng Fang,Mykola Pechenizkiy*

Main category: cs.AI

TL;DR: MEAL是首个针对持续多智能体强化学习（CMARL）的基准测试工具，利用JAX实现GPU加速，解决了现有CL基准测试在CPU上运行时的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在持续学习和多智能体强化学习的交叉领域研究不足，且计算效率低，限制了任务序列的长度和研究进展。

Method: 开发MEAL基准测试，利用JAX实现GPU加速，支持在标准台式机上快速运行100个任务的序列。

Result: 实验表明，简单结合现有CL和MARL方法在复杂环境中表现不佳，但通过消融研究确定了CMARL的关键架构和算法特征。

Conclusion: MEAL为CMARL研究提供了高效工具，并揭示了复杂环境中持续协调和适应的关键因素。

Abstract: Benchmarks play a crucial role in the development and analysis of
reinforcement learning (RL) algorithms, with environment availability strongly
impacting research. One particularly underexplored intersection is continual
learning (CL) in cooperative multi-agent settings. To remedy this, we introduce
MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark
tailored for continual multi-agent reinforcement learning (CMARL). Existing CL
benchmarks run environments on the CPU, leading to computational bottlenecks
and limiting the length of task sequences. MEAL leverages JAX for GPU
acceleration, enabling continual learning across sequences of 100 tasks on a
standard desktop PC in a few hours. We show that naively combining popular CL
and MARL methods yields strong performance on simple environments, but fails to
scale to more complex settings requiring sustained coordination and adaptation.
Our ablation study identifies architectural and algorithmic features critical
for CMARL on MEAL.

</details>


### [38] [Truncated Proximal Policy Optimization](https://arxiv.org/abs/2506.15050)
*Tiantian Fan,Lingjun Liu,Yu Yue,Jiaze Chen,Chengyi Wang,Qiying Yu,Chi Zhang,Zhiqi Lin,Ruofei Zhu,Yufeng Yuan,Xiaochen Zuo,Bole Ma,Mofan Zhang,Gaohong Liu,Ru Zhang,Haotian Zhou,Cong Xie,Ruidong Zhu,Zhi Zhang,Xin Liu,Mingxuan Wang,Lin Yan,Yonghui Wu*

Main category: cs.AI

TL;DR: 提出了一种名为T-PPO的新方法，通过优化策略更新和限制响应长度，提升了大型语言模型（LLM）的训练效率。


<details>
  <summary>Details</summary>
Motivation: PPO方法在训练长响应生成的LLM时效率低下，硬件利用率低，导致资源浪费。

Method: 提出了T-PPO，结合了Extended Generalized Advantage Estimation（EGAE）和选择性过滤机制，优化策略和价值模型的独立训练。

Result: 在AIME 2024上验证，T-PPO将训练效率提升至2.5倍，并优于现有方法。

Conclusion: T-PPO显著提升了LLM的训练效率，同时保持了收敛性能。

Abstract: Recently, test-time scaling Large Language Models (LLMs) have demonstrated
exceptional reasoning capabilities across scientific and professional tasks by
generating long chains-of-thought (CoT). As a crucial component for developing
these reasoning models, reinforcement learning (RL), exemplified by Proximal
Policy Optimization (PPO) and its variants, allows models to learn through
trial and error. However, PPO can be time-consuming due to its inherent
on-policy nature, which is further exacerbated by increasing response lengths.
In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a
novel extension to PPO that improves training efficiency by streamlining policy
update and length-restricted response generation. T-PPO mitigates the issue of
low hardware utilization, an inherent drawback of fully synchronized
long-generation procedures, where resources often sit idle during the waiting
periods for complete rollouts. Our contributions are two-folds. First, we
propose Extended Generalized Advantage Estimation (EGAE) for advantage
estimation derived from incomplete responses while maintaining the integrity of
policy learning. Second, we devise a computationally optimized mechanism that
allows for the independent optimization of the policy and value models. By
selectively filtering prompt and truncated tokens, this mechanism reduces
redundant computations and accelerates the training process without sacrificing
convergence performance. We demonstrate the effectiveness and efficacy of T-PPO
on AIME 2024 with a 32B base model. The experimental results show that T-PPO
improves the training efficiency of reasoning LLMs by up to 2.5x and
outperforms its existing competitors.

</details>


### [39] [HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges](https://arxiv.org/abs/2506.15196)
*Xianliang Yang,Ling Zhang,Haolong Qian,Lei Song,Jiang Bian*

Main category: cs.AI

TL;DR: HeurAgenix是一个基于大语言模型的两阶段超启发式框架，通过演化启发式并动态选择最优策略，解决了组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统启发式算法依赖人工设计且难以泛化，HeurAgenix旨在通过自动化设计提升性能。

Method: 框架分为启发式演化和动态选择两阶段，利用LLM提取策略并选择最优启发式，支持轻量级模型以降低推理成本。

Result: 在标准基准测试中，HeurAgenix优于现有LLM超启发式方法，甚至媲美专用求解器。

Conclusion: HeurAgenix展示了LLM在组合优化中的潜力，为自动化启发式设计提供了新思路。

Abstract: Heuristic algorithms play a vital role in solving combinatorial optimization
(CO) problems, yet traditional designs depend heavily on manual expertise and
struggle to generalize across diverse instances. We introduce
\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large
language models (LLMs) that first evolves heuristics and then selects among
them automatically. In the heuristic evolution phase, HeurAgenix leverages an
LLM to compare seed heuristic solutions with higher-quality solutions and
extract reusable evolution strategies. During problem solving, it dynamically
picks the most promising heuristic for each problem state, guided by the LLM's
perception ability. For flexibility, this selector can be either a
state-of-the-art LLM or a fine-tuned lightweight model with lower inference
cost. To mitigate the scarcity of reliable supervision caused by CO complexity,
we fine-tune the lightweight heuristic selector with a dual-reward mechanism
that jointly exploits singals from selection preferences and state perception,
enabling robust selection under noisy annotations. Extensive experiments on
canonical benchmarks show that HeurAgenix not only outperforms existing
LLM-based hyper-heuristics but also matches or exceeds specialized solvers.
Code is available at https://github.com/microsoft/HeurAgenix.

</details>


### [40] [Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels](https://arxiv.org/abs/2506.15225)
*Jiahao You,Ziye Jia,Chao Dong,Qihui Wu,Zhu Han*

Main category: cs.AI

TL;DR: 本文提出了一种基于无人机和船只协作的海上计算卸载与资源分配框架，利用Lyapunov优化和马尔可夫博弈解决不确定任务带来的挑战，并通过仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 海上物联网（MIoT）的计算需求快速增长，但不确定的任务导致计算卸载和资源分配效率低下，亟需一种高效解决方案。

Method: 提出协作MEC框架，结合Lyapunov优化处理任务不确定性，将问题转化为马尔可夫博弈，并设计异构智能体软演员-评论家算法求解。

Result: 仿真结果表明，所提方法能有效优化计算卸载和资源分配，减少总执行时间。

Conclusion: 通过无人机和船只的协作，结合优化算法，能够高效解决海上物联网中的计算卸载和资源分配问题。

Abstract: The computation demands from the maritime Internet of Things (MIoT) increase
rapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels
based multi-access edge computing (MEC) can fulfill these MIoT requirements.
However, the uncertain maritime tasks present significant challenges of
inefficient computation offloading and resource allocation. In this paper, we
focus on the maritime computation offloading and resource allocation through
the cooperation of UAVs and vessels, with consideration of uncertain tasks.
Specifically, we propose a cooperative MEC framework for computation offloading
and resource allocation, including MIoT devices, UAVs and vessels. Then, we
formulate the optimization problem to minimize the total execution time. As for
the uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the
unpredictable task arrivals and varying computational resource availability. By
converting the long-term constraints into short-term constraints, we obtain a
set of small-scale optimization problems. Further, considering the
heterogeneity of actions and resources of UAVs and vessels, we reformulate the
small-scale optimization problem into a Markov game (MG). Moreover, a
heterogeneous-agent soft actor-critic is proposed to sequentially update
various neural networks and effectively solve the MG problem. Finally,
simulations are conducted to verify the effectiveness in addressing
computational offloading and resource allocation.

</details>


### [41] [Efficient and Generalizable Environmental Understanding for Visual Navigation](https://arxiv.org/abs/2506.15377)
*Ruoyu Wang,Xinshu Li,Chen Wang,Lina Yao*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果关系的导航方法（CAN），通过引入因果理解模块提升导航任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法通常同时处理所有历史观测数据，忽略了数据内部的关联结构，限制了任务性能的进一步提升。

Method: 通过因果视角分析导航任务特性，提出CAN方法，包含因果理解模块以增强环境理解能力。

Result: 实验表明，CAN在多种任务和模拟环境中均优于基线方法，且因果理解模块在强化学习和监督学习场景中均有效。

Conclusion: CAN通过因果理解模块显著提升了导航任务的性能，且无需额外计算开销。

Abstract: Visual Navigation is a core task in Embodied AI, enabling agents to navigate
complex environments toward given objectives. Across diverse settings within
Navigation tasks, many necessitate the modelling of sequential data accumulated
from preceding time steps. While existing methods perform well, they typically
process all historical observations simultaneously, overlooking the internal
association structure within the data, which may limit the potential for
further improvements in task performance. We address this by examining the
unique characteristics of Navigation tasks through the lens of causality,
introducing a causal framework to highlight the limitations of conventional
sequential methods. Leveraging this insight, we propose Causality-Aware
Navigation (CAN), which incorporates a Causal Understanding Module to enhance
the agent's environmental understanding capability. Empirical evaluations show
that our approach consistently outperforms baselines across various tasks and
simulation environments. Extensive ablations studies attribute these gains to
the Causal Understanding Module, which generalizes effectively in both
Reinforcement and Supervised Learning settings without computational overhead.

</details>


### [42] [Managing Complex Failure Analysis Workflows with LLM-based Reasoning and Acting Agents](https://arxiv.org/abs/2506.15567)
*Aline Dobrovsky,Konstantin Schekotihin,Christian Burmer*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型（LLM）的规划代理（LPA），用于辅助故障分析（FA）工程师处理复杂任务，并展示了其有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 故障分析是一个复杂且知识密集型的过程，AI组件的集成可以自动化许多任务，但如何协调这些组件以形成高效的工作流程是一个挑战。

Method: 设计并实现了一个基于LLM的规划代理（LPA），结合了高级规划能力和外部工具使用，能够自主处理复杂查询、检索数据并生成可读响应。

Result: 评估结果表明，LPA在支持FA任务方面具有操作有效性和可靠性。

Conclusion: LPA为FA工程师提供了一种有效的工具，能够提升故障分析的效率和准确性。

Abstract: Failure Analysis (FA) is a highly intricate and knowledge-intensive process.
The integration of AI components within the computational infrastructure of FA
labs has the potential to automate a variety of tasks, including the detection
of non-conformities in images, the retrieval of analogous cases from diverse
data sources, and the generation of reports from annotated images. However, as
the number of deployed AI models increases, the challenge lies in orchestrating
these components into cohesive and efficient workflows that seamlessly
integrate with the FA process.
  This paper investigates the design and implementation of a Large Language
Model (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their
analysis cases. The LPA integrates LLMs with advanced planning capabilities and
external tool utilization, enabling autonomous processing of complex queries,
retrieval of relevant data from external systems, and generation of
human-readable responses. Evaluation results demonstrate the agent's
operational effectiveness and reliability in supporting FA tasks.

</details>


### [43] [The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games](https://arxiv.org/abs/2506.15624)
*Lyle Goodyear,Rachel Guo,Ramesh Johari*

Main category: cs.AI

TL;DR: 论文提出了一种系统构建自然语言状态表示的框架，用于在重复多智能体游戏中提示LLM代理，解决了以往研究中状态表示不一致的问题。


<details>
  <summary>Details</summary>
Motivation: LLM在动态环境中作为决策者表现出潜力，但其无状态特性需要自然语言历史表示。以往研究对游戏历史的编码方式不一致，影响了行为分析和研究可比性。

Method: 提出了一个框架，从动作信息性、奖励信息性和提示风格三个维度系统构建状态表示，并将其应用于动态自私路由游戏中。

Result: 研究发现，提供总结性历史表示、遗憾信息而非原始收益、以及有限他人动作信息的表示，能使LLM代理行为更接近博弈论均衡预测。

Conclusion: 自然语言状态表示对LLM代理行为有显著影响，特定表示方式能提升行为稳定性和与理论预测的一致性。

Abstract: Large Language Models (LLMs) have shown promise as decision-makers in dynamic
settings, but their stateless nature necessitates creating a natural language
representation of history. We present a unifying framework for systematically
constructing natural language "state" representations for prompting LLM agents
in repeated multi-agent games. Previous work on games with LLM agents has taken
an ad hoc approach to encoding game history, which not only obscures the impact
of state representation on agents' behavior, but also limits comparability
between studies. Our framework addresses these gaps by characterizing methods
of state representation along three axes: action informativeness (i.e., the
extent to which the state representation captures actions played); reward
informativeness (i.e., the extent to which the state representation describes
rewards obtained); and prompting style (or natural language compression, i.e.,
the extent to which the full text history is summarized).
  We apply this framework to a dynamic selfish routing game, chosen because it
admits a simple equilibrium both in theory and in human subject experiments
\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find
that there are key dependencies of LLM agent behavior on the natural language
state representation. In particular, we observe that representations which
provide agents with (1) summarized, rather than complete, natural language
representations of past history; (2) information about regrets, rather than raw
payoffs; and (3) limited information about others' actions lead to behavior
that more closely matches game theoretic equilibrium predictions, and with more
stable game play by the agents. By contrast, other representations can exhibit
either large deviations from equilibrium, higher variation in dynamic game play
over time, or both.

</details>


### [44] [The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy](https://arxiv.org/abs/2506.15639)
*James Weichert,Daniel Dunlap,Mohammed Farghally,Hoda Eldardiry*

Main category: cs.AI

TL;DR: 论文探讨了AI政策模块在计算机科学课程中的引入，旨在培养学生将伦理原则转化为实践的能力，并通过调查显示学生对该模块的积极反馈。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的广泛应用，现有的计算机科学课程未能充分培养学生应对AI伦理和政策的能力，因此需要引入相关教学内容。

Method: 开发并更新了AI政策模块，包括技术作业“AI监管”，并通过前后调查评估学生态度变化。

Result: 学生表现出对AI伦理影响的更多关注，并增强了参与AI政策讨论的信心。

Conclusion: AI政策模块及其作业是培养学生应对AI伦理挑战的有效工具。

Abstract: As artificial intelligence (AI) further embeds itself into many settings
across personal and professional contexts, increasing attention must be paid
not only to AI ethics, but also to the governance and regulation of AI
technologies through AI policy. However, the prevailing post-secondary
computing curriculum is currently ill-equipped to prepare future AI
practitioners to confront increasing demands to implement abstract ethical
principles and normative policy preferences into the design and development of
AI systems. We believe that familiarity with the 'AI policy landscape' and the
ability to translate ethical principles to practices will in the future
constitute an important responsibility for even the most technically-focused AI
engineers.
  Toward preparing current computer science (CS) students for these new
expectations, we developed an AI Policy Module to introduce discussions of AI
policy into the CS curriculum. Building on a successful pilot in fall 2024, in
this innovative practice full paper we present an updated and expanded version
of the module, including a technical assignment on "AI regulation". We present
the findings from our pilot of the AI Policy Module 2.0, evaluating student
attitudes towards AI ethics and policy through pre- and post-module surveys.
Following the module, students reported increased concern about the ethical
impacts of AI technologies while also expressing greater confidence in their
abilities to engage in discussions about AI regulation. Finally, we highlight
the AI Regulation Assignment as an effective and engaging tool for exploring
the limits of AI alignment and emphasizing the role of 'policy' in addressing
ethical challenges.

</details>


### [45] [Exploring and Exploiting the Inherent Efficiency within Large Reasoning Models for Self-Guided Efficiency Enhancement](https://arxiv.org/abs/2506.15647)
*Weixiang Zhao,Jiahe Guo,Yang Deng,Xingyu Sui,Yulin Hu,Yanyan Zhao,Wanxiang Che,Bing Qin,Tat-Seng Chua,Ting Liu*

Main category: cs.AI

TL;DR: 论文探讨了大型推理模型（LRMs）在复杂问题解决中的效率问题，提出了两种轻量级方法提升其推理效率。


<details>
  <summary>Details</summary>
Motivation: LRMs在模拟人类思考时存在过度推理（生成冗余内容）的问题，导致效率低下和推理成本增加。

Method: 1. 提出Efficiency Steering，一种无需训练的激活引导技术；2. 开发Self-Rewarded Efficiency RL，通过强化学习动态平衡任务准确性和简洁性。

Result: 实验表明，方法显著减少推理长度，同时保持或提升任务性能。

Conclusion: 通过引导模型内在能力，可以自指导地提升推理效率。

Abstract: Recent advancements in large reasoning models (LRMs) have significantly
enhanced language models' capabilities in complex problem-solving by emulating
human-like deliberative thinking. However, these models often exhibit
overthinking (i.e., the generation of unnecessarily verbose and redundant
content), which hinders efficiency and inflates inference cost. In this work,
we explore the representational and behavioral origins of this inefficiency,
revealing that LRMs inherently possess the capacity for more concise reasoning.
Empirical analyses show that correct reasoning paths vary significantly in
length, and the shortest correct responses often suffice, indicating untapped
efficiency potential. Exploiting these findings, we propose two lightweight
methods to enhance LRM efficiency. First, we introduce Efficiency Steering, a
training-free activation steering technique that modulates reasoning behavior
via a single direction in the model's representation space. Second, we develop
Self-Rewarded Efficiency RL, a reinforcement learning framework that
dynamically balances task accuracy and brevity by rewarding concise correct
solutions. Extensive experiments on seven LRM backbones across multiple
mathematical reasoning benchmarks demonstrate that our methods significantly
reduce reasoning length while preserving or improving task performance. Our
results highlight that reasoning efficiency can be improved by leveraging and
guiding the intrinsic capabilities of existing models in a self-guided manner.

</details>


### [46] [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence](https://arxiv.org/abs/2506.15677)
*Yining Hong,Rui Sun,Bingxuan Li,Xingcheng Yao,Maxine Wu,Alexander Chien,Da Yin,Ying Nian Wu,Zhecan James Wang,Kai-Wei Chang*

Main category: cs.AI

TL;DR: 论文提出了一种新型AI代理范式——Embodied Web Agents，旨在整合物理世界交互与网络规模推理能力，并开发了统一仿真平台和基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理通常仅专注于数字信息推理或物理世界交互，缺乏整合能力，限制了其在需要跨领域智能的任务中的应用。

Method: 开发了Embodied Web Agents任务环境和仿真平台，整合了3D环境和功能性网络接口，并构建了涵盖多任务的基准测试。

Result: 实验结果显示，现有AI系统与人类能力存在显著差距，揭示了跨领域智能的挑战与机遇。

Conclusion: 论文为物理认知与网络知识访问的交叉领域提供了新范式，相关资源已公开。

Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast
amount of digital information and knowledge obtained online; or interact with
the physical world through embodied perception, planning and action - but
rarely both. This separation limits their ability to solve tasks that require
integrated physical and digital intelligence, such as cooking from online
recipes, navigating with dynamic map data, or interpreting real-world landmarks
using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI
agents that fluidly bridge embodiment and web-scale reasoning. To
operationalize this concept, we first develop the Embodied Web Agents task
environments, a unified simulation platform that tightly integrates realistic
3D indoor and outdoor environments with functional web interfaces. Building
upon this platform, we construct and release the Embodied Web Agents Benchmark,
which encompasses a diverse suite of tasks including cooking, navigation,
shopping, tourism, and geolocation - all requiring coordinated reasoning across
physical and digital realms for systematic assessment of cross-domain
intelligence. Experimental results reveal significant performance gaps between
state-of-the-art AI systems and human capabilities, establishing both
challenges and opportunities at the intersection of embodied cognition and
web-scale knowledge access. All datasets, codes and websites are publicly
available at our project page https://embodied-web-agent.github.io/.

</details>
