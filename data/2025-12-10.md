<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 29]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.SE](#cs.SE) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals](https://arxiv.org/abs/2512.05998)
*Michael Todasco*

Main category: cs.AI

TL;DR: 通过虚构预测市场让LLM用虚拟货币下注，将评估任务转化为赌博游戏，能够产生可校准的置信度信号，使模型内部信念变得可见可用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用于评估其他模型，但这些判断通常缺乏置信度表示。本研究探索是否通过赌博游戏框架（虚构预测市场）能够提高预测准确性并产生校准的置信度信号。

Method: 生成100个数学和逻辑问题，6个基线模型（3个当前代，3个前代）回答问题。3个预测模型在两种条件下预测每个问题-基线组合：控制组（简单正确/错误预测）和激励组（预测加1-100,000 LLMCoin下注，起始资金1,000,000 LLMCoin）。

Result: 激励组显示略高的准确性（81.5% vs. 79.1%，p=0.089），学习速度显著更快（第1轮到第4轮改进12.0% vs. 2.9%，p=0.011）。下注大小与置信度相关：40,000+硬币的大额下注正确率约99%，<1,000硬币的小额下注正确率仅约74%。

Conclusion: 主要发现不是虚拟货币让模型更聪明，而是赌博机制创造了从二元输出中缺失的可读置信度信号。简单的财务框架可能帮助LLM成为风险感知的预测者，使其内部信念变得可见可用，为元评估系统和LLM间预测市场奠定基础。

Abstract: Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. "Whale" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.

</details>


### [2] [Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach](https://arxiv.org/abs/2512.06161)
*Gondy Leroy,Prakash Bisht,Sai Madhuri Kandula,Nell Maltman,Sydney Rice*

Main category: cs.AI

TL;DR: 本研究提出了一种基于BioBERT的透明可解释机器学习方法，用于分析临床文本以诊断自闭症谱系障碍，通过混合数据集训练策略获得了97%敏感性和98%特异性的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍（ASD）诊断过程漫长且需求日益增长，现有机器学习模型多为黑箱且通常基于单一数据集训练，限制了其泛化能力和临床可信度。

Method: 采用BioBERT语言模型分析非结构化临床文本，训练模型标记行为描述并映射到诊断标准，然后分配最终标签（ASD或非ASD）。评估了迁移学习能力，使用两种不同的真实世界数据集，比较了顺序训练和混合训练策略，并与黑箱模型进行对比。

Result: 透明模型表现出稳健性能，混合数据训练策略获得最佳结果（97%敏感性，98%特异性）。顺序训练导致性能略有下降。黑箱模型在顺序或混合数据训练下表现较差（90%敏感性，96%特异性）。透明方法整体优于黑箱方法。

Conclusion: 透明可解释的机器学习方法在ASD诊断中优于黑箱方法，混合数据集训练可获得最佳性能，为神经发育障碍诊断中更可信、可泛化且临床可操作的AI工具铺平了道路。

Abstract: Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.

</details>


### [3] [ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment](https://arxiv.org/abs/2512.06196)
*Charlie Masters,Marta Grześkiewicz,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: ARCANE框架将AI对齐问题转化为多智能体协作问题，通过动态生成自然语言评分标准来代表利益相关者偏好，实现可解释、无需重新训练即可调整的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体越来越多地部署到长期任务中，保持其与利益相关者偏好的一致性变得至关重要。需要可解释的奖励模型让利益相关者理解和审核模型目标，并且能够在交互时引导智能体，无需重新训练即可纳入偏好变化。

Method: 提出ARCANE框架，将对齐问题构建为多智能体协作问题，动态生成自然语言评分标准（加权可验证标准）。受效用理论启发，将评分标准学习构建为重构问题，应用正则化的组序列策略优化(GSPO)程序，平衡可解释性、忠实度和计算效率。

Result: 使用GDPVal基准的219个标记评分标准进行评估，在需要多步推理和工具使用的挑战性任务上测试ARCANE。学习的评分标准产生紧凑、易读的评估，并支持可配置的权衡（如正确性与简洁性）而无需重新训练。

Conclusion: 基于评分标准的奖励模型为复杂、长期AI系统提供了可解释、测试时自适应对齐的有前景路径，能够实现可解释、无需重新训练即可调整的偏好对齐。

Abstract: As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.

</details>


### [4] [On measuring grounding and generalizing grounding problems](https://arxiv.org/abs/2512.06205)
*Daniel Quigley,Eric Maynard*

Main category: cs.AI

TL;DR: 该论文将符号接地问题从二元判断重构为多维度审计框架，提出了真实性、保持性、忠实性、鲁棒性和组合性五个评估标准，并应用于四种接地模式和三个案例研究。


<details>
  <summary>Details</summary>
Motivation: 解决符号接地问题——即符号如何获得意义而非仅仅是形式操作——的传统二元判断方法存在局限，需要更系统化的评估框架来理解不同系统中的意义表征。

Method: 提出一个多维度审计框架，包含五个核心评估标准：真实性（机制是否在智能体内部并通过学习/进化获得）、保持性（原子意义是否保持完整）、忠实性（包括相关性和因果性）、鲁棒性（在扰动下的优雅降级）、组合性（整体是否系统地从部分构建）。将此框架应用于四种接地模式（符号、指称、向量、关系）和三个案例研究。

Result: 模型论语义学实现精确组合但缺乏因果保证；大语言模型在语言任务上显示相关拟合和局部鲁棒性，但在无接地交互的世界任务上缺乏成功选择；人类语言通过进化和发展获得强真实性满足所有标准。该框架为哲学、计算机科学、语言学和数学提供了共同语言和技术工具。

Conclusion: 通过将哲学上的表征问题操作化为技术评估框架，该研究为符号接地和意义研究提供了系统化的方法论，促进了跨学科对话和更深入的理解。

Abstract: The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.

</details>


### [5] [AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems](https://arxiv.org/abs/2512.06240)
*Chuanhao Nie,Yunbo Liu,Chao Wang*

Main category: cs.AI

TL;DR: 该论文探讨了AI在反洗钱(AML)工作流现代化中的应用，提出基于图检索增强生成(RAG-Graph)的KYC应用，实验显示该架构能提高检测准确性、降低误报率，并增强KYC流程的效率和透明度。


<details>
  <summary>Details</summary>
Motivation: 洗钱和金融欺诈每年造成数万亿美元损失，严重威胁全球金融稳定，传统监管方式面临挑战。需要现代化技术来提高反洗钱检测准确性、降低误报率，并减轻人工调查的运营负担。

Method: 论文首先综述AI在AML中的应用，然后提出基于图检索增强生成(RAG-Graph)的KYC应用架构。该架构将图技术与生成模型结合，用于增强KYC流程的效率和透明度。

Result: 实验结果表明，RAG-Graph架构在不同评估设置下表现出高忠实度和强答案相关性，显著提高了KYC CDD/EDD工作流的效率和透明度，有助于实现更可持续、资源优化的合规实践。

Conclusion: AI技术能够有效现代化反洗钱工作流，提高检测准确性并降低运营成本。未来研究方向包括：联邦学习保护隐私、公平可解释AI、强化学习自适应防御、人机协同可视化系统，以确保下一代AML架构的透明性、可问责性和鲁棒性。

Abstract: Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.

</details>


### [6] [How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion](https://arxiv.org/abs/2512.06296)
*Sooho Moon,Yunyong Ko*

Main category: cs.AI

TL;DR: 该论文提出了PROBE评估框架，用于知识图谱补全任务的更全面评估，关注预测锐度和流行度偏差鲁棒性两个关键维度。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全评估指标存在不足，忽视了预测锐度（评估单个预测的严格程度）和流行度偏差鲁棒性（预测低流行度实体的能力）两个关键维度。

Method: 提出PROBE评估框架，包含两个组件：1) 秩变换器（RT）- 基于所需预测锐度水平估计每个预测的分数；2) 秩聚合器（RA）- 以流行度感知的方式聚合所有分数。

Result: 在真实世界知识图谱上的实验表明，现有指标倾向于高估或低估KGC模型的准确性，而PROBE能够提供对KGC模型的全面理解和可靠的评估结果。

Conclusion: PROBE框架能够更全面地评估知识图谱补全模型，解决现有评估指标在预测锐度和流行度偏差鲁棒性方面的不足，提供更可靠的评估结果。

Abstract: Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.

</details>


### [7] [DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2512.06337)
*Xuan Xie,Xuan Wang,Wenjie Wang*

Main category: cs.AI

TL;DR: DaGRPO通过序列级梯度校正和离策略数据增强解决GRPO训练不稳定和样本效率低的问题，在数学推理和OOD泛化基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: GRPO虽然能有效激发LLM的长时程推理能力，但存在训练不稳定和样本效率低的问题。研究发现根本原因在于on-policy rollout中样本缺乏区分度：对于常规查询，高度同质的样本导致破坏性梯度冲突；对于困难查询，有效正样本稀缺导致优化无效。

Method: 提出DaGRPO方法，包含两个核心机制：1) 序列级梯度校正：利用细粒度评分动态掩码低区分度的样本对，从源头消除梯度冲突；2) 离策略数据增强：引入高质量锚点样本，为困难任务恢复训练信号。

Result: 在9个数学推理和OOD泛化基准上的实验表明，DaGRPO显著超越现有SFT、GRPO和混合基线，达到新的SOTA性能（如在数学基准上平均准确率提升+4.7%）。深入分析证实DaGRPO有效缓解梯度爆炸并加速长链推理能力的出现。

Conclusion: DaGRPO通过解决GRPO的区分度问题，显著提升了训练稳定性和样本效率，为LLM的长时程推理能力训练提供了更有效的后训练机制。

Abstract: The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.

</details>


### [8] [Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression](https://arxiv.org/abs/2512.06393)
*Qiming Bao,Xiaoxuan Fu*

Main category: cs.AI

TL;DR: 该研究通过四种压力测试评估大语言模型在逻辑推理中的泛化能力：规则删除、矛盾证据注入、逻辑等价重写和多定律等价叠加，发现模型对语义保持的逻辑变换具有稳定不变性，但对缺失或冲突证据的处理存在根本性脆弱。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在许多自然语言任务上表现出色，但它们在逻辑语境中对结构性扰动的泛化能力仍不清楚。研究者旨在开发一个受控评估框架，探究模型推理的可靠性，特别是对逻辑结构变化的适应能力。

Method: 引入一个包含四种针对性压力测试的评估框架：1) 规则删除（删除冗余或必要规则）；2) 矛盾证据注入；3) 逻辑等价重写（使用对偶、双重否定、蕴含、德摩根、恒等和交换律等变换）；4) 多定律等价叠加（同时应用2-5个逻辑变换）。在BERT、Qwen2和LLaMA类模型上进行实验。

Result: 所有模型在基础任务上达到完美准确率，对冗余规则删除和所有等价重写（单定律或多定律）完全泛化，但在必要规则删除下准确率骤降至25%，在明确矛盾存在时完全崩溃（0%准确率）。模型表现出对语义保持逻辑变换的稳定不变性，但对缺失或冲突证据的处理存在根本性脆弱。

Conclusion: 该框架提供了一个清晰的诊断工具来隔离推理失败模式，突显了当前大语言模型在逻辑泛化能力上的持久差距。模型虽然对逻辑等价变换具有鲁棒性，但在处理不完整或矛盾信息时表现出根本性脆弱。

Abstract: Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.
  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.

</details>


### [9] [UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems](https://arxiv.org/abs/2512.06406)
*Xianzong Wu,Xiaohong Li,Lili Quan,Qiang Hu*

Main category: cs.AI

TL;DR: UncertaintyZoo是一个统一的不确定性量化工具包，集成了29种方法，用于评估大语言模型的预测置信度，并在代码漏洞检测任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在安全关键场景中可能做出错误预测，导致潜在损失。虽然已有多种不确定性量化方法，但缺乏集成工具，阻碍了这些方法的实际应用和未来研究。

Method: 开发UncertaintyZoo工具包，统一集成29种不确定性量化方法，涵盖五大类别，提供标准化接口。在CodeBERT和ChatGLM3模型上进行代码漏洞检测任务的评估。

Result: UncertaintyZoo能够有效揭示预测不确定性，工具已在GitHub开源，并配有演示视频。

Conclusion: UncertaintyZoo填补了不确定性量化工具集的空白，促进了该领域的研究和实际应用，特别是在安全关键场景中评估大语言模型预测可靠性方面。

Abstract: Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.

</details>


### [10] [Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City](https://arxiv.org/abs/2512.06431)
*Mohamed Shamroukh,Mohamed Alkhuzamy Aziz*

Main category: cs.AI

TL;DR: 本研究为埃及基纳市开发了基于Voronoi图的空间分析算法，创建了本地化规划标准模型，评估公共服务覆盖效率，平均覆盖率为81.3%，发现市中心与郊区服务密度差异显著。


<details>
  <summary>Details</summary>
Motivation: 埃及国家公共服务规划标准往往无法适应地方独特特征，本研究旨在填补这一空白，为基纳市开发定制化的规划模型。

Method: 采用混合方法（描述性、分析性和实验性），利用Python编程开发基于Voronoi图的智能空间分析算法，创建城市特定规划标准并评估现有公共设施覆盖情况。

Result: 模型应用显示平均服务覆盖率为81.3%，救护车站效率最高（99.8%），公园和开放空间覆盖率最低（10%）。空间分析显示市中心服务密度高（>45个/平方公里），郊区显著降低（<5个/平方公里），Hajer基纳区未服务区域最多，第一区服务覆盖率最高。

Conclusion: 本研究成功开发了本地化规划标准模型和自动化评估算法，为埃及城市提供了可复制的数据驱动城市规划框架。

Abstract: National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.

</details>


### [11] [The Effect of Belief Boxes and Open-mindedness on Persuasion](https://arxiv.org/abs/2512.06573)
*Onur Bilgin,Abdullah As Sami,Sriram Sai Vujjini,John Licato*

Main category: cs.AI

TL;DR: 研究探索了在LLM智能体中引入"信念盒"（包含信念陈述的提示空间）如何影响其行为、信念改变倾向以及在多智能体场景中的说服力，特别是开放心态指令和同伴压力情境下的效果。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统在推理和决策应用中的增加，需要让基于LLM的智能体具备类似命题信念的能力。研究者想知道在提示空间中包含信念陈述（信念盒）如何实际影响智能体行为、信念倾向以及在多智能体场景中的说服力。

Method: 通过一系列实验探索信念盒技术，研究智能体在提示空间中包含信念陈述及其强度如何影响行为。特别关注开放心态指令的影响，以及在辩论中被对立观点包围（同伴压力场景）时信念改变的可能性。

Result: 研究发现：1）指示智能体保持开放心态会影响其信念改变的易感性；2）纳入信念陈述及其强度会影响智能体对对立观点的抵抗力和说服力；3）在辩论中被对立观点包围（同伴压力场景）时，信念改变的可能性特别受影响。

Conclusion: 结果证明了信念盒技术在推理和决策任务中的可行性和有效性，为LLM智能体构建类似命题信念的能力提供了实证支持。

Abstract: As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.

</details>


### [12] [FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection](https://arxiv.org/abs/2512.06629)
*Xiao-li Xia,Hou-biao Li*

Main category: cs.AI

TL;DR: FlatFormer通过"信息注入而非结构堆叠"的设计范式，解决了知识追踪模型在性能与复杂度之间的权衡问题，在保持高性能的同时大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 知识追踪模型面临"性能-复杂度陷阱"：捕捉复杂认知动态（如学习会话和记忆衰减）通常需要深度层次架构，这导致实时部署的计算成本过高。

Method: 提出FlatFormer架构，采用"信息注入而非结构堆叠"的设计范式。使用标准扁平Transformer，通过两种轻量级注入机制增强：(1) 混合输入编码策略，结合可学习的会话标识符和固定的正弦步长嵌入；(2) 将预计算的幂律偏置直接集成到注意力对数中，显式建模遗忘曲线。

Result: 在四个大规模数据集（如EdNet、Junyi）上的实验表明，FlatFormer实现了最先进的性能。在EdNet数据集上，相比最强的层次基线（HiTSKT），绝对AUC提高了8.3%，同时使用不到15%的参数，推理速度约快三倍。

Conclusion: 高认知保真度不一定需要架构复杂性，通过信息注入而非结构堆叠的设计范式，可以在保持高性能的同时显著降低计算复杂度。

Abstract: Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.

</details>


### [13] [LightSearcher: Efficient DeepSearch via Experiential Memory](https://arxiv.org/abs/2512.06653)
*Hengzhi Lan,Yue Yu,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Ting Bai*

Main category: cs.AI

TL;DR: LightSearcher是一个高效的强化学习框架，通过文本经验记忆和自适应奖励机制，在保持准确性的同时显著减少DeepSearch范式中的工具调用次数和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的RL驱动的DeepSearch系统存在准确性与效率之间的权衡问题：频繁调用外部搜索工具可以提高事实准确性，但会导致不必要的计算开销和效率下降。

Method: 提出了LightSearcher框架，包含两个核心组件：1）通过对比推理轨迹学习生成可解释的成功推理模式摘要的文本经验记忆；2）仅在正确答案场景中惩罚冗余工具调用的自适应奖励塑造机制。

Result: 在四个多跳QA基准测试中，LightSearcher在保持与SOTA基线ReSearch相当的准确性的同时，将搜索工具调用次数减少了39.6%，推理时间减少了48.6%，token消耗减少了21.2%。

Conclusion: LightSearcher通过创新的文本经验记忆和自适应奖励机制，有效平衡了DeepSearch范式中的准确性与效率权衡，实现了高效且准确的深度推理。

Abstract: DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.

</details>


### [14] [Academic journals' AI policies fail to curb the surge in AI-assisted academic writing](https://arxiv.org/abs/2512.06705)
*Yongyuan He,Yi Bu*

Main category: cs.AI

TL;DR: 对5114种期刊和520万篇论文的分析显示，尽管70%的期刊采用了AI使用政策，但AI写作工具的使用率在学科间急剧增长，政策存在与否无显著差异，且AI使用披露率极低（仅0.1%）。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在学术写作中的快速普及，期刊和出版商纷纷制定相关政策，但这些政策的实际效果尚不明确，需要实证评估。

Method: 分析了5114种期刊和超过520万篇论文，评估AI使用指南的实际影响；对16.4万篇科学出版物进行全文分析，特别关注2023年以来发表的7.5万篇论文。

Result: 1. 尽管70%的期刊采用了AI政策（主要是要求披露），但研究人员使用AI写作工具的情况在各学科中急剧增加；2. 有政策与无政策的期刊之间无显著差异；3. 非英语国家、物理科学和高开放获取期刊的增长率最高；4. 2023年以来发表的论文中，只有76篇（0.1%）明确披露了AI使用。

Conclusion: 当前政策在促进透明度或限制AI采用方面基本失败，需要重新评估伦理框架以促进科学中负责任的AI整合。

Abstract: The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.

</details>


### [15] [Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning](https://arxiv.org/abs/2512.06835)
*Tingyu Li,Zheng Sun,Jingxuan Wei,Siyuan Li,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: DoGe是一个双解耦框架，通过将学习过程分解为思考者和解决者两个组件，解决视觉语言模型在强化学习中面临的奖励黑客问题，并提供可扩展的自进化路径。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型通过强化学习实现显著推理能力，但在专业领域（如化学、地球科学、多模态数学）面临高质量多模态数据稀缺的问题。现有方法如合成数据和自奖励机制存在分布有限和对齐困难，导致奖励黑客问题，模型利用高奖励模式导致策略熵崩溃和训练不稳定。

Method: 提出DoGe（Decouple to Generalize）双解耦框架：1）将学习过程解耦为思考者和解决者两个组件，引导模型首先从上下文学习而非直接解决问题；2）采用两阶段强化学习后训练方法，从自由探索上下文到实际解决任务；3）构建演化课程学习管道，包括扩展的本地领域知识语料库和迭代演化的种子问题池。

Result: 实验表明，该方法在各种基准测试中始终优于基线，为实现自进化的大型视觉语言模型提供了可扩展的路径。

Conclusion: DoGe通过双解耦框架有效解决了视觉语言模型强化学习中的奖励黑客问题，通过合理的奖励信号量化和演化课程学习，为专业领域的自进化大型视觉语言模型提供了可行的解决方案。

Abstract: Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.

</details>


### [16] [JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models](https://arxiv.org/abs/2512.06859)
*Ce Chi,Xing Wang,Zhendong Wang,Xiaofan Liu,Ce Li,Zhiyan Song,Chen Zhao,Kexin Yang,Boshen Shi,Jingjing Yang,Chao Deng,Junlan Feng*

Main category: cs.AI

TL;DR: JT-DA-8B是一个专门用于复杂表格推理任务的8B参数大语言模型，通过构建包含34个表格推理任务的多样化训练语料，结合SFT和RL优化，在多种表格推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 针对表格推理场景中高质量监督数据缺乏的问题，需要构建专门的大语言模型来处理现实世界中的复杂表格推理任务。

Method: 1) 构建包含29个公开表格QA数据集和300万张表格的多样化训练语料；2) 提出自动流水线生成多步分析任务；3) 基于JT-Coder-8B基础模型训练；4) 使用LLM评分和工作流对齐过滤来提炼高质量表格数据；5) 结合监督微调(SFT)和强化学习(RL)优化模型；6) 提出四阶段表格推理工作流（表格预处理、表格感知、工具集成推理、提示工程）。

Result: JT-DA-8B在各种表格推理任务中表现出强大的性能，证明了数据中心生成和工作流驱动优化的有效性。

Conclusion: 通过构建全面的训练语料、采用数据质量筛选机制、结合SFT和RL优化以及设计系统化推理工作流，JT-DA-8B能够有效处理复杂的表格推理任务，为现实世界表格分析提供了有效的解决方案。

Abstract: In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.

</details>


### [17] [Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?](https://arxiv.org/abs/2512.06867)
*John Licato,Stephen Steinle,Brayden Hollis*

Main category: cs.AI

TL;DR: 研究探讨了人格提示在大型语言模型中对战略决策的影响，发现在世界统治棋盘游戏PERIL中，某些与战略思维相关的人格能提升游戏表现，但需要中介机制将人格转化为启发式值。


<details>
  <summary>Details</summary>
Motivation: 虽然人格提示在大型语言模型中似乎能触发不同的文本生成风格，但尚不清楚这些风格是否转化为可测量的行为差异，特别是在对抗性战略环境中如何影响决策制定。

Method: 引入结构化翻译过程作为中介机制，受探索性因子分析启发，将LLM生成的库存响应映射为启发式值，比较人格衍生的启发式策略与手动选择的策略。

Result: 某些与战略思维相关的人格确实能提高游戏表现，但仅当使用中介机制将人格转化为启发式值时；结构化翻译方法相比直接推断的启发式具有更高的可靠性和表面效度。

Conclusion: 人格提示确实影响LLM的决策制定，提出的启发式生成方法将心理测量学原理应用于LLM，有助于更好地研究人格类型对决策的影响。

Abstract: Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.

</details>


### [18] [On Memory: A comparison of memory mechanisms in world models](https://arxiv.org/abs/2512.06983)
*Eli J. Laird,Corey Clark*

Main category: cs.AI

TL;DR: 该研究分析了基于Transformer的世界模型的有效记忆跨度，通过引入记忆增强机制的分类法，探讨了记忆编码和记忆注入机制如何扩展世界模型的记忆能力，并展示了这些机制在视觉Transformer中改善长期规划的效果。


<details>
  <summary>Details</summary>
Motivation: 世界模型使智能体能够在想象环境中进行规划，但基于Transformer架构的有效记忆跨度有限，导致长序列推演中出现感知漂移，阻碍了在想象轨迹中完成环路闭合的能力。

Method: 研究通过分析多种记忆增强机制，引入了区分记忆编码和记忆注入机制的分类法，从残差流动态的角度理解它们扩展世界模型记忆的作用，并使用状态回忆评估任务测量每种机制的记忆回忆能力。

Result: 研究发现记忆机制能够改善视觉Transformer中的有效记忆跨度，并为在世界模型的想象中完成环路闭合提供了路径。

Conclusion: 记忆增强机制对于扩展基于Transformer的世界模型的长期规划能力至关重要，特别是在实现环路闭合和减少感知漂移方面具有重要价值。

Abstract: World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.

</details>


### [19] [ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes](https://arxiv.org/abs/2512.07081)
*Rongjia Zhou,Chengzhuo Li,Carl Yang,Jiaying Lu*

Main category: cs.AI

TL;DR: ClinNoteAgents是一个基于大语言模型的多智能体框架，能够将自由文本临床笔记转化为结构化风险因素表示和临床医生风格抽象，用于心衰30天再入院预测，在数据有限的医疗系统中提供可扩展和可解释的方法。


<details>
  <summary>Details</summary>
Motivation: 心衰是美国老年人再入院的主要原因之一，临床笔记包含丰富的患者信息但未充分利用。传统模型依赖专家规则、医学术语和本体来解释临床笔记，但笔记中常包含拼写错误、缩写和领域特定术语。

Method: 提出ClinNoteAgents，一个基于LLM的多智能体框架，将自由文本临床笔记转化为：(1) 临床和社会风险因素的结构化表示用于关联分析；(2) 临床医生风格抽象用于心衰30天再入院预测。

Result: 在3,544份来自2,065名患者（再入院率35.16%）的笔记上评估，在从自由文本提取风险因素、识别关键贡献因素和预测再入院风险方面表现出色。

Conclusion: 通过减少对结构化字段的依赖并最小化手动标注和模型训练，ClinNoteAgents为数据有限的医疗系统提供了可扩展和可解释的基于笔记的心衰再入院风险建模方法。

Abstract: Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.

</details>


### [20] [ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation](https://arxiv.org/abs/2512.07178)
*Latifa Dwiyanti,Sergio Ryan Wibisono,Hidetaka Nambo*

Main category: cs.AI

TL;DR: 该研究开发了一个Python包，将SHAP解释框架与大型语言模型（GPT）集成，生成上下文化的文本解释，以增强非技术用户对机器学习模型解释的理解。


<details>
  <summary>Details</summary>
Motivation: SHAP虽然能有效可视化特征重要性，但缺乏对非技术背景终端用户有意义的上下文解释。当前XAI方法在提供用户友好的、情境化的解释方面存在不足。

Method: 开发了一个Python包，将SHAP与OpenAI的GPT集成，通过用户定义的参数（如特征别名、描述和背景信息）来生成上下文化的文本解释。在医疗相关案例研究中应用，并通过Likert量表调查和后续访谈进行用户评估。

Result: 用户评估结果表明，与仅视觉输出相比，生成的解释被认为更容易理解和更符合上下文。初步发现表明，可视化与上下文化文本的结合可能支持更用户友好和可信的模型解释。

Conclusion: 将SHAP与大型语言模型集成可以增强解释的可理解性和上下文相关性，特别是在非技术用户的应用场景中。这种方法为开发更用户友好的XAI工具提供了有前景的方向。

Abstract: Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.

</details>


### [21] [PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations](https://arxiv.org/abs/2512.07179)
*Wonbeen Lee,Channyoung Lee,Junho Sohn,Hansam Cho*

Main category: cs.AI

TL;DR: 提出PICKT模型解决知识追踪中的冷启动问题，通过知识图谱处理多种输入数据格式，提升在真实ITS环境中的实用性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着个性化学习需求的增长，智能辅导系统需要准确追踪学生知识状态并提供个性化学习路径。现有知识追踪模型存在输入数据格式受限、新学生/新问题冷启动问题、以及真实服务环境中稳定性不足等局限性。

Method: 提出实用的互连概念知识追踪（PICKT）模型，利用知识图谱结构化概念间关系，考虑问题和概念文本信息，有效处理多种类型输入数据，解决冷启动问题。

Result: 在反映真实操作环境的实验中，模型表现出优异性能和实用性。相比现有模型，在新学生注册和新问题添加两个核心冷启动挑战上取得显著性能提升，并通过精细实验设计验证了模型的稳定性和实用性。

Conclusion: PICKT模型为下一代智能辅导系统的实际实施提供了重要的理论和技术基础，增强了在真实产品环境中的适用性。

Abstract: With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.

</details>


### [22] [M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling](https://arxiv.org/abs/2512.07314)
*Yuxiao Luo,Songming Zhang,Sijie Ruan,Siran Chen,Kang Liu,Yang Xu,Yu Zheng,Ling Yin*

Main category: cs.AI

TL;DR: M-STAR框架通过多尺度时空自回归方法生成长期人类移动轨迹，相比现有方法在保真度和生成速度上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归和扩散模型的方法在生成单日轨迹方面表现良好，但在长期轨迹生成（如周轨迹）方面效率低下，且缺乏显式的时空多尺度建模能力。

Method: 提出M-STAR框架，采用从粗到细的时空预测过程，结合多尺度时空标记器编码层次化移动模式，以及基于Transformer的解码器进行下一尺度自回归预测。

Result: 在两个真实世界数据集上的实验表明，M-STAR在保真度方面优于现有方法，并显著提高了生成速度。

Conclusion: M-STAR通过多尺度时空自回归框架有效解决了长期轨迹生成的效率和建模问题，为人类移动建模提供了新的解决方案。

Abstract: Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.

</details>


### [23] [A Geometric Unification of Concept Learning with Concept Cones](https://arxiv.org/abs/2512.07355)
*Alexandre Rocchi--Henry,Thomas Fel,Gianni Franchi*

Main category: cs.AI

TL;DR: 该论文提出了一个统一框架，将监督式概念瓶颈模型（CBMs）和无监督稀疏自编码器（SAEs）联系起来，认为两者都学习激活空间中的线性方向，形成概念锥。通过几何包含关系评估SAEs发现的概念与CBM定义概念的匹配程度。


<details>
  <summary>Details</summary>
Motivation: 传统上，概念瓶颈模型（CBMs）和稀疏自编码器（SAEs）这两种可解释性方法各自发展，缺乏交流。CBMs使用监督学习将激活与人工标注的概念对齐，而SAEs通过稀疏编码发现涌现概念。作者希望建立一个统一框架来连接这两种范式。

Method: 提出几何框架：两种方法都学习激活空间中的线性方向，其非负组合形成概念锥。监督和非监督方法的区别仅在于如何选择这个锥。基于此，建立操作桥梁：CBMs提供人工定义的参考几何，SAEs通过其学习锥与CBM锥的近似或包含程度来评估。

Result: 开发了定量指标，将SAE的归纳偏置（如类型、稀疏度、扩展比）与合理概念的出现联系起来。发现了稀疏度和扩展因子的"最佳点"，能最大化与CBM概念的几何和语义对齐。

Conclusion: 通过共享的几何框架统一了监督和非监督概念发现，提供了原则性指标来衡量SAE进展，并评估发现的概念与合理人类概念的对齐程度。

Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.

</details>


### [24] [LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services](https://arxiv.org/abs/2512.07436)
*Hang He,Chuhuai Yue,Chengqi Dong,Mingxue Tian,Zhenfeng Liu,Jiajun Chai,Xiaohan Wang,Yufei Zhang,Qun Liao,Guojun Yin,Wei Lin,Chengcheng Wan,Haiying Sun,Ting Su*

Main category: cs.AI

TL;DR: LocalSearchBench是首个针对本地生活服务的智能搜索基准测试，包含超过15万条高质量数据和300个多跳问答任务，实验显示即使最先进的推理模型在该领域表现也较差。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型主要关注通用信息检索，很少探索具有独特挑战的垂直领域。本地生活服务中的真实查询往往模糊且需要跨商家和产品的多跳推理，这些问题尚未得到充分解决。

Method: 构建LocalSearchBench基准测试，包含来自不同城市和业务类型的超过15万条高质量条目，基于真实用户查询构建300个多跳问答任务。同时开发LocalPlayground统一环境，集成多种工具供智能体交互。

Result: 实验结果显示，即使最先进的推理模型在LocalSearchBench上表现不佳：最佳模型（DeepSeek-V3.1）正确率仅为34.34%，大多数模型在完整性（平均77.33%）和忠实性（平均61.99%）方面存在问题。

Conclusion: 本地生活服务领域需要专门的基准测试和领域特定的智能体训练，现有通用模型在该垂直领域面临显著挑战。

Abstract: Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.

</details>


### [25] [Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement](https://arxiv.org/abs/2512.07611)
*Yongsheng Lian*

Main category: cs.AI

TL;DR: 本研究系统比较了三种强化学习算法（PPO、GRPO、DAPO）在提升大语言模型复杂推理能力方面的表现，通过受控迁移学习评估发现RL训练模型在所有任务上都优于基准模型，但改进程度因基准而异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索不同强化学习算法在提升大语言模型复杂推理能力方面的效果，为RL-based LLM训练提供实践指导，特别是比较PPO、GRPO和DAPO三种算法的性能差异。

Method: 采用受控迁移学习评估方法：首先在专门的Countdown Game上对模型进行微调，然后在通用推理基准测试套件上进行评估。进行参数分析，包括GRPO和DAPO中组大小的影响、KL惩罚系数的非单调影响，以及DAPO中动态采样组件的效果评估。

Result: 在所有任务中，RL训练模型都优于相应的基准模型，但改进程度因基准而异。增加GRPO和DAPO中的组大小能带来更稳定的训练动态和更高的准确性，KL惩罚系数的影响是非单调的。DAPO中的动态采样组件并未提升性能，实际上禁用DS时DAPO取得了最佳整体结果。

Conclusion: 强化学习能有效提升大语言模型的复杂推理能力，但不同算法的效果存在差异。GRPO和DAPO的组大小是重要参数，而DAPO的动态采样组件在实际应用中可能不需要。研究为RL-based LLM训练提供了实用的参数配置指导。

Abstract: This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.

</details>


### [26] [The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds](https://arxiv.org/abs/2512.07631)
*Shahar Lutati*

Main category: cs.AI

TL;DR: 该论文提出了Agent Capability Problem (ACP)框架，用于预测智能体在资源约束下能否解决问题，通过信息获取视角将问题解决建模为信息获取过程，并提供了成本预测的理论界限。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖经验启发式方法预测智能体能否在资源约束下解决问题，缺乏理论基础。需要一种能够预测智能体资源需求的理论框架，以便在实际搜索开始前就能评估任务可行性。

Method: 提出ACP框架，将问题解决视为信息获取过程：智能体需要I_total比特信息来识别解决方案，每个动作获得I_step比特信息，成本为C_step。由此推导出有效成本C_eff = (I_total/I_step) * C_step，用于预测资源需求。提供了C_eff作为期望成本下界的理论证明，并给出了紧密的概率上界。

Result: 实验验证表明ACP预测与实际智能体性能高度一致，能够持续限制搜索工作量，同时比贪婪和随机策略更高效。该框架可泛化到基于LLM的工作流程和智能体工作流，通过统一的信息论视角连接主动学习、贝叶斯优化和强化学习的原则。

Conclusion: ACP框架为预测智能体在资源约束下的问题解决能力提供了理论基础，通过信息获取视角统一了多种学习范式，为智能体资源分配决策提供了理论指导。

Abstract: When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\Itotal$ bits to identify a solution and gains $\Istep$ bits per action at cost $\Cstep$, yielding an effective cost $\Ceff = (\Itotal/\Istep), \Cstep$ that predicts resource requirements before search. We prove that $\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \

</details>


### [27] [Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE](https://arxiv.org/abs/2512.07710)
*Anxiang Zeng,Haibo Zhang,Hailing Zhang,Kaixiang Mo,Liang Yao,Ling Hu,Long Zhang,Shuman Liu,Shuyi Xie,Yanshi Li,Yizhang Chen,Yuepeng Sheng,Yuwei Huang,Zhaochen Xu,Zhiqiang Zhou,Ziqin Liew*

Main category: cs.AI

TL;DR: CompassMax-V3-Thinking是一个千亿规模的MoE推理模型，采用新的RL训练框架，核心原则是"每个提示都必须重要"。通过多项创新技术解决了大规模RL训练中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 将强化学习扩展到千亿规模时暴露出关键效率问题：零方差提示浪费rollout资源、长视野重要性采样不稳定、标准奖励模型导致优势反转，以及rollout处理的系统性瓶颈。

Method: 引入四项统一创新：1) 多阶段零方差消除，过滤非信息性提示并稳定基于组的策略优化；2) ESPO熵自适应优化方法，平衡token级和序列级重要性采样；3) 路由器重放策略，对齐训练时MoE路由器决策与推理时行为；4) 高吞吐RL系统，采用FP8精度rollout、重叠奖励计算和长度感知调度。

Result: 这些贡献形成了一个连贯的管道，使千亿规模MoE模型的RL训练变得稳定高效。最终模型在内部和公共评估中都表现出色。

Conclusion: 通过解决大规模RL训练中的关键效率瓶颈，CompassMax-V3-Thinking展示了在千亿规模MoE模型上实现稳定高效强化学习的可行性，为大规模推理模型训练提供了新框架。

Abstract: We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.

</details>


### [28] [RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2512.07761)
*Xiqiao Xiong,Ouxiang Li,Zhuo Liu,Moxin Li,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化学习的多轮越狱攻击方法，通过优化最终输出的危害性作为奖励，并引入过程奖励来提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到越狱攻击的威胁，影响其在现实应用中的安全部署。现有方法通常依赖单轮优化，不足以学习长期攻击策略，因此需要更有效的多轮攻击方法。

Method: 将问题建模为多轮强化学习任务，直接优化最终轮输出的危害性作为结果奖励。为缓解稀疏监督并促进长期攻击策略，提出两种启发式过程奖励：1) 控制中间输出的危害性以避免触发黑盒模型的拒绝机制；2) 保持中间输出的语义相关性以避免偏离主题。

Result: 在多个基准测试上的实验结果显示，该方法在多个模型上持续提高了攻击成功率，证明了该方法的有效性。

Conclusion: 提出的基于强化学习的多轮越狱攻击方法能够有效提升攻击成功率，为黑盒模型的安全评估提供了新的技术手段。

Abstract: Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.

</details>


### [29] [ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning](https://arxiv.org/abs/2512.07795)
*Nearchos Potamitis,Lars Klein,Akhil Arora*

Main category: cs.AI

TL;DR: ReasonBENCH是首个专门量化大语言模型推理不稳定性的基准测试，通过多轮运行协议提供统计可靠的质量和成本指标，揭示当前推理方法普遍存在的高不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理评估主要报告单次运行的准确率，忽略了随机解码带来的内在不确定性，导致无法可靠评估方法的稳定性、可重复性和成本一致性，存在评估盲区。

Method: 开发ReasonBENCH基准测试，包含：(1)标准化推理框架、模型和任务的模块化评估库；(2)报告统计可靠质量和成本指标的多轮运行协议；(3)鼓励方差感知报告的公开排行榜。

Result: 跨不同领域任务发现，绝大多数推理策略和模型表现出高不稳定性。即使平均性能相似的策略，置信区间宽度可能相差四倍，且性能最佳的方法通常成本更高且更不稳定。

Conclusion: 推理不稳定性会损害跨运行的可重复性，进而影响报告性能的可靠性。可重复性是可靠LLM推理的关键维度，ReasonBENCH为未来推理方法和不确定性量化技术提供了基础。

Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [30] [Sell Data to AI Algorithms Without Revealing It: Secure Data Valuation and Sharing via Homomorphic Encryption](https://arxiv.org/abs/2512.06033)
*Michael Yang,Ruijiang Gao,Zhiqiang,Zheng*

Main category: cs.CR

TL;DR: TIP协议通过同态加密和梯度影响函数，让买家在不解密原始数据的情况下量化外部数据对AI模型的效用，解决了数据市场中的价值-隐私困境。


<details>
  <summary>Details</summary>
Motivation: 人工智能的快速发展受到数据市场基本摩擦的阻碍：价值-隐私困境（阿罗信息悖论），买家无法在不检查数据的情况下验证数据集效用，但检查又会暴露数据隐私。

Method: 提出可信影响协议(TIP)，整合同态加密和基于梯度的影响函数，允许买家对数据点进行精确的盲评分。针对大语言模型，采用低秩梯度投影降低计算开销，同时保持与明文基线的接近完美保真度。

Result: 在医疗健康和生成式AI领域的实证模拟验证了框架的经济潜力：加密估值信号与实现的临床效用高度相关，揭示了预训练语料库中数据价值的重尾分布，少数文本驱动能力而多数文本降低能力。

Conclusion: 这些发现挑战了现行的固定费率补偿模式，为建立基于功绩的、安全的数据经济提供了可扩展的技术基础。

Abstract: The rapid expansion of Artificial Intelligence is hindered by a fundamental friction in data markets: the value-privacy dilemma, where buyers cannot verify a dataset's utility without inspection, yet inspection may expose the data (Arrow's Information Paradox). We resolve this challenge by introducing the Trustworthy Influence Protocol (TIP), a privacy-preserving framework that enables prospective buyers to quantify the utility of external data without ever decrypting the raw assets. By integrating Homomorphic Encryption with gradient-based influence functions, our approach allows for the precise, blinded scoring of data points against a buyer's specific AI model. To ensure scalability for Large Language Models (LLMs), we employ low-rank gradient projections that reduce computational overhead while maintaining near-perfect fidelity to plaintext baselines, as demonstrated across BERT and GPT-2 architectures. Empirical simulations in healthcare and generative AI domains validate the framework's economic potential: we show that encrypted valuation signals achieve a high correlation with realized clinical utility and reveal a heavy-tailed distribution of data value in pre-training corpora where a minority of texts drive capability while the majority degrades it. These findings challenge prevailing flat-rate compensation models and offer a scalable technical foundation for a meritocratic, secure data economy.

</details>


### [31] [Sift or Get Off the PoC: Applying Information Retrieval to Vulnerability Research with SiftRank](https://arxiv.org/abs/2512.06155)
*Caleb Gross*

Main category: cs.CR

TL;DR: SiftRank：一种基于LLM的O(n)复杂度排序算法，用于安全研究中的优先级排序，可直接处理数千个项目，无需预筛选步骤


<details>
  <summary>Details</summary>
Motivation: 安全研究面临资源限制和优先级排序问题，攻击面太大而分析资源有限。最有效的安全研究人员通常擅长直觉选择最有希望的调查方向，这本质上是一个从众多可能性中选择最优选项的问题。

Method: 将优先级排序问题重构为信息检索问题，使用文档排序技术，LLM作为通用排序器。SiftRank通过三个关键机制实现O(n)复杂度：1) 列表式排序，LLM在小批量（约10个项目）中排序文档；2) 基于拐点的收敛检测，当分数分布稳定时自适应终止排序；3) 迭代精炼，逐步聚焦于最相关文档。每个文档在多个随机批次中评估以减轻LLM判断不一致性。

Result: 在N-day漏洞分析中，成功从2,197个更改函数中识别出漏洞修复函数，耗时99秒，推理成本0.82美元。无需专门的嵌入或领域特定微调，仅需标准LLM API访问。

Conclusion: SiftRank为通常受限于手动分析的安全优先级排序问题提供了可扩展解决方案，可直接处理数千个项目，无需预筛选步骤，仅需标准LLM API访问，无需专门基础设施。

Abstract: Security research is fundamentally a problem of resource constraint and consequent prioritization. There is simply too much attack surface and too little time and energy to spend analyzing it all. The most effective security researchers are often those who are most skilled at intuitively deciding which part of an expansive attack surface to investigate. We demonstrate that this problem of selecting the most promising option from among many possibilities can be reframed as an information retrieval problem, and solved using document ranking techniques with LLMs performing the heavy lifting as general-purpose rankers. We present SiftRank, a ranking algorithm achieving O(n) complexity through three key mechanisms: listwise ranking using an LLM to order documents in small batches of approximately 10 items at a time; inflection-based convergence detection that adaptively terminates ranking when score distributions have stabilized; and iterative refinement that progressively focuses ranking effort on the most relevant documents. Unlike existing reranking approaches that require a separate first-stage retrieval step to narrow datasets to approximately 100 candidates, SiftRank operates directly on thousands of items, with each document evaluated across multiple randomized batches to mitigate inconsistent judgments by an LLM. We demonstrate practical effectiveness on N-day vulnerability analysis, successfully identifying a vulnerability-fixing function among 2,197 changed functions in a stripped binary firmware patch within 99 seconds at an inference cost of $0.82. Our approach enables scalable security prioritization for problems that are generally constrained by manual analysis, requiring only standard LLM API access without specialized infrastructure, embedding, or domain-specific fine-tuning. An open-source implementation of SiftRank may be found at https://github.com/noperator/siftrank.

</details>


### [32] [DEFEND: Poisoned Model Detection and Malicious Client Exclusion Mechanism for Secure Federated Learning-based Road Condition Classification](https://arxiv.org/abs/2512.06172)
*Sheng Liu,Panos Papadimitratos*

Main category: cs.CR

TL;DR: DEFEND：一种针对联邦学习道路状况分类中目标标签翻转攻击的防御机制，通过神经元幅度分析和GMM聚类检测中毒模型，排除恶意客户端，在攻击下保持与无攻击场景相同的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在智能交通系统中的应用面临安全威胁，特别是目标标签翻转攻击会误导模型预测，威胁交通安全。现有防御措施无法在攻击下维持接近无攻击场景的性能水平，缺乏针对TLFA的特定检测机制和客户端排除策略。

Method: 提出DEFEND防御机制：1）使用神经元幅度分析进行攻击目标识别；2）基于高斯混合模型聚类检测中毒模型；3）每轮丢弃中毒模型的贡献；4）自适应调整客户端评分，最终排除恶意客户端。

Result: 在多种联邦学习RCC模型和任务上的广泛评估表明，DEFEND能够有效抵御TLFA，性能优于7个基线防御方法，至少提升15.78%，在攻击下实现了与无攻击场景相同的性能水平。

Conclusion: DEFEND填补了联邦学习道路状况分类中针对目标标签翻转攻击的防御研究空白，通过专门的检测和排除机制，在保持隐私保护的同时确保了交通安全系统的可靠性。

Abstract: Federated Learning (FL) has drawn the attention of the Intelligent Transportation Systems (ITS) community. FL can train various models for ITS tasks, notably camera-based Road Condition Classification (RCC), in a privacy-preserving collaborative way. However, opening up to collaboration also opens FL-based RCC systems to adversaries, i.e., misbehaving participants that can launch Targeted Label-Flipping Attacks (TLFAs) and threaten transportation safety. Adversaries mounting TLFAs poison training data to misguide model predictions, from an actual source class (e.g., wet road) to a wrongly perceived target class (e.g., dry road). Existing countermeasures against poisoning attacks cannot maintain model performance under TLFAs close to the performance level in attack-free scenarios, because they lack specific model misbehavior detection for TLFAs and neglect client exclusion after the detection. To close this research gap, we propose DEFEND, which includes a poisoned model detection strategy that leverages neuron-wise magnitude analysis for attack goal identification and Gaussian Mixture Model (GMM)-based clustering. DEFEND discards poisoned model contributions in each round and adapts accordingly client ratings, eventually excluding malicious clients. Extensive evaluation involving various FL-RCC models and tasks shows that DEFEND can thwart TLFAs and outperform seven baseline countermeasures, with at least 15.78% improvement, with DEFEND remarkably achieving under attack the same performance as in attack-free scenarios.

</details>


### [33] [Privacy Loss of Noise Perturbation via Concentration Analysis of A Product Measure](https://arxiv.org/abs/2512.06253)
*Shuainan Liu,Tianxi Ji,Zhongshuo Fang,Lu Wei,Pan Li*

Main category: cs.CR

TL;DR: 本文提出了一种新的差分隐私噪声生成方案，通过利用噪声的球面对称性，将隐私损失表示为半径和方向随机变量的乘积测度，在相同隐私保证下比传统高斯噪声具有更小的期望噪声幅度，从而提高了高维查询结果的效用。


<details>
  <summary>Details</summary>
Motivation: 传统高斯噪声机制在实现差分隐私时，虽然能提供理论保证，但在高维空间中噪声幅度较大，降低了查询结果的实用性。作者观察到噪声扰动通常具有球面对称性（旋转不变性），希望利用这一几何特性设计更高效的噪声生成机制。

Method: 提出了一种新的噪声生成方案，将噪声分解为半径随机变量（控制噪声幅度）和方向随机变量（控制噪声与查询结果差向量的夹角）的乘积测度。通过几何视角分析隐私损失，推导出乘积测度的闭式矩界来证明(ε,δ)-差分隐私保证。

Result: 在相同的(ε,δ)-差分隐私保证下，新机制在高维空间中比传统高斯噪声产生更小的期望噪声幅度。在高维空间的凸和非凸经验风险最小化问题中应用该乘积噪声，验证了其能显著提高噪声结果的效用。

Conclusion: 通过利用噪声的球面对称性，提出的乘积噪声机制为高维差分隐私提供了一种更有效的解决方案，在保持相同隐私保护水平的同时显著降低了噪声对查询结果的影响，提高了数据发布的实用性。

Abstract: Noise perturbation is one of the most fundamental approaches for achieving $(ε,δ)$-differential privacy (DP) guarantees when releasing the result of a query or function $f(\cdot)\in\mathbb{R}^M$ evaluated on a sensitive dataset $\mathbf{x}$. In this approach, calibrated noise $\mathbf{n}\in\mathbb{R}^M$ is used to obscure the difference vector $f(\mathbf{x})-f(\mathbf{x}')$, where $\mathbf{x}'$ is known as a neighboring dataset. A DP guarantee is obtained by studying the tail probability bound of a privacy loss random variable (PLRV), defined as the Radon-Nikodym derivative between two distributions. When $\mathbf{n}$ follows a multivariate Gaussian distribution, the PLRV is characterized as a specific univariate Gaussian. In this paper, we propose a novel scheme to generate $\mathbf{n}$ by leveraging the fact that the perturbation noise is typically spherically symmetric (i.e., the distribution is rotationally invariant around the origin). The new noise generation scheme allows us to investigate the privacy loss from a geometric perspective and express the resulting PLRV using a product measure, $W\times U$; measure $W$ is related to a radius random variable controlling the magnitude of $\mathbf{n}$, while measure $U$ involves a directional random variable governing the angle between $\mathbf{n}$ and the difference $f(\mathbf{x})-f(\mathbf{x}')$. We derive a closed-form moment bound on the product measure to prove $(ε,δ)$-DP. Under the same $(ε,δ)$-DP guarantee, our mechanism yields a smaller expected noise magnitude than the classic Gaussian noise in high dimensions, thereby significantly improving the utility of the noisy result $f(\mathbf{x})+\mathbf{n}$. To validate this, we consider convex and non-convex empirical risk minimization (ERM) problems in high dimensional space and apply the proposed product noise to achieve privacy.

</details>


### [34] [Beyond Model Jailbreak: Systematic Dissection of the "Ten DeadlySins" in Embodied Intelligence](https://arxiv.org/abs/2512.06387)
*Yuhang Huang,Junchao Li,Boyang Ma,Xuelong Dai,Minghui Xu,Kaidi Xu,Yue Zhang,Jianping Wang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 首次对Unitree Go2平台进行整体安全分析，发现10个跨层漏洞（"十宗罪"），涵盖无线配置、核心模块和外部接口，可导致设备劫持、命令注入、信息泄露和完全物理控制


<details>
  <summary>Details</summary>
Motivation: 虽然语言模型越狱已受关注，但具身智能系统的整体安全栈仍未被充分探索。本研究旨在对Unitree Go2平台进行首次全面安全分析，揭示具身AI系统的系统性安全风险

Method: 采用BLE嗅探、流量拦截、APK逆向工程、云API测试和硬件探测等技术，对Unitree Go2平台进行跨层安全分析，识别三个架构层（无线配置、核心模块、外部接口）的漏洞

Result: 发现10个跨层漏洞，包括硬编码密钥、可预测握手令牌、WiFi凭据泄露、缺失TLS验证、静态SSH密码、多语言安全绕过行为、不安全的本地中继通道、弱绑定逻辑和无限制固件访问权限

Conclusion: 保护具身AI系统需要远超模型对齐的全面安全措施。研究提出了系统级经验教训和建议，强调需要构建在整个软硬件生态系统中保持鲁棒性的具身平台

Abstract: Embodied AI systems integrate language models with real world sensing, mobility, and cloud connected mobile apps. Yet while model jailbreaks have drawn significant attention, the broader system stack of embodied intelligence remains largely unexplored. In this work, we conduct the first holistic security analysis of the Unitree Go2 platform and uncover ten cross layer vulnerabilities the "Ten Sins of Embodied AI Security." Using BLE sniffing, traffic interception, APK reverse engineering, cloud API testing, and hardware probing, we identify systemic weaknesses across three architectural layers: wireless provisioning, core modules, and external interfaces. These include hard coded keys, predictable handshake tokens, WiFi credential leakage, missing TLS validation, static SSH password, multilingual safety bypass behavior, insecure local relay channels, weak binding logic, and unrestricted firmware access. Together, they allow adversaries to hijack devices, inject arbitrary commands, extract sensitive information, or gain full physical control.Our findings show that securing embodied AI requires far more than aligning the model itself. We conclude with system level lessons learned and recommendations for building embodied platforms that remain robust across their entire software hardware ecosystem.

</details>


### [35] [KyFrog: A High-Security LWE-Based KEM Inspired by ML-KEM](https://arxiv.org/abs/2512.06411)
*Victor Duarte Melo,Willian J. Buchanan*

Main category: cs.CR

TL;DR: KyFrog是一个保守的LWE密钥封装机制，通过使用大维度（n=1024）和小模数（q=1103）以及窄误差分布，在标准成本模型下达到约2^325的经典和量子安全级别，代价是产生约0.5 MiB的大密文。


<details>
  <summary>Details</summary>
Motivation: 探索与现有具有相对较小公钥和密文的方案不同的操作点，提供更高的安全裕度。

Method: 使用大维度（n=1024）、小素数模数（q=1103）和窄误差分布（标准差σ_s=σ_e=1.4），基于Lattice Estimator进行参数搜索和安全性评估。

Result: 在标准成本模型下达到约2^325的经典和量子安全级别，密文大小约为0.5 MiB，公钥和私钥大小与ML-KEM相当。

Conclusion: KyFrog通过牺牲密文大小获得了极高的安全裕度，为LWE密钥封装机制提供了一个保守但高安全性的设计选择。

Abstract: KyFrog is a conservative Learning-with-Errors (LWE) key-encapsulation mechanism designed to explore an alternative operating point compared to schemes with relatively small public keys and ciphertexts. KyFrog uses a larger dimension ($n = 1024$) and a small prime modulus $q = 1103$, together with narrow error distributions with standard deviations $σ_s = σ_e = 1.4$, to target approximately $2^{325}$ classical and quantum security against state-of-the-art lattice attacks under standard cost models, as estimated using the Lattice Estimator. The price paid for this security margin is an extremely large KEM ciphertext (about 0.5 MiB), while public and secret keys remain in the same ballpark as ML-KEM. We describe the design rationale, parameter search methodology, and implementation details of KyFrog, and we compare its asymptotic security and concrete parameter sizes with the ML-KEM standard. All code and data for this work are released as free and open-source software, with the full C++23 implementation and experimental scripts available at: https://github.com/victormeloasm/kyfrog

</details>


### [36] [Formalisation of Security for Federated Learning with DP and Attacker Advantage in IIIf for Satellite Swarms -- Extended Version](https://arxiv.org/abs/2512.06467)
*Florian Kammüller*

Main category: cs.CR

TL;DR: 本文扩展了联邦学习分布式动态系统的差分隐私形式化定义，并将其与攻击者优势概念关联，使用Isabelle证明助手进行机器验证，以卫星群系统作为案例研究。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式应用（如卫星群）中能有效应用，但存在数据梯度泄露（DLG）攻击。现有防御措施缺乏理论基础和安全性的严格评估，需要建立形式化框架来确保安全性。

Method: 在Isabelle Insider和Infrastructure框架（IIIf）内扩展联邦学习分布式动态系统的差分隐私形式化定义，将差分隐私与攻击者优势概念关联，使用机器支持的证明助手进行理论和应用验证。

Result: 建立了联邦学习分布式动态系统的形式化差分隐私框架，实现了攻击者优势与隐私保护的关联分析，并通过卫星群系统案例验证了该框架的适用性。

Conclusion: 通过形式化方法为联邦学习系统提供了严格的隐私保护理论基础，使用Isabelle证明助手确保了验证的可靠性，为分布式动态系统的安全设计提供了可验证的框架。

Abstract: In distributed applications, like swarms of satellites, machine learning can be efficiently applied even on small devices by using Federated Learning (FL). This allows to reduce the learning complexity by transmitting only updates to the general model in the server in the form of differences in stochastic gradient descent. FL naturally supports differential privacy but new attacks, so called Data Leakage from Gradient (DLG) have been discovered recently. There has been work on defenses against DLG but there is a lack of foundation and rigorous evaluation of their security. In the current work, we extend existing work on a formal notion of Differential Privacy for Federated Learning distributed dynamic systems and relate it to the notion of the attacker advantage. This formalisation is carried out within the Isabelle Insider and Infrastructure framework (IIIf) allowing the machine supported verification of theory and applications within the proof assistant Isabelle. Satellite swarm systems are used as a motivating use case but also as a validation case study.

</details>


### [37] [OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation](https://arxiv.org/abs/2512.06589)
*Xiaojun Jia,Jie Liao,Qi Guo,Teng Ma,Simeng Qin,Ranjie Duan,Tianlin Li,Yihao Huang,Zhitao Zeng,Dongxian Wu,Yiming Li,Wenqi Ren,Xiaochun Cao,Yang Liu*

Main category: cs.CR

TL;DR: OmniSafeBench-MM是一个全面的多模态越狱攻击-防御评估工具箱，集成了13种攻击方法、15种防御策略和多样化数据集，建立了三维评估协议来衡量危害性、意图对齐和响应细节水平。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）虽然具备统一的感知推理能力，但容易受到越狱攻击，绕过安全对齐并诱导有害行为。现有基准如JailBreakV-28K、MM-SafetyBench和HADES存在局限性：攻击场景有限、缺乏标准化防御评估、没有统一可复现的工具箱。

Method: 引入OmniSafeBench-MM工具箱，集成13种代表性攻击方法、15种防御策略，以及涵盖9个主要风险领域和50个细粒度类别的多样化数据集。数据集结构包括咨询性、命令性和陈述性查询类型。建立三维评估协议：1) 危害性（从低影响个体危害到灾难性社会威胁的多级尺度），2) 响应与查询的意图对齐，3) 响应细节水平。

Result: 对10个开源和8个闭源MLLMs进行了广泛实验，揭示了它们对多模态越狱攻击的脆弱性。通过将数据、方法和评估统一到开源可复现平台中，为未来研究提供了标准化基础。

Conclusion: OmniSafeBench-MM通过统一数据、方法和评估，提供了一个全面的多模态越狱攻击-防御评估工具箱，填补了现有基准的空白，为多模态AI安全研究提供了标准化、可复现的基础平台。

Abstract: Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.

</details>


### [38] [Towards Small Language Models for Security Query Generation in SOC Workflows](https://arxiv.org/abs/2512.06660)
*Saleha Muzammil,Rahul Reddy,Vishal Kamalakrishnan,Hadi Ahmadi,Wajih Ul Hassan*

Main category: cs.CR

TL;DR: 本文研究小型语言模型(SLMs)在企业安全领域实现自然语言到Kusto查询语言(KQL)的准确、经济高效翻译，提出了三旋钮框架，在微软数据集上达到0.987语法准确率和0.906语义准确率，比GPT-5降低10倍令牌成本。


<details>
  <summary>Details</summary>
Motivation: 安全运营中心分析师需要查询大量遥测数据，但编写正确的KQL查询需要专业知识，这种依赖关系在安全团队扩展时成为瓶颈。研究SLMs能否为安全运营提供准确、经济高效的自然语言到KQL翻译解决方案。

Method: 提出三旋钮框架：1) 针对SLMs的轻量级检索和错误感知提示，解决常见解析失败问题；2) 应用LoRA微调与原理蒸馏，通过链式思考解释传递教师模型的推理能力；3) 两阶段架构：SLM生成候选查询，低成本LLM法官进行模式感知优化和选择。

Result: 在微软NL2KQL Defender评估数据集上，两阶段方法达到0.987语法准确率和0.906语义准确率。在Microsoft Sentinel数据上达到0.964语法准确率和0.831语义准确率，比GPT-5降低高达10倍的令牌成本。

Conclusion: SLMs可以作为安全运营中自然语言查询的实用、可扩展基础，提供准确且经济高效的解决方案，显著降低专业查询的依赖瓶颈。

Abstract: Analysts in Security Operations Centers routinely query massive telemetry streams using Kusto Query Language (KQL). Writing correct KQL requires specialized expertise, and this dependency creates a bottleneck as security teams scale. This paper investigates whether Small Language Models (SLMs) can enable accurate, cost-effective natural-language-to-KQL translation for enterprise security. We propose a three-knob framework targeting prompting, fine-tuning, and architecture design. First, we adapt existing NL2KQL framework for SLMs with lightweight retrieval and introduce error-aware prompting that addresses common parser failures without increasing token count. Second, we apply LoRA fine-tuning with rationale distillation, augmenting each NLQ-KQL pair with a brief chain-of-thought explanation to transfer reasoning from a teacher model while keeping the SLM compact. Third, we propose a two-stage architecture that uses an SLM for candidate generation and a low-cost LLM judge for schema-aware refinement and selection. We evaluate nine models (five SLMs and four LLMs) across syntax correctness, semantic accuracy, table selection, and filter precision, alongside latency and token cost. On Microsoft's NL2KQL Defender Evaluation dataset, our two-stage approach achieves 0.987 syntax and 0.906 semantic accuracy. We further demonstrate generalizability on Microsoft Sentinel data, reaching 0.964 syntax and 0.831 semantic accuracy. These results come at up to 10x lower token cost than GPT-5, establishing SLMs as a practical, scalable foundation for natural-language querying in security operations.

</details>


### [39] [PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance](https://arxiv.org/abs/2512.06747)
*Jifar Wakuma Ayana,Huang Qiming*

Main category: cs.CR

TL;DR: PrivLLMSwarm是一个保护隐私的框架，通过安全多方计算（MPC）为无人机群协调执行安全的大语言模型推理，解决了现有LLM驱动无人机系统处理敏感操作数据时的隐私和安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型驱动的无人机系统以明文形式处理敏感操作数据，暴露了隐私和安全风险。在物联网环境中，无人机群需要协调和自主推理，但必须保护敏感数据不被泄露。

Method: 提出了PrivLLMSwarm框架，采用安全多方计算（MPC）进行安全的LLM推理。框架包含MPC优化的transformer组件，使用非线性激活函数的高效近似，使资源受限的空中平台能够进行实用的加密推理。通过强化学习在模拟中增强的微调GPT命令生成器提供可靠指令并保持机密性。

Result: 在城市规模模拟中的实验评估表明，PrivLLMSwarm实现了高语义准确性、低加密推理延迟，以及在隐私约束下的鲁棒编队控制。比较分析显示，与差分隐私、联邦学习和明文基线相比，PrivLLMSwarm提供了更优的隐私-效用平衡。

Conclusion: PrivLLMSwarm为隐私敏感的物联网应用（包括智慧城市监控和应急响应）中的安全、LLM驱动的无人机群建立了实用基础。完整的实现包括源代码、MPC组件和合成数据集已公开可用，支持可重复性。

Abstract: Large Language Models (LLMs) are emerging as powerful enablers for autonomous reasoning and natural-language coordination in unmanned aerial vehicle (UAV) swarms operating within Internet of Things (IoT) environments. However, existing LLM-driven UAV systems process sensitive operational data in plaintext, exposing them to privacy and security risks. This work introduces PrivLLMSwarm, a privacy-preserving framework that performs secure LLM inference for UAV swarm coordination through Secure Multi-Party Computation (MPC). The framework incorporates MPC-optimized transformer components with efficient approximations of nonlinear activations, enabling practical encrypted inference on resource-constrained aerial platforms. A fine-tuned GPT-based command generator, enhanced through reinforcement learning in simulation, provides reliable instructions while maintaining confidentiality. Experimental evaluation in urban-scale simulations demonstrates that PrivLLMSwarm achieves high semantic accuracy, low encrypted inference latency, and robust formation control under privacy constraints. Comparative analysis shows PrivLLMSwarm offers a superior privacy-utility balance compared to differential privacy, federated learning, and plaintext baselines. To support reproducibility, the full implementation including source code, MPC components, and a synthetic dataset is publicly available. PrivLLMSwarm establishes a practical foundation for secure, LLM-enabled UAV swarms in privacy-sensitive IoT applications including smart-city monitoring and emergency response.

</details>


### [40] [From Description to Score: Can LLMs Quantify Vulnerabilities?](https://arxiv.org/abs/2512.06781)
*Sima Jafarikhah,Daniel Thompson,Eva Deans,Hossein Siadati,Yi Liu*

Main category: cs.CR

TL;DR: 本研究评估了通用大语言模型（ChatGPT、Llama、Grok、DeepSeek、Gemini）在自动化漏洞评分（CVSS）方面的表现，发现LLMs在某些指标上显著优于基线，但性能因模型和指标而异，且CVE描述的质量限制了自动化评分的可靠性。


<details>
  <summary>Details</summary>
Motivation: 手动漏洞评分（如CVSS评分）是一个资源密集型过程，且容易受到主观解释的影响。随着CVE积压不断增加，需要自动化解决方案来减轻人工负担。

Method: 研究分析了31,000多个最近的CVE条目，使用五种通用大语言模型（ChatGPT、Llama、Grok、DeepSeek、Gemini）进行自动化CVSS评分。评估了模型在不同CVSS指标上的表现，并尝试了集成元分类器来提升性能。

Result: LLMs在某些指标（如可用性影响）上显著优于基线，但在其他指标（如攻击复杂度）上提升有限。ChatGPT-5获得了最高的精确度。模型倾向于错误分类相同的CVE，集成方法仅略微改善性能。分析发现CVE描述常缺乏关键上下文或包含模糊措辞，导致系统性错误分类。

Conclusion: 研究强调了改进漏洞描述质量和纳入更丰富上下文信息的重要性，以支持更可靠的自动化推理，减轻CVE积压处理的负担。虽然LLMs在自动化漏洞评分方面显示出潜力，但当前仍受限于CVE描述的质量问题。

Abstract: Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study investigates the potential of general-purpose large language models (LLMs), namely ChatGPT, Llama, Grok, DeepSeek, and Gemini, to automate this process by analyzing over 31{,}000 recent Common Vulnerabilities and Exposures (CVE) entries. The results show that LLMs substantially outperform the baseline on certain metrics (e.g., \textit{Availability Impact}), while offering more modest gains on others (e.g., \textit{Attack Complexity}). Moreover, model performance varies across both LLM families and individual CVSS metrics, with ChatGPT-5 attaining the highest precision. Our analysis reveals that LLMs tend to misclassify many of the same CVEs, and ensemble-based meta-classifiers only marginally improve performance. Further examination shows that CVE descriptions often lack critical context or contain ambiguous phrasing, which contributes to systematic misclassifications. These findings underscore the importance of enhancing vulnerability descriptions and incorporating richer contextual details to support more reliable automated reasoning and alleviate the growing backlog of CVEs awaiting triage.

</details>


### [41] [CKG-LLM: LLM-Assisted Detection of Smart Contract Access Control Vulnerabilities Based on Knowledge Graphs](https://arxiv.org/abs/2512.06846)
*Xiaoqi Li,Hailu Kuang,Wenkai Li,Zongwei Li,Shipeng Ye*

Main category: cs.CR

TL;DR: CKG-LLM：利用知识图谱和大型语言模型检测智能合约访问控制漏洞的框架


<details>
  <summary>Details</summary>
Motivation: 传统智能合约分析方法依赖中间表示（如抽象语法树、控制流图等），但这些方法在捕捉语义结构和控制逻辑方面存在局限。知识图谱能提供更丰富的实体关系结构化表示，支持使用图查询语言识别违规元素。

Method: 提出CKG-LLM框架，利用大型语言模型的推理和代码生成能力，将自然语言漏洞模式转换为可在合约知识图谱上执行的可执行查询，自动定位易受攻击的代码元素。

Result: 实验评估表明，CKG-LLM在检测访问控制漏洞方面相比现有工具具有更优越的性能。

Conclusion: CKG-LLM框架有效结合了知识图谱的结构化表示和LLM的推理能力，为智能合约漏洞检测提供了新方法，并讨论了未来研究方向。

Abstract: Traditional approaches for smart contract analysis often rely on intermediate representations such as abstract syntax trees, control-flow graphs, or static single assignment form. However, these methods face limitations in capturing both semantic structures and control logic. Knowledge graphs, by contrast, offer a structured representation of entities and relations, enabling richer intermediate abstractions of contract code and supporting the use of graph query languages to identify rule-violating elements. This paper presents CKG-LLM, a framework for detecting access-control vulnerabilities in smart contracts. Leveraging the reasoning and code generation capabilities of large language models, CKG-LLM translates natural-language vulnerability patterns into executable queries over contract knowledge graphs to automatically locate vulnerable code elements. Experimental evaluation demonstrates that CKG-LLM achieves superior performance in detecting access-control vulnerabilities compared to existing tools. Finally, we discuss potential extensions of CKG-LLM as part of future research directions.

</details>


### [42] [Patronus: Identifying and Mitigating Transferable Backdoors in Pre-trained Language Models](https://arxiv.org/abs/2512.06899)
*Tianhang Zhao,Wei Du,Haodong Zhao,Sufeng Duan,Gongshen Liu*

Main category: cs.CR

TL;DR: Patronus是一个针对预训练语言模型后门攻击的防御框架，通过输入侧触发器不变性应对参数变化，使用多触发器对比搜索算法，结合实时输入监控和对抗训练，在15个PLM和10个任务上实现≥98.7%的后门检测召回率。


<details>
  <summary>Details</summary>
Motivation: 现有后门防御方法主要依赖输出特征空间的异常检测，但下游任务微调会改变模型参数，导致输出分布偏移，使预计算的防御失效。需要一种能应对参数变化的鲁棒防御方法。

Method: 提出Patronus框架：1) 利用触发器在输入侧对参数变化的不变性；2) 引入多触发器对比搜索算法，将基于梯度的优化与对比学习目标结合，解决离散文本优化收敛问题；3) 采用双阶段缓解策略，结合实时输入监控和通过对抗训练的模型净化。

Result: 在15个预训练语言模型和10个任务上的广泛实验表明，Patronus达到≥98.7%的后门检测召回率，并将攻击成功率降低到干净设置水平，在所有设置中显著优于所有最先进的基线方法。

Conclusion: Patronus通过输入侧不变性和创新的优化算法，有效解决了现有后门防御方法在下游任务微调后失效的问题，为PLM供应链安全提供了鲁棒的防御方案。

Abstract: Transferable backdoors pose a severe threat to the Pre-trained Language Models (PLMs) supply chain, yet defensive research remains nascent, primarily relying on detecting anomalies in the output feature space. We identify a critical flaw that fine-tuning on downstream tasks inevitably modifies model parameters, shifting the output distribution and rendering pre-computed defense ineffective. To address this, we propose Patronus, a novel framework that use input-side invariance of triggers against parameter shifts. To overcome the convergence challenges of discrete text optimization, Patronus introduces a multi-trigger contrastive search algorithm that effectively bridges gradient-based optimization with contrastive learning objectives. Furthermore, we employ a dual-stage mitigation strategy combining real-time input monitoring with model purification via adversarial training. Extensive experiments across 15 PLMs and 10 tasks demonstrate that Patronus achieves $\geq98.7\%$ backdoor detection recall and reduce attack success rates to clean settings, significantly outperforming all state-of-the-art baselines in all settings. Code is available at https://github.com/zth855/Patronus.

</details>


### [43] [A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data](https://arxiv.org/abs/2512.07030)
*Zahra Lotfi,Mostafa Lotfi*

Main category: cs.CR

TL;DR: 该研究评估了五种监督学习模型在检测零日攻击方面的性能，通过网格搜索、降维和过采样技术提升模型效果，发现随机森林性能最佳但处理时间较长，最终选择XGBoost作为最佳模型。


<details>
  <summary>Details</summary>
Motivation: 零日攻击难以检测，因为其模式和特征与已知攻击不匹配。现有的监督学习模型在训练阶段学习已知攻击模式，对未见过的零日攻击检测效率低下。需要评估不同模型在零日攻击检测中的性能，并解决数据不平衡问题。

Method: 1. 评估五种监督学习模型在零日攻击检测中的性能和执行时间；2. 提出包含网格搜索、降维和过采样方法的框架以解决数据不平衡问题；3. 使用高度不平衡的数据集模拟真实攻击检测场景；4. 仅在测试阶段暴露零日攻击，模型在训练阶段不接触这些攻击。

Result: 1. 随机森林在过采样和非过采样条件下都表现最佳；2. 随机森林性能提升的代价是更长的处理时间；3. XGBoost因快速且高精度的零日攻击检测性能被选为最佳模型。

Conclusion: 研究通过系统评估发现，虽然随机森林在零日攻击检测中性能最优，但XGBoost在速度和准确性之间取得了更好的平衡，是实际应用中检测零日攻击的优选模型。提出的框架有效解决了数据不平衡问题，提升了监督学习模型对未知攻击的检测能力。

Abstract: Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.

</details>


### [44] [Managed TLS Under Migration: Authentication Authority Across CDN and Hosting Transitions](https://arxiv.org/abs/2512.07033)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.CR

TL;DR: 研究发现托管TLS平台在域名迁移后仍继续使用原证书直到过期，导致认证权限与DNS控制分离，存在多个环境可认证同一域名的安全窗口。


<details>
  <summary>Details</summary>
Motivation: 托管TLS简化了HTTPS部署，但将私钥和证书控制权从域名所有者转移到平台。这种控制权转移在提供商转换期间的影响尚未得到充分研究，需要了解托管TLS平台在域名迁移后的行为。

Method: 使用受控测量环境监测多个托管TLS平台在域名迁移后的行为。观察每个平台在委托期间颁发的证书的整个剩余生命周期，监控DNS解析器将流量导向新基础设施后的平台反应。

Result: 所有平台在域名迁移后继续提供相同的证书直到过期，没有平台撤销、替换或停用证书，委托结束后也没有颁发新证书。直接连接到原平台仍能使用旧证书完成TLS握手，认证能力独立于DNS状态持续存在。

Conclusion: 认证权限在整个委托期间颁发的证书生命周期内仍保留在原平台。DNS控制与认证材料控制之间的差距导致存在多个环境可认证同一域名的窗口期。随着托管TLS采用增长，需要更清晰的密钥退役和证书失效机制，确保认证权限在转换期间跟随操作权限。

Abstract: Managed TLS has become a common approach for deploying HTTPS, with platforms generating and storing private keys and automating certificate issuance on behalf of domain operators. This model simplifies operational management but shifts control of authentication material from the domain owner to the platform. The implications of this shift during provider transitions remain insufficiently examined. This study investigates how managed TLS platforms behave when a domain is moved away from the platform that originally issued and stored its certificate. A controlled measurement environment was used to monitor multiple platforms after migration. Each platform was observed for the full remaining lifetime of the certificate that had been active during delegation. The measurements show that platforms continue to serve the same certificate until it expires, even after DNS resolvers direct traffic toward new infrastructure. No platform revoked, replaced, or retired the certificate, and no new certificate was issued after delegation ended. Direct connections to the previous platform continued to complete TLS handshakes with the stale certificate, which confirms that authentication capability persisted independently of DNS state. These findings indicate that authentication authority remains with the previous platform for the entire lifetime of certificates issued during the delegation period. The gap between DNS control and control of authentication material introduces a window in which multiple environments can authenticate the same domain. As managed TLS adoption grows, clearer mechanisms for key retirement and certificate invalidation are needed to ensure that the authentication authority follows operational authority during transitions.

</details>


### [45] [Ideal Attribution and Faithful Watermarks for Language Models](https://arxiv.org/abs/2512.07038)
*Min Jae Song,Kameron Shahabi*

Main category: cs.CR

TL;DR: 提出理想归因机制作为字符串归因决策的形式化抽象，以账本记录模型与用户的交互历史，为水印方案提供理论基础和设计路线图。


<details>
  <summary>Details</summary>
Motivation: 当前水印方案缺乏统一的归因保证框架，需要建立形式化抽象来澄清不同方案的可实现保证，为未来水印方案设计提供明确目标。

Method: 引入理想归因机制，基于账本（记录模型与用户交互历史的只追加日志）和明确选择标准进行确定性决策，将水印方案设计目标框架化为对理想归因机制的忠实表示。

Result: 建立了统一的形式化框架，用理想归因机制替代零散的概率陈述，能够精确推理未来水印方案的期望特性，即使当前构造尚未实现这些特性。

Conclusion: 该框架提供了概念清晰度，明确了在理想化设置中可实现的保证，为实践中值得追求的水印方案设计提供了路线图。

Abstract: We introduce ideal attribution mechanisms, a formal abstraction for reasoning about attribution decisions over strings. At the core of this abstraction lies the ledger, an append-only log of the prompt-response interaction history between a model and its user. Each mechanism produces deterministic decisions based on the ledger and an explicit selection criterion, making it well-suited to serve as a ground truth for attribution. We frame the design goal of watermarking schemes as faithful representation of ideal attribution mechanisms. This novel perspective brings conceptual clarity, replacing piecemeal probabilistic statements with a unified language for stating the guarantees of each scheme. It also enables precise reasoning about desiderata for future watermarking schemes, even when no current construction achieves them, since the ideal functionalities are specified first. In this way, the framework provides a roadmap that clarifies which guarantees are attainable in an idealized setting and worth pursuing in practice.

</details>


### [46] [ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking](https://arxiv.org/abs/2512.07086)
*Yunzhe Li,Jianan Wang,Hongzi Zhu,James Lin,Shan Chang,Minyi Guo*

Main category: cs.CR

TL;DR: ThinkTrap是一种针对黑盒LLM服务的DoS攻击框架，通过输入空间优化诱导模型进入过长或无限生成循环，消耗计算资源


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为云服务部署，出现了通过无界推理进行DoS攻击的新威胁，攻击者可以设计特殊输入使模型进入过长或无限生成循环，耗尽后端计算资源

Method: ThinkTrap框架首先将离散token映射到连续嵌入空间，然后在低维子空间中进行高效的黑盒优化，利用输入稀疏性识别能诱导扩展或非终止生成的对抗提示

Result: 攻击在多个商业闭源LLM服务上评估，即使在严格请求频率限制下（通常10RPM），也能将服务吞吐量降至原容量的1%，在某些情况下导致完全服务故障

Conclusion: ThinkTrap展示了黑盒环境下对LLM服务进行DoS攻击的可行性，揭示了当前LLM部署模型的安全漏洞，需要新的防御机制

Abstract: Large Language Models (LLMs) have become foundational components in a wide range of applications, including natural language understanding and generation, embodied intelligence, and scientific discovery. As their computational requirements continue to grow, these models are increasingly deployed as cloud-based services, allowing users to access powerful LLMs via the Internet. However, this deployment model introduces a new class of threat: denial-of-service (DoS) attacks via unbounded reasoning, where adversaries craft specially designed inputs that cause the model to enter excessively long or infinite generation loops. These attacks can exhaust backend compute resources, degrading or denying service to legitimate users. To mitigate such risks, many LLM providers adopt a closed-source, black-box setting to obscure model internals. In this paper, we propose ThinkTrap, a novel input-space optimization framework for DoS attacks against LLM services even in black-box environments. The core idea of ThinkTrap is to first map discrete tokens into a continuous embedding space, then undertake efficient black-box optimization in a low-dimensional subspace exploiting input sparsity. The goal of this optimization is to identify adversarial prompts that induce extended or non-terminating generation across several state-of-the-art LLMs, achieving DoS with minimal token overhead. We evaluate the proposed attack across multiple commercial, closed-source LLM services. Our results demonstrate that, even far under the restrictive request frequency limits commonly enforced by these platforms, typically capped at ten requests per minute (10 RPM), the attack can degrade service throughput to as low as 1% of its original capacity, and in some cases, induce complete service failure.

</details>


### [47] [Breaking ECDSA with Electromagnetic Side-Channel Attacks: Challenges and Practicality on Modern Smartphones](https://arxiv.org/abs/2512.07292)
*Felix Oberhansl,Marc Schink,Nisha Jacob Kabakci,Michael Gruber,Dominik Klein,Sven Freud,Tobias Damm,Michael Hartmeier,Ivan Gavrilan,Silvan Streit,Jonas Stappenbeck,Andreas Seelos Zankl*

Main category: cs.CR

TL;DR: 该论文评估了现代智能手机对电磁侧信道攻击的脆弱性，针对Raspberry Pi 4和Fairphone 4设备，成功恢复了ECDSA密钥，并指出当前软件防护措施不足，强调需要独立认证的安全元件。


<details>
  <summary>Details</summary>
Motivation: 随着智能手机处理敏感任务（如支付、数字身份认证）的增加，以及硬件平台变得更加复杂（异构处理器集群、10纳米以下制程、2GHz以上频率），需要重新评估现代智能手机对物理侧信道攻击的脆弱性，特别是针对即将推出的欧洲数字身份钱包等关键应用。

Method: 1. 使用Raspberry Pi 4（Broadcom BCM2711 SoC）和Fairphone 4（Snapdragon 750G 5G SoC）作为测试平台；2. 开发针对现代SoC的新型攻击方法；3. 对OpenSSL实施Nonce@Once攻击以恢复ECDSA密钥；4. 评估libgcrypt防护措施的有效性；5. 分析Android加密实现并定义代表性威胁模型。

Result: 1. 成功从OpenSSL中恢复ECDSA密钥；2. 证明libgcrypt的防护措施无法完全缓解攻击；3. 通过案例研究展示了硬件和软件堆栈如何影响电磁侧信道攻击的可行性；4. 发现ECDSA软件实现存在弱点。

Conclusion: 现代智能手机仍然容易受到电磁侧信道攻击，当前软件防护措施不足。为确保敏感应用（如数字身份钱包）的安全，所有智能手机都需要配备独立认证的安全元件。

Abstract: Smartphones handle sensitive tasks such as messaging and payment and may soon support critical electronic identification through initiatives such as the European Digital Identity (EUDI) wallet, currently under development. Yet the susceptibility of modern smartphones to physical side-channel analysis (SCA) is underexplored, with recent work limited to pre-2019 hardware. Since then, smartphone system on chip (SoC) platforms have grown more complex, with heterogeneous processor clusters, sub 10 nm nodes, and frequencies over 2 GHz, potentially complicating SCA. In this paper, we assess the feasibility of electromagnetic (EM) SCA on a Raspberry Pi 4, featuring a Broadcom BCM2711 SoC and a Fairphone 4 featuring a Snapdragon 750G 5G SoC. Using new attack methodologies tailored to modern SoCs, we recover ECDSA secrets from OpenSSL by mounting the Nonce@Once attack of Alam et al. (Euro S&P 2021) and show that the libgcrypt countermeasure does not fully mitigate it. We present case studies illustrating how hardware and software stacks impact EM SCA feasibility. Motivated by use cases such as the EUDI wallet, we survey Android cryptographic implementations and define representative threat models to assess the attack. Our findings show weaknesses in ECDSA software implementations and underscore the need for independently certified secure elements (SEs) in all smartphones.

</details>


### [48] [PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning](https://arxiv.org/abs/2512.07342)
*Chen Gong,Zheng Liu,Kecen Li,Tianhao Wang*

Main category: cs.CR

TL;DR: PrivORL是首个差分隐私离线强化学习数据集合成方法，使用扩散模型和扩散变换器分别合成转移和轨迹，通过DP-SGD保护隐私，并引入好奇心驱动预训练提升数据多样性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习通过共享预收集数据集训练模型，避免了与环境直接交互，但在导航等关键领域应用时存在隐私泄露风险，需要保护数据集中的隐私信息。

Method: 提出PrivORL方法：1）使用扩散模型合成转移，扩散变换器合成轨迹；2）采用DP-SGD在敏感数据集上微调预训练合成器；3）引入好奇心驱动预训练，通过好奇心模块反馈提升合成数据多样性。

Result: 在五个敏感离线RL数据集上的实验表明，该方法在DP转移和轨迹合成方面相比基线方法具有更好的效用和保真度，能生成与敏感数据集高度相似的多样化合成数据。

Conclusion: PrivORL是首个差分隐私离线RL数据集合成框架，能有效保护隐私同时保持数据质量，为安全发布离线RL数据集进行下游分析和研究提供了可行方案。

Abstract: Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.
  To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.

</details>


### [49] [VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection](https://arxiv.org/abs/2512.07533)
*Yuzhou Nie,Hongwei Li,Chengquan Guo,Ruizhe Jiang,Zhun Wang,Bo Li,Dawn Song,Wenbo Guo*

Main category: cs.CR

TL;DR: VulnLLM-R是首个专门用于漏洞检测的推理大语言模型，通过程序状态推理而非简单模式匹配来提升泛化能力，在Python、C/C++、Java等语言的SOTA数据集上表现优于现有静态分析工具和大型推理模型。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的推理LLM通常规模超大、闭源或在漏洞检测方面性能有限，而传统方法主要依赖模式匹配，容易学习捷径且泛化能力不足。需要专门针对漏洞检测的推理模型来提升检测效果。

Method: 提出新颖的训练方法：专门的数据选择、推理数据生成、推理数据过滤和修正、测试阶段优化。使用该方法训练了一个70亿参数的推理模型，并构建了围绕该模型的智能体框架。

Result: VulnLLM-R在Python、C/C++、Java的SOTA数据集上表现出优于现有静态分析工具和开源/商业大型推理模型的效果和效率。在真实项目中超越了CodeQL和AFL++，并在活跃维护的仓库中发现了一系列零日漏洞。

Conclusion: 这是首次通过专门推理模型驱动的AI智能体实现真实世界项目级漏洞检测的开创性工作，展示了专门化推理模型在漏洞检测领域的巨大潜力。

Abstract: We propose VulnLLM-R, the~\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [50] [Auto-SPT: Automating Semantic Preserving Transformations for Code](https://arxiv.org/abs/2512.06042)
*Ashish Hooda,Mihai Christodorescu,Chuangang Ren,Aaron Wilson,Kassem Fawaz,Somesh Jha*

Main category: cs.SE

TL;DR: Auto-SPT：基于大语言模型自动生成代码语义保持变换的框架，用于提升代码克隆检测模型的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测模型主要在干净、结构化的代码数据集上训练，但现实世界代码会经历各种语义保持的变换（如重构、压缩、格式化、编译器优化），导致训练与测试数据之间存在关键差距

Method: 提出Auto-SPT框架，利用大语言模型自动构建代码合成数据生成器：1）使用LLM设计多样化的语义保持变换；2）为这些变换生成强实现；3）组合变换产生强变换效果。理论分析表明变换多样性影响组合强度

Result: Auto-SPT生成的语义保持变换比现有方法更多样化，能显著降低最先进代码克隆检测器的性能。实验还表明Auto-SPT可用于增强训练数据集，产生对现实世界对抗性代码变换具有鲁棒性的代码克隆检测模型

Conclusion: Auto-SPT框架能有效弥合代码克隆检测模型训练与测试数据之间的差距，通过自动生成多样化的语义保持变换，既能攻击现有模型，也能增强模型对现实世界代码变换的鲁棒性

Abstract: Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.

</details>


### [51] [Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring](https://arxiv.org/abs/2512.06060)
*Mohanakrishnan Hariharan*

Main category: cs.SE

TL;DR: 提出一个结合强化学习与自主智能体的框架，用于从业务需求文档自动生成软件测试用例，通过质量工程反馈实现持续改进。


<details>
  <summary>Details</summary>
Motivation: 传统基于大语言模型的系统从静态知识库生成测试用例，缺乏随时间改进性能的能力。需要一种能够从质量工程反馈中学习并持续优化的自动化测试生成方法。

Method: 提出强化学习增强的智能体RAG框架，结合专业智能体与混合向量-图知识库，使用PPO和DQN算法根据测试有效性、缺陷检测率和工作流指标优化智能体行为。

Result: 在企业级苹果项目上验证：测试生成准确率从94.8%提升至97.2%（提高2.4%），缺陷检测率提升10.8%。

Conclusion: 该框架建立了由质量工程专业知识驱动的持续知识精炼循环，逐步提升测试用例质量，增强而非替代人工测试能力。

Abstract: This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.

</details>


### [52] [Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples](https://arxiv.org/abs/2512.06123)
*Qilin Zhou,Zhengyuan Wei,Haipeng Wang,Zhuo Wang,W. K. Chan*

Main category: cs.SE

TL;DR: HiCert是一种新颖的基于掩码的认证检测技术，针对对抗性补丁攻击提供全面的补丁鲁棒性认证，能够认证更多良性样本（包括不一致和一致的样本），在无警告样本上实现更高准确率并显著降低虚假静默率。


<details>
  <summary>Details</summary>
Motivation: 现有的认证检测方法在认证被错误分类或其突变体被不一致预测到不同标签的样本时效果不佳，需要一种能够系统认证这些不一致样本和一致样本的全面补丁鲁棒性认证方法。

Method: HiCert是一种基于掩码的认证检测技术，通过关注突变体预测标签与真实标签不同的问题，制定了有害样本与其良性对应样本之间的形式关系。通过检查每个良性样本的潜在有害（即不一致）突变体的最大置信度边界，确保每个有害样本要么具有预测与有害样本本身相同的突变体的最小置信度低于此边界，要么至少有一个突变体预测标签与有害样本本身不同。

Result: 实验显示HiCert具有高效性并达到新的最先进性能：认证了显著更多的良性样本（包括不一致和一致的），在无警告样本上实现了显著更高的准确率，并显著降低了虚假静默率。

Conclusion: HiCert是第一个能够为认证检测提供如此全面的补丁鲁棒性认证的工作，通过系统认证不一致样本和一致样本，显著提升了对抗性补丁攻击的防御能力。

Abstract: Patch robustness certification is an emerging kind of provable defense technique against adversarial patch attacks for deep learning systems. Certified detection ensures the detection of all patched harmful versions of certified samples, which mitigates the failures of empirical defense techniques that could (easily) be compromised. However, existing certified detection methods are ineffective in certifying samples that are misclassified or whose mutants are inconsistently pre icted to different labels. This paper proposes HiCert, a novel masking-based certified detection technique. By focusing on the problem of mutants predicted with a label different from the true label with our formal analysis, HiCert formulates a novel formal relation between harmful samples generated by identified loopholes and their benign counterparts. By checking the bound of the maximum confidence among these potentially harmful (i.e., inconsistent) mutants of each benign sample, HiCert ensures that each harmful sample either has the minimum confidence among mutants that are predicted the same as the harmful sample itself below this bound, or has at least one mutant predicted with a label different from the harmful sample itself, formulated after two novel insights. As such, HiCert systematically certifies those inconsistent samples and consistent samples to a large extent. To our knowledge, HiCert is the first work capable of providing such a comprehensive patch robustness certification for certified detection. Our experiments show the high effectiveness of HiCert with a new state-of the-art performance: It certifies significantly more benign samples, including those inconsistent and consistent, and achieves significantly higher accuracy on those samples without warnings and a significantly lower false silent ratio.

</details>


### [53] [Systematically Thinking about the Complexity of Code Structuring Exercises at Introductory Level](https://arxiv.org/abs/2512.06178)
*Georgiana Haldeman,Peter Ohmann,Paul Denny*

Main category: cs.SE

TL;DR: 提出一个评估代码结构任务复杂度的框架，用于教授分解与抽象技能，包含三个维度：重复、代码模式和数据依赖


<details>
  <summary>Details</summary>
Motivation: 分解与抽象是计算思维的重要组成部分，但在入门编程课程中常被忽视。随着生成式AI降低语法重要性并提升高级代码推理的重要性，现在有新的机会来明确教授分解与抽象技能

Method: 引入一个系统评估代码结构任务复杂度的框架，定义三个维度（重复、代码模式、数据依赖），每个维度有多个复杂度级别。提供示例任务映射和交互工具来生成和探索分解与抽象问题

Result: 开发了一个支持教育任务设计的框架，帮助学生在过程式编程范式中培养分解与抽象技能。框架提供了任务复杂度评估的具体维度和级别

Conclusion: 该框架为教育工作者提供了系统的方法来设计和评估分解与抽象教学任务，有助于在生成式AI时代更好地培养学生的计算思维和高级代码推理能力

Abstract: Decomposition and abstraction is an essential component of computational thinking, yet it is not always emphasized in introductory programming courses. In addition, as generative AI further reduces the focus on syntax and increases the importance of higher-level code reasoning, there is renewed opportunity to teach DA explicitly. In this paper, we introduce a framework for systematically assessing the complexity of code structuring tasks, where students must identify and separate meaningful abstractions within existing, unstructured code. The framework defines three dimensions of task complexity, each with multiple levels: repetition, code pattern, and data dependency. To support practical use, we provide example tasks mapped to these levels and offer an interactive tool for generating and exploring DA problems. The framework is designed to support the development of educational tasks that build students' skills with DA in the procedural paradigm.

</details>


### [54] [CFCEval: Evaluating Security Aspects in Code Generated by Large Language Models](https://arxiv.org/abs/2512.06248)
*Cheng Cheng,Jinqiu Yang*

Main category: cs.SE

TL;DR: CFCEval是一个评估代码生成大语言模型质量和安全性的新框架，通过MLVBench基准和ELRM指标解决现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成大语言模型的评估存在方法学严谨性和全面性不足的问题，包括数据集偏差、CodeBLEU指标的缺陷（不精确的分词、结构限制、参考多样性低），需要更有效的评估框架来同时衡量代码质量和安全性。

Method: 提出CFCEval框架，包含：1）创建MLVBench新基准来缓解数据集偏差；2）设计ELRM新指标评估参考代码与生成代码的相关性；3）从四个维度评估生成代码：编程质量、漏洞修复能力、后转换修复能力和相关性。

Result: 实验表明CFCEval能更有效地捕捉生成代码的质量和安全方面，且其ELRM指标比CodeBLEU更接近人类判断，为代码生成大语言模型的评估提供了更可靠的方法。

Conclusion: CFCEval框架通过解决现有评估方法的局限性，为代码生成大语言模型的评估提供了更全面、更准确的解决方案，有助于推动该领域的进一步发展。

Abstract: Code-focused Large Language Models (LLMs), such as CodeX and Star-Coder, have demonstrated remarkable capabilities in enhancing developer productivity through context-aware code generation. However, evaluating the quality and security of LLM-generated code remains a significant challenge. Existing evaluation protocols for Code LLMs lack both methodological rigor and comprehensive scope. A key limitation is dataset bias, which arises from unintentional overlap between training and testing data. Furthermore, while CodeBLEU, a BLEU-based metric, is widely used to assess code similarity, it suffers from critical shortcomings, including imprecise tokenization, structural limitations, and low reference diversity. To address these challenges, we introduce CFCEval, a novel framework for evaluating the quality and security of code generated by LLMs. CFCEval mitigates dataset bias by creating a new benchmark, MLVBench, and incorporates ELRM, a new metric designed to assess the relevance between reference code and generated code. CFCEval evaluates generated code across four dimensions: programming quality, vulnerability-fixing capability, post-transformation fixing capability, and relevance. Our experiments show that CFCEval not only captures both quality and security aspects of generated code more effectively but also that its ELRM aligns more closely with human judgments than CodeBLEU, thus paving the way for future advancements in Code LLMs evaluation.

</details>


### [55] [Translating PL/I Macro Procedures into Java Using Automatic Templatization and Large Language Models](https://arxiv.org/abs/2512.06448)
*Takaaki Tateishi,Yasuharu Katsuno*

Main category: cs.SE

TL;DR: 本文提出了一种名为"模板化"的新方法，通过符号执行生成代码模板作为中间表示，帮助LLM将包含宏过程的PL/I程序翻译成可维护的Java代码。


<details>
  <summary>Details</summary>
Motivation: 企业系统现代化需要将PL/I程序翻译成Java等现代语言，但PL/I宏过程作为字符串操作程序会生成PL/I代码，这使得自动化翻译变得复杂。现有的LLM方法难以将PL/I宏过程翻译成能够重现原始宏生成PL/I代码行为的Java程序。

Method: 提出模板化方法：使用符号执行生成代码模板（带有命名占位符的代码）作为中间表示。将符号值视为宏生成代码的一部分，通过符号执行宏过程并生成代码模板，帮助LLM生成可读且可维护的Java代码。

Result: 在10个PL/I宏过程上的初步实验表明，通过模板化的LLM翻译成功生成了能够重现宏生成PL/I程序行为的Java程序。

Conclusion: 模板化方法通过符号执行生成代码模板作为中间表示，有效解决了LLM在翻译PL/I宏过程时的困难，能够生成行为正确且可维护的Java代码。

Abstract: Modernizing legacy enterprise systems often involves translating PL/I programs into modern languages such as Java. This task becomes significantly more complex when PL/I macro procedures are involved. The PL/I macro procedures are considered string-manipulating programs that generate PL/I code, and they make automated translation more complex. Recently, large language models (LLMs) have been explored for automated code translation. However, LLM-based code translation struggles to translate the PL/I macro procedures to Java programs that reproduce the behavior of the plain PL/I code generated by the original PL/I macro procedures.
  This paper proposes a novel method called templatization, which uses symbolic execution to generate code templates (code with named placeholders) as an intermediate representation. In this approach, symbolic values are treated as parts of macro-generated code. By symbolically executing macro procedures and generating code templates, our approach facilitates LLMs to generate readable and maintainable Java code. Our preliminary experiment on ten PL/I macro procedures shows that the LLM-based translation through templatization successfully generates Java programs that reproduce the behavior of the macro-generated PL/I programs.

</details>


### [56] [Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs](https://arxiv.org/abs/2512.06836)
*Weixing Zhang,Regina Hebig,Daniel Strüber*

Main category: cs.SE

TL;DR: LLM可用于文本DSL语法和实例的协同演化，在小规模案例中表现良好，但面临大规模实例的可扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 文本DSL语法演化时，现有模型驱动工程中的协同演化技术会丢失注释和布局等辅助信息，这些信息对软件理解和维护很重要。需要探索能保留这些信息的解决方案。

Method: 使用Claude-3.5和GPT-4o两种先进语言模型，在七个案例语言上进行实验，评估LLM在直接处理文本实例时实现语法和实例协同演化的可行性。

Result: LLM在小规模实例（代表实践中遇到的部分案例）中迁移文本实例的能力良好，但在扩展到更大实例时面临显著的可扩展性挑战。

Conclusion: LLM在文本DSL协同演化方面有潜力，特别是在保留辅助信息方面，但需要解决可扩展性问题，这些发现对未来研究有指导意义。

Abstract: Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.

</details>


### [57] [BabelCoder: Agentic Code Translation with Specification Alignment](https://arxiv.org/abs/2512.06902)
*Fazle Rabbi,Soumit Kanti Saha,Tri Minh Triet Pham,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: BabelCoder是一个基于多智能体协作的代码翻译框架，通过翻译、测试、修复三个专门智能体的分工合作，显著提升了跨语言代码迁移的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统演进，开发者需要在多种编程语言间工作，经常面临代码迁移需求。虽然自动代码翻译提供了有前景的解决方案，但传统方法准确率有限，且未能有效利用代码的上下文和结构信息。现有方法缺乏结构化的多智能体协作框架来提升翻译质量。

Method: BabelCoder采用智能体框架，将代码翻译任务分解为三个专门智能体：翻译智能体负责生成代码，测试智能体负责验证正确性，修复智能体负责修正错误。这种分工协作机制使每个智能体专注于特定方面，共同提升翻译质量。

Result: 在四个基准数据集上与四种最先进基线方法比较，BabelCoder在94%的情况下优于现有方法，提升幅度为0.5%-13.5%，平均准确率达到94.16%。

Conclusion: BabelCoder通过多智能体协作框架有效解决了代码翻译中的准确性问题，为跨语言代码迁移提供了更可靠的自动化解决方案。

Abstract: As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.

</details>


### [58] [MINES: Explainable Anomaly Detection through Web API Invariant Inference](https://arxiv.org/abs/2512.06906)
*Wenjie Zhang,Yun Lin,Chun Fung Amos Kwok,Xiwen Teoh,Xiaofei Xie,Frank Liauw,Hongyu Zhang,Jin Song Dong*

Main category: cs.SE

TL;DR: MINES：一种通过推断API数据库约束来检测Web应用异常的方案，从模式层面而非原始日志实例生成可解释的API不变量，能有效区分日志噪声并检测超出日志记录范围的异常行为。


<details>
  <summary>Details</summary>
Motivation: 现代Web应用基于API运行，其暴露易受攻击或非法访问，导致系统异常。现有日志学习方案面临两个主要问题：异常日志与正常日志相似度高，缺乏关键区分信息；日志实例存在噪声，导致模型学习虚假相关性，产生表面化的异常检测规则。

Method: MINES将API签名转换为表模式以增强原始数据库模式，推断增强数据库模式上的潜在数据库约束，捕捉API与数据库表之间的关系。使用LLM基于两个表结构提取潜在关系，利用正常日志实例来拒绝或接受LLM生成的不变量，最后将推断的约束转换为不变量并生成Python代码验证运行时日志。

Result: 在TrainTicket、NiceFish、Gitea、Mastodon和NextCloud等基准上针对Web篡改攻击进行广泛评估，与LogRobust、LogFormer、WebNorm等基线对比。结果显示MINES在实现高异常召回率的同时几乎引入零误报，达到了新的最先进水平。

Conclusion: MINES通过从模式层面推断可解释的API不变量，有效解决了传统日志学习方案中噪声干扰和虚假相关性问题，为Web应用异常检测提供了更精确、更可靠的解决方案。

Abstract: Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.

</details>


### [59] [Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization](https://arxiv.org/abs/2512.07022)
*Genevieve Caumartin,Glaucia Melo*

Main category: cs.SE

TL;DR: 该论文研究如何利用LLM驱动的智能体通过轻量级查询重构和摘要来改进文件级缺陷定位性能，相比传统方法显著提升检索准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基于信息检索的缺陷定位方法依赖未处理的缺陷描述，其中常包含噪声信息，导致检索准确率低下。虽然大型语言模型在查询重构方面有所改进，但其对智能体性能的影响尚未被充分探索。

Method: 使用开源、未经微调的大型语言模型从缺陷报告中提取关键信息（如标识符和代码片段），并进行检索前的查询重构。智能体随后使用这些预处理后的查询来编排BM25检索，实现大规模自动化的定位工作流程。

Result: 采用最佳查询重构技术后，智能体在首次文件检索中的排名比BM25基线提高了35%，相比SWE-agent的文件检索性能提升了高达22%。

Conclusion: LLM驱动的智能体通过轻量级查询重构和摘要能够显著提升文件级缺陷定位的准确性和效率，为大规模软件仓库中的缺陷定位提供了有效的自动化解决方案。

Abstract: Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.

</details>


### [60] [RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations](https://arxiv.org/abs/2512.07122)
*Liping Han,Tingting Nie,Le Yu,Mingzhe Hu,Tao Yue*

Main category: cs.SE

TL;DR: 提出基于大语言模型的无人机风险配置实时修复方法RisConFix，通过监控飞行状态并利用LLM分析参数关系生成修复方案，在ArduPilot案例中达到97%修复成功率。


<details>
  <summary>Details</summary>
Motivation: 无人机飞行控制软件包含大量可配置参数，虽然厂商提供了推荐值以确保安全稳定运行，但某些推荐参数组合仍可能导致不稳定飞行行为，降低无人机鲁棒性。需要一种实时修复风险配置的方法。

Method: 提出RisConFix方法：1）持续监控无人机运行状态；2）检测到异常飞行行为时自动触发修复机制；3）利用大语言模型分析配置参数与飞行状态的关系；4）生成纠正性参数更新以恢复飞行稳定性；5）采用迭代过程确保配置有效性，如果异常持续则触发下一修复周期。

Result: 在ArduPilot案例研究中（包含1,421组错误配置），RisConFix实现了最佳修复成功率97%，最优平均修复次数1.17次，证明其能够有效且高效地实时修复风险配置。

Conclusion: RisConFix通过结合大语言模型和迭代修复机制，能够实时检测并修复无人机飞行控制软件中的风险配置参数，显著提高无人机系统的鲁棒性和安全性。

Abstract: Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.

</details>


### [61] [Towards Benchmarking Design Pattern Detection Under Obfuscation: Reproducing and Evaluating Attention-Based Detection Method](https://arxiv.org/abs/2512.07193)
*Manthan Shenoy,Andreas Rausch*

Main category: cs.SE

TL;DR: 该论文研究了基于注意力的设计模式检测分类器在语义鲁棒性方面的局限性，发现它们过度依赖表面语法特征，在代码混淆后性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估基于注意力的设计模式检测方法（DPDAtt）的语义鲁棒性，特别是它们对结构和行为语义的依赖程度，以了解这些分类器是否真正理解了代码的深层语义。

Method: 方法包括：1）复现DPDAtt（基于注意力的设计模式检测方法）；2）创建混淆版的DPDAtt语料库，替换类名、方法名等标识符和字符串字面量，同时保留控制流、继承和逻辑；3）在混淆数据集上评估分类器性能。

Result: 研究发现DPDAtt中的训练分类器显著依赖表面语法特征，当这些线索通过混淆被移除时，会导致大量误分类，表明这些分类器未能真正理解代码的深层语义。

Conclusion: 结论强调需要开发更鲁棒的检测工具，能够捕获源代码的深层语义含义。作者提出的混淆语料库（包含34个Java源文件）可作为可重复使用的概念验证基准，用于评估最先进的设计模式检测器在真实语义泛化能力方面的表现。

Abstract: This paper investigates the semantic robustness of attention-based classifiers for design pattern detection, particularly focusing on their reliance on structural and behavioral semantics. We reproduce the DPDAtt, an attention-based design pattern detection approach using learning-based classifiers, and evaluate its performance under obfuscation. To this end, we curate an obfuscated version of the DPDAtt Corpus, where the name identifiers in code such as class names, method names, etc., and string literals like print statements and comment blocks are replaced while preserving control flow, inheritance, and logic. Our findings reveal that these trained classifiers in DPDAtt depend significantly on superficial syntactic features, leading to substantial misclassification when such cues are removed through obfuscation. This work highlights the need for more robust detection tools capable of capturing deeper semantic meanings in source code. We propose our curated Obfuscated corpus (containing 34 Java source files) as a reusable proof-of-concept benchmark for evaluating state-of-the-art design pattern detectors on their true semantic generalization capabilities.

</details>


### [62] [The Human Need for Storytelling: Reflections on Qualitative Software Engineering Research With a Focus Group of Experts](https://arxiv.org/abs/2512.07293)
*Roberto Verdecchia,Justus Bogner*

Main category: cs.SE

TL;DR: 本文通过专家小组讨论，回顾了软件工程领域定性研究的现状、重要性、发展历程、当前面临的障碍以及未来展望。


<details>
  <summary>Details</summary>
Motivation: 在软件工程研究长期以定量方法为主导的背景下，定性研究自20世纪80年代末引入以来逐渐获得认可。本文旨在通过专家讨论，反思定性软件工程研究的现状，探讨其重要性、发展历程、当前挑战和未来方向。

Method: 采用专家焦点小组讨论的形式，邀请了三位国际专家（Rashina Hoda、Carolyn Seaman、Klaas Stol）参与对话。讨论于2025年10月25日进行，内容经过主持人和编辑整理，作为ACM SIGSOFT SEN-ESE专栏文章发表。

Result: 专家们讨论了定性研究在软件工程领域的重要性、历史演变过程、当前实践中的常见障碍，并对定性研究的未来发展方向进行了展望。讨论内容反映了定性研究在软件工程领域的现状和面临的挑战。

Conclusion: 定性研究在软件工程领域已经建立了自己的地位，但仍面临一些实践障碍。通过专家讨论，本文为理解定性研究的价值、历史发展和未来趋势提供了重要见解，有助于推动该领域的发展。

Abstract: From its first adoption in the late 80s, qualitative research has slowly but steadily made a name for itself in what was, and perhaps still is, the predominantly quantitative software engineering (SE) research landscape. As part of our regular column on empirical software engineering (ACM SIGSOFT SEN-ESE), we reflect on the state of qualitative SE research with a focus group of experts. Among other things, we discuss why qualitative SE research is important, how it evolved over time, common impediments faced while practicing it today, and what the future of qualitative SE research might look like. Joining the conversation are Rashina Hoda (Monash University, Australia), Carolyn Seaman (University of Maryland, United States), and Klaas Stol (University College Cork, Ireland). The content of this paper is a faithful account of our conversation from October 25, 2025, which we moderated and edited for our column.

</details>


### [63] [Do LLMs Trust the Code They Write?](https://arxiv.org/abs/2512.07404)
*Francisco Ribeiro,Claudio Spiess,Prem Devanbu,Sarah Nadi*

Main category: cs.SE

TL;DR: 论文探索大语言模型内部是否编码了代码正确性表示，通过对比正确和错误代码的隐藏状态来提取这种表示，并利用它改进代码生成质量评估


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码生成方面很有效，但经常输出错误代码。模型输出概率与正确性相关性不强，且只反映生成过程的最终输出。受LLM内部编码真实性概念的启发，探索LLM是否类似地表示代码正确性

Method: 通过对比同一编程任务下正确和错误代码对的隐藏状态，在四个LLM中识别代码正确性表示。利用提取的正确性表示改进代码质量评估，无需测试执行

Result: 提取的正确性表示优于标准对数似然排名和模型口头置信度。内部正确性信号可用于选择更高质量的代码样本，无需测试执行

Conclusion: 利用内部表示可以增强代码生成系统，使LLM更可靠，从而提高对自动生成代码的信心。这项工作展示了如何通过内部表征改进代码生成质量评估

Abstract: Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.

</details>


### [64] [AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution](https://arxiv.org/abs/2512.07501)
*Weilin Luo,Xueyi Liang,Haotian Deng,Yanan Liu,Hai Wan*

Main category: cs.SE

TL;DR: AutoICE：基于LLM驱动进化搜索的自动形式化验证代码合成方法，通过多样化初始化、协作交叉和自反思变异，显著提升代码验证成功率至90.36%


<details>
  <summary>Details</summary>
Motivation: 从自然语言需求自动合成可验证代码能确保软件正确性和可靠性，降低形式化方法的应用门槛。现有方法因缺乏领域特定预训练语料库而存在严重语法和语义错误，且难以有效形式化隐式知识。

Method: 提出AutoICE方法，采用LLM驱动的进化搜索框架，包含：1）多样化个体初始化；2）协作交叉实现多样化迭代更新；3）自反思变异促进隐式知识发现，以减轻单智能体迭代中的错误传播问题。

Result: AutoICE成功验证90.36%的代码，优于现有最佳方法。在开发者友好数据集变体上，验证成功率达到88.33%，显著超越现有最佳方法的65%成功率。

Conclusion: AutoICE通过进化搜索框架有效解决了现有自动形式化方法中的语法语义错误和隐式知识形式化问题，显著提升了代码验证成功率，为自然语言到可验证代码的自动合成提供了有效解决方案。

Abstract: Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\% verification success rate, significantly surpassing the $65$\% success rate of the SOTA approach.

</details>


### [65] [Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach](https://arxiv.org/abs/2512.07814)
*Hua Yang,Alejandro Velasco,Sen Fang,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 该研究发现代码大语言模型中不同PII类型的泄露风险存在显著差异，且泄露风险与训练动态相关：易学习的PII类型（如IP地址）泄露风险更高，而难学习的类型（如密钥和密码）泄露较少。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多将PII视为单一类别，忽视了不同类型PII的异质性风险。本研究旨在探究不同PII类型在代码大语言模型中被学习和泄露的可能性是否存在差异，以及这种关系是否具有因果性。

Method: 方法包括：构建包含多种PII类型的数据集；对不同规模的代表性模型进行微调；计算真实PII数据的训练动态；构建结构因果模型来估计可学习性对泄露的因果效应。

Result: 结果显示：不同PII类型的泄露风险存在显著差异，且与训练动态相关。易学习的实例（如IP地址）表现出更高的泄露率，而较难的类型（如密钥和密码）泄露频率较低。模糊类型表现出混合行为。

Conclusion: 本研究首次提供了泄露风险具有类型依赖性的因果证据，并为开发类型感知和可学习性感知的代码大语言模型防御机制提供了指导。

Abstract: Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.

</details>


### [66] [Studying the Role of Reusing Crowdsourcing Knowledge in Software Development](https://arxiv.org/abs/2512.07824)
*Rabe Abdalkareem*

Main category: cs.SE

TL;DR: 研究通过大规模实证分析发现，重用Stack Overflow和npm等众包平台知识能提升开发效率，但会增加依赖开销和维护成本，需要改进持续集成来降低风险。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注众包对开发效率和上市时间的影响，但缺乏对软件质量的实证研究，且不清楚开发者如何使用众包知识。

Method: 对Stack Overflow和npm等知名众包平台进行大规模实证研究，分析重用众包知识对软件项目的影响。

Result: 重用众包知识（特别是代码）能辅助软件开发实践，但会导致软件项目面临依赖开销增加、维护工作量上升等质量问题。

Conclusion: 基于研究发现，通过改进持续集成（CI）等软件质量保证方法，可以降低依赖众包知识的风险，提高开发者生产力并节约资源。

Abstract: Crowdsourcing platforms, such as Stack Overflow, have changed and impacted the software development practice. In these platforms, developers share and reuse their software development and programming experience. Therefore, a plethora of research work focused on crowdsourcing in software engineering and showed that, among other things, crowdsourced development tends to increase developers' productivity and reduce time-to-market. However, in crowdsourcing, the empirical studies of software quality are lacking, and simple questions, such as what developers use the crowdsourcing knowledge for, are unanswered.
  Therefore, our research focused on studying the impact of reusing crowdsourcing knowledge on software projects. To do so, we conduct several large-scale empirical studies on some of the well-known crowdsourcing platforms, including Stack Overflow and npm. Our results showed that reusing knowledge from these crowdsourcing platforms has the potential to assist software development practice, specifically in the form of reusing crowdsourced code. However, using such knowledge affects the quality of the software in several aspects, such as making the software projects suffer from dependency overhead and increasing the maintenance effort. Based on these findings, we use the gained knowledge to make sound data-driven decisions where we examine software quality assurance methods to mitigate the risk of relying on crowd sourcing knowledge in software development. We examine the use of continuous integration (CI). Our analysis showed how CI can be improved to increase developers' productivity and save their resources.

</details>
