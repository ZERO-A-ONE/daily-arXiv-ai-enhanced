{"id": "2506.11285", "categories": ["cs.MA", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11285", "abs": "https://arxiv.org/abs/2506.11285", "authors": ["Jianhong Wang", "Yang Li", "Samuel Kaski", "Jonathan Lawry"], "title": "Shapley Machine: A Game-Theoretic Framework for N-Agent Ad Hoc Teamwork", "comment": "25 pages", "summary": "Open multi-agent systems are increasingly important in modeling real-world\napplications, such as smart grids, swarm robotics, etc. In this paper, we aim\nto investigate a recently proposed problem for open multi-agent systems,\nreferred to as n-agent ad hoc teamwork (NAHT), where only a number of agents\nare controlled. Existing methods tend to be based on heuristic design and\nconsequently lack theoretical rigor and ambiguous credit assignment among\nagents. To address these limitations, we model and solve NAHT through the lens\nof cooperative game theory. More specifically, we first model an open\nmulti-agent system, characterized by its value, as an instance situated in a\nspace of cooperative games, generated by a set of basis games. We then extend\nthis space, along with the state space, to accommodate dynamic scenarios,\nthereby characterizing NAHT. Exploiting the justifiable assumption that basis\ngame values correspond to a sequence of n-step returns with different horizons,\nwe represent the state values for NAHT in a form similar to $\\lambda$-returns.\nFurthermore, we derive Shapley values to allocate state values to the\ncontrolled agents, as credits for their contributions to the ad hoc team.\nDifferent from the conventional approach to shaping Shapley values in an\nexplicit form, we shape Shapley values by fulfilling the three axioms uniquely\ndescribing them, well defined on the extended game space describing NAHT. To\nestimate Shapley values in dynamic scenarios, we propose a TD($\\lambda$)-like\nalgorithm. The resulting reinforcement learning (RL) algorithm is referred to\nas Shapley Machine. To our best knowledge, this is the first time that the\nconcepts from cooperative game theory are directly related to RL concepts. In\nexperiments, we demonstrate the effectiveness of Shapley Machine and verify\nreasonableness of our theory."}
{"id": "2506.11212", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.11212", "abs": "https://arxiv.org/abs/2506.11212", "authors": ["Carla F. Griggio", "Boel Nelson", "Zefan Sramek", "Aslan Askarov"], "title": "User Perceptions and Attitudes Toward Untraceability in Messaging Platforms", "comment": null, "summary": "Mainstream messaging platforms offer a variety of features designed to\nenhance user privacy, such as disappearing messages, password-protected chats,\nand end-to-end encryption (E2EE), which primarily protect message contents.\nBeyond contents, the transmission of messages generates metadata that can\nreveal who communicates with whom, when and how often. In this paper, we study\nuser perceptions of \"untraceability\", i.e., preventing third parties from\ntracing who communicates with whom, with the goal of informing the design of\nprivacy-enhancing features in messaging platforms and untraceable communication\nprotocols that depend on large anonymity sets and widespread user adoption. We\nexplore this from a broad conceptual standpoint: rather than studying mental\nmodels of a particular solution, we analyze how users reason about what\nfeatures should be incorporated by two fictitious platforms, Texty and Chatty,\nto prevent third parties from knowing who communicates with whom. Through a\nvignette-based survey with 189 participants, we found that users associate the\nconcept of untraceability with a wide range of privacy enhancing technologies,\nimplying a diverse set of threat models. Overall, the features suggested by\nparticipants show awareness of privacy threats stemming from forms of\nsurveillance and unauthorized access to message contents. Many participants\nalso associated untraceability with the notion of anonymity, but interpreted it\nas senders and receivers concealing their identity from each other rather than\nonly from third parties. We discuss the gap between users' perceptions of\nuntraceability and the threat models addressed by untraceable communication\nprotocols, as well as how different privacy attitudes point to challenges and\nopportunities for the adoption of untraceable communication tools in messaging\nplatforms."}
{"id": "2506.11475", "categories": ["cs.MA", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11475", "abs": "https://arxiv.org/abs/2506.11475", "authors": ["Syeda Kisaa Fatima", "Tehreem Zubair", "Noman Ahmed", "Asifullah Khan"], "title": "AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction", "comment": null, "summary": "This paper introduces LUCID-MA (Learning and Understanding Crime through\nDialogue of Multiple Agents), an innovative AI powered framework where multiple\nAI agents collaboratively analyze and understand crime data. Our system that\nconsists of three core components: an analysis assistant that highlights\nspatiotemporal crime patterns, a feedback component that reviews and refines\nanalytical results and a prediction component that forecasts future crime\ntrends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it\nruns completely offline and allows the agents undergo self-improvement through\n100 rounds of communication with less human interaction. A scoring function is\nincorporated to evaluate agent's performance, providing visual plots to track\nlearning progress. This work demonstrates the potential of AutoGen-style agents\nfor autonomous, scalable, and iterative analysis in social science domains\nmaintaining data privacy through offline execution."}
{"id": "2506.11325", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.11325", "abs": "https://arxiv.org/abs/2506.11325", "authors": ["Evangelos Froudakis", "Athanasios Avgetidis", "Sean Tyler Frankum", "Roberto Perdisci", "Manos Antonakakis", "Angelos Keromytis"], "title": "Uncovering Reliable Indicators: Improving IoC Extraction from Threat Reports", "comment": null, "summary": "Indicators of Compromise (IoCs) are critical for threat detection and\nresponse, marking malicious activity across networks and systems. Yet, the\neffectiveness of automated IoC extraction systems is fundamentally limited by\none key issue: the lack of high-quality ground truth. Current extraction tools\nrely either on manually extracted ground truth, which is labor-intensive and\ncostly, or on automated ground truth creation methods that include\nnon-malicious artifacts, leading to inflated false positive (FP) rates and\nunreliable threat intelligence. In this work, we analyze the shortcomings of\nexisting ground truth creation strategies and address them by introducing the\nfirst hybrid human-in-the-loop pipeline for IoC extraction, which combines a\nlarge language model-based classifier (LANCE) with expert analyst validation.\nOur system improves precision through explainable, context-aware labeling and\nreduces analysts' work factor by 43% compared to manual annotation, as\ndemonstrated in our evaluation with six analysts. Using this approach, we\nproduce PRISM, a high-quality, publicly available benchmark of 1,791 labeled\nIoCs from 50 real-world threat reports. PRISM supports both fair evaluation and\ntraining of IoC extraction methods and enables reproducible research grounded\nin expert-validated indicators."}
{"id": "2506.11803", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.11803", "abs": "https://arxiv.org/abs/2506.11803", "authors": ["Yingfan Deng", "Anhao Zhou", "Yuan Yuan", "Xian Zhang", "Yifei Zou", "Dongxiao Yu"], "title": "PE-MA: Parameter-Efficient Co-Evolution of Multi-Agent Systems", "comment": "arXiv admin note: text overlap with arXiv:2312.10815 by other authors", "summary": "Multi-Agent Systems have recently emerged as a promising paradigm for\ncollaborative reasoning and solving complex tasks. However, the design of\ncollaborative learning algorithms in multi-agent systems faces several\nchallenges, including high communication overhead and insufficient agent-level\npersonalization. In this paper, we propose PE-MA (Parameter-Efficient\nMulti-Agent Co-Evolution), a novel collaboration framework that supports\nefficient, scalable, and personalized co-evolution in multi-agent systems. In\nPE-MA, each agent maintains a lightweight personalized adapter to support\nagent-specific behavior, while a shared adapter is collaboratively optimized\nacross neighboring agents. This design balances global coordination with local\nadaptation under heterogeneous environments. We achieve an asymptotically\noptimal convergence rate of O( 1/(NK)^(1/2) ), where N is the number of agents\nand K the local update steps."}
{"id": "2506.11423", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.11423", "abs": "https://arxiv.org/abs/2506.11423", "authors": ["Manish Bhatt"], "title": "Bhatt Conjectures: On Necessary-But-Not-Sufficient Benchmark Tautology for Human Like Reasoning", "comment": null, "summary": "Debates about whether Large Language or Reasoning Models (LLMs/LRMs) truly\nreason or merely pattern-match suffer from shifting goal posts. In my personal\nopinion, two analytic--hence \"tautological\"--benchmarks cut through that fog in\nmy mental model. In this paper, I attempt to write down my mental model in\nconcrete terms."}
{"id": "2506.11444", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.11444", "abs": "https://arxiv.org/abs/2506.11444", "authors": ["Kecen Li", "Zhicong Huang", "Xinwen Hou", "Cheng Hong"], "title": "GaussMarker: Robust Dual-Domain Watermark for Diffusion Models", "comment": "Accepted at ICML 2025", "summary": "As Diffusion Models (DM) generate increasingly realistic images, related\nissues such as copyright and misuse have become a growing concern. Watermarking\nis one of the promising solutions. Existing methods inject the watermark into\nthe single-domain of initial Gaussian noise for generation, which suffers from\nunsatisfactory robustness. This paper presents the first dual-domain DM\nwatermarking approach using a pipelined injector to consistently embed\nwatermarks in both the spatial and frequency domains. To further boost\nrobustness against certain image manipulations and advanced attacks, we\nintroduce a model-independent learnable Gaussian Noise Restorer (GNR) to refine\nGaussian noise extracted from manipulated images and enhance detection\nrobustness by integrating the detection scores of both watermarks. GaussMarker\nefficiently achieves state-of-the-art performance under eight image distortions\nand four advanced attacks across three versions of Stable Diffusion with better\nrecall and lower false positive rates, as preferred in real applications."}
{"id": "2506.10984", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10984", "abs": "https://arxiv.org/abs/2506.10984", "authors": ["Ahilan Ayyachamy Nadar Ponnusamy"], "title": "Application Modernization with LLMs: Addressing Core Challenges in Reliability, Security, and Quality", "comment": null, "summary": "AI-assisted code generation tools have revolutionized software development,\noffering unprecedented efficiency and scalability. However, multiple studies\nhave consistently highlighted challenges such as security vulnerabilities,\nreliability issues, and inconsistencies in the generated code. Addressing these\nconcerns is crucial to unlocking the full potential of this transformative\ntechnology. While advancements in foundational and code-specialized language\nmodels have made notable progress in mitigating some of these issues,\nsignificant gaps remain, particularly in ensuring high-quality, trustworthy\noutputs.\n  This paper builds upon existing research on leveraging large language models\n(LLMs) for application modernization. It explores an opinionated approach that\nemphasizes two core capabilities of LLMs: code reasoning and code generation.\nThe proposed framework integrates these capabilities with human expertise to\ntackle application modernization challenges effectively. It highlights the\nindispensable role of human involvement and guidance in ensuring the success of\nAI-assisted processes.\n  To demonstrate the framework's utility, this paper presents a detailed case\nstudy, walking through its application in a real-world scenario. The analysis\nincludes a step-by-step breakdown, assessing alternative approaches where\napplicable. This work aims to provide actionable insights and a robust\nfoundation for future research in AI-driven application modernization. The\nreference implementation created for this paper is available on GitHub."}
{"id": "2506.11458", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.11458", "abs": "https://arxiv.org/abs/2506.11458", "authors": ["Dustin Ray", "Caroline El Jazmi"], "title": "Computational Attestations of Polynomial Integrity Towards Verifiable Machine-Learning", "comment": "21 pages, Future Technologies Conference (FTC) 2024", "summary": "Machine-learning systems continue to advance at a rapid pace, demonstrating\nremarkable utility in various fields and disciplines. As these systems continue\nto grow in size and complexity, a nascent industry is emerging which aims to\nbring machine-learning-as-a-service (MLaaS) to market. Outsourcing the\noperation and training of these systems to powerful hardware carries numerous\nadvantages, but challenges arise when privacy and the correctness of work\ncarried out must be ensured. Recent advancements in the field of zero-knowledge\ncryptography have led to a means of generating arguments of integrity for any\ncomputation, which in turn can be efficiently verified by any party, in any\nplace, at any time. In this work we prove the correct training of a\ndifferentially-private (DP) linear regression over a dataset of 50,000 samples\non a single machine in less than 6 minutes, verifying the entire computation in\n0.17 seconds. To our knowledge, this result represents the fastest known\ninstance in the literature of provable-DP over a dataset of this size. We\nbelieve this result constitutes a key stepping-stone towards end-to-end private\nMLaaS."}
{"id": "2506.10985", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10985", "abs": "https://arxiv.org/abs/2506.10985", "authors": ["Raman Mohammed Hussein", "Bryar A. Hassan"], "title": "Collaboration Tools and their Role in Agile Software Projects", "comment": "https://www.middleeastconference.org/_files/ugd/614b1f_82fa5f91169a44278723a921b27e2864.pdf\n  ISBN: 979-8-89695-015-8", "summary": "The purpose of this review is to understand the importance of collaboration\ntools which are Slack, Microsoft Teams, Confluence in Agile and software\nprojects. Agile methodologies rely on flexibility, using cycles and integration\nthroughout various levels of developing cycles. However, it is still a great\nproblem for many teams to collaborate and communicate even if staff members and\nteams are working remotely. In terms of collaboration, the applications and\ntechnologies mean better organization of work, increased mutually\nunderstandable openness and fast and efficient inter team and interpersonal\ninteractions to enhance results of projects into productivity. This paper\nexamines how these tools fit the Agile principles, how they facilitate\niterative development, and encouraging effective initiation and tracking of\ntasks in small and large projects. The insights focus on how Slack, Microsoft\nTeams, and Confluence are essential for gaining better task coordination,\nsupporting knowledge sharing, and adopting agile values across cross-functional\ncontexts."}
{"id": "2506.11521", "categories": ["cs.CR", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.11521", "abs": "https://arxiv.org/abs/2506.11521", "authors": ["Jinming Wen", "Xinyi Wu", "Shuai Zhao", "Yanhao Jia", "Yuwen Li"], "title": "Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models", "comment": null, "summary": "Multimodal large language models (MLLMs), which bridge the gap between\naudio-visual and natural language processing, achieve state-of-the-art\nperformance on several audio-visual tasks. Despite the superior performance of\nMLLMs, the scarcity of high-quality audio-visual training data and\ncomputational resources necessitates the utilization of third-party data and\nopen-source MLLMs, a trend that is increasingly observed in contemporary\nresearch. This prosperity masks significant security risks. Empirical studies\ndemonstrate that the latest MLLMs can be manipulated to produce malicious or\nharmful content. This manipulation is facilitated exclusively through\ninstructions or inputs, including adversarial perturbations and malevolent\nqueries, effectively bypassing the internal security mechanisms embedded within\nthe models. To gain a deeper comprehension of the inherent security\nvulnerabilities associated with audio-visual-based multimodal models, a series\nof surveys investigates various types of attacks, including adversarial and\nbackdoor attacks. While existing surveys on audio-visual attacks provide a\ncomprehensive overview, they are limited to specific types of attacks, which\nlack a unified review of various types of attacks. To address this issue and\ngain insights into the latest trends in the field, this paper presents a\ncomprehensive and systematic review of audio-visual attacks, which include\nadversarial attacks, backdoor attacks, and jailbreak attacks. Furthermore, this\npaper also reviews various types of attacks in the latest audio-visual-based\nMLLMs, a dimension notably absent in existing surveys. Drawing upon\ncomprehensive insights from a substantial review, this paper delineates both\nchallenges and emergent trends for future research on audio-visual attacks and\ndefense."}
{"id": "2506.10986", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10986", "abs": "https://arxiv.org/abs/2506.10986", "authors": ["Mouna Dhaouadi", "Bentley James Oakes", "Michalis Famelis"], "title": "CoMRAT: Commit Message Rationale Analysis Tool", "comment": null, "summary": "In collaborative open-source development, the rationale for code changes is\noften captured in commit messages, making them a rich source of valuable\ninformation. However, research on rationale in commit messages remains limited.\nIn this paper, we present CoMRAT, a tool for analyzing decision and rationale\nsentences rationale in commit messages. CoMRAT enables a) researchers to\nproduce metrics and analyses on rationale information in any Github module, and\nb) developers to check the amount of rationale in their commit messages. A\npreliminary evaluation suggests the tool's usefulness and usability in both\nthese research and development contexts."}
{"id": "2506.11586", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11586", "abs": "https://arxiv.org/abs/2506.11586", "authors": ["Shashank Balla"], "title": "SecONNds: Secure Outsourced Neural Network Inference on ImageNet", "comment": null, "summary": "The widespread adoption of outsourced neural network inference presents\nsignificant privacy challenges, as sensitive user data is processed on\nuntrusted remote servers. Secure inference offers a privacy-preserving\nsolution, but existing frameworks suffer from high computational overhead and\ncommunication costs, rendering them impractical for real-world deployment. We\nintroduce SecONNds, a non-intrusive secure inference framework optimized for\nlarge ImageNet-scale Convolutional Neural Networks. SecONNds integrates a novel\nfully Boolean Goldreich-Micali-Wigderson (GMW) protocol for secure comparison\n-- addressing Yao's millionaires' problem -- using preprocessed Beaver's bit\ntriples generated from Silent Random Oblivious Transfer. Our novel protocol\nachieves an online speedup of 17$\\times$ in nonlinear operations compared to\nstate-of-the-art solutions while reducing communication overhead. To further\nenhance performance, SecONNds employs Number Theoretic Transform (NTT)\npreprocessing and leverages GPU acceleration for homomorphic encryption\noperations, resulting in speedups of 1.6$\\times$ on CPU and 2.2$\\times$ on GPU\nfor linear operations. We also present SecONNds-P, a bit-exact variant that\nensures verifiable full-precision results in secure computation, matching the\nresults of plaintext computations. Evaluated on a 37-bit quantized SqueezeNet\nmodel, SecONNds achieves an end-to-end inference time of 2.8 s on GPU and 3.6 s\non CPU, with a total communication of just 420 MiB. SecONNds' efficiency and\nreduced computational load make it well-suited for deploying privacy-sensitive\napplications in resource-constrained environments. SecONNds is open source and\ncan be accessed from: https://github.com/shashankballa/SecONNds."}
{"id": "2506.10987", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10987", "abs": "https://arxiv.org/abs/2506.10987", "authors": ["Shaoyi Yang"], "title": "Chain of Draft for Software Engineering: Challenges in Applying Concise Reasoning to Code Tasks", "comment": null, "summary": "Large language models (LLMs) have become vital tools for software\ndevelopment, but they often require verbose intermediate reasoning for complex\ncode tasks, leading to high latency and costs. This research extends the Chain\nof Draft (CoD) method to software engineering, designing and evaluating\nmultiple CoD variants tailored for code tasks. Through comprehensive\nexperiments on all 300 samples from the SWE-bench benchmark, we found that all\nCoD variants used significantly fewer tokens than Chain of Thought (CoT), with\nBaseline CoD being most efficient at 55.4% of CoT's tokens. While this\nrepresents substantial efficiency gains - translating to approximately 45%\nreduction in processing time and API costs - it differs from the extreme 7.6%\nreported in the original CoD paper for mathematical reasoning. This difference\nstems from the inherent complexity and context-dependency of software tasks,\nwhich require more detailed reasoning to maintain solution quality. Our\nmulti-dimensional quality assessment revealed that CoD variants maintain over\n90% of CoT's code quality across key metrics including correctness,\ncompatibility, and maintainability, making them practical alternatives for\nreal-world development scenarios where efficiency matters. This research\ndemonstrates how domain-specific characteristics influence prompting strategy\neffectiveness and provides a framework for balancing efficiency with solution\nquality in software engineering applications. Our findings offer practical\nguidance for optimizing LLM-based development workflows through appropriate\nprompting strategy selection based on project requirements."}
{"id": "2506.11612", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11612", "abs": "https://arxiv.org/abs/2506.11612", "authors": ["Zhijie Liu", "Qiyi Tang", "Sen Nie", "Shi Wu", "Liang Feng Zhang", "Yutian Tang"], "title": "KEENHash: Hashing Programs into Function-Aware Embeddings for Large-Scale Binary Code Similarity Analysis", "comment": null, "summary": "Binary code similarity analysis (BCSA) is a crucial research area in many\nfields such as cybersecurity. Specifically, function-level diffing tools are\nthe most widely used in BCSA: they perform function matching one by one for\nevaluating the similarity between binary programs. However, such methods need a\nhigh time complexity, making them unscalable in large-scale scenarios (e.g.,\n1/n-to-n search). Towards effective and efficient program-level BCSA, we\npropose KEENHash, a novel hashing approach that hashes binaries into\nprogram-level representations through large language model (LLM)-generated\nfunction embeddings. KEENHash condenses a binary into one compact and\nfixed-length program embedding using K-Means and Feature Hashing, allowing us\nto do effective and efficient large-scale program-level BCSA, surpassing the\nprevious state-of-the-art methods. The experimental results show that KEENHash\nis at least 215 times faster than the state-of-the-art function matching tools\nwhile maintaining effectiveness. Furthermore, in a large-scale scenario with\n5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while\nthese tools will cost at least 56 days. We also evaluate KEENHash on the\nprogram clone search of large-scale BCSA across extensive datasets in 202,305\nbinaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of\nthem by at least 23.16%, and displays remarkable superiority over them in the\nlarge-scale BCSA security scenario of malware detection."}
{"id": "2506.10988", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10988", "abs": "https://arxiv.org/abs/2506.10988", "authors": ["Bowen Tian", "Zhengyang Xu", "Mingqiang Wu", "Songning Lai", "Yutai Yue"], "title": "You Only Train Once: A Flexible Training Framework for Code Vulnerability Detection Driven by Vul-Vector", "comment": "Under Review", "summary": "With the pervasive integration of computer applications across industries,\nthe presence of vulnerabilities within code bases poses significant risks. The\ndiversity of software ecosystems coupled with the intricate nature of modern\nsoftware engineering has led to a shift from manual code vulnerability\nidentification towards the adoption of automated tools. Among these, deep\nlearning-based approaches have risen to prominence due to their superior\naccuracy; however, these methodologies encounter several obstacles. Primarily,\nthey necessitate extensive labeled datasets and prolonged training periods, and\ngiven the rapid emergence of new vulnerabilities, the frequent retraining of\nmodels becomes a resource-intensive endeavor, thereby limiting their\napplicability in cutting-edge scenarios. To mitigate these challenges, this\npaper introduces the \\underline{\\textbf{YOTO}}--\\underline{\\textbf{Y}}ou\n\\underline{\\textbf{O}}nly \\underline{\\textbf{T}}rain \\underline{\\textbf{O}}nce\nframework. This innovative approach facilitates the integration of multiple\ntypes of vulnerability detection models via parameter fusion, eliminating the\nneed for joint training. Consequently, YOTO enables swift adaptation to newly\ndiscovered vulnerabilities, significantly reducing both the time and\ncomputational resources required for model updates."}
{"id": "2506.11635", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11635", "abs": "https://arxiv.org/abs/2506.11635", "authors": ["Shaun Shuster", "Eyal Zaloof", "Asaf Shabtai", "Rami Puzis"], "title": "FAA Framework: A Large Language Model-Based Approach for Credit Card Fraud Investigations", "comment": null, "summary": "The continuous growth of the e-commerce industry attracts fraudsters who\nexploit stolen credit card details. Companies often investigate suspicious\ntransactions in order to retain customer trust and address gaps in their fraud\ndetection systems. However, analysts are overwhelmed with an enormous number of\nalerts from credit card transaction monitoring systems. Each alert\ninvestigation requires from the fraud analysts careful attention, specialized\nknowledge, and precise documentation of the outcomes, leading to alert fatigue.\nTo address this, we propose a fraud analyst assistant (FAA) framework, which\nemploys multi-modal large language models (LLMs) to automate credit card fraud\ninvestigations and generate explanatory reports. The FAA framework leverages\nthe reasoning, code execution, and vision capabilities of LLMs to conduct\nplanning, evidence collection, and analysis in each investigation step. A\ncomprehensive empirical evaluation of 500 credit card fraud investigations\ndemonstrates that the FAA framework produces reliable and efficient\ninvestigations comprising seven steps on average. Thus we found that the FAA\nframework can automate large parts of the workload and help reduce the\nchallenges faced by fraud analysts."}
{"id": "2506.10989", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10989", "abs": "https://arxiv.org/abs/2506.10989", "authors": ["Rogelio Cruz", "Jonatan Contreras", "Francisco Guerrero", "Ezequiel Rodriguez", "Carlos Valdez", "Citlali Carrillo"], "title": "Prompt engineering and framework: implementation to increase code reliability based guideline for LLMs", "comment": null, "summary": "In this paper, we propose a novel prompting approach aimed at enhancing the\nability of Large Language Models (LLMs) to generate accurate Python code.\nSpecifically, we introduce a prompt template designed to improve the quality\nand correctness of generated code snippets, enabling them to pass tests and\nproduce reliable results. Through experiments conducted on two state-of-the-art\nLLMs using the HumanEval dataset, we demonstrate that our approach outperforms\nwidely studied zero-shot and Chain-of-Thought (CoT) methods in terms of the\nPass@k metric. Furthermore, our method achieves these improvements with\nsignificantly reduced token usage compared to the CoT approach, making it both\neffective and resource-efficient, thereby lowering the computational demands\nand improving the eco-footprint of LLM capabilities. These findings highlight\nthe potential of tailored prompting strategies to optimize code generation\nperformance, paving the way for broader applications in AI-driven programming\ntasks."}
{"id": "2506.11669", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.11669", "abs": "https://arxiv.org/abs/2506.11669", "authors": ["Guanjie Li", "Tom H. Luan", "Chengzhe Lai", "Jinkai Zheng", "Rongxing Lu"], "title": "DTHA: A Digital Twin-Assisted Handover Authentication Scheme for 5G and Beyond", "comment": null, "summary": "With the rapid development and extensive deployment of the fifth-generation\nwireless system (5G), it has achieved ubiquitous high-speed connectivity and\nimproved overall communication performance. Additionally, as one of the\npromising technologies for integration beyond 5G, digital twin in cyberspace\ncan interact with the core network, transmit essential information, and further\nenhance the wireless communication quality of the corresponding mobile device\n(MD). However, the utilization of millimeter-wave, terahertz band, and\nultra-dense network technologies presents urgent challenges for MD in 5G and\nbeyond, particularly in terms of frequent handover authentication with target\nbase stations during faster mobility, which can cause connection interruption\nand incur malicious attacks. To address such challenges in 5G and beyond, in\nthis paper, we propose a secure and efficient handover authentication scheme by\nutilizing digital twin. Acting as an intelligent intermediate, the authorized\ndigital twin can handle computations and assist the corresponding MD in\nperforming secure mutual authentication and key negotiation in advance before\nattaching the target base stations in both intra-domain and inter-domain\nscenarios. In addition, we provide the formal verification based on BAN logic,\nRoR model, and ProVerif, and informal analysis to demonstrate that the proposed\nscheme can offer diverse security functionality. Performance evaluation shows\nthat the proposed scheme outperforms most related schemes in terms of\nsignaling, computation, and communication overheads."}
{"id": "2506.10990", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.DC", "I.2.0"], "pdf": "https://arxiv.org/pdf/2506.10990", "abs": "https://arxiv.org/abs/2506.10990", "authors": ["Roberto Vergallo", "Luís Cruz", "Alessio Errico", "Luca Mainetti"], "title": "On the Effectiveness of the 'Follow-the-Sun' Strategy in Mitigating the Carbon Footprint of AI in Cloud Instances", "comment": "24 pages, 4 figures, 10 tables", "summary": "'Follow-the-Sun' (FtS) is a theoretical computational model aimed at\nminimizing the carbon footprint of computer workloads. It involves dynamically\nmoving workloads to regions with cleaner energy sources as demand increases and\nenergy production relies more on fossil fuels. With the significant power\nconsumption of Artificial Intelligence (AI) being a subject of extensive\ndebate, FtS is proposed as a strategy to mitigate the carbon footprint of\ntraining AI models. However, the literature lacks scientific evidence on the\nadvantages of FtS to mitigate the carbon footprint of AI workloads. In this\npaper, we present the results of an experiment conducted in a partial synthetic\nscenario to address this research gap. We benchmarked four AI algorithms in the\nanomaly detection domain and measured the differences in carbon emissions in\nfour cases: no strategy, FtS, and two strategies previously introduced in the\nstate of the art, namely Flexible Start and Pause and Resume. To conduct our\nexperiment, we utilized historical carbon intensity data from the year 2021 for\nseven European cities. Our results demonstrate that the FtS strategy not only\nachieves average reductions of up to 14.6% in carbon emissions (with peaks of\n16.3%) but also helps in preserving the time needed for training."}
{"id": "2506.11679", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11679", "abs": "https://arxiv.org/abs/2506.11679", "authors": ["Tran Thanh Lam Nguyen", "Barbara Carminati", "Elena Ferrari"], "title": "LLMs on support of privacy and security of mobile apps: state of the art and research directions", "comment": null, "summary": "Modern life has witnessed the explosion of mobile devices. However, besides\nthe valuable features that bring convenience to end users, security and privacy\nrisks still threaten users of mobile apps. The increasing sophistication of\nthese threats in recent years has underscored the need for more advanced and\nefficient detection approaches. In this chapter, we explore the application of\nLarge Language Models (LLMs) to identify security risks and privacy violations\nand mitigate them for the mobile application ecosystem. By introducing\nstate-of-the-art research that applied LLMs to mitigate the top 10 common\nsecurity risks of smartphone platforms, we highlight the feasibility and\npotential of LLMs to replace traditional analysis methods, such as dynamic and\nhybrid analysis of mobile apps. As a representative example of LLM-based\nsolutions, we present an approach to detect sensitive data leakage when users\nshare images online, a common behavior of smartphone users nowadays. Finally,\nwe discuss open research challenges."}
{"id": "2506.10991", "categories": ["cs.SE", "D.2.9; H.4.1"], "pdf": "https://arxiv.org/pdf/2506.10991", "abs": "https://arxiv.org/abs/2506.10991", "authors": ["Hoang Vu", "Henrik Leopold", "Han van der Aa"], "title": "What is Business Process Automation Anyway?", "comment": "Accepted at HICSS 2023", "summary": "Many organizations strive to increase the level of automation in their\nbusiness processes. While automation historically was mainly concerned with\nautomating physical labor, current automation efforts mostly focus on\nautomation in a digital manner, thus targeting work that is related to the\ninteraction between humans and computers. This type of automation, commonly\nreferred to as business process automation, has many facets. Yet, academic\nliterature mainly focuses on Robotic Process Automation, a specific automation\ncapability. Recognizing that leading vendors offer automation capabilities\ngoing way beyond that, we use this paper to develop a detailed understanding of\nbusiness process automation in industry. To this end, we conduct a structured\nmarket analysis of the 18 predominant vendors of business process automation\nsolutions as identified by Gartner. As a result, we provide a comprehensive\noverview of the business process automation capabilities currently offered by\nindustrial vendors. We show which types and facets of automation exist and\nwhich aspects represent promising directions for the future."}
{"id": "2506.11687", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.11687", "abs": "https://arxiv.org/abs/2506.11687", "authors": ["Francisco Aguilera-Martínez", "Fernando Berzal"], "title": "Differential Privacy in Machine Learning: From Symbolic AI to LLMs", "comment": "arXiv admin note: text overlap with arXiv:2303.00654 by other authors", "summary": "Machine learning models should not reveal particular information that is not\notherwise accessible. Differential privacy provides a formal framework to\nmitigate privacy risks by ensuring that the inclusion or exclusion of any\nsingle data point does not significantly alter the output of an algorithm, thus\nlimiting the exposure of private information. This survey paper explores the\nfoundational definitions of differential privacy, reviews its original\nformulations and tracing its evolution through key research contributions. It\nthen provides an in-depth examination of how DP has been integrated into\nmachine learning models, analyzing existing proposals and methods to preserve\nprivacy when training ML models. Finally, it describes how DP-based ML\ntechniques can be evaluated in practice. %Finally, it discusses the broader\nimplications of DP, highlighting its potential for public benefit, its\nreal-world applications, and the challenges it faces, including vulnerabilities\nto adversarial attacks. By offering a comprehensive overview of differential\nprivacy in machine learning, this work aims to contribute to the ongoing\ndevelopment of secure and responsible AI systems."}
{"id": "2506.10992", "categories": ["cs.SE", "D.2.9"], "pdf": "https://arxiv.org/pdf/2506.10992", "abs": "https://arxiv.org/abs/2506.10992", "authors": ["Hoang Vu", "Jennifer Haase", "Henrik Leopold", "Jan Mendling"], "title": "Towards a Theory on Process Automation Effects", "comment": "Accepted at HICSS 2023", "summary": "Process automation is a crucial strategy for improving business processes,\nbut little attention has been paid to the effects that automation has once it\nis operational. This paper addresses this research problem by reviewing the\nliterature on human-automation interaction. Although many of the studies in\nthis field have been conducted in different domains, they provide a foundation\nfor developing propositions about process automation effects. Our analysis\nfocuses on how humans perceive automation technology when working within a\nprocess, allowing us to propose an effective engagement model between\ntechnology, process participants, process managers, and software developers.\nThis paper offers insights and recommendations that can help organizations\noptimize their use of process automation. We further derive novel research\nquestions for a discourse within the process automation community."}
{"id": "2506.11939", "categories": ["cs.CR", "cs.AI", "D.2; I.2"], "pdf": "https://arxiv.org/pdf/2506.11939", "abs": "https://arxiv.org/abs/2506.11939", "authors": ["Ranindya Paramitha", "Yuan Feng", "Fabio Massacci"], "title": "Today's Cat Is Tomorrow's Dog: Accounting for Time-Based Changes in the Labels of ML Vulnerability Detection Approaches", "comment": "Accepted at The ACM International Conference on the Foundations of\n  Software Engineering (FSE) 2025. Published in the Proceedings of the ACM on\n  Software Engineering (PACMSE), Issue FSE 2025", "summary": "Vulnerability datasets used for ML testing implicitly contain retrospective\ninformation. When tested on the field, one can only use the labels available at\nthe time of training and testing (e.g. seen and assumed negatives). As\nvulnerabilities are discovered across calendar time, labels change and past\nperformance is not necessarily aligned with future performance. Past works only\nconsidered the slices of the whole history (e.g. DiverseVUl) or individual\ndifferences between releases (e.g. Jimenez et al. ESEC/FSE 2019). Such\napproaches are either too optimistic in training (e.g. the whole history) or\ntoo conservative (e.g. consecutive releases). We propose a method to\nrestructure a dataset into a series of datasets in which both training and\ntesting labels change to account for the knowledge available at the time. If\nthe model is actually learning, it should improve its performance over time as\nmore data becomes available and data becomes more stable, an effect that can be\nchecked with the Mann-Kendall test. We validate our methodology for\nvulnerability detection with 4 time-based datasets (3 projects from BigVul\ndataset + Vuldeepecker's NVD) and 5 ML models (Code2Vec, CodeBERT, LineVul,\nReGVD, and Vuldeepecker). In contrast to the intuitive expectation (more\nretrospective information, better performance), the trend results show that\nperformance changes inconsistently across the years, showing that most models\nare not learning."}
{"id": "2506.10993", "categories": ["cs.SE", "cs.FL", "68N30, 68Q60", "D.2.4; D.2.1; F.3.1"], "pdf": "https://arxiv.org/pdf/2506.10993", "abs": "https://arxiv.org/abs/2506.10993", "authors": ["Muhammad Naeem", "Cristina Seceleanu"], "title": "Contract-based Verification of Digital Twins", "comment": "Accepted at ICECCS 2025, to appear in Lecture Notes in Computer\n  Science (LNCS), Springer", "summary": "Digital twins are becoming powerful tools in industrial applications,\noffering virtual representations of cyber-physical systems. However,\nverification of these models remains a significant challenge due to the\npotentially large datasets used by the digital twin. This paper introduces an\ninnovative methodology for verifying neural network-based digital twin models,\nin a black-box fashion, by integrating model checking into the process. The\nlatter relies on defining and applying system-level contracts that capture the\nsystem's requirements, to verify the behavior of digital twin models,\nimplemented in Simulink. We develop an automated solution that simulates the\ndigital twin model for certain inputs, and feeds the predicted outputs together\nwith the inputs to the contract model described as a network of timed automata\nin the UPPAAL model checker. The latter verifies whether the predicted outputs\nfulfill the specified contracts. This approach allows us to identify scenarios\nwhere the digital twin's behavior fails to meet the contracts, without\nrequiring the digital twin's design technicalities. We apply our method to a\nboiler system case study for which we identify prediction errors via contract\nverification. Our work demonstrates the effectiveness of integrating model\nchecking with digital twin models for continuous improvement."}
{"id": "2506.11954", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11954", "abs": "https://arxiv.org/abs/2506.11954", "authors": ["Eric Filiol"], "title": "Technical Evaluation of a Disruptive Approach in Homomorphic AI", "comment": "This is the extended version of the talk presented at CyberWiseCon\n  2025 in Vilnius, Lituania in May 21$^{st}$-23$^{rd}$, 2025", "summary": "We present a technical evaluation of a new, disruptive cryptographic approach\nto data security, known as HbHAI (Hash-based Homomorphic Artificial\nIntelligence). HbHAI is based on a novel class of key-dependent hash functions\nthat naturally preserve most similarity properties, most AI algorithms rely on.\nAs a main claim, HbHAI makes now possible to analyze and process data in its\ncryptographically secure form while using existing native AI algorithms without\nmodification, with unprecedented performances compared to existing homomorphic\nencryption schemes.\n  We tested various HbHAI-protected datasets (non public preview) using\ntraditional unsupervised and supervised learning techniques (clustering,\nclassification, deep neural networks) with classical unmodified AI algorithms.\nThis paper presents technical results from an independent analysis conducted\nwith those different, off-the-shelf AI algorithms. The aim was to assess the\nsecurity, operability and performance claims regarding HbHAI techniques. As a\nresults, our results confirm most these claims, with only a few minor\nreservations."}
{"id": "2506.10994", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10994", "abs": "https://arxiv.org/abs/2506.10994", "authors": ["April Clarke"], "title": "Improving Software Team Communication Through Social Interventions in Project Management Tools", "comment": "ICSE 2025 Doctoral Track. arXiv admin note: substantial text overlap\n  with arXiv:2502.01923", "summary": "Productive software engineering teams require effective communication and\nbalanced contributions between team members. However, teams are often\nineffective at these skills, which is detrimental to project success.\nProject-based university courses are an opportunity for students to practise\nthese skills, but we have yet to establish how we can guide students towards\nimproving their communication and coordination. We aim to develop project\nmanagement tool features, informed by social network analysis, that nudge\nstudents in software engineering group projects towards beneficial behaviours.\nTo do this, we will first evaluate the suitability of social network analysis\ntechniques for identifying areas of improvement in teams' communication. Then,\nwe will develop features in a project management tool that aid students in\nidentifying and addressing these areas of improvement, and evaluate them in the\ncontext of a software engineering group project."}
{"id": "2506.11970", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2506.11970", "abs": "https://arxiv.org/abs/2506.11970", "authors": ["Chris S. Lin", "Jeonghyun Woo", "Prashant J. Nair", "Gururaj Saileshwar"], "title": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an Efficient in-DRAM Rowhammer Mitigation", "comment": "8 pages, including appendices. The paper is presented at DRAMSec\n  2025. (see https://dramsec.ethz.ch/)", "summary": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM."}
{"id": "2506.10995", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10995", "abs": "https://arxiv.org/abs/2506.10995", "authors": ["Jorge Martinez-Gil"], "title": "Evaluating Small-Scale Code Models for Code Clone Detection", "comment": "20 pages", "summary": "Detecting code clones is relevant to software maintenance and code\nrefactoring. This challenge still presents unresolved cases, mainly when\nstructural similarity does not reflect functional equivalence, though recent\ncode models show promise. Therefore, this research aims to systematically\nmeasure the performance of several newly introduced small code models in\nclassifying code pairs as clones or non-clones. The evaluation is based on five\ndatasets: BigCloneBench, CodeJam, Karnalim, POJ104, and PoolC, as well as six\ncode models: CodeBERT, GraphCodeBERT, Salesforce T5, UniXCoder, PLBART, and\nPolycoder. Most models performed well across standard metrics, including\naccuracy, precision, recall, and F1-score. However, a marginal fraction of\nclones remains challenging to detect, especially when the code looks similar\nbut performs different operations. The source code that illustrates our\napproach is available at:\nhttps://github.com/jorge-martinez-gil/small-code-models"}
{"id": "2506.11022", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11022", "abs": "https://arxiv.org/abs/2506.11022", "authors": ["Shivani Shukla", "Himanshu Joshi", "Romilla Syed"], "title": "Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox", "comment": "Keywords - Large Language Models, Security Vulnerabilities,\n  AI-Generated Code, Iterative Feedback, Software Security, Secure Coding\n  Practices, Feedback Loops, LLM Prompting Strategies", "summary": "The rapid adoption of Large Language Models(LLMs) for code generation has\ntransformed software development, yet little attention has been given to how\nsecurity vulnerabilities evolve through iterative LLM feedback. This paper\nanalyzes security degradation in AI-generated code through a controlled\nexperiment with 400 code samples across 40 rounds of \"improvements\" using four\ndistinct prompting strategies. Our findings show a 37.6% increase in critical\nvulnerabilities after just five iterations, with distinct vulnerability\npatterns emerging across different prompting approaches. This evidence\nchallenges the assumption that iterative LLM refinement improves code security\nand highlights the essential role of human expertise in the loop. We propose\npractical guidelines for developers to mitigate these risks, emphasizing the\nneed for robust human validation between LLM iterations to prevent the\nparadoxical introduction of new security issues during supposedly beneficial\ncode \"improvements\"."}
{"id": "2506.10996", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10996", "abs": "https://arxiv.org/abs/2506.10996", "authors": ["Saadiq Rauf Khan", "Vinit Chandak", "Sougata Mukherjea"], "title": "Evaluating LLMs for Visualization Tasks", "comment": null, "summary": "Information Visualization has been utilized to gain insights from complex\ndata. In recent times, Large Language Models (LLMs) have performed very well in\nmany tasks. In this paper, we showcase the capabilities of different popular\nLLMs to generate code for visualization based on simple prompts. We also\nanalyze the power of LLMs to understand some common visualizations by answering\nsimple questions. Our study shows that LLMs could generate code for some\nvisualizations as well as answer questions about them. However, LLMs also have\nseveral limitations. We believe that our insights can be used to improve both\nLLMs and Information Visualization systems."}
{"id": "2506.10997", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.10997", "abs": "https://arxiv.org/abs/2506.10997", "authors": ["Hanumanthrao Kannan", "Alejandro Salado"], "title": "A Theory-driven Interpretation and Elaboration of Verification and Validation", "comment": null, "summary": "This paper presents a formal theory of verification and validation (V&V)\nwithin systems engineering, grounded in the axiom that V&V are fundamentally\nknowledge-building activities. Using dynamic epistemic modal logic, we develop\nprecise definitions of verification and validation, articulating their roles in\nconfirming and contextualizing knowledge about systems. The theory formalizes\nthe interplay between epistemic states, evidence, and reasoning processes,\nallowing for the derivation of theorems that clarify the conceptual\nunderpinnings of V&V. By providing a formal foundation, this work addresses\nambiguities in traditional V&V practices, offering a structured framework to\nenhance precision and consistency in systems engineering methodologies. The\ninsights gained have implications for both academic research and practical\napplications, fostering a deeper understanding of V&V as critical components of\nengineering knowledge generation."}
{"id": "2506.10998", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10998", "abs": "https://arxiv.org/abs/2506.10998", "authors": ["Kangping Xu", "Yifan Luo", "Yang Yuan", "Andrew Chi-Chih Yao"], "title": "Towards Automated Formal Verification of Backend Systems with LLMs", "comment": null, "summary": "Software testing plays a critical role in ensuring that systems behave as\nintended. However, existing automated testing approaches struggle to match the\ncapabilities of human engineers due to key limitations such as test locality,\nlack of general reliability, and business logic blindness. In this work, we\npropose a novel framework that leverages functional programming and type\nsystems to translate Scala backend code into formal Lean representations. Our\npipeline automatically generates theorems that specify the intended behavior of\nAPIs and database operations, and uses LLM-based provers to verify them. When a\ntheorem is proved, the corresponding logic is guaranteed to be correct and no\nfurther testing is needed. If the negation of a theorem is proved instead, it\nconfirms a bug. In cases where neither can be proved, human intervention is\nrequired. We evaluate our method on realistic backend systems and find that it\ncan formally verify over 50% of the test requirements, which suggests that half\nof a testing engineer's workload can be automated. Additionally, with an\naverage cost of only $2.19 per API, LLM-based verification is significantly\nmore cost-effective than manual testing and can be scaled easily through\nparallel execution. Our results indicate a promising direction for scalable,\nAI-powered software testing, with the potential to greatly improve engineering\nproductivity as models continue to advance."}
{"id": "2506.10999", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10999", "abs": "https://arxiv.org/abs/2506.10999", "authors": ["Atul Kumar", "Diptikalyan Saha", "Toshikai Yasue", "Kohichi Ono", "Saravanan Krishnan", "Sandeep Hans", "Fumiko Satoh", "Gerald Mitchell", "Sachin Kumar"], "title": "Automated Validation of COBOL to Java Transformation", "comment": "arXiv admin note: text overlap with arXiv:2504.10548", "summary": "Recent advances in Large Language Model (LLM) based Generative AI techniques\nhave made it feasible to translate enterpriselevel code from legacy languages\nsuch as COBOL to modern languages such as Java or Python. While the results of\nLLM-based automatic transformation are encouraging, the resulting code cannot\nbe trusted to correctly translate the original code. We propose a framework and\na tool to help validate the equivalence of COBOL and translated Java. The\nresults can also help repair the code if there are some issues and provide\nfeedback to the AI model to improve. We have developed a\nsymbolic-execution-based test generation to automatically generate unit tests\nfor the source COBOL programs which also mocks the external resource calls. We\ngenerate equivalent JUnit test cases with equivalent mocking as COBOL and run\nthem to check semantic equivalence between original and translated programs."}
{"id": "2506.11000", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11000", "abs": "https://arxiv.org/abs/2506.11000", "authors": ["Ketai Qiu"], "title": "Ever-Improving Test Suite by Leveraging Large Language Models", "comment": "Accepted by 33rd ACM International Conference on the Foundations of\n  Software Engineering (FSE Companion '25), June 23--28, 2025, Trondheim,\n  Norway", "summary": "Augmenting test suites with test cases that reflect the actual usage of the\nsoftware system is extremely important to sustain the quality of long lasting\nsoftware systems. In this paper, we propose E-Test, an approach that\nincrementally augments a test suite with test cases that exercise behaviors\nthat emerge in production and that are not been tested yet. E-Test leverages\nLarge Language Models to identify already-tested, not-yet-tested, and\nerror-prone unit execution scenarios, and augment the test suite accordingly.\nOur experimental evaluation shows that E-Test outperforms the main\nstate-of-the-art approaches to identify inadequately tested behaviors and\noptimize test suites."}
{"id": "2506.11001", "categories": ["cs.SE", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11001", "abs": "https://arxiv.org/abs/2506.11001", "authors": ["S. Tucker Browne", "Mark M. Bailey"], "title": "Rethinking Technological Readiness in the Era of AI Uncertainty", "comment": "12 pages", "summary": "Artificial intelligence (AI) is poised to revolutionize military combat\nsystems, but ensuring these AI-enabled capabilities are truly mission-ready\npresents new challenges. We argue that current technology readiness assessments\nfail to capture critical AI-specific factors, leading to potential risks in\ndeployment. We propose a new AI Readiness Framework to evaluate the maturity\nand trustworthiness of AI components in military systems. The central thesis is\nthat a tailored framework - analogous to traditional Technology Readiness\nLevels (TRL) but expanded for AI - can better gauge an AI system's reliability,\nsafety, and suitability for combat use. Using current data evaluation tools and\ntesting practices, we demonstrate the framework's feasibility for near-term\nimplementation. This structured approach provides military decision-makers with\nclearer insight into whether an AI-enabled system has met the necessary\nstandards of performance, transparency, and human integration to be deployed\nwith confidence, thus advancing the field of defense technology management and\nrisk assessment."}
{"id": "2506.11002", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11002", "abs": "https://arxiv.org/abs/2506.11002", "authors": ["Roberto Verdecchia", "Justus Bogner"], "title": "Notes On Writing Effective Empirical Software Engineering Papers: An Opinionated Primer", "comment": null, "summary": "While mastered by some, good scientific writing practices within Empirical\nSoftware Engineering (ESE) research appear to be seldom discussed and\ndocumented. Despite this, these practices are implicit or even explicit\nevaluation criteria of typical software engineering conferences and journals.\nIn this pragmatic, educational-first document, we want to provide guidance to\nthose who may feel overwhelmed or confused by writing ESE papers, but also\nthose more experienced who still might find an opinionated collection of\nwriting advice useful. The primary audience we had in mind for this paper were\nour own BSc, MSc, and PhD students, but also students of others. Our documented\nadvice therefore reflects a subjective and personal vision of writing ESE\npapers. By no means do we claim to be fully objective, generalizable, or\nrepresentative of the whole discipline. With that being said, writing papers in\nthis way has worked pretty well for us so far. We hope that this guide can at\nleast partially do the same for others."}
{"id": "2506.11003", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11003", "abs": "https://arxiv.org/abs/2506.11003", "authors": ["Ruiyang Xu", "Jialun Cao", "Mingyuan Wu", "Wenliang Zhong", "Yaojie Lu", "Ben He", "Xianpei Han", "Shing-Chi Cheung", "Le Sun"], "title": "EmbedAgent: Benchmarking Large Language Models in Embedded System Development", "comment": "21 pages", "summary": "Large Language Models (LLMs) have shown promise in various tasks, yet few\nbenchmarks assess their capabilities in embedded system development.In this\npaper, we introduce EmbedAgent, a paradigm designed to simulate real-world\nroles in embedded system development, such as Embedded System Programmer,\nArchitect, and Integrator. This paradigm enables LLMs to be tested in tasks\nthat bridge the gap between digital and physical systems, allowing for a more\ncomprehensive assessment of their capabilities. To evaluate LLMs on these\ntasks, we propose Embedbench, the first comprehensive benchmark for embedded\nsystem programming, circuit design, and cross-platform migration.Embedbench\nconsists of 126 cases, covering 9 electronic components across 3 hardware\nplatforms. Through extensive experiments on 10 mainstream LLMs, we uncover\nseveral key findings. Surprisingly, despite the simplicity of the cases,\nDeepSeek-R1 achieves only a 55.6% pass@1 rate when provided with schematic\ninformation, and 50.0% when tasked with generating the schematics itself. In\nthe cross-platform migration tasks, LLMs show relatively strong performance\nwith MicroPython on the Raspberry Pi Pico (with the top model achieving 73.8%\npass@1), but perform poorly on ESP-IDF, where the best model reaches only 29.4%\npass@1.Interestingly, we observe that general-purpose chat LLMs like\nDeepSeek-V3 often fail to utilize relevant pre-trained knowledge in this\ndomain, while reasoning LLMs tend to overthink and overlook efficient knowledge\nduring pretraining. Based on these insights, we propose two strategies:\nretrieval augmented generation and compiler feedback-to enhance LLM\nperformance. These strategies result in significant improvements, with\nDeepseek-R1 reaching a 65.1% pass@1 with correct schematics, and 53.1% without.\nAdditionally, the accuracy of the Arduino to ESP32 migration task improves from\n21.4% to 27.8%."}
{"id": "2506.11005", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11005", "abs": "https://arxiv.org/abs/2506.11005", "authors": ["Mouna Dhaouadi", "Bentley Oakes", "Michalis Famelis"], "title": "Automated Extraction and Analysis of Developer's Rationale in Open Source Software", "comment": null, "summary": "Contributors to open source software must deeply understand a project's\nhistory to make coherent decisions which do not conflict with past reasoning.\nHowever, inspecting all related changes to a proposed contribution requires\nintensive manual effort, and previous research has not yet produced an\nautomated mechanism to expose and analyze these conflicts. In this article, we\npropose such an automated approach for rationale analyses, based on an\ninstantiation of Kantara, an existing high-level rationale extraction and\nmanagement architecture. Our implementation leverages pre-trained models and\nLarge Language Models, and includes structure-based mechanisms to detect\nreasoning conflicts and problems which could cause design erosion in a project\nover time. We show the feasibility of our extraction and analysis approach\nusing the OOM-Killer module of the Linux Kernel project, and investigate the\napproach's generalization to five other highly active open source projects. The\nresults confirm that our automated approach can support rationale analyses with\nreasonable performance, by finding interesting relationships and to detect\npotential conflicts and reasoning problems. We also show the effectiveness of\nthe automated extraction of decision and rationale sentences and the prospects\nfor generalizing this to other open source projects. This automated approach\ncould therefore be used by open source software developers to proactively\naddress hidden issues and to ensure that new changes do not conflict with past\ndecisions."}
{"id": "2506.11006", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11006", "abs": "https://arxiv.org/abs/2506.11006", "authors": ["Sai Krishna", "Balvinder Singh", "Sujoy Roychowdhury", "Giriprasad Sridhara", "Sourav Mazumdar", "Magnus Sandelin", "Dimitris Rentas", "Maciej Nalepa", "Karol Sawicki", "Jakub Gajda"], "title": "Test code generation at Ericsson using Program Analysis Augmented Fine Tuned LLMs", "comment": "Accepted at International Conference on Evaluation and Assessment in\n  Software Engineering (EASE), 2025", "summary": "We describe test code generation using Large Language Models (LLMs) in\nEricsson. Our input is a test step in natural language (English) and our output\nis code (Java) which accomplishes the test step. We describe how straight\nforward prompting does not suffice and results in LLM assuming functions and\nsignatures which are not present in the code repository. We then show how we\nalleviate the problem by a combination of Retrieval Augmented Generation (RAG)\nalong with prompt engineering that expanded the simple prompt with additional\ncontextual information using static program analysis. We then describe further\nimprovements that we obtained by fine-tuning the underlying LLM. The fine\ntuning is done based on a custom designed prompt template which has\npre-dependent classes, their public methods as well two exemplar outputs\nobtained from RAG. Our results establish that our fine tuned models help\nimprove the correspondence or conformity with the original developer written\ntest code as measured by the traditional metrics of F1-score based on the\nmethods used in the generated code. Fine tuning of a 8x7b Mixture of Experts\n(MoE) model leads to an average improvement of 8\\% over the base model and is\ncomparable to the scores on a much larger 8x22b MoE model."}
{"id": "2506.11007", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11007", "abs": "https://arxiv.org/abs/2506.11007", "authors": ["Rock Sabetto", "Emily Escamilla", "Devesh Agarwal", "Sujay Kandwal", "Justin F. Brunelle", "Scott Rosen", "Nitin Naik", "Samruddhi Thaker", "Eric O. Scott", "Jacob Zimmer", "Amit Madan", "Arun Sridharan", "Doug Wendt", "Michael Doyle", "Christopher Glasz", "Jasper Phillips", "William Macke", "Colin Diggs", "Michael Bartholf", "Zachary Robin", "Paul Ursino"], "title": "Impact of Comments on LLM Comprehension of Legacy Code", "comment": null, "summary": "Large language models (LLMs) have been increasingly integrated into software\nengineering and maintenance tasks due to their high performance with software\nengineering tasks and robust understanding of modern programming languages.\nHowever, the ability of LLMs to comprehend code written with legacy languages\nremains a research gap challenged by real-world legacy systems lacking or\ncontaining inaccurate documentation that may impact LLM comprehension. To\nassess LLM comprehension of legacy languages, there is a need for objective LLM\nevaluation. In order to objectively measure LLM comprehension of legacy\nlanguages, we need an efficient, quantitative evaluation method. We leverage\nmultiple-choice question answering (MCQA), an emerging LLM evaluation\nmethodology, to evaluate LLM comprehension of legacy code and the impact of\ncomment prevalence and inaccurate comments. In this work, we present\npreliminary findings on the impact of documentation on LLM comprehension of\nlegacy code and outline strategic objectives for future work."}
{"id": "2506.11008", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11008", "abs": "https://arxiv.org/abs/2506.11008", "authors": ["David Noever"], "title": "Encoding Software For Perpetuity: A Compact Representation Of Apollo 11 Guidance Code", "comment": null, "summary": "This brief note presents a novel method for encoding historic Apollo 11 Lunar\nModule guidance computer code into a single, compact Quick Response Code (QR\ncode) format, creating an accessible digital artifact for transmission and\narchival purposes. By applying tokenization, selective content preservation,\nand minimal HTML/JavaScript techniques, we successfully compressed key\ncomponents of the original Assembly Language Code (AGC) into a shareable,\npreservable, and scannable 3 kilobyte (KB) image. We evaluate multiple\ncompression strategies and their tradeoffs in terms of size, readability, and\nhistorical significance. This method addresses the challenge of making\nhistorically significant software artifacts available through modern mobile\ndevices without requiring specialized hardware or internet connectivity. While\nnumerous digital preservation methods exist for historic software, this\napproach balances accessibility with historical significance, offering a\ncomplementary method to traditional archival techniques. This work contributes\nto the broader field of computing heritage preservation by demonstrating how\nlandmark software can be made accessible instantly through contemporary mobile\ntechnologies."}
{"id": "2506.11009", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11009", "abs": "https://arxiv.org/abs/2506.11009", "authors": ["Jirat Pasuksmit", "Wannita Takerngsaksiri", "Patanamon Thongtanunam", "Chakkrit Tantithamthavorn", "Ruixiong Zhang", "Shiyan Wang", "Fan Jiang", "Jing Li", "Evan Cook", "Kun Chen", "Ming Wu"], "title": "Human-In-The-Loop Software Development Agents: Challenges and Future Directions", "comment": "The International Conference on Mining Software Repositories (MSR)\n  2025, Industry track", "summary": "Multi-agent LLM-driven systems for software development are rapidly gaining\ntraction, offering new opportunities to enhance productivity. At Atlassian, we\ndeployed Human-in-the-Loop Software Development Agents to resolve Jira work\nitems and evaluated the generated code quality using functional correctness\ntesting and GPT-based similarity scoring. This paper highlights two major\nchallenges: the high computational costs of unit testing and the variability in\nLLM-based evaluations. We also propose future research directions to improve\nevaluation frameworks for Human-In-The-Loop software development tools."}
{"id": "2506.11011", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11011", "abs": "https://arxiv.org/abs/2506.11011", "authors": ["Abhi Desai"], "title": "Enhancing Inventory Management with Progressive Web Applications (PWAs): A Scalable Solution for Small and Large Enterprises", "comment": null, "summary": "Efficient inventory management is crucial for both small and large\nenterprises to optimize operational workflows and reduce overhead costs. This\npaper explores the development and implementation of a Progressive Web\nApplication (PWA) designed to enhance the inventory management experience. The\napplication integrates key functionalities such as barcode and QR code\nscanning, geolocation-based warehouse identification, and cross-device\naccessibility. By leveraging PWA technology, the solution ensures offline\ncapabilities, responsive user experience, and seamless adaptability across\nvarious platforms. The study discusses the challenges and benefits of\nimplementing PWA in inventory management systems, including its limitations in\nperformance compared to native applications. Insights from the development\nprocess provide a roadmap for future developers looking to integrate PWA\ntechnology into enterprise applications. This research contributes to the\ngrowing domain of web-based inventory solutions, offering a scalable and\ncost-effective alternative to traditional inventory management software."}
{"id": "2506.11013", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11013", "abs": "https://arxiv.org/abs/2506.11013", "authors": ["Filipe Fernandes", "Cláudia Werner"], "title": "Toward a Brazilian Research Agenda in Quantum Software Engineering: A Systematic Mapping Study", "comment": "11 pages, 13 figures", "summary": "Context: Quantum Software Engineering (QSE) has emerged as a key field to\nsupport the development of reliable, maintainable, and scalable quantum\napplications, bridging advances in quantum computing with established practices\nin software engineering. Problem: Despite its growth, the field still suffers\nfrom fragmented knowledge, with a lack of standardized methodologies, tools,\nand guidelines tailored to the unique features of the quantum paradigm.\nAdditionally, countries like Brazil have had limited participation in the\ndevelopment of this emerging domain. Objective: This study aims to map the\nstate of the art in QSE by identifying current research trends, recurring\ncontributions, and existing gaps that can guide future investigations and\nstrategic initiatives. Methodology: A systematic mapping study was conducted\nanalyzing selected publications based on inclusion and exclusion criteria.\nArticles were categorized by study type, research type, and alignment with the\nSWEBOK knowledge areas. Results: Most of the reviewed studies are primary\nresearch articles written in English, with a strong focus on Software\nEngineering Models and Methods, Software Architecture, and Software Testing.\nConceptual proposals and technical solutions predominate, while empirical\nvalidations remain limited. Conclusions: Findings confirm that QSE is a\npromising but still maturing field. The standardization of practices, expansion\nof empirical studies, and inclusion of researchers from developing countries\nare crucial for advancing the discipline. Additionally, Brazilian contributions\nare still scarce, highlighting the urgent need to establish a national research\nagenda. As a main contribution, this study proposes a Brazilian Research Agenda\nin QSE, outlining priority areas and opportunities to foster a local scientific\ncommunity and accelerate progress in this emerging field."}
{"id": "2506.11014", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11014", "abs": "https://arxiv.org/abs/2506.11014", "authors": ["Benedetta Donato", "Leonardo Mariani", "Daniela Micucci", "Oliviero Riganelli", "Marco Somaschini"], "title": "MultiMind: A Plug-in for the Implementation of Development Tasks Aided by AI Assistants", "comment": null, "summary": "The integration of AI assistants into software development workflows is\nrapidly evolving, shifting from automation-assisted tasks to collaborative\ninteractions between developers and AI. Large Language Models (LLMs) have\ndemonstrated their effectiveness in several development activities, including\ncode completion, test case generation, and documentation production. However,\nembedding AI-assisted tasks within Integrated Development Environments (IDEs)\npresents significant challenges. It requires designing mechanisms to invoke AI\nassistants at the appropriate time, coordinate interactions with multiple\nassistants, process the generated outputs, and present feedback in a way that\nseamlessly integrates with the development workflow. To address these issues,\nwe introduce MultiMind, a Visual Studio Code plug-in that streamlines the\ncreation of AI-assisted development tasks. MultiMind provides a modular and\nextensible framework, enabling developers to cost-effectively implement and\nexperiment with new AI-powered interactions without the need for complex IDE\ncustomizations. MultiMind has been tested in two use cases: one for the\nautomatic generation of code comments and the other about the definition of\nAI-powered chat."}
{"id": "2506.11016", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11016", "abs": "https://arxiv.org/abs/2506.11016", "authors": ["Lelanthran Manickum"], "title": "ZjsComponent: A Pragmatic Approach to Modular, Reusable UI Fragments for Web Development", "comment": "12 pages, 7 figures", "summary": "In this paper, I present ZjsComponent, a lightweight and framework-agnostic\nweb component designed for creating modular, reusable UI elements with minimal\ndeveloper overhead. ZjsComponent is an example implementation of an approach to\ncreating components and object instances that can be used purely from HTML.\nUnlike traditional approaches to components, the approach implemented by\nZjsComponent does not require build-steps, transpiling, pre-compilation, any\nspecific ecosystem or any other dependency. All that is required is that the\nbrowser can load and execute Javascript as needed by Web Components.\nZjsComponent allows dynamic loading and isolation of HTML+JS fragments,\noffering developers a simple way to build reusable interfaces with ease. This\napproach is dependency-free, provides significant DOM and code isolation, and\nsupports simple lifecycle hooks as well as traditional methods expected of an\ninstance of a class."}
{"id": "2506.11018", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11018", "abs": "https://arxiv.org/abs/2506.11018", "authors": ["Grigory Tsiperman"], "title": "Formation of requirements traceability in the process of information systems design", "comment": "12 pages, 4 figures, 2025 the 8th International Conference on\n  Information Management", "summary": "The traceability of requirements in the information system design process is\nconsidered an essential property of the project, one of its quality\ncharacteristics. The point here is that traceability provides the methods of\nvalidation and verification of software systems, and that the system model\nbased on requirements traceability reduces the system's dependence on\ndevelopers and, in general, makes it as straightforward as possible. One of the\nchallenges of the traceability process, dubbed \"The grand challenge of\ntraceability\" among traceability researchers, is its integration into the\ndesign process. In this paper, to achieve this goal, we propose the application\nof the Adaptive Clustering Method (ACM) of Information Systems developed by the\nauthor, which is based on the idea of a seamless system architecture that\nprovides explicit interconnection of project artifacts of different levels of\nabstraction."}
{"id": "2506.11019", "categories": ["cs.SE", "68T01, 68N30", "I.2.6; D.2.7"], "pdf": "https://arxiv.org/pdf/2506.11019", "abs": "https://arxiv.org/abs/2506.11019", "authors": ["Vincent Koc", "Jacques Verre", "Douglas Blank", "Abigail Morgan"], "title": "Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)", "comment": "16 pages, 5 figures, conference preprint submission. Conceptual\n  systems architecture paper on telemetry-driven prompt optimization and IDE\n  design patterns for AI development. Builds on Opik MCP open-source\n  architecture and Comet trace infrastructure", "summary": "AI development environments are evolving into observability first platforms\nthat integrate real time telemetry, prompt traces, and evaluation feedback into\nthe developer workflow. This paper introduces telemetry aware integrated\ndevelopment environments (IDEs) enabled by the Model Context Protocol (MCP), a\nsystem that connects IDEs with prompt metrics, trace logs, and versioned\ncontrol for real time refinement. We present design patterns for local prompt\niteration, CI based optimization, and autonomous agents that adapt behavior\nusing telemetry. Rather than focusing on a single algorithm, we describe an\narchitecture that supports integration with frameworks like DSPy, PromptWizard,\nand Prompts as Programs. We demonstrate this through Opik, an open source MCP\nserver for LLM telemetry, and position our approach within the emerging LLMOps\necosystem. This work lays a foundation for future research on prompt\noptimization, IDE agent tooling, and empirical benchmarking in telemetry rich\nAI development workflows."}
{"id": "2506.11020", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11020", "abs": "https://arxiv.org/abs/2506.11020", "authors": ["Thayná Camargo da Silva"], "title": "Extracting Knowledge Graphs from User Stories using LangChain", "comment": "Master thesis work", "summary": "This thesis introduces a novel methodology for the automated generation of\nknowledge graphs from user stories by leveraging the advanced capabilities of\nLarge Language Models. Utilizing the LangChain framework as a basis, the User\nStory Graph Transformer module was developed to extract nodes and relationships\nfrom user stories using an LLM to construct accurate knowledge graphs.This\ninnovative technique was implemented in a script to fully automate the\nknowledge graph extraction process. Additionally, the evaluation was automated\nthrough a dedicated evaluation script, utilizing an annotated dataset for\nassessment. By enhancing the visualization and understanding of user\nrequirements and domain concepts, this method fosters better alignment between\nsoftware functionalities and user expectations, ultimately contributing to more\neffective and user-centric software development processes."}
{"id": "2506.11021", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11021", "abs": "https://arxiv.org/abs/2506.11021", "authors": ["Chaitanya Ravuri", "Saman Amarasinghe"], "title": "Eliminating Hallucination-Induced Errors in LLM Code Generation with Functional Clustering", "comment": "9 pages, 1 figure", "summary": "Modern code-generation LLMs can already solve a large fraction of programming\nproblems, yet they still hallucinate subtle bugs that make their outputs unsafe\nfor autonomous deployment. We present functional clustering, a black-box\nwrapper that eliminates nearly all hallucination-induced errors while providing\na tunable confidence score. The wrapper samples many candidate programs,\nexecutes each on a self-generated test suite, and clusters candidates whose I/O\nbehavior is identical; the empirical mass of the largest cluster serves as an\nexact confidence estimate. A single scalar threshold on this estimate lets\nusers trade coverage for reliability with exponential guarantees. On\nLiveCodeBench our verifier preserves baseline pass@1 on solvable tasks yet\nslashes the error rate of returned answers from ~65% to 2%, and drives it to 0%\nat a conservative threshold while still answering 15.6% of prompts. Manual\naudits show that the few residual mistakes stem from prompt misinterpretation,\nnot random generation noise, narrowing future work to specification clarity.\nBecause the method requires only sampling and sandbox execution, it applies\nunchanged to closed-source APIs and future models, offering a practical path\ntoward dependable, autonomous code generation. Our code is available on Github\n(https://github.com/20ChaituR/functional-clustering)."}
{"id": "2506.11022", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11022", "abs": "https://arxiv.org/abs/2506.11022", "authors": ["Shivani Shukla", "Himanshu Joshi", "Romilla Syed"], "title": "Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox", "comment": "Keywords - Large Language Models, Security Vulnerabilities,\n  AI-Generated Code, Iterative Feedback, Software Security, Secure Coding\n  Practices, Feedback Loops, LLM Prompting Strategies", "summary": "The rapid adoption of Large Language Models(LLMs) for code generation has\ntransformed software development, yet little attention has been given to how\nsecurity vulnerabilities evolve through iterative LLM feedback. This paper\nanalyzes security degradation in AI-generated code through a controlled\nexperiment with 400 code samples across 40 rounds of \"improvements\" using four\ndistinct prompting strategies. Our findings show a 37.6% increase in critical\nvulnerabilities after just five iterations, with distinct vulnerability\npatterns emerging across different prompting approaches. This evidence\nchallenges the assumption that iterative LLM refinement improves code security\nand highlights the essential role of human expertise in the loop. We propose\npractical guidelines for developers to mitigate these risks, emphasizing the\nneed for robust human validation between LLM iterations to prevent the\nparadoxical introduction of new security issues during supposedly beneficial\ncode \"improvements\"."}
{"id": "2506.11051", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11051", "abs": "https://arxiv.org/abs/2506.11051", "authors": ["Sung Une Lee", "Liming Dong", "Zhenchang Xing", "Muhammad Ejaz Ahmed", "Stefan Avgoustakis"], "title": "Software Security Mapping Framework: Operationalization of Security Requirements", "comment": "28 pages, 13 figures, 6 tables", "summary": "The escalating complexity of modern software development environments has\nheightened concerns around supply chain security. However, existing frameworks\noften fall short in translating abstract security principles into concrete,\nactionable practices. This paper introduces the Software Security Mapping\nFramework, a structured solution designed to operationalize security\nrequirements across hierarchical levels -- from high-level regulatory standards\n(e.g., ISM, Australia cybersecurity standard published by the Australian\nSignals Directorate), through mid-level frameworks (e.g., NIST SSDF, the U.S.\nSecure Software Development Framework), to fine-grained technical activities\n(e.g., SLSA, a software supply chain security framework). Developed through\ncollaborative research with academic experts and industry practitioners, the\nframework systematically maps 131 refined security requirements to over 400\nactionable operational steps spanning the software development lifecycle. It is\ngrounded in four core security goals: Secure Software Environment, Secure\nSoftware Development, Software Traceability, and Vulnerability Management. Our\napproach leverages the KAOS goal modeling methodology to establish traceable\nlinkages between strategic goals and tactical operations, enhancing clarity,\naccountability, and practical implementation. To facilitate adoption, we\nprovide a web-based navigation tool for interactive exploration of the\nframework. A real-world case study based on the Log4j vulnerability illustrates\nthe framework's utility by generating a tailored checklist aligned with\nindustry best practices. Additionally, we offer a structured, machine-readable\nOSCAL Catalog Model of the Software Security Mapping Framework, enabling\norganizations to automate implementation, streamline compliance processes, and\nrespond effectively to evolving security risks."}
{"id": "2506.11058", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11058", "abs": "https://arxiv.org/abs/2506.11058", "authors": ["Ziga Kovacic", "Celine Lee", "Justin Chiu", "Wenting Zhao", "Kevin Ellis"], "title": "Refactoring Codebases through Library Design", "comment": "26 pages", "summary": "Maintainable and general software allows developers to build robust\napplications efficiently, yet achieving these qualities often requires\nrefactoring specialized solutions into reusable components. This challenge\nbecomes particularly relevant as code agents become increasingly accurate at\nsolving isolated programming problems. We investigate code agents' capacity to\nrefactor code in ways supporting growth and reusability. We present both a\nmethod and a benchmark for refactoring: Librarian, a sample-and-rerank method\nfor generating reusable libraries, and Minicode, a benchmark where code agents\nmust minimize and refactor multiple independent solutions into a joint library.\nCompared to state-of-the-art code agents, Librarian achieves strong results on\nboth compression and correctness on Minicode, obtaining compression rates\n1.6-2x better than coding agents while also improving correctness. We\nopen-source our code and benchmark at https://code-refactor.github.io/."}
{"id": "2506.11059", "categories": ["cs.SE", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11059", "abs": "https://arxiv.org/abs/2506.11059", "authors": ["Hanxi Guo", "Siyuan Cheng", "Kaiyuan Zhang", "Guangyu Shen", "Xiangyu Zhang"], "title": "CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs", "comment": null, "summary": "Large language models (LLMs) have become integral to modern software\ndevelopment, producing vast amounts of AI-generated source code. While these\nmodels boost programming productivity, their misuse introduces critical risks,\nincluding code plagiarism, license violations, and the propagation of insecure\nprograms. As a result, robust detection of AI-generated code is essential. To\nsupport the development of such detectors, a comprehensive benchmark that\nreflects real-world conditions is crucial. However, existing benchmarks fall\nshort -- most cover only a limited set of programming languages and rely on\nless capable generative models. In this paper, we present CodeMirage, a\ncomprehensive benchmark that addresses these limitations through three major\nadvancements: (1) it spans ten widely used programming languages, (2) includes\nboth original and paraphrased code samples, and (3) incorporates outputs from\nten state-of-the-art production-level LLMs, including both reasoning and\nnon-reasoning models from six major providers. Using CodeMirage, we evaluate\nten representative detectors across four methodological paradigms under four\nrealistic evaluation configurations, reporting results using three\ncomplementary metrics. Our analysis reveals nine key findings that uncover the\nstrengths and weaknesses of current detectors, and identify critical challenges\nfor future work. We believe CodeMirage offers a rigorous and practical testbed\nto advance the development of robust and generalizable AI-generated code\ndetectors."}
{"id": "2506.11060", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11060", "abs": "https://arxiv.org/abs/2506.11060", "authors": ["Ramneet Singh", "Sathvik Joel", "Abhav Mehrotra", "Nalin Wadhwa", "Ramakrishna B Bairi", "Aditya Kanade", "Nagarajan Natarajan"], "title": "Code Researcher: Deep Research Agent for Large Systems Code and Commit History", "comment": null, "summary": "Large Language Model (LLM)-based coding agents have shown promising results\non coding benchmarks, but their effectiveness on systems code remains\nunderexplored. Due to the size and complexities of systems code, making changes\nto a systems codebase is a daunting task, even for humans. It requires\nresearching about many pieces of context, derived from the large codebase and\nits massive commit history, before making changes. Inspired by the recent\nprogress on deep research agents, we design the first deep research agent for\ncode, called Code Researcher, and apply it to the problem of generating patches\nfor mitigating crashes reported in systems code. Code Researcher performs\nmulti-step reasoning about semantics, patterns, and commit history of code to\ngather sufficient context. The context is stored in a structured memory which\nis used for synthesizing a patch. We evaluate Code Researcher on kBenchSyz, a\nbenchmark of Linux kernel crashes, and show that it significantly outperforms\nstrong baselines, achieving a crash-resolution rate of 58%, compared to 37.5%\nby SWE-agent. On an average, Code Researcher explores 10 files in each\ntrajectory whereas SWE-agent explores only 1.33 files, highlighting Code\nResearcher's ability to deeply explore the codebase. Through another experiment\non an open-source multimedia software, we show the generalizability of Code\nResearcher. Our experiments highlight the importance of global context\ngathering and multi-faceted reasoning for large codebases."}
{"id": "2506.11066", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11066", "abs": "https://arxiv.org/abs/2506.11066", "authors": ["Jiahui Geng", "Fengyu Cai", "Shaobo Cui", "Qing Li", "Liangwei Chen", "Chenyang Lyu", "Haonan Li", "Derui Zhu", "Walter Pretschner", "Heinz Koeppl", "Fakhri Karray"], "title": "CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval", "comment": null, "summary": "Code retrieval is essential in modern software development, as it boosts code\nreuse and accelerates debugging. However, current benchmarks primarily\nemphasize functional relevance while neglecting critical dimensions of software\nquality. Motivated by this gap, we introduce CoQuIR, the first large-scale,\nmultilingual benchmark specifically designed to evaluate quality-aware code\nretrieval across four key dimensions: correctness, efficiency, security, and\nmaintainability. CoQuIR provides fine-grained quality annotations for 42,725\nqueries and 134,907 code snippets in 11 programming languages, and is\naccompanied by two quality-centric evaluation metrics: Pairwise Preference\nAccuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark 23\nretrieval models, covering both open-source and proprietary systems, and find\nthat even top-performing models frequently fail to distinguish buggy or\ninsecure code from their more robust counterparts. Furthermore, we conduct\npreliminary investigations into training methods that explicitly encourage\nretrievers to recognize code quality. Using synthetic datasets, we demonstrate\npromising improvements in quality-aware metrics across various models, without\nsacrificing semantic relevance. Downstream code generation experiments further\nvalidate the effectiveness of our approach. Overall, our work highlights the\nimportance of integrating quality signals into code retrieval systems, laying\nthe groundwork for more trustworthy and robust software development tools."}
{"id": "2506.11076", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11076", "abs": "https://arxiv.org/abs/2506.11076", "authors": ["Minyu Chen", "Guoqiang Li", "Ling-I Wu", "Ruibang Liu"], "title": "DCE-LLM: Dead Code Elimination with Large Language Models", "comment": "Accepted by regular paper in NAACL 2025, with 13 pages, 5 figures", "summary": "Dead code introduces several challenges in software development, such as\nincreased binary size and maintenance difficulties. It can also obscure logical\nerrors and be exploited for obfuscation in malware. For LLM-based code-related\ntasks, dead code introduces vulnerabilities that can mislead these models,\nraising security concerns. Although modern compilers and IDEs offer dead code\nelimination, sophisticated patterns can bypass these tools. A universal\napproach that includes classification, location, explanation, and correction is\nneeded, yet current tools often require significant manual effort. We present\nDCE-LLM, a framework for automated dead code elimination using a small CodeBERT\nmodel with an attribution-based line selector to efficiently locate suspect\ncode. LLMs then generate judgments and explanations, fine-tuned on a\nlarge-scale, annotated dead code dataset to provide detailed explanations and\npatches. DCE-LLM outperforms existing tools, with advanced unreachability\ndetection, automated correction, and support for multiple programming\nlanguages. Experimental results show DCE-LLM achieves over 94% F1 scores for\nunused and unreachable code, significantly surpassing GPT-4o by 30%."}
{"id": "2506.11084", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11084", "abs": "https://arxiv.org/abs/2506.11084", "authors": ["Yordan Kalmukov"], "title": "Research and Analysis of Employers' Opinion on the Necessary Skills that Students in the Field of Web Programming Should Possess", "comment": null, "summary": "In the era of artificial intelligence (AI) and chatbots, based on large\nlanguage models that can generate programming code in any language, write texts\nand summarize information, it is obvious that the requirements of employers for\ngraduating students have already changed. The modern IT world offers\nsignificant automation of programming through software frameworks and a huge\nset of third-party libraries and application programming interfaces (APIs). All\nthese tools provide most of the necessary functionality out of the box (already\nimplemented), and quite naturally the question arises as to what is more useful\nfor students - to teach how to use these ready-made tools or the basic\nprinciples of working and development of web applications from scratch. This\npaper analyzes the results of a survey conducted among IT employers, aimed to\nidentify what, in their opinion, are the necessary technical skills that\ngraduating students in the field of Web Programming should possess in order to\njoin the company's work as quickly and effectively as possible."}
{"id": "2506.11085", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.LO", "I.2.6; H.3.3; I.2.3"], "pdf": "https://arxiv.org/pdf/2506.11085", "abs": "https://arxiv.org/abs/2506.11085", "authors": ["Justin Asher"], "title": "LeanExplore: A search engine for Lean 4 declarations", "comment": "16 pages, 1 figure. Project website: https://www.leanexplore.com/ ,\n  Code: https://github.com/justincasher/lean-explore", "summary": "The expanding Lean 4 ecosystem poses challenges for navigating its vast\nlibraries. This paper introduces LeanExplore, a search engine for Lean 4\ndeclarations. LeanExplore enables users to semantically search for statements,\nboth formally and informally, across select Lean 4 packages (including\nBatteries, Init, Lean, Mathlib, PhysLean, and Std). This search capability is\npowered by a hybrid ranking strategy, integrating scores from a multi-source\nsemantic embedding model (capturing conceptual meaning from formal Lean code,\ndocstrings, AI-generated informal translations, and declaration titles), BM25+\nfor keyword-based lexical relevance, and a PageRank-based score reflecting\ndeclaration importance and interconnectedness. The search engine is accessible\nvia a dedicated website (https://www.leanexplore.com/) and a Python API\n(https://github.com/justincasher/lean-explore). Furthermore, the database can\nbe downloaded, allowing users to self-host the service. LeanExplore integrates\neasily with LLMs via the model context protocol (MCP), enabling users to chat\nwith an AI assistant about Lean declarations or utilize the search engine for\nbuilding theorem-proving agents. This work details LeanExplore's architecture,\ndata processing, functionalities, and its potential to enhance Lean 4 workflows\nand AI-driven mathematical research"}
{"id": "2506.11107", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11107", "abs": "https://arxiv.org/abs/2506.11107", "authors": ["Weibo Gao", "Qi Liu", "Rui Li", "Yuze Zhao", "Hao Wang", "Linan Yre", "Fangzhou Yao", "Zheng Zhang"], "title": "Denoising Programming Knowledge Tracing with a Code Graph-based Tuning Adaptor", "comment": "Accepted by KDD August 2025", "summary": "Programming Knowledge Tracking (PKT) aims to dynamically diagnose learners'\nmastery levels of programming knowledge based on their coding activities,\nfacilitating more effective and personalized programming education. However,\ncurrent PKT studies primarily focus on the implicit relationship between code\ncontent and knowledge assessment, often overlooking two types of noise signals\nin long-term programming activities: unwanted signals from unrelated\nsubmissions and weak signals from minor modifications. This practical challenge\nsignificantly limits model performance and application. To address this issue,\nwe propose Coda, a Code graph-based tuning adaptor designed to enhance existing\nPKT models by identifying and mitigating the impact of noise. Specifically,\nCoda first transforms the loose code sequences submitted by each learner into a\ncompact code graph. By leveraging this code graph, unwanted signals can be\nidentified from a semantic similarity perspective. We then apply a\ncluster-aware GCN to the code graph, which improves the discrimination of weak\nsignals and enables their clustering for identification. Finally, a lightweight\nyet effective adaptor is incorporated into the PKT task through optimization\nwith two noise feature-based constraints and a navigational regularization\nterm, to correct knowledge states affected by noise. It is worth mentioning\nthat the Coda framework is model-agnostic and can be adapted to most existing\nPKT solutions. Extensive experimental results on four real-world datasets\ndemonstrate that Coda effectively performs the PKT task in the presence of\nnoisy programming records, outperforming typical baselines."}
{"id": "2506.11141", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.11141", "abs": "https://arxiv.org/abs/2506.11141", "authors": ["Philippe J. Giabbanelli", "John Beverley", "Istvan David", "Andreas Tolk"], "title": "From over-reliance to smart integration: using Large-Language Models as translators between specialized modeling and simulation tools", "comment": "Accepted at the Winter Simulation conference 2025, December, Seattle\n  USA", "summary": "Large Language Models (LLMs) offer transformative potential for Modeling &\nSimulation (M&S) through natural language interfaces that simplify workflows.\nHowever, over-reliance risks compromising quality due to ambiguities, logical\nshortcuts, and hallucinations. This paper advocates integrating LLMs as\nmiddleware or translators between specialized tools to mitigate complexity in\nM&S tasks. Acting as translators, LLMs can enhance interoperability across\nmulti-formalism, multi-semantics, and multi-paradigm systems. We address two\nkey challenges: identifying appropriate languages and tools for modeling and\nsimulation tasks, and developing efficient software architectures that\nintegrate LLMs without performance bottlenecks. To this end, the paper explores\nLLM-mediated workflows, emphasizes structured tool integration, and recommends\nLow-Rank Adaptation-based architectures for efficient task-specific\nadaptations. This approach ensures LLMs complement rather than replace\nspecialized tools, fostering high-quality, reliable M&S processes."}
{"id": "2506.11153", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11153", "abs": "https://arxiv.org/abs/2506.11153", "authors": ["Changxin Ke", "Rui Zhang", "Shuo Wang", "Li Ding", "Guangli Li", "Yuanbo Wen", "Shuoming Zhang", "Ruiyuan Xu", "Jin Qin", "Jiaming Guo", "Chenxi Wang", "Ling Li", "Qi Guo", "Yunji Chen"], "title": "Mutual-Supervised Learning for Sequential-to-Parallel Code Translation", "comment": "28 pages", "summary": "The rise of GPU-based high-performance computing (HPC) has driven the\nwidespread adoption of parallel programming models such as CUDA. Yet, the\ninherent complexity of parallel programming creates a demand for the automated\nsequential-to-parallel approaches. However, data scarcity poses a significant\nchallenge for machine learning-based sequential-to-parallel code translation.\nAlthough recent back-translation methods show promise, they still fail to\nensure functional equivalence in the translated code. In this paper, we propose\na novel Mutual-Supervised Learning (MSL) framework for sequential-to-parallel\ncode translation to address the functional equivalence issue. MSL consists of\ntwo models, a Translator and a Tester. Through an iterative loop consisting of\nCo-verify and Co-evolve steps, the Translator and the Tester mutually generate\ndata for each other and improve collectively. The Tester generates unit tests\nto verify and filter functionally equivalent translated code, thereby evolving\nthe Translator, while the Translator generates translated code as augmented\ninput to evolve the Tester. Experimental results demonstrate that MuSL\nsignificantly enhances the performance of the base model: when applied to\nQwen2.5-Coder, it not only improves Pass@1 by up to 28.91% and boosts Tester\nperformance by 68.90%, but also outperforms the previous state-of-the-art\nmethod CodeRosetta by 1.56 and 6.92 in BLEU and CodeBLEU scores, while\nachieving performance comparable to DeepSeek-R1 and GPT-4.1. Our code is\navailable at https://github.com/kcxain/musl."}
{"id": "2506.11176", "categories": ["cs.SE", "cs.DC", "cs.DM", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.11176", "abs": "https://arxiv.org/abs/2506.11176", "authors": ["Anatoly A. Krasnovsky", "Alexander Zorkin"], "title": "Model Discovery and Graph Simulation: A Lightweight Alternative to Chaos Engineering", "comment": null, "summary": "Microservice applications are prone to cascading failures because of dense\ninter-service dependencies. Ensuring resilience usually demands fault-injection\nexperiments in production-like setups. We propose \\textit{model discovery} --\nan automated CI/CD step that extracts a live dependency graph from trace data\n-- and show that this lightweight representation is sufficient for accurate\nresilience prediction. Using the DeathStarBench Social Network, we build the\ngraph, simulate failures via Monte-Carlo, and run matching chaos experiments on\nthe real system. The graph model closely matches reality: with no replication,\n16 trials yield an observed resilience of 0.186 versus a predicted 0.161; with\nreplication, both observed and predicted values converge to 0.305 (mean\nabsolute error \\leq 0.0004). These results indicate that even a simple,\nautomatically discovered graph can estimate microservice availability with high\nfidelity, offering rapid design-time insight without full-scale failure\ntesting."}
{"id": "2506.11180", "categories": ["cs.SE", "cs.AI", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.11180", "abs": "https://arxiv.org/abs/2506.11180", "authors": ["Luis Miguel Vieira da Silva", "Aljosha Köcher", "Felix Gehlhoff"], "title": "Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing", "comment": null, "summary": "Explicit modeling of capabilities and skills -- whether based on ontologies,\nAsset Administration Shells, or other technologies -- requires considerable\nmanual effort and often results in representations that are not easily\naccessible to Large Language Models (LLMs). In this work-in-progress paper, we\npresent an alternative approach based on the recently introduced Model Context\nProtocol (MCP). MCP allows systems to expose functionality through a\nstandardized interface that is directly consumable by LLM-based agents. We\nconduct a prototypical evaluation on a laboratory-scale manufacturing system,\nwhere resource functions are made available via MCP. A general-purpose LLM is\nthen tasked with planning and executing a multi-step process, including\nconstraint handling and the invocation of resource functions via MCP. The\nresults indicate that such an approach can enable flexible industrial\nautomation without relying on explicit semantic models. This work lays the\nbasis for further exploration of external tool integration in LLM-driven\nproduction systems."}
{"id": "2506.11237", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.11237", "abs": "https://arxiv.org/abs/2506.11237", "authors": ["Ngoc Phuoc An Vo", "Brent Paulovicks", "Vadim Sheinin"], "title": "LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement for Natural Language to Bash in IT Automation", "comment": "10 pages", "summary": "In an effort to automatically evaluate and select the best model and improve\ncode quality for automatic incident remediation in IT Automation, it is crucial\nto verify if the generated code for remediation action is syntactically and\nsemantically correct and whether it can be executed correctly as intended.\nThere are three approaches: 1) conventional methods use surface form similarity\nmetrics (token match, exact match, etc.) which have numerous limitations, 2)\nexecution-based evaluation focuses more on code functionality based on\npass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs\nfor automated evaluation to judge if it is a correct answer for a given problem\nbased on pre-defined metrics. In this work, we focused on enhancing\nLLM-as-a-Judge using bidirectional functionality matching and logic\nrepresentation for reference-less automatic validation and refinement for Bash\ncode generation to select the best model for automatic incident remediation in\nIT Automation. We used execution-based evaluation as ground-truth to evaluate\nour LLM-as-a-Judge metrics. Results show high accuracy and agreement with\nexecution-based evaluation (and up to 8% over baseline). Finally, we built\nReflection code agents to utilize judgments and feedback from our evaluation\nmetrics which achieved significant improvement (up to 24% increase in accuracy)\nfor automatic code refinement."}
{"id": "2506.11266", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11266", "abs": "https://arxiv.org/abs/2506.11266", "authors": ["Benjamin Elder", "Anupama Murthi", "Jungkoo Kang", "Ankita Rajaram Naik", "Kiran Kate", "Kinjal Basu", "Danish Contractor"], "title": "Invocable APIs derived from NL2SQL datasets for LLM Tool-Calling Evaluation", "comment": "10+32 pages, 5 figures", "summary": "Large language models (LLMs) are routinely deployed as agentic systems, with\naccess to tools that interact with live environments to accomplish tasks. In\nenterprise deployments these systems need to interact with API collections that\ncan be extremely large and complex, often backed by databases. In order to\ncreate datasets with such characteristics, we explore how existing NL2SQL\n(Natural Language to SQL query) datasets can be used to automatically create\nNL2API datasets. Specifically, this work describes a novel data generation\npipeline that exploits the syntax of SQL queries to construct a functionally\nequivalent sequence of API calls. We apply this pipeline to one of the largest\nNL2SQL datasets, BIRD-SQL to create a collection of over 2500 APIs that can be\nserved as invocable tools or REST-endpoints. We pair natural language queries\nfrom BIRD-SQL to ground-truth API sequences based on this API pool. We use this\ncollection to study the performance of 10 public LLMs and find that all models\nstruggle to determine the right set of tools (consisting of tasks of intent\ndetection, sequencing with nested function calls, and slot-filling). We find\nthat models have extremely low task completion rates (7-47 percent - depending\non the dataset) which marginally improves to 50 percent when models are\nemployed as ReACT agents that interact with the live API environment. The best\ntask completion rates are far below what may be required for effective\ngeneral-use tool-calling agents, suggesting substantial scope for improvement\nin current state-of-the-art tool-calling LLMs. We also conduct detailed\nablation studies, such as assessing the impact of the number of tools available\nas well as the impact of tool and slot-name obfuscation. We compare the\nperformance of models on the original SQL generation tasks and find that\ncurrent models are sometimes able to exploit SQL better than APIs."}
{"id": "2506.11295", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.8; I.2.0"], "pdf": "https://arxiv.org/pdf/2506.11295", "abs": "https://arxiv.org/abs/2506.11295", "authors": ["Renato Cordeiro Ferreira"], "title": "A Tale of Two Systems: Characterizing Architectural Complexity on Machine Learning-Enabled Systems", "comment": "8 pages, 3 figures (3 diagrams), submitted to the ECSA2025. arXiv\n  admin note: substantial text overlap with arXiv:2506.08153", "summary": "How can the complexity of ML-enabled systems be managed effectively? The goal\nof this research is to investigate how complexity affects ML-Enabled Systems\n(MLES). To address this question, this research aims to introduce a\nmetrics-based architectural model to characterize the complexity of MLES. The\ngoal is to support architectural decisions, providing a guideline for the\ninception and growth of these systems. This paper brings, side-by-side, the\narchitecture representation of two systems that can be used as case studies for\ncreating the metrics-based architectural model: the SPIRA and the Ocean Guard\nMLES."}
{"id": "2506.11400", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11400", "abs": "https://arxiv.org/abs/2506.11400", "authors": ["Yupeng Jiang", "Yao Deng", "Sebastian Schroder", "Linfeng Liang", "Suhaas Gambhir", "Alice James", "Avishkar Seth", "James Pirrie", "Yihao Zhang", "Xi Zheng"], "title": "A Step-by-Step Guide to Creating a Robust Autonomous Drone Testing Pipeline", "comment": null, "summary": "Autonomous drones are rapidly reshaping industries ranging from aerial\ndelivery and infrastructure inspection to environmental monitoring and disaster\nresponse. Ensuring the safety, reliability, and efficiency of these systems is\nparamount as they transition from research prototypes to mission-critical\nplatforms. This paper presents a step-by-step guide to establishing a robust\nautonomous drone testing pipeline, covering each critical stage:\nSoftware-in-the-Loop (SIL) Simulation Testing, Hardware-in-the-Loop (HIL)\nTesting, Controlled Real-World Testing, and In-Field Testing. Using practical\nexamples, including the marker-based autonomous landing system, we demonstrate\nhow to systematically verify drone system behaviors, identify integration\nissues, and optimize performance. Furthermore, we highlight emerging trends\nshaping the future of drone testing, including the integration of Neurosymbolic\nand LLMs, creating co-simulation environments, and Digital Twin-enabled\nsimulation-based testing techniques. By following this pipeline, developers and\nresearchers can achieve comprehensive validation, minimize deployment risks,\nand prepare autonomous drones for safe and reliable real-world operations."}
{"id": "2506.11442", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11442", "abs": "https://arxiv.org/abs/2506.11442", "authors": ["Yiyang Jin", "Kunzhao Xu", "Hang Li", "Xueting Han", "Yanmin Zhou", "Cheng Li", "Jing Bai"], "title": "ReVeal: Self-Evolving Code Agents via Iterative Generation-Verification", "comment": null, "summary": "Recent advances in reinforcement learning (RL) with verifiable outcome\nrewards have significantly improved the reasoning capabilities of large\nlanguage models (LLMs), especially when combined with multi-turn tool\ninteractions. However, existing methods lack both meaningful verification\nsignals from realistic environments and explicit optimization for verification,\nleading to unreliable self-verification. To address these limitations, we\npropose ReVeal, a multi-turn reinforcement learning framework that interleaves\ncode generation with explicit self-verification and tool-based evaluation.\nReVeal enables LLMs to autonomously generate test cases, invoke external tools\nfor precise feedback, and improves performance via a customized RL algorithm\nwith dense, per-turn rewards. As a result, ReVeal fosters the co-evolution of a\nmodel's generation and verification capabilities through RL training, expanding\nthe reasoning boundaries of the base model, demonstrated by significant gains\nin Pass@k on LiveCodeBench. It also enables test-time scaling into deeper\ninference regimes, with code consistently evolving as the number of turns\nincreases during inference, ultimately surpassing DeepSeek-R1-Zero-Qwen-32B.\nThese findings highlight the promise of ReVeal as a scalable and effective\nparadigm for building more robust and autonomous AI agents."}
{"id": "2506.11451", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11451", "abs": "https://arxiv.org/abs/2506.11451", "authors": ["Md Nahidul Islam Opu", "Md Shahidul Islam", "Sara Rouhani", "Shaiful Chowdhury"], "title": "Understanding the Issue Types in Open Source Blockchain-based Software Projects with the Transformer-based BERTopic", "comment": null, "summary": "Blockchain-based software systems are increasingly deployed across diverse\ndomains, yet a systematic understanding of their development challenges remains\nlimited. This paper presents a large-scale empirical study of 497,742 issues\nmined from 1,209 open-source blockchain projects hosted on GitHub. Employing\nBERTopic, a transformer-based topic modeling technique, we identify 49 distinct\nissue topics and organize them hierarchically into 11 major subcategories. Our\nanalysis reveals that both general software development issues and\nblockchain-specific concerns are nearly equally represented, with Wallet\nManagement and UI Enhancement emerging as the most prominent topics. We further\nexamine the temporal evolution of issue categories and resolution times,\nfinding that Wallet issues not only dominate in frequency but also exhibit the\nlongest resolution time. Conversely, Mechanisms issues are resolved\nsignificantly faster. Issue frequency surged after 2016 with the rise of\nEthereum and decentralized applications, but declined after 2022. These\nfindings enhance our understanding of blockchain software maintenance,\ninforming the development of specialized tools and practices to improve\nrobustness and maintainability."}
{"id": "2506.11484", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11484", "abs": "https://arxiv.org/abs/2506.11484", "authors": ["Haoshen", "Ming Hu", "Xiaofei Xie", "Jiaye Li", "Mingsong Chen"], "title": "VulStamp: Vulnerability Assessment using Large Language Model", "comment": null, "summary": "Although modern vulnerability detection tools enable developers to\nefficiently identify numerous security flaws, indiscriminate remediation\nefforts often lead to superfluous development expenses. This is particularly\ntrue given that a substantial portion of detected vulnerabilities either\npossess low exploitability or would incur negligible impact in practical\noperational environments. Consequently, vulnerability severity assessment has\nemerged as a critical component in optimizing software development efficiency.\nExisting vulnerability assessment methods typically rely on manually crafted\ndescriptions associated with source code artifacts. However, due to variability\nin description quality and subjectivity in intention interpretation, the\nperformance of these methods is seriously limited. To address this issue, this\npaper introduces VulStamp, a novel intention-guided framework, to facilitate\ndescription-free vulnerability assessment. Specifically, VulStamp adopts static\nanalysis together with Large Language Model (LLM) to extract the intention\ninformation of vulnerable code. Based on the intention information, VulStamp\nuses a prompt-tuned model for vulnerability assessment. Furthermore, to\nmitigate the problem of imbalanced data associated with vulnerability types,\nVulStamp integrates a Reinforcement Learning (RL)-based prompt-tuning method to\ntrain the assessment model."}
{"id": "2506.11525", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11525", "abs": "https://arxiv.org/abs/2506.11525", "authors": ["Michael Grohs", "Nadine Cordes", "Jana-Rebecca Rehse"], "title": "A Procedural Framework for Assessing the Desirability of Process Deviations", "comment": null, "summary": "Conformance checking techniques help process analysts to identify where and\nhow process executions deviate from a process model. However, they cannot\ndetermine the desirability of these deviations, i.e., whether they are\nproblematic, acceptable or even beneficial for the process. Such desirability\nassessments are crucial to derive actions, but process analysts typically\nconduct them in a manual, ad-hoc way, which can be time-consuming, subjective,\nand irreplicable. To address this problem, this paper presents a procedural\nframework to guide process analysts in systematically assessing deviation\ndesirability. It provides a step-by-step approach for identifying which input\nfactors to consider in what order to categorize deviations into mutually\nexclusive desirability categories, each linked to action recommendations. The\nframework is based on a review and conceptualization of existing literature on\ndeviation desirability, which is complemented by empirical insights from\ninterviews with process analysis practitioners and researchers. We evaluate the\nframework through a desirability assessment task conducted with practitioners,\nindicating that the framework effectively enables them to streamline the\nassessment for a thorough yet concise evaluation."}
{"id": "2506.11548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11548", "abs": "https://arxiv.org/abs/2506.11548", "authors": ["Fabian C. Peña"], "title": "Augmenting the Generality and Performance of Large Language Models for Software Engineering", "comment": null, "summary": "Large Language Models (LLMs) are revolutionizing software engineering (SE),\nwith special emphasis on code generation and analysis. However, their\napplications to broader SE practices including conceptualization, design, and\nother non-code tasks, remain partially underexplored. This research aims to\naugment the generality and performance of LLMs for SE by (1) advancing the\nunderstanding of how LLMs with different characteristics perform on various\nnon-code tasks, (2) evaluating them as sources of foundational knowledge in SE,\nand (3) effectively detecting hallucinations on SE statements. The expected\ncontributions include a variety of LLMs trained and evaluated on\ndomain-specific datasets, new benchmarks on foundational knowledge in SE, and\nmethods for detecting hallucinations. Initial results in terms of performance\nimprovements on various non-code tasks are promising."}
{"id": "2506.11559", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11559", "abs": "https://arxiv.org/abs/2506.11559", "authors": ["Gábor Antal", "Dénes Bán", "Martin Isztin", "Rudolf Ferenc", "Péter Hegedűs"], "title": "Leveraging GPT-4 for Vulnerability-Witnessing Unit Test Generation", "comment": null, "summary": "In the life-cycle of software development, testing plays a crucial role in\nquality assurance. Proper testing not only increases code coverage and prevents\nregressions but it can also ensure that any potential vulnerabilities in the\nsoftware are identified and effectively fixed. However, creating such tests is\na complex, resource-consuming manual process. To help developers and security\nexperts, this paper explores the automatic unit test generation capability of\none of the most widely used large language models, GPT-4, from the perspective\nof vulnerabilities. We examine a subset of the VUL4J dataset containing real\nvulnerabilities and their corresponding fixes to determine whether GPT-4 can\ngenerate syntactically and/or semantically correct unit tests based on the code\nbefore and after the fixes as evidence of vulnerability mitigation. We focus on\nthe impact of code contexts, the effectiveness of GPT-4's self-correction\nability, and the subjective usability of the generated test cases. Our results\nindicate that GPT-4 can generate syntactically correct test cases 66.5\\% of the\ntime without domain-specific pre-training. Although the semantic correctness of\nthe fixes could be automatically validated in only 7. 5\\% of the cases, our\nsubjective evaluation shows that GPT-4 generally produces test templates that\ncan be further developed into fully functional vulnerability-witnessing tests\nwith relatively minimal manual effort.\n  Therefore, despite the limited data, our initial findings suggest that GPT-4\ncan be effectively used in the generation of vulnerability-witnessing tests. It\nmay not operate entirely autonomously, but it certainly plays a significant\nrole in a partially automated process."}
{"id": "2506.11561", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11561", "abs": "https://arxiv.org/abs/2506.11561", "authors": ["Gábor Antal", "Bence Bogenfürst", "Rudolf Ferenc", "Péter Hegedűs"], "title": "Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study", "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown promise for\nautomated vulnerability detection and repair in software systems. This paper\ninvestigates the performance of GPT-4o in repairing Java vulnerabilities from a\nwidely used dataset (Vul4J), exploring how different contextual information\naffects automated vulnerability repair (AVR) capabilities. We compare the\nlatest GPT-4o's performance against previous results with GPT-4 using identical\nprompts. We evaluated nine additional prompts crafted by us that contain\nvarious contextual information such as CWE or CVE information, and manually\nextracted code contexts. Each prompt was executed three times on 42\nvulnerabilities, and the resulting fix candidates were validated using Vul4J's\nautomated testing framework.\n  Our results show that GPT-4o performed 11.9\\% worse on average than GPT-4\nwith the same prompt, but was able to fix 10.5\\% more distinct vulnerabilities\nin the three runs together. CVE information significantly improved repair\nrates, while the length of the task description had minimal impact. Combining\nCVE guidance with manually extracted code context resulted in the best\nperformance. Using our \\textsc{Top}-3 prompts together, GPT-4o repaired 26\n(62\\%) vulnerabilities at least once, outperforming both the original baseline\n(40\\%) and its reproduction (45\\%), suggesting that ensemble prompt strategies\ncould improve vulnerability repair in zero-shot settings."}
{"id": "2506.11588", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11588", "abs": "https://arxiv.org/abs/2506.11588", "authors": ["Simone Romano", "Alberto Conforti", "Gloria Guidetti", "Sara Viotti", "Rachele Ceschin", "Giuseppe Scanniello"], "title": "MBSR at Work: Perspectives from an Instructor and Software Developers", "comment": null, "summary": "In this paper, we present the preliminary findings from a qualitative study\n(i.e., semi-structured interviews) on how a Mindfulness-Based Stress Reduction\n(MBSR) program, carried out in the Software Development (SD) working context,\nis perceived by the software developers of a multinational company who\nparticipated in the MBSR program and by the instructor who led it. MBSR is a\ndeeply personal and experiential practice in helping individuals manage stress,\nparticularly in high-pressure environments such as workplaces, healthcare\nsettings, education, and other demanding professional or personal situations.\nAlthough MBSR has been experimented in different working contexts;\nsurprisingly, it has never been studied in the SD working context where there\nare several stress factors that developers experience (e.g., time pressure and\nuncertainty about the content of a particular task and its outcome). In this\nrespect, qualitative research can generate valuable insights into the\napplication of MBSR in the SD working context that cannot be captured by\nstandardized quantitative measures. Being MBSR instructors and software\ndevelopers the key stakeholders in delivering an MBSR program in the SD working\ncontext, understanding their first-hand experiences can provide a more detailed\npicture of the investigated phenomenon. The most important takeaway result of\nour research can be summarized as follows: despite initial skepticism, the\ndevelopers recognized personal improvements due to the MBSR practice, though\nthe integration of MBSR techniques in the working context remained challenging."}
{"id": "2506.11591", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11591", "abs": "https://arxiv.org/abs/2506.11591", "authors": ["Hyunsun Hong", "Jongmoon Baik"], "title": "Retrieval-Augmented Code Review Comment Generation", "comment": null, "summary": "Automated code review comment generation (RCG) aims to assist developers by\nautomatically producing natural language feedback for code changes. Existing\napproaches are primarily either generation-based, using pretrained language\nmodels, or information retrieval-based (IR), reusing comments from similar past\nexamples. While generation-based methods leverage code-specific pretraining on\nlarge code-natural language corpora to learn semantic relationships between\ncode and natural language, they often struggle to generate low-frequency but\nsemantically important tokens due to their probabilistic nature. In contrast,\nIR-based methods excel at recovering such rare tokens by copying from existing\nexamples but lack flexibility in adapting to new code contexts-for example,\nwhen input code contains identifiers or structures not found in the retrieval\ndatabase. To bridge the gap between generation-based and IR-based methods, this\nwork proposes to leverage retrieval-augmented generation (RAG) for RCG by\nconditioning pretrained language models on retrieved code-review exemplars. By\nproviding relevant examples that illustrate how similar code has been\npreviously reviewed, the model is better guided to generate accurate review\ncomments. Our evaluation on the Tufano et al. benchmark shows that RAG-based\nRCG outperforms both generation-based and IR-based RCG. It achieves up to\n+1.67% higher exact match and +4.25% higher BLEU scores compared to\ngeneration-based RCG. It also improves the generation of low-frequency\nground-truth tokens by up to 24.01%. We additionally find that performance\nimproves as the number of retrieved exemplars increases."}
{"id": "2506.11597", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11597", "abs": "https://arxiv.org/abs/2506.11597", "authors": ["Simone Romano", "Francesco Paolo Sferratore", "Giuseppe Scanniello"], "title": "Further Evidence on a Controversial Topic about Human-Based Experiments: Professionals vs. Students", "comment": null, "summary": "Most Software Engineering (SE) human-based controlled experiments rely on\nstudents as participants, raising concerns about their external validity.\nSpecifically, the realism of results obtained from students and their\napplicability to the software industry remains in question. In this short\npaper, we bring further evidence on this controversial point. To do so, we\ncompare 62 students and 42 software professionals on a bug-fixing task on the\nsame Java program. The students were enrolled in a Bachelor's program in\nComputer Science, while the professionals were employed by two multinational\ncompanies (for one of them, the professionals were from two offices). Some\nvariations in the experimental settings of the two groups (students and\nprofessionals) were present. For instance, the experimental environment of the\nexperiment with professionals was more realistic; i.e., they faced some stress\nfactors such as interruptions during the bug-fixing task. Considering the\ndifferences between the two groups of participants, the gathered data show that\nthe students outperformed the professionals in fixing bugs. This diverges to\nsome extent from past empirical evidence. Rather than presenting definitive\nconclusions, our results aim to catalyze the discussion on the use of students\nin experiments and pave the way for future investigations. Specifically, our\nresults encourage us to examine the complex factors influencing SE tasks,\nmaking experiments as more realistic as possible."}
{"id": "2506.11598", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11598", "abs": "https://arxiv.org/abs/2506.11598", "authors": ["Ahmed Zaki", "Cristian Cadar"], "title": "Understanding API Usage and Testing: An Empirical Study of C Libraries", "comment": "The 29th International Conference on Evaluation and Assessment in\n  Software Engineering, 17 to 20 June, 2025, Istanbul, Turkey", "summary": "For library developers, understanding how their Application Programming\nInterfaces (APIs) are used in the field can be invaluable. Knowing how clients\nare using their APIs allows for data-driven decisions on prioritising bug\nreports, feature requests, and testing activities. For example, the priority of\na bug report concerning an API can be partly determined by how widely that API\nis used.\n  In this paper, we present an empirical study in which we analyse API usage\nacross 21 popular open-source C libraries, such as OpenSSL and SQLite, with a\ncombined total of 3,061 C/C++ clients. We compare API usage by clients with how\nwell library test suites exercise the APIs to offer actionable insights for\nlibrary developers. To our knowledge, this is the first study that compares API\nusage and API testing at scale for the C/C++ ecosystem. Our study shows that\nlibrary developers do not prioritise their effort based on how clients use\ntheir API, with popular APIs often poorly tested. For example, in LMDB, a\npopular key-value store, 45% of the APIs are used by clients but not tested by\nthe library test suite. We further show that client test suites can be\nleveraged to improve library testing e.g., improving coverage in LMDB by 14.7%\nwith the important advantage that those tests are representative of how the\nAPIs are used in the field.\n  For our empirical study, we have developed LibProbe, a framework that can be\nused to analyse a large corpus of clients for a given library and produce\nvarious metrics useful to library developers."}
{"id": "2506.11614", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11614", "abs": "https://arxiv.org/abs/2506.11614", "authors": ["Yonggang Tao", "Jingling Xue"], "title": "Accelerating Delta Debugging through Probabilistic Monotonicity Assessment", "comment": "Accepted by EASE 2025 (The 29th International Conference on\n  Evaluation and Assessment in Software Engineering), 17-20 June 2025,\n  Istanbul, Turkey. 11 pages", "summary": "Delta debugging assumes search space monotonicity: if a program causes a\nfailure, any supersets of that program will also induce the same failure,\npermitting the exclusion of subsets of non-failure-inducing programs. However,\nthis assumption does not always hold in practice. This paper introduces\nProbabilistic Monotonicity Assessment (PMA), enhancing the efficiency of\nDDMIN-style algorithms without sacrificing effectiveness. PMA dynamically\nmodels and assesses the search space's monotonicity based on prior tests tried\nduring the debugging process and uses a confidence function to quantify\nmonotonicity, thereby enabling the probabilistic exclusion of subsets of\nnon-failure-inducing programs. Our approach significantly reduces redundant\ntests that would otherwise be performed, without compromising the quality of\nthe reduction.\n  We evaluated PMA against two leading DDMIN-style tools, CHISEL and ProbDD.\nOur findings indicate that PMA cuts processing time by 59.2% compared to\nCHISEL, accelerates the reduction process (i.e., the number of tokens deleted\nper second) by 3.32x, and decreases the sizes of the final reduced programs by\n6.7%. Against ProbDD, PMA reduces processing time by 22.0%, achieves a 1.34x\nspeedup in the reduction process, and further decreases the sizes of the final\nreduced programs by 3.0%. These findings affirm PMA's role in significantly\nimproving delta debugging's efficiency while maintaining or enhancing its\neffectiveness."}
{"id": "2506.11659", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11659", "abs": "https://arxiv.org/abs/2506.11659", "authors": ["Simin Sun", "Yuchuan Jin", "Miroslaw Staron"], "title": "An Empirical study on LLM-based Log Retrieval for Software Engineering Metadata Management", "comment": null, "summary": "Developing autonomous driving systems (ADSs) involves generating and storing\nextensive log data from test drives, which is essential for verification,\nresearch, and simulation. However, these high-frequency logs, recorded over\nvarying durations, pose challenges for developers attempting to locate specific\ndriving scenarios. This difficulty arises due to the wide range of signals\nrepresenting various vehicle components and driving conditions, as well as\nunfamiliarity of some developers' with the detailed meaning of these signals.\nTraditional SQL-based querying exacerbates this challenge by demanding both\ndomain expertise and database knowledge, often yielding results that are\ndifficult to verify for accuracy.\n  This paper introduces a Large Language Model (LLM)-supported approach that\ncombines signal log data with video recordings from test drives, enabling\nnatural language based scenario searches while reducing the need for\nspecialized knowledge. By leveraging scenario distance graphs and relative gap\nindicators, it provides quantifiable metrics to evaluate the reliability of\nquery results. The method is implemented as an API for efficient database\nquerying and retrieval of relevant records, paired with video frames for\nintuitive visualization. Evaluation on an open industrial dataset demonstrates\nimproved efficiency and reliability in scenario retrieval, eliminating\ndependency on a single data source and conventional SQL."}
{"id": "2506.11697", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11697", "abs": "https://arxiv.org/abs/2506.11697", "authors": ["Yiwei Hu", "Zhen Li", "Kedie Shu", "Shenghua Guan", "Deqing Zou", "Shouhuai Xu", "Bin Yuan", "Hai Jin"], "title": "SoK: Automated Vulnerability Repair: Methods, Tools, and Assessments", "comment": "The full version of \"SoK: Automated Vulnerability Repair: Methods,\n  Tools, and Assessments\" accepted by the 34th USENIX Security Symposium\n  (USENIX Security 2025)", "summary": "The increasing complexity of software has led to the steady growth of\nvulnerabilities. Vulnerability repair investigates how to fix software\nvulnerabilities. Manual vulnerability repair is labor-intensive and\ntime-consuming because it relies on human experts, highlighting the importance\nof Automated Vulnerability Repair (AVR). In this SoK, we present the\nsystematization of AVR methods through the three steps of AVR workflow:\nvulnerability analysis, patch generation, and patch validation. We assess AVR\ntools for C/C++ and Java programs as they have been widely studied by the\ncommunity. Since existing AVR tools for C/C++ programs are evaluated with\ndifferent datasets, which often consist of a few vulnerabilities, we construct\nthe first C/C++ vulnerability repair benchmark dataset, dubbed Vul4C, which\ncontains 144 vulnerabilities as well as their exploits and patches. We use\nVul4C to evaluate seven AVR tools for C/C++ programs and use the third-party\nVul4J dataset to evaluate two AVR tools for Java programs. We also discuss\nfuture research directions."}
{"id": "2506.11722", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11722", "abs": "https://arxiv.org/abs/2506.11722", "authors": ["Eduard C. Groen", "Fabiano Dalpiaz", "Martijn van Vliet", "Boris Winter", "Joerg Doerr", "Sjaak Brinkkemper"], "title": "Classification of Quality Characteristics in Online User Feedback using Linguistic Analysis, Crowdsourcing and LLMs", "comment": "Accepted at the Journal of Systems and Software (JSS); online\n  appendix and supplementary material available at\n  https://doi.org/10.5281/zenodo.15604749", "summary": "Software qualities such as usability or reliability are among the strongest\ndeterminants of mobile app user satisfaction and constitute a significant\nportion of online user feedback on software products, making it a valuable\nsource of quality-related feedback to guide the development process. The\nabundance of online user feedback warrants the automated identification of\nquality characteristics, but the online user feedback's heterogeneity and the\nlack of appropriate training corpora limit the applicability of supervised\nmachine learning. We therefore investigate the viability of three approaches\nthat could be effective in low-data settings: language patterns (LPs) based on\nquality-related keywords, instructions for crowdsourced micro-tasks, and large\nlanguage model (LLM) prompts. We determined the feasibility of each approach\nand then compared their accuracy. For the complex multiclass classification of\nquality characteristics, the LP-based approach achieved a varied precision\n(0.38-0.92) depending on the quality characteristic, and low recall;\ncrowdsourcing achieved the best average accuracy in two consecutive phases\n(0.63, 0.72), which could be matched by the best-performing LLM condition\n(0.66) and a prediction based on the LLMs' majority vote (0.68). Our findings\nshow that in this low-data setting, the two approaches that use crowdsourcing\nor LLMs instead of involving experts achieve accurate classifications, while\nthe LP-based approach has only limited potential. The promise of crowdsourcing\nand LLMs in this context might even extend to building training corpora."}
{"id": "2506.11874", "categories": ["cs.SE", "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2506.11874", "abs": "https://arxiv.org/abs/2506.11874", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "A Short Survey on Formalising Software Requirements using Large Language Models", "comment": "Submitted to SAIV 2025 as extended abstract and received valuable\n  comments improving our draft. This version is the improved one after\n  addressing suggestions from reviewers for improving the draft", "summary": "This paper presents a focused literature survey on the use of large language\nmodels (LLM) to assist in writing formal specifications for software. A summary\nof thirty-five key papers is presented, including examples for specifying\nprograms written in Dafny, C and Java. This paper arose from the project\nVERIFAI - Traceability and verification of natural language requirements that\naddresses the challenges in writing formal specifications from requirements\nthat are expressed in natural language. Our methodology employed multiple\nacademic databases to identify relevant research. The AI-assisted tool Elicit\nfacilitated the initial paper selection, which were manually screened for final\nselection. The survey provides valuable insights and future directions for\nutilising LLMs while formalising software requirements."}
{"id": "2506.11928", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11928", "abs": "https://arxiv.org/abs/2506.11928", "authors": ["Zihan Zheng", "Zerui Cheng", "Zeyu Shen", "Shang Zhou", "Kaiyuan Liu", "Hansen He", "Dongruixuan Li", "Stanley Wei", "Hangyi Hao", "Jianzhu Yao", "Peiyao Sheng", "Zixuan Wang", "Wenhao Chai", "Aleksandra Korolova", "Peter Henderson", "Sanjeev Arora", "Pramod Viswanath", "Jingbo Shang", "Saining Xie"], "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?", "comment": "Project Page at https://livecodebenchpro.com/", "summary": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning."}
{"id": "2506.11612", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11612", "abs": "https://arxiv.org/abs/2506.11612", "authors": ["Zhijie Liu", "Qiyi Tang", "Sen Nie", "Shi Wu", "Liang Feng Zhang", "Yutian Tang"], "title": "KEENHash: Hashing Programs into Function-Aware Embeddings for Large-Scale Binary Code Similarity Analysis", "comment": null, "summary": "Binary code similarity analysis (BCSA) is a crucial research area in many\nfields such as cybersecurity. Specifically, function-level diffing tools are\nthe most widely used in BCSA: they perform function matching one by one for\nevaluating the similarity between binary programs. However, such methods need a\nhigh time complexity, making them unscalable in large-scale scenarios (e.g.,\n1/n-to-n search). Towards effective and efficient program-level BCSA, we\npropose KEENHash, a novel hashing approach that hashes binaries into\nprogram-level representations through large language model (LLM)-generated\nfunction embeddings. KEENHash condenses a binary into one compact and\nfixed-length program embedding using K-Means and Feature Hashing, allowing us\nto do effective and efficient large-scale program-level BCSA, surpassing the\nprevious state-of-the-art methods. The experimental results show that KEENHash\nis at least 215 times faster than the state-of-the-art function matching tools\nwhile maintaining effectiveness. Furthermore, in a large-scale scenario with\n5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while\nthese tools will cost at least 56 days. We also evaluate KEENHash on the\nprogram clone search of large-scale BCSA across extensive datasets in 202,305\nbinaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of\nthem by at least 23.16%, and displays remarkable superiority over them in the\nlarge-scale BCSA security scenario of malware detection."}
