<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.CR](#cs.CR) [Total: 12]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization](https://arxiv.org/abs/2601.03359)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: 提出一种多智能体工作流，将主任务描述优化与约束条件解耦，通过量化分数反馈迭代改进提示词，显著提升LLM输出对形式约束的遵守程度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常生成内容相关但不符合形式约束的输出，传统提示词优化方法只关注重述主任务描述，忽略了作为响应验收标准的细粒度约束条件。

Method: 提出新颖的多智能体工作流，将主任务描述优化与其约束条件解耦，使用量化分数作为反馈，迭代重写和改进提示词。

Result: 评估表明该方法产生的修订提示词在Llama 3.1 8B和Mixtral-8x 7B等模型上获得了显著更高的合规性分数。

Conclusion: 通过解耦任务描述与约束条件，并使用量化反馈进行迭代优化，可以有效提高LLM输出对形式约束的遵守程度，解决概念正确但程序错误的问题。

Abstract: Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.

</details>


### [2] [Exploration Through Introspection: A Self-Aware Reward Model](https://arxiv.org/abs/2601.03389)
*Michael Petrowski,Milica Gašić*

Main category: cs.AI

TL;DR: 论文提出了一种基于内省探索的强化学习框架，通过隐马尔可夫模型推断"疼痛信念"作为学习信号，研究自我意识对智能体学习能力的影响，并比较正常与慢性疼痛感知模型的性能差异。


<details>
  <summary>Details</summary>
Motivation: 理解人工智能如何建模内部心理状态对于推进心智理论至关重要。证据表明自我认知和他人认知存在统一系统，本研究通过让强化学习智能体在网格世界中推断自身内部状态来探索这种自我意识。

Method: 引入受生物疼痛启发的内省探索组件，使用隐马尔可夫模型从在线观察中推断"疼痛信念"，将该信号整合到主观奖励函数中，构建计算框架比较正常与慢性疼痛感知模型的差异。

Result: 内省智能体总体上显著优于标准基线智能体，能够复现复杂的人类行为模式，验证了自我意识模型在提升学习能力方面的有效性。

Conclusion: 基于内省探索的计算框架为研究人工智能中的自我意识提供了有效工具，证明了自我状态推断能够显著提升智能体性能，并为理解正常与病理性疼痛感知差异提供了计算模型基础。

Abstract: Understanding how artificial agents model internal mental states is central to advancing Theory of Mind in AI. Evidence points to a unified system for self- and other-awareness. We explore this self-awareness by having reinforcement learning agents infer their own internal states in gridworld environments. Specifically, we introduce an introspective exploration component that is inspired by biological pain as a learning signal by utilizing a hidden Markov model to infer "pain-belief" from online observations. This signal is integrated into a subjective reward function to study how self-awareness affects the agent's learning abilities. Further, we use this computational framework to investigate the difference in performance between normal and chronic pain perception models. Results show that introspective agents in general significantly outperform standard baseline agents and can replicate complex human-like behaviors.

</details>


### [3] [Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms](https://arxiv.org/abs/2601.03470)
*Michael C. Darling,Alan H. Hesu,Michael A. Mardikes,Brian C. McGuigan,Reed M. Milewicz*

Main category: cs.AI

TL;DR: 提出基于成熟度的认证框架，通过明确测量机制为具身AI系统提供认证，以不确定性量化为例证，通过无人机系统检测案例验证可行性


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对具身AI系统的结构化认证框架，需要可量化的评估机制来确保其可信度，特别是在多目标权衡的复杂环境中

Method: 提出基于成熟度的认证框架，包含结构化评估框架、定量评分机制和多目标权衡导航方法，以不确定性量化作为示例测量机制，通过无人机系统检测案例进行验证

Result: 展示了该框架的可行性，通过不确定性量化机制能够有效评估具身AI系统的可信度，并在无人机检测案例中验证了方法的实用性

Conclusion: 基于成熟度的认证框架为具身AI系统提供了可量化的认证方法，通过明确的测量机制和多目标权衡导航，能够有效评估和提升系统的可信度

Abstract: We propose a maturity-based framework for certifying embodied AI systems through explicit measurement mechanisms. We argue that certifiable embodied AI requires structured assessment frameworks, quantitative scoring mechanisms, and methods for navigating multi-objective trade-offs inherent in trustworthiness evaluation. We demonstrate this approach using uncertainty quantification as an exemplar measurement mechanism and illustrate feasibility through an Uncrewed Aircraft System (UAS) detection case study.

</details>


### [4] [CPGPrompt: Translating Clinical Guidelines into LLM-Executable Decision Support](https://arxiv.org/abs/2601.03475)
*Ruiqi Deng,Geoffrey Martin,Tony Wang,Gongbo Zhang,Yi Liu,Chunhua Weng,Yanshan Wang,Justin F Rousseau,Yifan Peng*

Main category: cs.AI

TL;DR: CPGPrompt系统将临床指南转化为结构化决策树，利用LLM动态导航进行患者评估，在专科转诊决策上表现优异，但在多类别路径分类上存在领域差异。


<details>
  <summary>Details</summary>
Motivation: 临床实践指南（CPGs）为患者护理提供循证建议，但将其整合到人工智能中面临挑战。现有方法如基于规则的系统存在可解释性差、指南依从性不一致和领域适用性窄等限制。

Method: 开发CPGPrompt自动提示系统，将叙述性临床指南转化为结构化决策树，利用大型语言模型动态导航决策树进行患者病例评估。使用三个领域（头痛、腰痛、前列腺癌）的合成病例进行测试，评估系统在二元专科转诊决策和细粒度路径分类任务上的表现。

Result: 二元专科转诊分类在所有领域表现一致强劲（F1：0.85-1.00），召回率高（1.00±0.00）。多类别路径分配表现降低，存在领域差异：头痛（F1：0.47）、腰痛（F1：0.72）、前列腺癌（F1：0.77）。性能差异反映了各指南的结构特点：头痛指南在否定处理方面存在挑战，腰痛指南需要时间推理，前列腺癌路径受益于可量化的实验室测试。

Conclusion: CPGPrompt系统成功将临床指南整合到LLM中，在专科转诊决策上表现优异，但在复杂路径分类任务上仍需改进。不同指南的结构特点影响系统性能，未来需要针对特定挑战进行优化。

Abstract: Clinical practice guidelines (CPGs) provide evidence-based recommendations for patient care; however, integrating them into Artificial Intelligence (AI) remains challenging. Previous approaches, such as rule-based systems, face significant limitations, including poor interpretability, inconsistent adherence to guidelines, and narrow domain applicability. To address this, we develop and validate CPGPrompt, an auto-prompting system that converts narrative clinical guidelines into large language models (LLMs).
  Our framework translates CPGs into structured decision trees and utilizes an LLM to dynamically navigate them for patient case evaluation. Synthetic vignettes were generated across three domains (headache, lower back pain, and prostate cancer) and distributed into four categories to test different decision scenarios. System performance was assessed on both binary specialty-referral decisions and fine-grained pathway-classification tasks.
  The binary specialty referral classification achieved consistently strong performance across all domains (F1: 0.85-1.00), with high recall (1.00 $\pm$ 0.00). In contrast, multi-class pathway assignment showed reduced performance, with domain-specific variations: headache (F1: 0.47), lower back pain (F1: 0.72), and prostate cancer (F1: 0.77). Domain-specific performance differences reflected the structure of each guideline. The headache guideline highlighted challenges with negation handling. The lower back pain guideline required temporal reasoning. In contrast, prostate cancer pathways benefited from quantifiable laboratory tests, resulting in more reliable decision-making.

</details>


### [5] [Personalization of Large Foundation Models for Health Interventions](https://arxiv.org/abs/2601.03482)
*Stefan Konigorski,Johannes E. Vedder,Babajide Alamu Owoyele,İbrahim Özkan*

Main category: cs.AI

TL;DR: 大型基础模型在医疗AI中面临个性化治疗的根本性矛盾，无法替代N-of-1试验，但两者可以互补结合


<details>
  <summary>Details</summary>
Motivation: 大型基础模型在医疗AI中展现出潜力，但能否提供真正个性化的治疗建议仍存疑问。研究发现存在多重矛盾：泛化悖论（模型在某个临床研究中准确率高，在其他研究中却表现随机）、隐私-性能悖论、规模-特异性悖论、自动化-同理心悖论。此外，个性化推荐需要因果理解而非仅仅是预测能力，这也是开放性问题。

Method: 提出混合框架：大型基础模型擅长利用多模态数据从群体模式中快速生成假设，而N-of-1试验（交叉自我实验，个性化医学中个体因果推断的金标准）擅长为特定个体提供因果验证。该框架结合两者优势：大型基础模型生成带有不确定性估计的干预候选排名，触发后续的N-of-1试验。

Result: 论文论证大型基础模型无法替代N-of-1试验，但两者具有互补性。通过明确预测与因果之间的界限，并明确解决已识别的悖论性张力，可以负责任地将AI整合到个性化医学中。

Conclusion: 大型基础模型和N-of-1试验是互补的：前者擅长基于群体模式的快速假设生成，后者擅长个体因果验证。结合两者的混合框架能够实现个性化并应对已识别的悖论。明确预测与因果的界限对于负责任地整合AI到个性化医学至关重要。

Abstract: Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using multimodal data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.

</details>


### [6] [STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules](https://arxiv.org/abs/2601.03537)
*Di Wu,Yanyan Zhao,Xin Lu,Mingzhe Li,Bing Qin*

Main category: cs.AI

TL;DR: STAR-S是一个通过自我教学循环学习安全规则推理来防御大语言模型越狱攻击的框架


<details>
  <summary>Details</summary>
Motivation: 现有的通过训练模型基于安全规则进行推理的方法难以确定何种形式的安全推理能有效防御越狱攻击，这很难明确设计或直接获得

Method: 提出STAR-S框架，将安全规则推理学习整合到自我教学循环中：在安全规则指导下引出推理和反思，然后通过微调增强安全推理能力，重复此过程形成协同循环

Result: 实验表明STAR-S能有效防御越狱攻击，性能优于基线方法

Conclusion: STAR-S通过自我教学循环学习安全规则推理，为防御大语言模型越狱攻击提供了有效解决方案

Abstract: Defending against jailbreak attacks is crucial for the safe deployment of Large Language Models (LLMs). Recent research has attempted to improve safety by training models to reason over safety rules before responding. However, a key issue lies in determining what form of safety reasoning effectively defends against jailbreak attacks, which is difficult to explicitly design or directly obtain. To address this, we propose \textbf{STAR-S} (\textbf{S}elf-\textbf{TA}ught \textbf{R}easoning based on \textbf{S}afety rules), a framework that integrates the learning of safety rule reasoning into a self-taught loop. The core of STAR-S involves eliciting reasoning and reflection guided by safety rules, then leveraging fine-tuning to enhance safety reasoning. Repeating this process creates a synergistic cycle. Improvements in the model's reasoning and interpretation of safety rules allow it to produce better reasoning data under safety rule prompts, which is then utilized for further training. Experiments show that STAR-S effectively defends against jailbreak attacks, outperforming baselines. Code is available at: https://github.com/pikepokenew/STAR_S.git.

</details>


### [7] [ReEfBench: Quantifying the Reasoning Efficiency of LLMs](https://arxiv.org/abs/2601.03550)
*Zhizhang Fu,Yuancheng Gu,Chenkai Hu,Hanmeng Liu,Yue Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种神经符号框架来评估LLM的推理过程，发现扩展token生成并非深度推理的必要条件，并揭示了训练数据混合和模型蒸馏的关键限制。


<details>
  <summary>Details</summary>
Motivation: 当前CoT评估方法存在局限性，无法区分性能提升是来自真正的推理能力还是仅仅来自更长的输出。需要一种非侵入性、全面的过程中心评估方法来深入理解LLM的推理行为。

Method: 提出了一种神经符号框架，用于对推理过程进行非侵入性、全面的评估。通过这个框架识别了四种不同的行为原型，并诊断了失败模式。研究了推理模式、训练策略和模型规模的影响。

Result: 研究发现：1）扩展token生成并非深度推理的必要条件；2）训练中混合长短CoT数据可能导致过早饱和和崩溃；3）将大模型蒸馏到小模型虽然能复制行为长度，但由于内在容量限制无法复制逻辑效能。

Conclusion: 需要更精细的评估方法来真正理解LLM的推理能力，训练策略和模型规模对推理行为有重要影响，简单的长度扩展不能等同于深度推理能力的提升。

Abstract: Test-time scaling has enabled Large Language Models (LLMs) to tackle complex reasoning, yet the limitations of current Chain-of-Thought (CoT) evaluation obscures whether performance gains stem from genuine reasoning or mere verbosity. To address this, (1) we propose a novel neuro-symbolic framework for the non-intrusive, comprehensive process-centric evaluation of reasoning. (2) Through this lens, we identify four distinct behavioral prototypes and diagnose the failure modes. (3) We examine the impact of inference mode, training strategy, and model scale. Our analysis reveals that extended token generation is not a prerequisite for deep reasoning. Furthermore, we reveal critical constraints: mixing long and short CoT data in training risks in premature saturation and collapse, while distillation into smaller models captures behavioral length but fails to replicate logical efficacy due to intrinsic capacity limits.

</details>


### [8] [Controllable LLM Reasoning via Sparse Autoencoder-Based Steering](https://arxiv.org/abs/2601.03595)
*Yi Fang,Wenjie Wang,Mingfeng Xue,Boyi Deng,Fengli Xu,Dayiheng Liu,Fuli Feng*

Main category: cs.AI

TL;DR: 提出SAE-Steering方法，通过稀疏自编码器分解推理策略纠缠的隐藏状态，并识别策略特定特征作为控制向量，实现对大型推理模型推理策略的有效控制。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然能自主选择推理策略，但这种自主选择常产生低效甚至错误的推理路径。现有方法难以控制细粒度推理策略，因为策略概念在隐藏状态中纠缠在一起。

Method: 使用稀疏自编码器将策略纠缠的隐藏状态分解为解缠的特征空间，提出SAE-Steering两阶段特征识别流程：首先通过放大策略特定关键词的logits召回特征（过滤99%以上特征），然后按控制效果对剩余特征排序。

Result: SAE-Steering在控制效果上比现有方法提升超过15%。通过控制推理策略，能将模型从错误路径重定向到正确路径，实现7%的绝对准确率提升。

Conclusion: SAE-Steering方法能有效识别和控制大型推理模型的推理策略，提高推理的可靠性和灵活性，为模型可控推理提供了新途径。

Abstract: Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing methods struggle to control fine-grained reasoning strategies due to conceptual entanglement in LRMs' hidden states. To address this, we leverage Sparse Autoencoders (SAEs) to decompose strategy-entangled hidden states into a disentangled feature space. To identify the few strategy-specific features from the vast pool of SAE features, we propose SAE-Steering, an efficient two-stage feature identification pipeline. SAE-Steering first recalls features that amplify the logits of strategy-specific keywords, filtering out over 99\% of features, and then ranks the remaining features by their control effectiveness. Using the identified strategy-specific features as control vectors, SAE-Steering outperforms existing methods by over 15\% in control effectiveness. Furthermore, controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones, achieving a 7\% absolute accuracy improvement.

</details>


### [9] [Architecting Agentic Communities using Design Patterns](https://arxiv.org/abs/2601.03624)
*Zoran Milosevic,Fethi Rabhi*

Main category: cs.AI

TL;DR: 论文提出基于企业分布式系统标准、形式化方法和行业实践的AI智能体系统架构设计模式，分为LLM智能体、Agentic AI和智能体社区三个层次，重点关注智能体社区作为企业应用的核心协调框架。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和智能体AI技术的快速发展，需要系统性的架构指导来构建复杂的生产级系统。当前缺乏结合实践指导和形式化验证的架构方法，特别是在多智能体协调和企业应用场景中。

Method: 提出三层设计模式分类：LLM智能体（任务特定自动化）、Agentic AI（自适应目标寻求者）、智能体社区（组织框架，AI智能体与人类通过正式角色、协议和治理结构协调）。基于分布式系统协调原则，建立形式化框架，通过协作协议规范AI智能体与人类在治理生态系统中的角色。

Result: 建立了结合实践指导和形式化验证能力的架构框架，能够通过问责机制表达组织、法律和伦理规则，确保智能体间通信、协商和意图建模的可操作和可验证治理。通过临床试验匹配案例研究验证了该框架。

Conclusion: 该框架为从业者提供可操作的指导，同时保持企业部署在动态多智能体生态系统中所必需的形式化严谨性，填补了智能体AI系统架构指导的空白。

Abstract: The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.

</details>


### [10] [How Does the Thinking Step Influence Model Safety? An Entropy-based Safety Reminder for LRMs](https://arxiv.org/abs/2601.03662)
*Su-Hyeon Kim,Hyundong Jin,Yejin Lee,Yo-Sub Han*

Main category: cs.AI

TL;DR: SafeRemind是一种解码时防御方法，通过在大型推理模型的思考步骤中动态注入安全提醒短语，显著提升模型安全性而不影响推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式思考步骤取得显著成功，但这些步骤可能放大不安全行为。现有防御机制忽视了LRM独特的推理动态，因此需要专门针对推理过程的安全防御方法。

Method: 提出SafeRemind方法，利用熵触发器在决策锁定点进行干预，动态地将安全提醒短语注入到思考步骤中，将潜在有害轨迹重定向到更安全的结果，无需参数更新。

Result: 在5个LRM和6个基准测试上的广泛评估表明，SafeRemind显著提升了安全性，最高改善45.5个百分点，同时保持了核心推理效用。

Conclusion: 思考步骤中的安全提醒短语对确保LRM安全至关重要，SafeRemind作为一种有效的解码时防御方法，能够在不影响推理能力的情况下大幅提升模型安全性。

Abstract: Large Reasoning Models (LRMs) achieve remarkable success through explicit thinking steps, yet the thinking steps introduce a novel risk by potentially amplifying unsafe behaviors. Despite this vulnerability, conventional defense mechanisms remain ineffective as they overlook the unique reasoning dynamics of LRMs. In this work, we find that the emergence of safe-reminding phrases within thinking steps plays a pivotal role in ensuring LRM safety. Motivated by this finding, we propose SafeRemind, a decoding-time defense method that dynamically injects safe-reminding phrases into thinking steps. By leveraging entropy triggers to intervene at decision-locking points, SafeRemind redirects potentially harmful trajectories toward safer outcomes without requiring any parameter updates. Extensive evaluations across five LRMs and six benchmarks demonstrate that SafeRemind substantially enhances safety, achieving improvements of up to 45.5%p while preserving core reasoning utility.

</details>


### [11] [Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction](https://arxiv.org/abs/2601.03672)
*Chen Zhang,Kepu Zhang,Jiatong Zhang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: SandwichR是一种新颖的推理方法，通过"答案-推理-答案"范式，在保持CoT推理准确性的同时大幅降低延迟，解决了查询修正中的延迟-准确性权衡问题。


<details>
  <summary>Details</summary>
Motivation: 查询修正是现代搜索管道的关键入口点，需要在实时延迟约束下保持高准确性。CoT推理虽然能提高准确性，但延迟过高无法满足实时要求。提前输出答案可以降低延迟，但在自回归解码下，早期答案与后续推理无关，无法利用推理能力提高准确性。

Method: 提出SandwichR方法，采用"答案-推理-答案"范式：先生成初始修正，然后进行显式推理过程，最后生成精炼的最终修正。使用一致性感知强化学习策略，通过专门的一致性奖励强制初始修正与最终修正对齐，同时基于边界的拒绝采样优先处理推理能带来最大修正增益的边界样本。还构建了高质量的查询修正数据集。

Result: 实验结果表明，SandwichR在准确性上达到与标准CoT相当的最先进水平，同时实现了40-70%的延迟降低，成功解决了在线搜索中的延迟-准确性权衡问题。

Conclusion: SandwichR通过将快速初始答案与事后推理对齐，实现了低延迟查询修正而不牺牲推理感知的准确性，为实时搜索系统中的查询修正提供了有效的解决方案。

Abstract: Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.

</details>


### [12] [Personalized Medication Planning via Direct Domain Modeling and LLM-Generated Heuristics](https://arxiv.org/abs/2601.03687)
*Yonatan Vernik,Alexander Tuisov,David Izhaki,Hana Weitman,Gal A. Kaminka,Alexander Shleyfman*

Main category: cs.AI

TL;DR: 使用LLM生成特定问题启发式函数，结合GBFS搜索算法，将个性化用药规划的药物数量从7种扩展到28种以上，显著提升规划覆盖率和效率


<details>
  <summary>Details</summary>
Motivation: 个性化用药规划需要为每位患者选择药物并确定给药方案，但现有基于通用领域无关启发式函数的自动化规划方法最多只能处理7种药物，这在临床实践中远远不够，需要扩展到更多药物数量

Method: 通过程序化方式指定领域（定义初始状态和状态转移过程），使用LLM生成特定问题启发式函数，结合固定的贪婪最佳优先搜索（GBFS）算法进行规划

Result: 该方法显著提升了规划覆盖率和规划时间，能够处理至少28种药物，将个性化用药规划向实际应用推进了一步

Conclusion: 使用LLM自动生成领域和问题特定启发式函数的方法，能够有效扩展个性化用药规划的规模，使其更接近临床应用需求

Abstract: Personalized medication planning involves selecting medications and determining a dosing schedule to achieve medical goals specific to each individual patient. Previous work successfully demonstrated that automated planners, using general domain-independent heuristics, are able to generate personalized treatments, when the domain and problems are modeled using a general domain description language (\pddlp). Unfortunately, this process was limited in practice to consider no more than seven medications. In clinical terms, this is a non-starter. In this paper, we explore the use of automatically-generated domain- and problem-specific heuristics to be used with general search, as a method of scaling up medication planning to levels allowing closer work with clinicians. Specifically, we specify the domain programmatically (specifying an initial state and a successor generation procedure), and use an LLM to generate a problem specific heuristic that can be used by a fixed search algorithm (GBFS). The results indicate dramatic improvements in coverage and planning time, scaling up the number of medications to at least 28, and bringing medication planning one step closer to practical applications.

</details>


### [13] [EntroCoT: Enhancing Chain-of-Thought via Adaptive Entropy-Guided Segmentation](https://arxiv.org/abs/2601.03769)
*Zihang Li,Yuhang Wang,Yikun Zong,Wenhan Yu,Xiaokun Yuan,Runhan Jiang,Zirui Liu,Tong Yang,Arthur Jiang*

Main category: cs.AI

TL;DR: EntroCoT：通过熵基分割和蒙特卡洛评估自动识别和优化低质量思维链监督数据，提升数学推理能力


<details>
  <summary>Details</summary>
Motivation: 现有微调数据集存在"答案正确但推理错误"的问题，即最终答案正确但中间步骤存在幻觉、冗余或逻辑错误，这影响了模型推理能力的提升

Method: 提出EntroCoT框架：1）基于熵的机制在不确定节点分割推理轨迹；2）蒙特卡洛rollout机制评估每个步骤的边际贡献；3）准确过滤欺骗性推理样本，构建高质量数据集

Result: 在数学基准测试上的广泛实验表明，使用EntroCoT构建的子集进行微调，其性能始终优于全数据集监督的基线方法

Conclusion: EntroCoT能有效识别和优化低质量思维链监督数据，构建高质量推理数据集，显著提升大语言模型的数学推理能力

Abstract: Chain-of-Thought (CoT) prompting has significantly enhanced the mathematical reasoning capabilities of Large Language Models. We find existing fine-tuning datasets frequently suffer from the "answer right but reasoning wrong" probelm, where correct final answers are derived from hallucinated, redundant, or logically invalid intermediate steps. This paper proposes EntroCoT, a unified framework for automatically identifying and refining low-quality CoT supervision traces. EntroCoT first proposes an entropy-based mechanism to segment the reasoning trace into multiple steps at uncertain junctures, and then introduces a Monte Carlo rollout-based mechanism to evaluate the marginal contribution of each step. By accurately filtering deceptive reasoning samples, EntroCoT constructs a high-quality dataset where every intermediate step in each reasoning trace facilitates the final answer. Extensive experiments on mathematical benchmarks demonstrate that fine-tuning on the subset constructed by EntroCoT consistently outperforms the baseslines of full-dataset supervision.

</details>


### [14] [ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition](https://arxiv.org/abs/2601.03822)
*Muyang Zhao,Qi Qi,Hao Sun*

Main category: cs.AI

TL;DR: 论文提出ROI-Reasoning框架，通过元认知微调和理性感知强化学习，让大语言模型在严格计算预算下进行推理决策，优化多任务的计算资源分配。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具备强大的推理能力，但无法自动判断不同任务所需的计算量，在严格的计算预算约束下，需要一种能够预测任务难度、评估投资回报率并战略性地分配计算资源的方法。

Method: 提出ROI-Reasoning两阶段框架：1) 元认知微调阶段，训练模型在生成前预测推理成本和预期效用，做出明确的解决或跳过决策；2) 理性感知强化学习阶段，在严格的token预算下优化序列决策，学习长期分配策略。

Result: 在预算约束的数学推理基准测试中，ROI-Reasoning在保持整体得分的同时，显著降低了在严格计算预算下的遗憾值。

Conclusion: 该研究为LLMs引入了内在的预算感知理性能力，通过形式化为有序随机多选择背包问题，实现了在有限计算资源下的最优推理决策，为资源受限环境下的高效推理提供了新思路。

Abstract: Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.

</details>


### [15] [Defeasible Conditionals using Answer Set Programming](https://arxiv.org/abs/2601.03840)
*Racquel Dennison,Jesse Heyninck,Thomas Meyer*

Main category: cs.AI

TL;DR: 本文提出了一种使用答案集编程（ASP）计算理性闭包（RC）的声明式方法，实现了从知识库自动构建最小排序模型并进行蕴涵检查，相比现有命令式实现具有更好的计算效率。


<details>
  <summary>Details</summary>
Motivation: 可废止蕴涵处理从不完整信息中得出合理结论的问题，KLM框架是其基础模型，理性闭包是该框架中最突出的算法之一。现有实现多为命令式方法，需要更高效、声明式的计算方法。

Method: 使用答案集编程（ASP）为理性闭包提供声明式定义，能够自动从给定知识库构建最小排序模型，并支持对指定查询进行蕴涵检查。

Result: 形式化证明了ASP编码的正确性，并通过实验评估将ASP实现与现有命令式实现（InfOCF求解器）进行性能比较，结果显示ASP方法遵循RC的理论基础且提供更好的计算效率。

Conclusion: ASP为理性闭包提供了一种有效的声明式计算方法，既保持了理论正确性，又提高了计算效率，为可废止推理的实际应用提供了更好的工具。

Abstract: Defeasible entailment is concerned with drawing plausible conclusions from incomplete information. A foundational framework for modelling defeasible entailment is the KLM framework. Introduced by Kraus, Lehmann, and Magidor, the KLM framework outlines several key properties for defeasible entailment. One of the most prominent algorithms within this framework is Rational Closure (RC). This paper presents a declarative definition for computing RC using Answer Set Programming (ASP). Our approach enables the automatic construction of the minimal ranked model from a given knowledge base and supports entailment checking for specified queries. We formally prove the correctness of our ASP encoding and conduct empirical evaluations to compare the performance of our implementation with that of existing imperative implementations, specifically the InfOCF solver. The results demonstrate that our ASP-based approach adheres to RC's theoretical foundations and offers improved computational efficiency.

</details>


### [16] [XAI-LAW: A Logic Programming Tool for Modeling, Explaining, and Learning Legal Decisions](https://arxiv.org/abs/2601.03844)
*Agostino Dovier,Talissa Dreossi,Andrea Formisano,Benedetta Strizzolo*

Main category: cs.AI

TL;DR: 使用答案集编程（ASP）对意大利刑法典进行建模，并基于司法判例半自动学习法律规则，开发支持法律专家在刑事审判阶段进行推理和预测法律结果的工具。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够支持法律专家在刑事审判阶段进行推理和预测法律结果的工具，通过将法律条文编码为形式化逻辑规则，提高法律决策的透明度和可解释性。

Method: 使用答案集编程（ASP）对意大利刑法典中的"人身犯罪"和财产犯罪条款进行建模和编码，通过验证先前判决来完善模型，并利用ASP的"支持性"稳定模型提供解释。同时集成归纳逻辑编程系统从案例中归纳法律规则。

Result: 开发了一个能够处理法律条文编码中可能出现的矛盾、为新案件生成可能决策并提供解释的工具。该工具能够基于先前判决验证模型，并通过归纳逻辑编程从案例中学习法律规则。

Conclusion: ASP方法能够有效建模意大利刑法典并支持法律推理，提供的自动解释功能使司法决策过程更加透明和可解释，同时通过归纳学习能够从案例中提取法律规则，为法律专家提供有价值的决策支持。

Abstract: We propose an approach to model articles of the Italian Criminal Code (ICC), using Answer Set Programming (ASP), and to semi-automatically learn legal rules from examples based on prior judicial decisions. The developed tool is intended to support legal experts during the criminal trial phase by providing reasoning and possible legal outcomes. The methodology involves analyzing and encoding articles of the ICC in ASP, including "crimes against the person" and property offenses. The resulting model is validated on a set of previous verdicts and refined as necessary. During the encoding process, contradictions may arise; these are properly handled by the system, which also generates possible decisions for new cases and provides explanations through a tool that leverages the "supportedness" of stable models. The automatic explainability offered by the tool can also be used to clarify the logic behind judicial decisions, making the decision-making process more interpretable. Furthermore, the tool integrates an inductive logic programming system for ASP, which is employed to generalize legal rules from case examples.

</details>


### [17] [Formally Explaining Decision Tree Models with Answer Set Programming](https://arxiv.org/abs/2601.03845)
*Akihiro Takemura,Masayuki Otani,Katsumi Inoue*

Main category: cs.AI

TL;DR: 本文提出了一种基于答案集编程（ASP）的方法，用于为决策树模型生成多种解释类型，相比SAT方法更具灵活性且支持枚举所有可能解释。


<details>
  <summary>Details</summary>
Motivation: 决策树模型（如随机森林和梯度提升决策树）在机器学习中广泛应用，但其复杂结构难以解释，特别是在需要正式论证的安全关键应用中。现有工作已证明可以通过自动推理技术推导逻辑和溯因解释。

Method: 提出使用答案集编程（ASP）生成多种解释类型的方法，包括充分解释、对比解释、多数解释和树特定解释。相比基于SAT的方法，ASP方法在编码用户偏好方面更具灵活性，并支持枚举所有可能的解释。

Result: 在多样化数据集上进行了实证评估，证明了该方法的有效性，并与现有方法进行了比较，展示了其优势和局限性。

Conclusion: 基于ASP的方法为决策树模型解释提供了更灵活和全面的解决方案，能够生成多种类型的解释并支持完整枚举，在可解释AI领域具有应用价值。

Abstract: Decision tree models, including random forests and gradient-boosted decision trees, are widely used in machine learning due to their high predictive performance.  However, their complex structures often make them difficult to interpret, especially in safety-critical applications where model decisions require formal justification.  Recent work has demonstrated that logical and abductive explanations can be derived through automated reasoning techniques.  In this paper, we propose a method for generating various types of explanations, namely, sufficient, contrastive, majority, and tree-specific explanations, using Answer Set Programming (ASP).  Compared to SAT-based approaches, our ASP-based method offers greater flexibility in encoding user preferences and supports enumeration of all possible explanations.  We empirically evaluate the approach on a diverse set of datasets and demonstrate its effectiveness and limitations compared to existing methods.

</details>


### [18] [Investigating the Grounding Bottleneck for a Large-Scale Configuration Problem: Existing Tools and Constraint-Aware Guessing](https://arxiv.org/abs/2601.03850)
*Veronika Semmelrock,Gerhard Friedrich*

Main category: cs.AI

TL;DR: ASP技术在处理大规模配置问题时面临内存瓶颈，研究者通过增量求解和约束感知猜测方法显著降低了内存需求


<details>
  <summary>Details</summary>
Motivation: 研究当前ASP求解技术是否能够扩展到大规模配置问题，特别是电子系统配置这类可能包含超过30,000个组件的复杂问题

Method: 采用增量求解方法，并基于对基础化过程的分析开发了约束感知猜测方法，以解决基础化瓶颈问题

Result: 增量求解方法在实践中有效，但内存需求仍构成显著限制；约束感知猜测方法显著降低了内存需求

Conclusion: 当前ASP技术在处理大规模配置问题时存在内存瓶颈，但通过增量求解和约束感知猜测等优化方法可以显著改善性能，推动ASP技术的应用边界

Abstract: Answer set programming (ASP) aims to realize the AI vision: The user specifies the problem, and the computer solves it. Indeed, ASP has made this vision true in many application domains. However, will current ASP solving techniques scale up for large configuration problems? As a benchmark for such problems, we investigated the configuration of electronic systems, which may comprise more than 30,000 components. We show the potential and limits of current ASP technology, focusing on methods that address the so-called grounding bottleneck, i.e., the sharp increase of memory demands in the size of the problem instances. To push the limits, we investigated the incremental solving approach, which proved effective in practice. However, even in the incremental approach, memory demands impose significant limits. Based on an analysis of grounding, we developed the method constraint-aware guessing, which significantly reduced the memory need.

</details>


### [19] [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](https://arxiv.org/abs/2601.03948)
*Rui Sun,Yifan Sun,Sheng Xu,Li Zhao,Jing Li,Daxin Jiang,Cheng Hua,Zuo Bai*

Main category: cs.AI

TL;DR: Trade-R1：通过过程级推理验证将可验证奖励与随机金融环境连接，解决标准RL在金融决策中的奖励黑客问题


<details>
  <summary>Details</summary>
Motivation: 强化学习在数学和编程等可验证奖励领域表现出色，但在金融决策中面临挑战：市场具有随机性，奖励虽然可验证但本质上是噪声的，导致标准RL退化为奖励黑客

Method: 提出Trade-R1框架，通过过程级推理验证连接可验证奖励与随机环境。核心创新是将冗长金融文档的推理评估转化为结构化RAG任务，构建三角一致性度量来评估检索证据、推理链和决策之间的对齐，作为噪声市场回报的有效性过滤器。探索两种奖励整合策略：固定效应语义奖励（FSR）和动态效应语义奖励（DSR）

Result: 在不同国家资产选择实验中，该范式减少了奖励黑客问题，DSR实现了优越的跨市场泛化能力，同时保持了最高的推理一致性

Conclusion: Trade-R1通过过程级推理验证有效解决了金融决策中RL的奖励黑客问题，为随机环境中的可验证奖励应用提供了新范式

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.

</details>


### [20] [Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models](https://arxiv.org/abs/2601.03969)
*Wei Wu,Liyi Chen,Congxi Xiao,Tianfu Wang,Qimeng Wang,Chengqiang Lu,Yan Gao,Yi Wu,Yao Hu,Hui Xiong*

Main category: cs.AI

TL;DR: 论文提出DOT方法解决大模型推理中的过度冗长问题，通过动态截断冗余token减少78%推理token使用，同时提升准确率


<details>
  <summary>Details</summary>
Motivation: 现有增强推理模型在简单查询上过度冗长，导致部署成本高；现有基于长度惩罚的方法存在优化冲突，且未深入探究过度思考的生成机制

Method: 提出动态异常截断(DOT)：在训练时选择性抑制冗余token，仅针对完全正确rollout组中的极端长度尾部；结合辅助KL正则化和预测动态采样确保稳定收敛

Result: 在多个模型规模上显著扩展效率-性能帕累托前沿；在AIME-24上减少78%推理token使用，同时提高准确率，超越现有高效推理方法

Conclusion: DOT方法有效解决长度偏移现象，在保持复杂问题长程推理能力的同时，显著提升推理效率，为高效推理提供了新思路

Abstract: Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.

</details>


### [21] [MobileDreamer: Generative Sketch World Model for GUI Agent](https://arxiv.org/abs/2601.04035)
*Yilin Cao,Yufeng Zhong,Zhixiong Zeng,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Wenji Mao,Wan Guanglu*

Main category: cs.AI

TL;DR: MobileDreamer：基于世界模型的移动GUI智能体前瞻框架，通过文本草图世界模型预测动作后状态，提升长时程任务性能


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体多为反应式，主要依赖当前屏幕信息进行决策，在长时程任务中表现受限。构建世界模型能够预测动作结果，支持更好的决策制定。

Method: 提出MobileDreamer框架，包含文本草图世界模型和滚动想象策略。文本草图世界模型将数字图像转换为关键任务相关草图，采用顺序不变学习策略保持GUI元素空间信息；滚动想象策略利用世界模型预测能力优化动作选择过程。

Result: 在Android World上的实验表明，MobileDreamer达到最先进性能，任务成功率提升5.25%。世界模型评估进一步验证了文本草图建模能准确预测关键GUI元素。

Conclusion: MobileDreamer通过高效的世界模型前瞻框架，显著提升了移动GUI智能体在长时程任务中的性能，为实际部署提供了可行的解决方案。

Abstract: Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.

</details>


### [22] [ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows](https://arxiv.org/abs/2601.04060)
*Jinwei Su,Qizhen Lan,Zeyu Wang,Yinghui Xia,Hairu Wen,Yiqun Duan,Xi Xiao,Tianyu Shi,Yang Jingsong,Lewei He*

Main category: cs.AI

TL;DR: ComfySearch：一个基于验证引导的智能体框架，用于在ComfyUI平台上探索组件空间并生成功能性工作流，显著提升执行成功率、解决方案率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: ComfyUI平台上的AI生成内容已从单一模型发展到模块化工作流，但大量组件和严格图约束下保持长期结构一致性困难，导致执行通过率低、工作流质量有限。

Method: 提出ComfySearch框架，采用智能体探索组件空间，通过验证引导的工作流构建方法生成功能性ComfyUI管道。

Result: 实验表明ComfySearch在复杂创意任务上显著优于现有方法，实现了更高的执行通过率、解决方案率和更强的泛化能力。

Conclusion: ComfySearch有效解决了ComfyUI平台中组件空间探索和工作流构建的挑战，为模块化AI内容生成提供了更可靠的解决方案。

Abstract: AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.

</details>


### [23] [Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions](https://arxiv.org/abs/2601.04170)
*Abhishek Rath*

Main category: cs.AI

TL;DR: 该研究提出了"智能体漂移"概念，指多智能体LLM系统中行为、决策质量和协作一致性随时间逐渐退化的问题，并开发了量化框架和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统在复杂任务分解和协作解决问题方面表现出强大能力，但其长期行为稳定性尚未得到充分研究。需要理解智能体在长时间交互序列中行为退化的现象及其影响。

Method: 提出了智能体漂移的理论框架，包括语义漂移、协调漂移和行为漂移三种表现形式。开发了智能体稳定性指数（ASI），一个包含12个维度的复合度量框架，用于量化漂移现象。通过模拟分析和理论建模进行研究。

Result: 研究表明，未受控制的智能体漂移会导致任务完成准确率显著下降，并增加人工干预需求。理论分析表明，提出的缓解策略（情景记忆整合、漂移感知路由协议、自适应行为锚定）能显著减少漂移相关错误，同时保持系统吞吐量。

Conclusion: 这项工作为监测、测量和缓解生产级智能AI系统中的智能体漂移建立了基础方法论，对企业部署可靠性和AI安全研究具有直接意义。

Abstract: Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).
  We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.
  We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [24] [The Anatomy of a Successful Student Scrum Team: Motivation, Personalities, and Academic Adaptation](https://arxiv.org/abs/2601.03364)
*Nadia Damianova,Santiago Berrezueta-Guzman*

Main category: cs.SE

TL;DR: 本研究通过为期一年的案例研究，分析了8人学生团队在混合工作环境下如何调整Scrum实践以适应学术约束，发现轻量级工具协调、灵活冲刺周期和团队动力对项目成功至关重要。


<details>
  <summary>Details</summary>
Motivation: 虽然敏捷方法和Scrum在软件工程教育中被广泛教授，但缺乏关于这些实践在长期学生主导项目中如何适应学术约束和混合工作环境的实证证据。

Method: 采用为期一年的案例研究，分析8人学生开发团队设计和实施虚拟现实校园游戏的项目。通过Discord、Notion、GitHub的定性观察和工件，结合贡献指标和自定义沟通有效性指数（得分：0.76/1.00），评估三个维度：协作工具有效性、混合工作对沟通和生产率的影响、Scrum与学术时间表的协调可行性。

Result: 研究发现：(1)轻量级工具协调即使在远程期间也能保持稳定进展；(2)一周冲刺和灵活仪式有助于协调Scrum与学术义务；(3)共享动机、角色清晰度和兼容工作风格与流程机制同等重要。

Conclusion: 为教师和学生在混合项目式学习环境中采用敏捷方法提出了实用建议，强调轻量级工具协调、灵活流程调整和团队动力管理的重要性。

Abstract: Agile methods, and Scrum in particular, are widely taught in software engineering education; however, there is limited empirical evidence on how these practices function in long-running, student-led projects under academic and hybrid work constraints. This paper presents a year-long case study of an eight-person student development team tasked with designing and implementing a virtual reality game that simulates a university campus and provides program-related educational content. We analyze how the team adapted Scrum practices (sprint structure, roles, backlog management) to fit semester rhythms, exams, travel, and part-time availability, and how communication and coordination were maintained in a hybrid on-site/remote environment. Using qualitative observations and artifacts from Discord, Notion, and GitHub, as well as contribution metrics and a custom communication effectiveness index (score: 0.76/1.00), we evaluate three dimensions: (1) the effectiveness of collaboration tools, (2) the impact of hybrid work on communication and productivity, and (3) the feasibility of aligning Scrum with academic timelines. Our findings show that (i) lightweight, tool-mediated coordination enabled stable progress even during remote periods; (ii) one-week sprints and flexible ceremonies helped reconcile Scrum with academic obligations; and (iii) shared motivation, role clarity, and compatible working styles were as critical as process mechanics. We propose practical recommendations for instructors and student teams adopting agile methods in hybrid, project-based learning settings.

</details>


### [25] [An Empirical Analysis of Community and Coding Patterns in OSS4SG vs. Conventional OSS](https://arxiv.org/abs/2601.03430)
*Mohamed Ouf,Shayan Noei,Zeph Van Iterson,Mariam Guizani,Ying Zou*

Main category: cs.SE

TL;DR: OSS4SG项目相比传统OSS项目拥有更稳定、粘性更强的社区，贡献者参与度更均衡，核心贡献者在代码质量和问题解决中都发挥关键作用。


<details>
  <summary>Details</summary>
Motivation: 虽然传统开源软件已有大量研究，但针对社会公益开源软件（OSS4SG）的社区动态和贡献者模式了解甚少，特别是其使命驱动性质如何影响开发实践。

Method: 对1,039个GitHub仓库进行大规模实证研究，包括422个OSS4SG项目和617个传统OSS项目，比较社区结构、贡献者参与度和编码实践。

Result: OSS4SG项目社区更稳定（63.4%粘性），传统OSS项目更易吸引新贡献者（75.4%磁性）；OSS4SG全年参与度稳定，传统OSS有季节性波动；OSS4SG核心贡献者同时负责代码质量和问题解决，传统OSS中核心贡献者专注代码质量，临时贡献者负责问题解决。

Conclusion: OSS4SG项目的使命驱动性质塑造了独特的社区动态，形成了更稳定、粘性更强的社区结构，这对确保社会公益开源项目的可持续性和长期影响力具有重要意义。

Abstract: Open Source Software for Social Good (OSS4SG) projects aim to address critical societal challenges, such as healthcare access and community safety. Understanding the community dynamics and contributor patterns in these projects is essential for ensuring their sustainability and long-term impact. However, while extensive research has focused on conventional Open Source Software (OSS), little is known about how the mission-driven nature of OSS4SG influences its development practices. To address this gap, we conduct a large-scale empirical study of 1,039 GitHub repositories, comprising 422 OSS4SG and 617 conventional OSS projects, to compare community structure, contributor engagement, and coding practices. Our findings reveal that OSS4SG projects foster significantly more stable and "sticky" (63.4%) communities, whereas conventional OSS projects are more "magnetic" (75.4%), attracting a high turnover of contributors. OSS4SG projects also demonstrate consistent engagement throughout the year, while conventional OSS communities exhibit seasonal fluctuations. Additionally, OSS4SG projects rely heavily on core contributors for both code quality and issue resolution, while conventional OSS projects leverage casual contributors for issue resolution, with core contributors focusing primarily on code quality.

</details>


### [26] [CodeEval: A pedagogical approach for targeted evaluation of code-trained Large Language Models](https://arxiv.org/abs/2601.03432)
*Danny Brahman,Mohammad Mahoor*

Main category: cs.SE

TL;DR: 论文提出了CodeEval基准数据集和RunCodeEval执行框架，用于多维度评估大语言模型的Python编程能力，填补现有基准在代码生成评估方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估主要关注常识推理、语言理解和逻辑推理能力，但在代码生成能力评估方面存在显著差距。现有的基准数据集无法精确定位模型的具体优势和弱点，阻碍了针对性地提升模型的代码合成推理能力。

Method: 1. 提出CodeEval多维度基准数据集，涵盖Python编程的24个不同方面，包含三个熟练级别（初级、中级、高级）以及基于类和基于函数的问题类型；2. 开发RunCodeEval开源执行框架，提供即用型评估管道，处理测试执行、上下文设置和指标生成。

Result: CodeEval基准数据集能够对LLMs进行严格评估，RunCodeEval框架使研究人员能够快速获得模型在不同复杂度级别、问题类型和编程类别上的详细优劣势分析。

Conclusion: 该方法填补了LLMs代码生成能力评估的空白，通过教学式基准测试方法，为针对性评估和改进LLMs的编程能力提供了有效工具，有助于指导模型编程能力的提升。

Abstract: Large Language Models (LLMs) are predominantly assessed based on their common sense reasoning, language comprehension, and logical reasoning abilities. While models trained in specialized domains like mathematics or coding have demonstrated remarkable advancements in logical reasoning, there remains a significant gap in evaluating their code generation capabilities. Existing benchmark datasets fall short in pinpointing specific strengths and weaknesses, impeding targeted enhancements in models' reasoning abilities to synthesize code. To bridge this gap, our paper introduces an innovative, pedagogical benchmarking method that mirrors the evaluation processes encountered in academic programming courses. We introduce CodeEval, a multi-dimensional benchmark dataset designed to rigorously evaluate LLMs across 24 distinct aspects of Python programming. The dataset covers three proficiency levels - beginner, intermediate, and advanced - and includes both class-based and function-based problem types with detailed problem specifications and comprehensive test suites. To facilitate widespread adoption, we also developed RunCodeEval, an open-source execution framework that provides researchers with a ready-to-use evaluation pipeline for CodeEval. RunCodeEval handles test execution, context setup, and metrics generation, enabling researchers to quickly obtain detailed insights into model strengths and weaknesses across complexity levels, problem types, and programming categories. This combination enables targeted evaluation and guides improvements in LLMs' programming proficiencies.

</details>


### [27] [Bootstrapping Code Translation with Weighted Multilanguage Exploration](https://arxiv.org/abs/2601.03512)
*Yuhan Wu,Huan Zhang,Wei Cheng,Chen Shen,Jingyue Yang,Wei Hu*

Main category: cs.SE

TL;DR: BootTrans是一种利用测试套件功能不变性和跨语言可移植性的引导方法，通过双池架构和语言感知权重机制解决多语言代码翻译中的数据稀缺和优化不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多语言代码翻译面临两个关键挑战：1）并行数据稀缺且缺乏可执行的测试预言；2）处理不同语言对时存在优化不平衡问题。

Method: 提出BootTrans引导方法，利用测试套件的功能不变性和跨语言可移植性，将丰富的枢轴语言单元测试适配为多语言RL训练的通用验证预言。采用双池架构（种子池和探索池）通过执行引导的经验收集逐步扩展训练数据，并设计语言感知权重机制，基于兄弟语言间的相对性能动态优先处理较难的翻译方向。

Result: 在HumanEval-X和TransCoder-Test基准测试上的广泛实验表明，该方法在所有翻译方向上都显著优于基线LLM，消融实验验证了引导和权重组件的有效性。

Conclusion: BootTrans通过利用测试套件的跨语言可移植性和动态语言感知权重，有效解决了多语言代码翻译中的数据稀缺和优化不平衡问题，实现了显著的性能提升。

Abstract: Code translation across multiple programming languages is essential yet challenging due to two vital obstacles: scarcity of parallel data paired with executable test oracles, and optimization imbalance when handling diverse language pairs. We propose BootTrans, a bootstrapping method that resolves both obstacles. Its key idea is to leverage the functional invariance and cross-lingual portability of test suites, adapting abundant pivot-language unit tests to serve as universal verification oracles for multilingual RL training. Our method introduces a dual-pool architecture with seed and exploration pools to progressively expand training data via execution-guided experience collection. Furthermore, we design a language-aware weighting mechanism that dynamically prioritizes harder translation directions based on relative performance across sibling languages, mitigating optimization imbalance. Extensive experiments on the HumanEval-X and TransCoder-Test benchmarks demonstrate substantial improvements over baseline LLMs across all translation directions, with ablations validating the effectiveness of both bootstrapping and weighting components.

</details>


### [28] [Deploy-Master: Automating the Deployment of 50,000+ Agent-Ready Scientific Tools in One Day](https://arxiv.org/abs/2601.03513)
*Yi Wang,Zhenting Huang,Zhaohan Ding,Ruoxue Liao,Yuan Huang,Xinzijian Liu,Jiajun Xie,Siheng Chen,Linfeng Zhang*

Main category: cs.SE

TL;DR: Deploy-Master是一个自动化工作流，用于大规模发现、构建、验证和发布开源科学软件工具，解决了科学软件部署困难的问题，在一天内成功构建了50,112个可执行工具。


<details>
  <summary>Details</summary>
Motivation: 开源科学软件虽然丰富，但大多数工具难以编译、配置和重用，这限制了科学计算的可重复性、大规模评估以及与现代AI-for-Science和智能体工作流的集成。这种部署瓶颈阻碍了科学软件的广泛应用。

Method: Deploy-Master采用一站式智能体工作流，包括：1）基于90+科学工程领域分类的大规模工具发现；2）从50万个公共仓库中筛选出52,550个候选工具；3）构建规范推断；4）基于执行的验证；5）容器化部署。系统将异构开源仓库转化为可运行的容器化能力，并通过最小可执行命令验证每个工具。

Result: 在一天内完成了52,550次构建尝试，成功为50,112个科学工具构建了可重复的运行时环境。所有成功工具都注册到SciencePedia中供搜索和重用。研究还提供了50,000个工具规模的部署跟踪数据，揭示了吞吐量、成本分布、失败模式和规范不确定性等大规模部署特征。

Conclusion: Deploy-Master不仅提供了可运行的工具集合，还通过大规模部署实践揭示了科学软件难以操作化的根本原因。研究强调了共享、可观察的执行基础对于可扩展的AI-for-Science和智能体科学的重要性，为解决科学软件部署瓶颈提供了系统性方案。

Abstract: Open-source scientific software is abundant, yet most tools remain difficult to compile, configure, and reuse, sustaining a small-workshop mode of scientific computing. This deployment bottleneck limits reproducibility, large-scale evaluation, and the practical integration of scientific tools into modern AI-for-Science (AI4S) and agentic workflows.
  We present Deploy-Master, a one-stop agentic workflow for large-scale tool discovery, build specification inference, execution-based validation, and publication. Guided by a taxonomy spanning 90+ scientific and engineering domains, our discovery stage starts from a recall-oriented pool of over 500,000 public repositories and progressively filters it to 52,550 executable tool candidates under license- and quality-aware criteria. Deploy-Master transforms heterogeneous open-source repositories into runnable, containerized capabilities grounded in execution rather than documentation claims. In a single day, we performed 52,550 build attempts and constructed reproducible runtime environments for 50,112 scientific tools. Each successful tool is validated by a minimal executable command and registered in SciencePedia for search and reuse, enabling direct human use and optional agent-based invocation.
  Beyond delivering runnable tools, we report a deployment trace at the scale of 50,000 tools, characterizing throughput, cost profiles, failure surfaces, and specification uncertainty that become visible only at scale. These results explain why scientific software remains difficult to operationalize and motivate shared, observable execution substrates as a foundation for scalable AI4S and agentic science.

</details>


### [29] [On the Robustness of Fairness Practices: A Causal Framework for Systematic Evaluation](https://arxiv.org/abs/2601.03621)
*Verya Monjezi,Ashish Kumar,Ashutosh Trivedi,Gang Tan,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 该论文探讨机器学习公平性实践在现实数据问题（如错误标签、缺失数据和分布偏移）下的可靠性问题


<details>
  <summary>Details</summary>
Motivation: 机器学习算法在社会经济应用中做出关键决策时可能存在公平性问题，当前软件工程和ML社区已提出多种公平性干预措施，但这些实践在现实数据问题下的可靠性尚未得到充分验证

Method: 论文通过系统性地评估现有公平性实践在存在错误标签、缺失数据和分布偏移等现实数据问题下的表现，分析这些推荐实践的可靠性

Result: 研究发现当前推荐的公平性实践在现实数据问题下可能不够可靠，无法很好地泛化到存在数据质量问题的实际场景中

Conclusion: 需要更稳健的公平性评估框架和干预措施，以应对现实世界中的数据质量问题，确保机器学习系统在实际部署中的公平性

Abstract: Machine learning (ML) algorithms are increasingly deployed to make critical decisions in socioeconomic applications such as finance, criminal justice, and autonomous driving. However, due to their data-driven and pattern-seeking nature, ML algorithms may develop decision logic that disproportionately distributes opportunities, benefits, resources, or information among different population groups, potentially harming marginalized communities. In response to such fairness concerns, the software engineering and ML communities have made significant efforts to establish the best practices for creating fair ML software. These include fairness interventions for training ML models, such as including sensitive features, selecting non-sensitive attributes, and applying bias mitigators. But how reliably can software professionals tasked with developing data-driven systems depend on these recommendations? And how well do these practices generalize in the presence of faulty labels, missing data, or distribution shifts? These questions form the core theme of this paper.

</details>


### [30] [Verbatim Data Transcription Failures in LLM Code Generation: A State-Tracking Stress Test](https://arxiv.org/abs/2601.03640)
*Mohd Ariful Haque,Kishor Datta Gupta,Mohammad Ashiqur Rahman,Roy George*

Main category: cs.SE

TL;DR: 该论文提出了一个专门测试LLM代码生成中精确转录能力的基准测试，要求模型将高精度十进制常数原样嵌入Python代码并进行简单聚合计算。


<details>
  <summary>Details</summary>
Motivation: 现实世界软件任务（如加密常数、协议测试向量、白名单、校准表）需要将数据精确转录到代码中，这些任务对操作敏感，因为小的遗漏或改动可能在生成语法有效程序的同时保持静默错误。现有代码生成评估主要关注算法推理，缺乏对数据完整性的专门测试。

Method: 设计了一个最小化的转录到代码基准测试：给定高精度十进制常数列表，模型必须生成嵌入这些常数原样的Python代码并执行简单聚合计算。描述了提示变体、基于精确字符串包含的评估协议，以及用于表征状态跟踪和长视野生成失败的分析框架。

Result: 该基准测试作为一个紧凑的压力测试，旨在补充现有的代码生成评估，重点关注数据完整性而非算法推理。通过精确字符串包含评估来检测转录错误。

Conclusion: 该论文引入了一个专门针对LLM代码生成中数据完整性可靠性的基准测试，填补了现有评估在精确转录能力测试方面的空白，有助于识别状态跟踪和长视野生成中的失败模式。

Abstract: Many real-world software tasks require exact transcription of provided data into code, such as cryptographic constants, protocol test vectors, allowlists, and calibration tables. These tasks are operationally sensitive because small omissions or alterations can remain silent while producing syntactically valid programs. This paper introduces a deliberately minimal transcription-to-code benchmark to isolate this reliability concern in LLM-based code generation. Given a list of high-precision decimal constants, a model must generate Python code that embeds the constants verbatim and performs a simple aggregate computation. We describe the prompting variants, evaluation protocol based on exact-string inclusion, and analysis framework used to characterize state-tracking and long-horizon generation failures. The benchmark is intended as a compact stress test that complements existing code-generation evaluations by focusing on data integrity rather than algorithmic reasoning.

</details>


### [31] [Assessing and Improving the Representativeness of Code Generation Benchmarks Using Knowledge Units (KUs) of Programming Languages -- An Empirical Study](https://arxiv.org/abs/2601.03780)
*Md Ahasanuzzaman,Bram Adams,Emad Fallahzadeh,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 该研究首次系统分析了代码生成基准测试中编程概念的代表性问题，发现HumanEval和MBPP等基准测试仅覆盖了一半的编程知识单元，且分布严重偏斜，而真实项目使用了所有知识单元且分布均衡。研究者提出了基于提示的LLM框架来合成新任务以重新平衡基准测试，结果显示现有基准测试高估了LLM的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试（如HumanEval）评估LLM性能时，如果基准测试中的编程概念不能代表真实项目中的概念使用，评估结果可能不完整。然而，基准测试中代码概念的代表性问题尚未得到系统研究。

Method: 1. 提出知识单元（KUs）作为分析框架，将编程语言能力组织为连贯的集合
2. 分析两个广泛使用的Python基准测试（HumanEval和MBPP）的KU覆盖情况
3. 与30个真实Python项目进行对比
4. 提出基于提示的LLM框架，合成KU基础任务以重新平衡基准测试分布
5. 生成440个新任务并增强现有基准测试

Result: 1. 每个基准测试仅覆盖了20个已识别KUs中的一半，而真实项目使用了所有KUs且分布相对均衡
2. 基准测试任务呈现高度偏斜的KU分布
3. 增强后的基准测试显著提高了KU覆盖率，分布对齐度提升了60%以上
4. 在增强基准测试上评估最先进的LLM，发现性能出现一致且统计显著的下降（12.54-44.82%）
5. 表明现有基准测试由于有限的KU覆盖而高估了LLM性能

Conclusion: 现有代码生成基准测试在编程概念的代表性方面存在严重不足，导致高估了LLM的实际性能。提出的基于提示的LLM框架能够有效增强基准测试的代表性，为构建更现实的LLM代码生成能力评估提供了可操作的指导。

Abstract: Large Language Models (LLMs) such as GPT-4, Claude and LLaMA have shown impressive performance in code generation, typically evaluated using benchmarks (e.g., HumanEval). However, effective code generation requires models to understand and apply a wide range of language concepts. If the concepts exercised in benchmarks are not representative of those used in real-world projects, evaluations may yield incomplete. Despite this concern, the representativeness of code concepts in benchmarks has not been systematically examined.
  To address this gap, we present the first empirical study that analyzes the representativeness of code generation benchmarks through the lens of Knowledge Units (KUs) - cohesive sets of programming language capabilities provided by language constructs and APIs. We analyze KU coverage in two widely used Python benchmarks, HumanEval and MBPP, and compare them with 30 real-world Python projects. Our results show that each benchmark covers only half of the identified 20 KUs, whereas projects exercise all KUs with relatively balanced distributions. In contrast, benchmark tasks exhibit highly skewed KU distributions.
  To mitigate this misalignment, we propose a prompt-based LLM framework that synthesizes KU-based tasks to rebalance benchmark KU distributions and better align them with real-world usage. Using this framework, we generate 440 new tasks and augment existing benchmarks. The augmented benchmarks substantially improve KU coverage and achieve over a 60% improvement in distributional alignment. Evaluations of state-of-the-art LLMs on these augmented benchmarks reveal consistent and statistically significant performance drops (12.54-44.82%), indicating that existing benchmarks overestimate LLM performance due to their limited KU coverage. Our findings provide actionable guidance for building more realistic evaluations of LLM code-generation capabilities.

</details>


### [32] [Once Upon a Team: Investigating Bias in LLM-Driven Software Team Composition and Task Allocation](https://arxiv.org/abs/2601.03857)
*Alessandra Parziale,Gianmario Voria,Valeria Pontillo,Amleto Di Salle,Patrizio Pelliccione,Gemma Catolino,Fabio Palomba*

Main category: cs.SE

TL;DR: 研究发现大型语言模型在软件工程团队组建和任务分配中存在系统性偏见，候选人的国家和代词等人口属性显著影响选择可能性和任务分配，加剧了人口不平等问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用于提高生产力和支持软件工程任务，但当应用于团队组建和任务分配等社会敏感决策时，引发了公平性担忧。先前研究表明LLM可能再现刻板印象，但这些分析仍是探索性的，且孤立地考察敏感属性。

Method: 通过分析候选人国家和代词的组合效应，使用三个大型语言模型和3000个模拟决策，研究LLM在团队组建和任务分配中是否存在偏见，同时考虑了专业知识相关因素。

Result: 发现系统性差异：人口属性显著影响选择可能性和任务分配，即使考虑了专业知识因素。任务分布进一步反映了刻板印象，技术和领导角色在不同群体中分配不均。

Conclusion: 大型语言模型在软件工程环境中加剧了人口不平等，强调了需要公平意识评估的重要性。

Abstract: LLMs are increasingly used to boost productivity and support software engineering tasks. However, when applied to socially sensitive decisions such as team composition and task allocation, they raise concerns of fairness. Prior studies have revealed that LLMs may reproduce stereotypes; however, these analyses remain exploratory and examine sensitive attributes in isolation. This study investigates whether LLMs exhibit bias in team composition and task assignment by analyzing the combined effects of candidates' country and pronouns. Using three LLMs and 3,000 simulated decisions, we find systematic disparities: demographic attributes significantly shaped both selection likelihood and task allocation, even when accounting for expertise-related factors. Task distributions further reflected stereotypes, with technical and leadership roles unevenly assigned across groups. Our findings indicate that LLMs exacerbate demographic inequities in software engineering contexts, underscoring the need for fairness-aware assessment.

</details>


### [33] [Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design](https://arxiv.org/abs/2601.03878)
*Giovanni Rosa,David Moreno-Lumbreras,Gregorio Robles,Jesús M. González-Barahona*

Main category: cs.SE

TL;DR: 该研究设计了一个使用CURRANTE工具的实证研究，分析人类在规范和测试细化中的干预如何影响LLM生成代码的质量和动态过程。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型越来越多地集成到软件开发工作流中，但它们在结构化、规范驱动过程中的行为仍然缺乏深入理解。需要研究人类在规范和测试细化中的干预如何影响LLM生成代码的质量和动态。

Method: 使用CURRANTE工具（Visual Studio Code扩展）进行人类在环工作流研究。该工具引导开发者通过三个顺序阶段：规范定义、测试生成与细化、函数生成。参与者将解决LiveCodeBench数据集中的中等难度问题，工具记录细粒度交互日志、有效性指标（通过率、全通过完成率）、效率指标（通过时间）和迭代行为。

Result: 研究结果将提供人类干预对LLM生成代码质量和动态影响的实证洞察，为下一代开发环境的设计提供依据。

Conclusion: 该研究将为设计能够协调人类推理与模型驱动代码生成的下一代开发环境提供实证基础。

Abstract: Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation.

</details>


### [34] [Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures](https://arxiv.org/abs/2601.03988)
*Nicolas Lacroix,Mireille Blay-Fornarino,Sébastien Mosser,Frederic Precioso*

Main category: cs.SE

TL;DR: 本文评估小型语言模型（SLMs）在提取机器学习管道阶段方面的能力，以解决现有方法在可扩展性和领域多样性方面的局限性，并推进对数据科学实践的理解。


<details>
  <summary>Details</summary>
Motivation: 从源代码中提取机器学习管道阶段对于深入理解数据科学实践至关重要。然而，机器学习生态系统的不断演变（算法、库、数据集等）带来了多样性挑战。现有方法要么依赖不可扩展的手动标注，要么使用无法很好支持领域多样性的机器学习分类器，因此需要更灵活可靠的解决方案。

Method: 采用验证性研究设计，基于两个具有代表性的参考工作。首先使用Cochran's Q检验比较多个SLMs，选出最佳模型。然后通过两个独立的McNemar's检验将该模型与参考研究进行比较。进一步通过额外的Cochran's Q检验分析分类定义变化对性能的影响。最后使用Pearson卡方检验进行拟合优度分析，将本研究对数据科学实践的见解与先前研究进行比较。

Result: 研究结果表明小型语言模型在提取机器学习管道阶段方面具有潜力，能够解决现有方法的局限性。通过统计检验比较了多个SLMs的性能，并评估了最佳模型相对于现有方法的优势。同时分析了分类定义变化对性能的影响，并将研究结果与先前数据科学实践的研究进行了比较。

Conclusion: 小型语言模型能够利用其代码理解和分类能力，为提取机器学习管道阶段提供更灵活可靠的解决方案，从而推进对数据科学实践的理解。该方法能够更好地支持机器学习领域的多样性，并为相关研究提供新的技术途径。

Abstract: Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.
  Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.
  Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.

</details>


### [35] [Smells Depend on the Context: An Interview Study of Issue Tracking Problems and Smells in Practice](https://arxiv.org/abs/2601.04124)
*Lloyd Montgomery,Clara Lüders,Christian Rahe,Walid Maalej*

Main category: cs.SE

TL;DR: 该研究通过访谈26位软件工程从业者，识别了问题跟踪系统中的14个常见问题，并评估了31种文献中讨论的ITS异味（潜在问题实践）的实际相关性，发现许多异味在实践中并不出现或不构成问题。


<details>
  <summary>Details</summary>
Motivation: 尽管问题跟踪系统（ITS）在软件开发中被广泛使用，研究人员也对其数据进行了大量分析以自动化或辅助特定活动，但对ITS中软件工程团队面临的挑战以及某些实践和工作方式（如留空"优先级"等字段）何时被视为问题的研究仍然很少。

Method: 采用深度访谈研究方法，采访了来自不同组织和行业的26位经验丰富的软件工程从业者，询问他们遇到的一般问题以及文献中讨论的31种ITS异味（潜在问题实践）的相关性，然后对访谈记录应用主题分析。

Result: 识别了14个常见问题，包括问题可查找性、僵尸问题、工作流程臃肿和缺乏工作流程执行等。参与者还表示，许多ITS异味在实际中并不出现或不构成问题。研究发现ITS问题和异味高度依赖于上下文因素，如ITS配置、工作流程阶段和团队规模。

Conclusion: ITS问题和异味具有高度情境依赖性，需要根据具体上下文进行评估。研究讨论了潜在的解决方案，包括配置、监控和可视化ITS异味的工具，以应对这些挑战。

Abstract: Issue Tracking Systems (ITSs) enable software developers and managers to collect and resolve issues collaboratively. While researchers have extensively analysed ITS data to automate or assist specific activities such as issue assignments, duplicate detection, or priority prediction, developer studies on ITSs remain rare. Particularly, little is known about the challenges Software Engineering (SE) teams encounter in ITSs and when certain practices and workarounds (such as leaving issue fields like "priority" empty) are considered problematic. To fill this gap, we conducted an in-depth interview study with 26 experienced SE practitioners from different organisations and industries. We asked them about general problems encountered, as well as the relevance of 31 ITS smells (aka potentially problematic practices) discussed in the literature. By applying Thematic Analysis to the interview notes, we identified 14 common problems including issue findability, zombie issues, workflow bloat, and lack of workflow enforcement. Participants also stated that many of the ITS smells do not occur or are not problematic. Our results suggest that ITS problems and smells are highly dependent on context factors such as ITS configuration, workflow stage, and team size. We also discuss potential tooling solutions to configure, monitor, and visualise ITS smells to cope with these challenges.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [36] [How Real is Your Jailbreak? Fine-grained Jailbreak Evaluation with Anchored Reference](https://arxiv.org/abs/2601.03288)
*Songyang Liu,Chaozhuo Li,Rui Pu,Litian Zhang,Chenxu Wang,Zejian Chen,Yuting Zhang,Yiming Hei*

Main category: cs.CR

TL;DR: FJAR是一个细粒度的越狱评估框架，通过锚定参考和五级分类来更准确地评估大语言模型的越狱攻击成功率，解决了现有方法因粗粒度分类而高估攻击成功率的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的越狱攻击自动评估方法主要依赖粗粒度分类（主要关注有害性），导致攻击成功率被严重高估。需要更精细的评估框架来准确衡量越狱攻击的实际效果。

Method: 提出FJAR框架：1）将越狱响应分为五个细粒度类别：拒绝、无关、无帮助、不正确、成功，基于响应满足恶意查询意图的程度；2）引入无害树分解方法构建高质量的锚定参考，通过分解原始查询来指导评估者判断响应是否真正满足原始查询。

Result: 大量实验表明，FJAR在与人判断对齐方面达到最高水平，能有效识别越狱失败的根本原因，为改进攻击策略提供可操作的指导。

Conclusion: FJAR框架通过细粒度分类和锚定参考，显著提高了越狱攻击评估的准确性，解决了现有方法高估攻击成功率的问题，为大语言模型安全评估提供了更可靠的工具。

Abstract: Jailbreak attacks present a significant challenge to the safety of Large Language Models (LLMs), yet current automated evaluation methods largely rely on coarse classifications that focus mainly on harmfulness, leading to substantial overestimation of attack success. To address this problem, we propose FJAR, a fine-grained jailbreak evaluation framework with anchored references. We first categorized jailbreak responses into five fine-grained categories: Rejective, Irrelevant, Unhelpful, Incorrect, and Successful, based on the degree to which the response addresses the malicious intent of the query. This categorization serves as the basis for FJAR. Then, we introduce a novel harmless tree decomposition approach to construct high-quality anchored references by breaking down the original queries. These references guide the evaluator in determining whether the response genuinely fulfills the original query. Extensive experiments demonstrate that FJAR achieves the highest alignment with human judgment and effectively identifies the root causes of jailbreak failures, providing actionable guidance for improving attack strategies.

</details>


### [37] [AgentMark: Utility-Preserving Behavioral Watermarking for Agents](https://arxiv.org/abs/2601.03294)
*Kaibo Huang,Jin Tan,Yukun Wei,Wanling Li,Zipei Zhang,Hui Tian,Zhongliang Yang,Linna Zhou*

Main category: cs.CR

TL;DR: AgentMark：一个为LLM智能体规划行为嵌入多比特标识符的水印框架，在保持效用的同时实现行为溯源


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体被部署来自主解决复杂任务，对知识产权保护和监管溯源的需求日益迫切。传统内容水印只能识别LLM生成的输出，无法直接识别控制多步执行的高层规划行为（如工具选择和子目标选择）。规划行为层水印面临独特挑战：决策中的微小分布偏差在长期运行中会累积影响效用，且许多智能体作为黑盒难以直接干预。

Method: 提出AgentMark行为水印框架，通过从智能体引出显式行为分布，并应用保持分布的条件采样，在规划决策中嵌入多比特标识符。该方法可在黑盒API下部署，并与动作层内容水印兼容。

Result: 在具身、工具使用和社交环境中的实验表明，AgentMark具有实用的多比特容量，能够从部分日志中稳健恢复标识符，并保持效用。

Conclusion: AgentMark填补了智能体规划行为水印的空白，为LLM智能体的知识产权保护和监管溯源提供了有效解决方案，代码已开源。

Abstract: LLM-based agents are increasingly deployed to autonomously solve complex tasks, raising urgent needs for IP protection and regulatory provenance. While content watermarking effectively attributes LLM-generated outputs, it fails to directly identify the high-level planning behaviors (e.g., tool and subgoal choices) that govern multi-step execution. Critically, watermarking at the planning-behavior layer faces unique challenges: minor distributional deviations in decision-making can compound during long-term agent operation, degrading utility, and many agents operate as black boxes that are difficult to intervene in directly. To bridge this gap, we propose AgentMark, a behavioral watermarking framework that embeds multi-bit identifiers into planning decisions while preserving utility. It operates by eliciting an explicit behavior distribution from the agent and applying distribution-preserving conditional sampling, enabling deployment under black-box APIs while remaining compatible with action-layer content watermarking. Experiments across embodied, tool-use, and social environments demonstrate practical multi-bit capacity, robust recovery from partial logs, and utility preservation. The code is available at https://github.com/Tooooa/AgentMark.

</details>


### [38] [TRYLOCK: Defense-in-Depth Against LLM Jailbreaks via Layered Preference and Representation Engineering](https://arxiv.org/abs/2601.03300)
*Scott Thornton*

Main category: cs.CR

TL;DR: TRYLOCK是一个多层防御架构，通过权重级安全对齐、激活级控制、自适应强度选择和输入规范化四层机制，在Mistral-7B-Instruct上实现88.0%的相对攻击成功率降低，同时减少过度拒绝。


<details>
  <summary>Details</summary>
Motivation: 大语言模型仍然容易受到越狱攻击，单层防御往往需要在安全性和可用性之间做出权衡。现有防御方法存在局限性，需要一种深度防御架构来提供更全面的保护。

Method: 提出TRYLOCK深度防御架构，包含四个异构机制：1) 通过DPO进行权重级安全对齐；2) 通过表示工程进行激活级控制；3) 通过轻量级侧分类器选择自适应转向强度；4) 输入规范化以中和基于编码的绕过攻击。

Result: 在包含249个提示的五个攻击家族测试集上，TRYLOCK将攻击成功率从46.5%降低到5.6%，实现88.0%的相对降低。各层提供独特覆盖：RepE阻止了36%绕过DPO的攻击，规范化捕获了14%的编码攻击。自适应侧分类器将过度拒绝从60%降低到48%。

Conclusion: TRYLOCK展示了安全性和可用性不必相互排斥，通过深度防御架构可以同时实现高安全性和低过度拒绝。发现了非单调转向现象，并提供了RepE-DPO干扰的机制假设。所有组件都已开源以确保可复现性。

Abstract: Large language models remain vulnerable to jailbreak attacks, and single-layer defenses often trade security for usability. We present TRYLOCK, the first defense-in-depth architecture that combines four heterogeneous mechanisms across the inference stack: weight-level safety alignment via DPO, activation-level control via Representation Engineering (RepE) steering, adaptive steering strength selected by a lightweight sidecar classifier, and input canonicalization to neutralize encoding-based bypasses. On Mistral-7B-Instruct evaluated against a 249-prompt attack set spanning five attack families, TRYLOCK achieves 88.0% relative ASR reduction (46.5% to 5.6%), with each layer contributing unique coverage: RepE blocks 36% of attacks that bypass DPO alone, while canonicalization catches 14% of encoding attacks that evade both. We discover a non-monotonic steering phenomenon -- intermediate strength (alpha=1.0) degrades safety below baseline -- and provide mechanistic hypotheses explaining RepE-DPO interference. The adaptive sidecar reduces over-refusal from 60% to 48% while maintaining identical attack defense, demonstrating that security and usability need not be mutually exclusive. We release all components -- trained adapters, steering vectors, sidecar classifier, preference pairs, and complete evaluation methodology -- enabling full reproducibility.

</details>


### [39] [Listen to Rhythm, Choose Movements: Autoregressive Multimodal Dance Generation via Diffusion and Mamba with Decoupled Dance Dataset](https://arxiv.org/abs/2601.03323)
*Oran Duan,Yinghua Shen,Yingzhu Lv,Luyang Jie,Yaxin Liu,Qiong Wu*

Main category: cs.CR

TL;DR: LRCM是一个多模态引导的扩散框架，支持多样化输入模态和自回归舞蹈动作生成，解决了现有方法语义控制粗糙和长序列连贯性差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前舞蹈动作生成方法存在语义控制粗糙和长序列连贯性差的问题，需要更好的多模态引导和长序列生成能力。

Method: 提出特征解耦范式分离运动捕捉数据、音频节奏和专业标注的全局/局部文本描述；采用音频潜在Conformer和文本潜在Cross-Conformer的扩散架构，并引入Motion Temporal Mamba Module实现平滑的长时自回归合成。

Result: 实验结果表明LRCM在功能能力和量化指标上都表现出色，在多模态输入场景和扩展序列生成方面展现出显著潜力。

Conclusion: LRCM框架通过多模态引导和自回归生成，有效提升了舞蹈动作生成的语义控制和长序列连贯性，具有实际应用价值。

Abstract: Advances in generative models and sequence learning have greatly promoted research in dance motion generation, yet current methods still suffer from coarse semantic control and poor coherence in long sequences. In this work, we present Listen to Rhythm, Choose Movements (LRCM), a multimodal-guided diffusion framework supporting both diverse input modalities and autoregressive dance motion generation. We explore a feature decoupling paradigm for dance datasets and generalize it to the Motorica Dance dataset, separating motion capture data, audio rhythm, and professionally annotated global and local text descriptions. Our diffusion architecture integrates an audio-latent Conformer and a text-latent Cross-Conformer, and incorporates a Motion Temporal Mamba Module (MTMM) to enable smooth, long-duration autoregressive synthesis. Experimental results indicate that LRCM delivers strong performance in both functional capability and quantitative metrics, demonstrating notable potential in multimodal input scenarios and extended sequence generation. We will release the full codebase, dataset, and pretrained models publicly upon acceptance.

</details>


### [40] [DeepLeak: Privacy Enhancing Hardening of Model Explanations Against Membership Leakage](https://arxiv.org/abs/2601.03429)
*Firas Ben Hmida,Zain Sbeih,Philemon Hailemariam,Birhanu Eshete*

Main category: cs.CR

TL;DR: DeepLeak系统用于审计和缓解后解释方法中的隐私风险，通过更强的成员推理攻击量化解释方法的成员信息泄露，提出轻量级缓解策略减少泄露，并识别驱动泄露的算法特性。


<details>
  <summary>Details</summary>
Motivation: 机器学习可解释性在医疗诊断和贷款审批等高风险场景中至关重要，但这些领域同时需要严格的隐私保护，导致可解释性与隐私之间存在张力。现有研究显示解释方法可能泄露成员信息，但从业者缺乏系统指导来选择或部署既能保持透明度又能保护隐私的解释技术。

Method: DeepLeak系统包含三个核心部分：(1) 全面的泄露分析：开发更强的解释感知成员推理攻击来量化代表性解释方法在默认配置下的成员信息泄露程度；(2) 轻量级加固策略：引入实用的模型无关缓解措施，包括灵敏度校准噪声、归因裁剪和掩码，显著减少成员泄露同时保持解释效用；(3) 根本原因分析：通过受控实验确定驱动泄露的算法特性（如归因稀疏性和灵敏度）。

Result: 在图像基准测试中评估15种解释技术，发现默认设置下的成员信息泄露比先前报告高出74.9%。提出的缓解措施将泄露减少高达95%（最低46.5%），平均效用损失仅≤3.3%。

Conclusion: DeepLeak为隐私敏感机器学习中的可解释性提供了一条系统、可复现的安全路径，帮助从业者在保持解释效用的同时显著降低隐私风险。

Abstract: Machine learning (ML) explainability is central to algorithmic transparency in high-stakes settings such as predictive diagnostics and loan approval. However, these same domains require rigorous privacy guaranties, creating tension between interpretability and privacy. Although prior work has shown that explanation methods can leak membership information, practitioners still lack systematic guidance on selecting or deploying explanation techniques that balance transparency with privacy.
  We present DeepLeak, a system to audit and mitigate privacy risks in post-hoc explanation methods. DeepLeak advances the state-of-the-art in three ways: (1) comprehensive leakage profiling: we develop a stronger explanation-aware membership inference attack (MIA) to quantify how much representative explanation methods leak membership information under default configurations; (2) lightweight hardening strategies: we introduce practical, model-agnostic mitigations, including sensitivity-calibrated noise, attribution clipping, and masking, that substantially reduce membership leakage while preserving explanation utility; and (3) root-cause analysis: through controlled experiments, we pinpoint algorithmic properties (e.g., attribution sparsity and sensitivity) that drive leakage.
  Evaluating 15 explanation techniques across four families on image benchmarks, DeepLeak shows that default settings can leak up to 74.9% more membership information than previously reported. Our mitigations cut leakage by up to 95% (minimum 46.5%) with only <=3.3% utility loss on average. DeepLeak offers a systematic, reproducible path to safer explainability in privacy-sensitive ML.

</details>


### [41] [Security Parameter Analysis of the LINEture Post-Quantum Digital Signature Scheme](https://arxiv.org/abs/2601.03465)
*Yevgen Kotukh,Gennady Khalimov*

Main category: cs.CR

TL;DR: 本文对LINEture后量子数字签名方案的安全参数进行了全面密码分析，研究了三个主要参数对密码强度的影响，揭示了参数l的双重性质，并提出了最优参数选择规则。


<details>
  <summary>Details</summary>
Motivation: LINEture是一种基于初等阿贝尔2-群上矩阵代数的后量子数字签名方案，需要对其安全参数进行深入分析以评估其密码强度，特别是参数l的双重性质需要进一步研究。

Method: 采用密码分析方法，重点研究三个主要参数：字大小m（二次影响）、向量维度l和会话密钥中子矩阵数量q（线性影响）。通过深入分析验证机制，建立了参数间的阈值关系。

Result: 分析显示参数l具有双重性质：不影响猜测攻击抵抗性，但在验证机制中建立l×m位的验证屏障。建立了阈值关系l < (q-1)×m，低于此阈值时l成为安全关键参数。提出了最优选择规则l ≈ (q-1)×m以实现最大密码效率。

Conclusion: 研究为LINEture后量子签名方案提供了参数选择的指导原则，揭示了参数l的双重安全作用，并与NIST PQC标准进行了比较分析，提供了实际参数建议。

Abstract: This paper presents a comprehensive cryptographic analysis of the security parameters of the LINEture post-quantum digital signature scheme, which is constructed using matrix algebra over elementary abelian 2-groups. We investigate the influence of three principal parameters. First, the word size m (exhibiting quadratic impact), the second is a vector dimension l, and the third is a number of submatrices in the session key q (exhibiting linear impact) on cryptographic strength. Our analysis reveals a dualistic nature of the parameter l. According to the previous analysis, it does not affect resistance to guessing attacks. A deeper examination of the verification mechanism demonstrates that l establishes a kind of verification barrier of l times m bits. We establish the threshold relationship l less q minus 1 times m, below which parameter l becomes security-critical. The optimal selection rule l near q minus 1 times m is proposed for maximum cryptographic efficiency. Comparative analysis with NIST PQC standards and practical parameter recommendations are provided.

</details>


### [42] [Full-Stack Knowledge Graph and LLM Framework for Post-Quantum Cyber Readiness](https://arxiv.org/abs/2601.03504)
*Rasmus Erlemann,Charles Colyer Morris,Sanjyot Sathe*

Main category: cs.CR

TL;DR: 提出基于知识图谱的企业级后量子密码就绪度评估框架，通过图论风险函数量化密码暴露风险，结合大语言模型与人工验证实现可扩展分析


<details>
  <summary>Details</summary>
Motivation: 大规模量子计算威胁现有公钥密码体系，企业缺乏可扩展的定量框架来评估密码暴露程度和优先迁移策略

Method: 构建异构知识图谱建模企业密码资产、依赖关系和漏洞，使用图论风险函数量化后量子暴露风险，通过Shapley值分解跨密码域归因风险，集成大语言模型与人工验证确保数据质量和可扩展性

Result: 框架生成可解释、标准化的就绪度指标，支持持续监控、比较分析和修复优先级排序

Conclusion: 该知识图谱框架为企业提供了可扩展的定量方法，用于评估后量子密码就绪度并指导迁移优先级决策

Abstract: The emergence of large-scale quantum computing threatens widely deployed public-key cryptographic systems, creating an urgent need for enterprise-level methods to assess post-quantum (PQ) readiness. While PQ standards are under development, organizations lack scalable and quantitative frameworks for measuring cryptographic exposure and prioritizing migration across complex infrastructures. This paper presents a knowledge graph based framework that models enterprise cryptographic assets, dependencies, and vulnerabilities to compute a unified PQ readiness score. Infrastructure components, cryptographic primitives, certificates, and services are represented as a heterogeneous graph, enabling explicit modeling of dependency-driven risk propagation. PQ exposure is quantified using graph-theoretic risk functionals and attributed across cryptographic domains via Shapley value decomposition. To support scalability and data quality, the framework integrates large language models with human-in-the-loop validation for asset classification and risk attribution. The resulting approach produces explainable, normalized readiness metrics that support continuous monitoring, comparative analysis, and remediation prioritization.

</details>


### [43] [A Critical Analysis of the Medibank Health Data Breach and Differential Privacy Solutions](https://arxiv.org/abs/2601.03508)
*Zhuohan Cui,Qianqian Lang,Zikun Song*

Main category: cs.CR

TL;DR: 提出基于熵感知差分隐私的医疗数据保护框架，针对Medibank数据泄露问题，通过自适应预算分配和加密访问机制，在降低91%重识别风险的同时保持分析效用损失低于24%。


<details>
  <summary>Details</summary>
Motivation: 针对2022年Medibank健康保险数据泄露事件暴露的问题：970万个人敏感医疗记录因未加密存储、集中访问和缺乏隐私保护分析而泄露，需要解决这些安全漏洞。

Method: 提出熵感知差分隐私框架，整合拉普拉斯和高斯机制，采用自适应预算分配。设计包括TLS加密数据库访问、字段级机制选择和平滑敏感性模型，高熵属性获得更强的噪声注入。

Result: 使用合成Medibank数据集（N=131,000）进行实验验证，结果显示重识别概率降低90.3%，同时分析效用损失保持在24%以下。框架符合GDPR第32条和澳大利亚隐私原则11.1。

Conclusion: 该工作通过结合严格的隐私保证和实际可用性，为医疗数据保护提供了可扩展且技术可行的解决方案，为弹性、可信且符合法规的医疗分析提供了路径。

Abstract: This paper critically examines the 2022 Medibank health insurance data breach, which exposed sensitive medical records of 9.7 million individuals due to unencrypted storage, centralized access, and the absence of privacy-preserving analytics. To address these vulnerabilities, we propose an entropy-aware differential privacy (DP) framework that integrates Laplace and Gaussian mechanisms with adaptive budget allocation. The design incorporates TLS-encrypted database access, field-level mechanism selection, and smooth sensitivity models to mitigate re-identification risks. Experimental validation was conducted using synthetic Medibank datasets (N = 131,000) with entropy-calibrated DP mechanisms, where high-entropy attributes received stronger noise injection. Results demonstrate a 90.3% reduction in re-identification probability while maintaining analytical utility loss below 24%. The framework further aligns with GDPR Article 32 and Australian Privacy Principle 11.1, ensuring regulatory compliance. By combining rigorous privacy guarantees with practical usability, this work contributes a scalable and technically feasible solution for healthcare data protection, offering a pathway toward resilient, trustworthy, and regulation-ready medical analytics.

</details>


### [44] [Deontic Knowledge Graphs for Privacy Compliance in Multimodal Disaster Data Sharing](https://arxiv.org/abs/2601.03587)
*Kelvin Uzoma Echenim,Karuna Pande Joshi*

Main category: cs.CR

TL;DR: 提出基于义务知识图谱的灾害响应数据共享框架，整合灾害管理知识图谱与政策知识图谱，支持允许、阻止、允许但转换三种决策，实现细粒度隐私合规控制。


<details>
  <summary>Details</summary>
Motivation: 灾害响应需要共享异构数据（表格记录、无人机图像等），但面临重叠的隐私法规要求。现有系统通常采用二元访问控制，在时间关键的工作流程中过于脆弱，需要更灵活的合规机制。

Method: 构建灾害管理知识图谱（DKG）与政策知识图谱（PKG），基于IoT-Reg和FEMA/DHS隐私驱动。提出释放决策函数，支持三种结果：允许、阻止、允许但转换（绑定转换义务并通过溯源验证合规性）。

Result: 在包含510万三元组DKG和31.6万图像的测试中，实现了精确匹配决策正确性、亚秒级单决策延迟，以及在单图和联邦工作负载下的交互式查询性能。

Conclusion: 该框架通过知识图谱整合与细粒度决策机制，解决了灾害响应中异构数据共享的隐私合规挑战，提供了比传统二元访问控制更灵活、可靠的解决方案。

Abstract: Disaster response requires sharing heterogeneous artifacts, from tabular assistance records to UAS imagery, under overlapping privacy mandates. Operational systems often reduce compliance to binary access control, which is brittle in time-critical workflows. We present a novel deontic knowledge graph-based framework that integrates a Disaster Management Knowledge Graph (DKG) with a Policy Knowledge Graph (PKG) derived from IoT-Reg and FEMA/DHS privacy drivers. Our release decision function supports three outcomes: Allow, Block, and Allow-with-Transform. The latter binds obligations to transforms and verifies post-transform compliance via provenance-linked derived artifacts; blocked requests are logged as semantic privacy incidents. Evaluation on a 5.1M-triple DKG with 316K images shows exact-match decision correctness, sub-second per-decision latency, and interactive query performance across both single-graph and federated workloads.

</details>


### [45] [Jailbreaking LLMs & VLMs: Mechanisms, Evaluation, and Unified Defense](https://arxiv.org/abs/2601.03594)
*Zejian Chen,Chaozhuo Li,Chao Li,Xi Zhang,Litian Zhang,Yiming He*

Main category: cs.CR

TL;DR: 本文系统综述了大语言模型和视觉语言模型的越狱攻击与防御，提出了三维分析框架（攻击、防御、评估），并提出了统一的防御原则。


<details>
  <summary>Details</summary>
Motivation: 越狱漏洞源于结构性因素（不完整训练数据、语言歧义、生成不确定性），现有研究缺乏从纯文本到多模态环境的全面分析，需要统一的防御框架。

Method: 提出三维调查框架：1)攻击维度（模板/编码、上下文学习操纵、强化/对抗学习、LLM辅助、微调攻击等）；2)防御维度（提示级混淆、输出评估、模型级对齐）；3)评估维度（攻击成功率、毒性分数、查询/时间成本等）。

Result: 区分了幻觉与越狱的意图和触发机制，整合了共享机制，提出了统一防御原则：感知层的变体一致性和梯度敏感性检测、生成层的安全感知解码和输出审查、参数层的对抗增强偏好对齐。

Conclusion: 本文提供了从纯文本到多模态环境的全面越狱攻击与防御综述，总结了现有多模态安全基准，并讨论了未来方向（自动红队、跨模态协同防御、标准化评估）。

Abstract: This paper provides a systematic survey of jailbreak attacks and defenses on Large Language Models (LLMs) and Vision-Language Models (VLMs), emphasizing that jailbreak vulnerabilities stem from structural factors such as incomplete training data, linguistic ambiguity, and generative uncertainty. It further differentiates between hallucinations and jailbreaks in terms of intent and triggering mechanisms. We propose a three-dimensional survey framework: (1) Attack dimension-including template/encoding-based, in-context learning manipulation, reinforcement/adversarial learning, LLM-assisted and fine-tuned attacks, as well as prompt- and image-level perturbations and agent-based transfer in VLMs; (2) Defense dimension-encompassing prompt-level obfuscation, output evaluation, and model-level alignment or fine-tuning; and (3) Evaluation dimension-covering metrics such as Attack Success Rate (ASR), toxicity score, query/time cost, and multimodal Clean Accuracy and Attribute Success Rate. Compared with prior works, this survey spans the full spectrum from text-only to multimodal settings, consolidating shared mechanisms and proposing unified defense principles: variant-consistency and gradient-sensitivity detection at the perception layer, safety-aware decoding and output review at the generation layer, and adversarially augmented preference alignment at the parameter layer. Additionally, we summarize existing multimodal safety benchmarks and discuss future directions, including automated red teaming, cross-modal collaborative defense, and standardized evaluation.

</details>


### [46] [Human Challenge Oracle: Designing AI-Resistant, Identity-Bound, Time-Limited Tasks for Sybil-Resistant Consensus](https://arxiv.org/abs/2601.03923)
*Homayoun Maleki,Nekane Sainz,Jon Legarda*

Main category: cs.CR

TL;DR: HCO是一种新的安全原语，通过实时、限速的人类验证来对抗Sybil攻击，利用人类认知努力作为难以并行化的稀缺资源


<details>
  <summary>Details</summary>
Motivation: 现有防御措施（如CAPTCHA和一次性人格证明机制）主要解决身份创建问题，对长期、大规模的Sybil参与提供有限保护，特别是随着自动化求解器和AI系统的不断改进

Method: 引入人类挑战预言机（HCO），发布与个体身份加密绑定的短期、时间限制的挑战，必须在实时解决。核心洞察是实时人类认知努力（如感知、注意力和交互推理）构成难以并行化或跨身份分摊的稀缺资源

Result: 在明确且温和的假设下，维持s个活跃身份的成本在每个时间窗口内随s线性增长。具体浏览器实现表明这些挑战对人类在几秒内可轻松解决，而对当代自动化系统在严格时间限制下仍然困难

Conclusion: HCO为持续、限速的人类验证提供了新的安全原语，能够有效对抗Sybil攻击，利用人类认知稀缺性作为防御基础

Abstract: Sybil attacks remain a fundamental obstacle in open online systems, where adversaries can cheaply create and sustain large numbers of fake identities. Existing defenses, including CAPTCHAs and one-time proof-of-personhood mechanisms, primarily address identity creation and provide limited protection against long-term, large-scale Sybil participation, especially as automated solvers and AI systems continue to improve.
  We introduce the Human Challenge Oracle (HCO), a new security primitive for continuous, rate-limited human verification. HCO issues short, time-bound challenges that are cryptographically bound to individual identities and must be solved in real time. The core insight underlying HCO is that real-time human cognitive effort, such as perception, attention, and interactive reasoning, constitutes a scarce resource that is inherently difficult to parallelize or amortize across identities.
  We formalize the design goals and security properties of HCO and show that, under explicit and mild assumptions, sustaining s active identities incurs a cost that grows linearly with s in every time window. We further describe abstract classes of admissible challenges and concrete browser-based instantiations, and present an initial empirical study illustrating that these challenges are easily solvable by humans within seconds while remaining difficult for contemporary automated systems under strict time constraints.

</details>


### [47] [SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2601.03979)
*Andreea-Elena Bodea,Stephen Meisenbacher,Alexandra Klymenko,Florian Matthes*

Main category: cs.CR

TL;DR: 本文对检索增强生成（RAG）系统中的隐私风险进行了首次系统性综述，提出了隐私风险分类法和RAG隐私过程图，总结了风险测量与缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着RAG技术在结合LLM与领域知识库方面的广泛应用，数据隐私问题日益凸显。现有研究对RAG隐私风险的各个方面进行了探索，但缺乏系统性的整理和统一框架，需要全面调查RAG中的隐私风险及其测量与缓解方法。

Method: 通过系统性文献综述方法，收集和分析关注RAG隐私的研究工作，将发现系统化为全面的隐私风险集、缓解技术和评估策略，并创建了两个主要成果：RAG隐私风险分类法和RAG隐私过程图。

Result: 建立了首个RAG隐私风险与缓解措施的系统化框架，揭示了缓解RAG隐私风险时的重要考虑因素，评估了现有缓解措施的成熟度，为RAG隐私研究提供了基础性参考。

Conclusion: 本文填补了RAG隐私研究领域的系统性空白，不仅首次系统化了风险与缓解措施，还提供了实用的分类框架和过程图，为未来RAG系统的隐私保护设计和评估奠定了基础。

Abstract: The continued promise of Large Language Models (LLMs), particularly in their natural language understanding and generation capabilities, has driven a rapidly increasing interest in identifying and developing LLM use cases. In an effort to complement the ingrained "knowledge" of LLMs, Retrieval-Augmented Generation (RAG) techniques have become widely popular. At its core, RAG involves the coupling of LLMs with domain-specific knowledge bases, whereby the generation of a response to a user question is augmented with contextual and up-to-date information. The proliferation of RAG has sparked concerns about data privacy, particularly with the inherent risks that arise when leveraging databases with potentially sensitive information. Numerous recent works have explored various aspects of privacy risks in RAG systems, from adversarial attacks to proposed mitigations. With the goal of surveying and unifying these works, we ask one simple question: What are the privacy risks in RAG, and how can they be measured and mitigated? To answer this question, we conduct a systematic literature review of RAG works addressing privacy, and we systematize our findings into a comprehensive set of privacy risks, mitigation techniques, and evaluation strategies. We supplement these findings with two primary artifacts: a Taxonomy of RAG Privacy Risks and a RAG Privacy Process Diagram. Our work contributes to the study of privacy in RAG not only by conducting the first systematization of risks and mitigations, but also by uncovering important considerations when mitigating privacy risks in RAG systems and assessing the current maturity of proposed mitigations.

</details>
