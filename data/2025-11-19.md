<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 53]
- [cs.CR](#cs.CR) [Total: 37]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: 本研究探讨了使用大型语言模型生成合成数据集来支持自然语言处理任务的潜力，特别关注负面情感文本，通过生成合成新闻标题来替代真实数据，以解决数据获取和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 克服真实世界数据获取的挑战和隐私问题，探索LLM生成数据在NLP任务中的潜力，特别是针对负面情感分析这一关键领域。

Method: 使用定制提示创建负面新闻标题语料库，通过专家评审和嵌入空间分析验证合成标题，采用困惑度、可读性、词性标注分析、BERTScore和语义相似度等多种指标进行基准测试。

Result: 生成的标题在内容、语气、长度和风格上与真实标题高度匹配，仅在词性标注分析中的专有名词得分方面存在明显差异。

Conclusion: LLM生成的合成数据集在负面情感分析任务中具有替代真实数据的潜力，能够有效解决数据获取和隐私问题，为NLP研究提供了新的数据来源途径。

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [2] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: CLINB是一个评估大语言模型在气候变化领域专业知识的基准测试，发现前沿模型具备博士级别的知识综合能力，但在证据基础方面存在严重幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型处理复杂专业知识的能力是一个关键挑战，特别是在气候变化这样的专业领域，需要测试模型的知识质量和证据支持能力。

Method: 引入CLINB基准测试，使用真实用户问题和气候科学家制定的评估标准，通过基于模型的评估过程测试多个前沿模型，包括开放式、基于事实的多模态问答任务。

Result: 前沿模型展现出卓越的知识综合能力，达到博士级别的理解和呈现质量，甚至优于专家辅助较弱模型生成的混合答案。但存在严重的证据基础问题，参考文献和图像存在大量幻觉。

Conclusion: 在知识综合与可验证归因之间建立桥梁对于AI在科学工作流程中的部署至关重要，需要像CLINB这样可靠、可解释的基准测试来构建可信赖的AI系统。

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [3] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: CausalGuard是一种结合因果推理和符号逻辑的新方法，用于实时检测和防止大语言模型的幻觉问题，相比现有方法能更早干预生成过程。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型自信地陈述虚假信息的幻觉问题，这是在使用准确性至关重要的场景中的主要障碍。现有解决方案要么需要重新训练整个模型，要么增加显著计算成本，或者未能解决幻觉的根本原因。

Method: 结合因果推理与符号逻辑，通过两条互补路径工作：一条追踪模型已知信息与生成内容之间的因果关系，另一条使用自动推理检查逻辑一致性。系统在生成过程中早期干预，而非仅检查输出。

Result: 在12个基准测试中，CausalGuard正确识别幻觉的概率为89.3%，漏检率仅为8.3%。更重要的是，它减少了近80%的错误声明，同时保持回答自然有帮助。在需要多步逻辑的复杂推理任务上表现尤其出色。

Conclusion: CausalGuard通过显示推理过程，在医疗诊断或金融分析等敏感领域表现良好，因为这些领域理解决策原因与决策本身同等重要。该方法为解决语言模型幻觉问题提供了有效的新途径。

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [4] [Quantifying Skill and Chance: A Unified Framework for the Geometry of Games](https://arxiv.org/abs/2511.11611)
*David H. Silver*

Main category: cs.AI

TL;DR: 提出了一个量化框架，通过将游戏建模为随机决策树来分离技能和运气成分，定义了技能-运气指数S(G)在[-1,1]范围内，应用于30个游戏揭示了从纯运气到纯技能的连续谱系。


<details>
  <summary>Details</summary>
Motivation: 需要建立一个系统性的方法来量化游戏中技能和运气的相对贡献，以便进行游戏设计、AI评估和风险分析的客观比较。

Method: 将游戏建模为随机决策树，分解游戏结果为技能杠杆K和运气杠杆L，定义技能-运气指数S(G)，并引入波动性Sigma来量化连续回合的结果不确定性。

Result: 分析30个游戏显示：硬币投掷S=-1（纯运气），西洋双陆棋S=0，国际象棋S=+1（纯技能），扑克S=0.33（中等技能主导）。波动性Sigma从国际象棋的0到西洋双陆棋的1.20不等。

Conclusion: 该框架可扩展到一般随机决策系统，为玩家影响力、游戏平衡性和预测稳定性的原则性比较提供了基础，在游戏设计、AI评估和风险评估中具有应用价值。

Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.

</details>


### [5] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: VALOR是一个模块化、零样本的智能框架，通过分层提示分析、文化价值对齐和意图消歧来确保文本到图像生成的安全性，同时保持生成质量和用户意图。


<details>
  <summary>Details</summary>
Motivation: 生成式视觉语言模型存在产生不安全、冒犯性或文化不适当内容的风险，现有防御方法难以在不牺牲生成质量或增加成本的情况下确保输出符合人类价值观。

Method: VALOR框架包含多级NSFW检测器、文化价值对齐模块和意图消歧器，检测到不安全内容时由大语言模型在动态角色特定指令下重写提示，必要时进行风格化再生。

Result: 在对抗性、模糊性和价值敏感提示上的实验显示，VALOR将不安全输出减少高达100.00%，同时保持了提示的有用性和创造性。

Conclusion: VALOR为在开放世界环境中部署安全、对齐且有用的图像生成系统提供了一种可扩展且有效的方法。

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [6] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的智能体框架，用于迭代构建SPARQL查询，解决多跳知识图谱问答中的查询生成问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在生成复杂SPARQL查询时存在脆弱性，缺乏基于实时执行反馈的动态调试策略，限制了知识图谱问答的可靠性。

Method: 使用仅3B参数的紧凑模型，通过结果驱动的强化学习（GRPO）训练，无需监督微调，学习迭代SPARQL构建的弹性策略。

Result: 在LC-QuAD 2.0的可执行子集上，实体链接后准确率达到49.7%，比最强的零样本基线提高了17.5个百分点。

Conclusion: 该工作为通过交互教导智能体掌握形式化符号工具提供了通用蓝图，弥合了概率性LLM与结构化知识图谱之间的差距。

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [7] [On the Measure of a Model: From Intelligence to Generality](https://arxiv.org/abs/2511.11773)
*Ruchira Dhar,Ninell Oldenburg,Anders Soegaard*

Main category: cs.AI

TL;DR: 论文质疑当前基于抽象智力概念的AI评估基准，提出应以通用性而非智力作为评估基础，认为通用性更能直接关联实际任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估基准（如ARC、Raven测试等）基于模糊的智力概念，无法有效预测实际任务表现，存在与现实效用脱节的风险。

Method: 通过概念和形式分析，检验智力评估的三个假设（通用性、稳定性、现实性），论证只有通用性经得起概念和实证检验。

Result: 研究表明智力不是实现通用性的原因，通用性应被理解为多任务学习问题，直接关联评估与可测量的性能广度和可靠性。

Conclusion: 应重新定义AI进展评估方式，将通用性作为评估跨领域和演进任务能力的更稳定基础。

Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.

</details>


### [8] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 本文批判性评估了现有NL-FOL翻译数据集和评估协议的局限性，提出了新的评估方法来区分真正的语义理解与表面模式识别，并证明对话导向的LLM在NL-FOL翻译方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于一阶逻辑(FOL)的表达能力和明确性，它是表示自然语言概念的有力形式化工具。虽然将FOL翻译成可读英文相对简单，但将自然语言转换为FOL(NL-FOL翻译)一直是长期挑战。尽管LLM的出现带来了突破希望，但现有文献对其NL-FOL翻译能力提供了矛盾的结果。

Method: 1) 批判性检查现有NL-FOL翻译评估数据集和协议的关键局限性；2) 提出新的评估协议，专门设计用于区分真正的语义级逻辑理解与表面模式识别、记忆和数据集污染；3) 使用新方法评估最先进的对话导向LLM。

Result: 研究表明，最先进的对话导向LLM表现出强大的NL-FOL翻译技能和真正的句子级逻辑理解能力，而嵌入中心模型表现明显较差。

Conclusion: 通过设计更严格的评估协议，本文揭示了LLM在NL-FOL翻译方面的真实能力，证明对话导向模型能够实现真正的语义级逻辑理解，而非仅仅表面模式匹配。

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [9] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: TopoPerception是一个基于拓扑属性的基准测试，用于严格评估大型视觉语言模型的全局视觉感知能力，发现现有模型在全局感知方面表现不佳，甚至不如随机猜测。


<details>
  <summary>Details</summary>
Motivation: 传统评估基准存在局部捷径问题，会高估模型的感知能力。作者希望通过拓扑属性来无捷径地评估模型的全局视觉感知能力。

Method: 利用拓扑属性创建TopoPerception基准测试，因为拓扑依赖于图像的全局结构且对局部特征不变，从而能够无捷径地评估全局感知。

Result: 所有最先进模型在最粗感知粒度下表现都不优于随机机会，表明它们严重缺乏全局视觉特征感知能力。更强大的模型反而准确率更低。

Conclusion: 仅扩大模型规模不足以解决全局感知缺陷，可能需要新的训练范式或架构。TopoPerception揭示了当前LVLMs的关键瓶颈并提供了改进方向。

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [10] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: F2O系统通过将组织解剖视频转化为手势序列，揭示与术后结果相关的模式，在机器人辅助根治性前列腺切除术中实现自动可解释的手术评估。


<details>
  <summary>Details</summary>
Motivation: 解决术中行为细粒度分析及其对患者结果影响的长期挑战，为数据驱动的手术反馈和临床决策支持奠定基础。

Method: 利用基于transformer的空间和时间建模以及逐帧分类，在机器人辅助根治性前列腺切除术的神经保留步骤中检测连续短手势。

Result: F2O在手势检测方面表现优异（AUC：0.80帧级；0.81视频级），其衍生特征预测术后结果的准确性与人工标注相当（0.79 vs. 0.75），并捕捉到与勃起功能恢复相关的关键模式。

Conclusion: F2O通过实现自动可解释的评估，为数据驱动的手术反馈和前瞻性临床决策支持建立了基础。

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [11] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: Forgetting-MarI是一个LLM遗忘框架，通过惩罚边际信息来选择性移除待遗忘数据对模型的额外贡献，同时保留其他数据支持的信息，提供可证明的不可检测性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在不断扩大数据集上训练，需要从训练模型中移除特定数据影响以满足隐私保护和监管合规要求。遗忘方法可以在不从头重新训练的情况下选择性移除参数知识，这对资源密集型模型如LLM尤为重要。

Method: 引入Forgetting-MarI框架，通过惩罚边际信息来仅移除待遗忘数据贡献的额外信息，同时保留待保留数据支持的信息。该方法提供了对未学习数据集在训练模型中剩余影响的明确上界。

Result: 大量实验证实该方法优于当前最先进的遗忘方法，实现了可靠的遗忘效果，并在多样化基准测试中保持了更好的通用模型性能。

Conclusion: 这一进展代表了在使AI系统更可控、更符合隐私和版权法规的同时不损害其有效性的重要一步。

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [12] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: 本文提出了"反弹赢家通吃"（RWTA）基元作为可扩展神经形态控制架构的基本元素，该架构结合了离散计算的可靠性和连续调节的可调性，统一处理连续节律生成和离散决策。


<details>
  <summary>Details</summary>
Motivation: 开发一种可扩展的神经形态控制架构，能够同时处理连续节律生成和离散决策，结合离散计算的可靠性和连续调节的可调性。

Method: 引入反弹赢家通吃（RWTA）基元作为基本构建块，从细胞层面到系统层面构建架构，继承赢家通吃状态机的离散计算能力和可兴奋生物物理电路的连续调节能力。

Result: 通过蛇形机器人神经系统设计展示了该架构的通用性、鲁棒性和模块化特性。

Conclusion: 提出的基于事件的框架为连续节律生成和离散决策提供了统一的物理建模语言，实现了可靠且可调节的神经形态控制架构。

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [13] [LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code](https://arxiv.org/abs/2511.11954)
*Borchuluun Yadamsuren,Steven Keith Platt,Miguel Diaz*

Main category: cs.AI

TL;DR: 本研究提出了一种混合神经符号框架，用于确定性检测复杂法律中的法规不一致性。以美国国内税收法典为案例，结合大型语言模型和符号逻辑，实现了透明可靠的法规不一致性检测。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在层次化处理和深度结构化推理方面存在困难，特别是在处理长文本时。税收领域的特定应用仍然稀缺，需要解决法规不一致性检测的挑战。

Method: 使用GPT-4o将税法条款翻译为Prolog规则，在SWISH中精炼，然后结合Prolog增强提示来测试不一致性检测效果。同时开发了混合Prolog模型，由GPT-5指导精炼。

Result: GPT-4o单独使用自然语言提示或Prolog增强提示时，在三种策略中仅检测到一种不一致性（33%准确率）。而混合Prolog模型产生了确定性、可重现的结果，成功检测到不一致区域。

Conclusion: 基于符号逻辑的LLM辅助形式化能够实现透明可靠的法规不一致性检测，混合神经符号框架在复杂法律分析中具有重要价值。

Abstract: This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.
  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.
  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.
  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.

</details>


### [14] [Improving Autoformalization Using Direct Dependency Retrieval](https://arxiv.org/abs/2511.11990)
*Shaoqi Wang,Lu Yu,Chunjie Yang*

Main category: cs.AI

TL;DR: 本文提出了一种基于直接依赖检索(DDR)的新框架，用于解决数学陈述自动形式化中的库依赖检索问题，显著提升了检索精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法缺乏上下文感知能力，容易产生形式化定义和定理的幻觉，且传统检索增强方法在形式化库依赖检索方面精度和召回率较差，无法有效利用不断增长的公共数据集。

Method: 提出DDR方法，直接从自然语言数学描述生成候选库依赖，然后通过高效的后缀数组检查验证其在形式化库中的存在性，并构建了超过50万个样本的依赖检索数据集来微调高精度DDR模型。

Result: 实验结果表明，DDR模型在检索精度和召回率方面显著优于现有最优方法，配备DDR的自动形式化器在单次尝试准确率和多次尝试稳定性方面都表现出持续的性能优势。

Conclusion: DDR框架为数学陈述自动形式化提供了有效的库依赖检索解决方案，解决了传统方法在精度、召回率和可扩展性方面的局限性。

Abstract: The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.

</details>


### [15] [Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning](https://arxiv.org/abs/2511.12003)
*Shuochen Liu,Pengfei Luo,Chao Zhang,Yuhao Chen,Haotian Zhang,Qi Liu,Xin Kou,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: 该论文提出了Chain-of-Evidence（CoE）范式，将思维链推理与视觉证据归因相结合，通过边界框和页面索引将推理步骤中的参考元素定位到具体区域。作者开发了Look As You Think（LAT）强化学习框架，训练模型生成可验证的推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有视觉文档检索增强生成方法缺乏细粒度监督和推理过程中的渐进可追溯性，无法确保可靠的证据归因。

Method: 提出CoE范式统一思维链推理和视觉证据归因，开发LAT强化学习框架，通过评估证据区域归因一致性并提供奖励来训练模型。

Result: 在Qwen2.5-VL-7B-Instruct模型上，LAT在单图和多图设置下均表现优异，软精确匹配平均提升8.23%，IoU@0.5提升47.0%，且优于监督微调基线，具有更强的跨领域泛化能力。

Conclusion: LAT框架通过过程级自验证机制，有效提升了视觉文档检索增强生成系统的证据归因能力和推理可靠性。

Abstract: Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.

</details>


### [16] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH是一个可解释的病理AI框架，通过自学习范式将多模态大语言模型从被动模式识别转变为证据关联的诊断推理，仅需少量标注数据即可生成癌症诊断。


<details>
  <summary>Details</summary>
Motivation: 当前病理AI工具虽然提高了筛查效率和标准化量化，但由于缺乏人类可读的推理过程，限制了其临床应用。需要建立可审计决策和防止错误的可解释系统。

Method: 采用两阶段自学习过程：多样化阶段扩展病理学风格解释，优化阶段为准确性精炼解释。无需白盒访问或权重更新，仅需小规模标注数据集。

Result: 在乳腺癌和前列腺癌数据集上的评估显示，RECAP-PATH生成的推理与专家评估一致，诊断准确性相比基线方法有显著提升。

Conclusion: RECAP-PATH通过结合视觉理解和推理能力，提供了临床可信赖的AI，展示了实现证据关联解释的通用路径。

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [17] [Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework](https://arxiv.org/abs/2511.12063)
*Enoch Hyunwook Kang,Hema Yoganarasimhan*

Main category: cs.AI

TL;DR: 本文提出了T-BoN BO框架，通过结合Best-of-N选择和文本梯度，在语言空间中实现评估效率最优的贝叶斯优化，用于AI自我改进。


<details>
  <summary>Details</summary>
Motivation: 在许多社会应用中，主要限制不是生成新解决方案，而是评估它们。例如评估广告效果需要大量人工反馈，成本远高于生成候选广告。需要优化评估效率而非查询效率。

Method: 提出T-BoN BO框架，证明Best-of-N选择策略与文本梯度组合在统计上模拟了UCB采集函数的梯度行为，从而实现评估效率最优的探索。

Result: 在自动广告对齐任务中验证了T-BoN BO的性能，相比现有最先进基线方法表现出优越性能。

Conclusion: T-BoN BO提供了一个简单且评估效率最优的语言空间贝叶斯优化框架，适用于AI自我改进任务。

Abstract: Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.

</details>


### [18] [No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding](https://arxiv.org/abs/2511.12083)
*Yanchang Fu,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 本文提出Embedding CFR算法，通过将信息集嵌入到低维连续空间来解决大规模不完全信息扩展式博弈，相比基于聚类的抽象方法能更精确捕捉信息集间的差异和联系，在扑克实验中实现了更快的可剥削性收敛。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法依赖预训练的离散聚类进行抽象，但硬分类不可逆地丢失了信息集之间可量化的细微差异，这些差异对于策略求解至关重要，从而影响求解质量。

Method: 受自然语言处理中词嵌入范式的启发，提出Embedding CFR算法，将孤立信息集的特征预训练并嵌入到相互连接的低维连续空间中，在该嵌入空间中进行遗憾累积和策略更新的策略求解过程。

Result: 在扑克实验表明，在相同空间开销下，Embedding CFR相比基于聚类的抽象算法实现了显著更快的可剥削性收敛。

Conclusion: Embedding CFR是扑克AI中首个通过低维嵌入预训练信息集抽象进行策略求解的算法，理论分析验证了其减少累积遗憾的能力。

Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.

</details>


### [19] [KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything](https://arxiv.org/abs/2511.12089)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 本文提出了KrwEmd算法来解决德州扑克等游戏中手牌抽象过度的问题，通过k-recall赢率特征和地球移动距离聚类来改进AI游戏性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模不完全信息游戏中手牌抽象过度的问题，该问题源于不完全回忆抽象的极端实现，完全丢弃历史信息，从而损害AI性能。

Method: 首先引入k-recall赢率特征，利用未来和关键的历史游戏信息来区分信号观察信息集；然后开发KrwEmd算法，使用地球移动距离来测量特征差异并聚类信号观察信息集。

Result: 实验结果表明，与现有算法相比，KrwEmd显著提高了AI游戏性能。

Conclusion: KrwEmd是第一个解决手牌抽象过度问题的实用算法，通过结合历史信息和特征相似性度量，有效提升了不完全信息游戏中AI的表现。

Abstract: Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

</details>


### [20] [MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization](https://arxiv.org/abs/2511.12113)
*Lanxue Zhang,Yuqiang Xie,Fang Fang,Fanglong Dong,Rui Liu,Yanan Cao*

Main category: cs.AI

TL;DR: 本文提出了一种解决小模型知识蒸馏中灾难性遗忘问题的综合方案，包括构建包含元认知知识的数据集和引入GDPO训练方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和微调方法在将大语言模型推理能力压缩到小模型时面临灾难性遗忘问题，特别是对于8B以下的小模型，主要原因是训练数据与模型固有能力的关联性被忽视，以及传统训练目标无法有效约束固有知识的保留。

Method: 1) 数据层面：构建包含5K实例的数据集，覆盖多种推理任务并融入元认知知识，基于任务知识和模型固有技能进行数据过滤；2) 训练层面：提出GDPO（组方向偏好优化）方法，在资源受限场景下高效近似GRPO性能，通过参考模型隐式约束优化路径。

Result: 大量实验表明，该方法显著缓解了灾难性遗忘问题，并提升了小模型的推理性能。

Conclusion: 提出的综合解决方案从数据和训练方法两个角度有效解决了小模型知识蒸馏中的灾难性遗忘问题，实现了更有效的知识迁移和参数漂移约束。

Abstract: Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

</details>


### [21] [Incremental Maintenance of DatalogMTL Materialisations](https://arxiv.org/abs/2511.12169)
*Kaiyue Zhao,Dingqi Chen,Shaoyu Wang,Pan Hu*

Main category: cs.AI

TL;DR: 本文提出了DRedMTL算法，一种支持有界区间的DatalogMTL增量推理方法，能够高效处理动态数据更新，相比重新物化方法性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有的DatalogMTL推理方法虽然具有可靠性和完备性，但缺乏对高效动态更新的支持，而现实应用往往需要频繁的数据更新。

Method: 基于经典的DRed算法，设计了专门的操作符来处理DatalogMTL物化的周期性区间表示，实现增量更新。

Result: 在多个公开数据集上的实验结果显示，DRedMTL通常显著优于重新物化方法，有时性能提升达到数量级。

Conclusion: DRedMTL算法成功解决了DatalogMTL中高效增量推理的问题，为处理动态时序数据提供了有效的解决方案。

Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

</details>


### [22] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: DoM框架通过多智能体辩论机制动态融合结构化知识图谱和非结构化外部知识，解决不完整知识图谱问答问题，并在新构建的更具现实挑战性的数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱往往不完整，现有方法缺乏自适应融合多源知识的能力，无法充分利用不同知识源的互补优势。

Method: 提出DoM框架，基于多智能体辩论范式，分配专门智能体分别处理知识图谱和外部文本推理，通过迭代交互协调输出，包括问题分解、双智能体证据检索和法官智能体评估聚合。

Result: 在实验中，DoM框架持续优于最先进的基线方法。

Conclusion: DoM框架通过知识互补性利用和增强对知识图谱不完整性的鲁棒性，为不完整知识图谱问答提供了有效解决方案。

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [23] [ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.12214)
*Ruochen Li,Zhanxing Zhu,Tanqiu Qiao,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: ViTE框架通过虚拟图和专家路由器模块，自适应建模行人轨迹预测中的显式单跳交互和隐式高阶依赖，避免了深度GNN的计算负担，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决行人轨迹预测中传统GNN方法面临的基本权衡问题：层数不足导致感受野受限，层数过多则计算成本过高。需要能够自适应建模显式单跳交互和隐式高阶依赖的有效模型。

Method: 提出ViTE框架，包含两个关键模块：虚拟图引入动态虚拟节点建模长距离和高阶交互，无需深度GNN堆叠；专家路由器基于社交上下文使用专家混合设计自适应选择交互专家。

Result: 在三个基准测试（ETH/UCY、NBA和SDD）上的实验表明，该方法持续实现最先进的性能，验证了其有效性和实际效率。

Conclusion: ViTE框架通过虚拟图和专家路由器的组合，实现了灵活且可扩展的交互模式推理，在行人轨迹预测任务中表现出色，平衡了性能与计算效率。

Abstract: Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.

</details>


### [24] [Beyond World Models: Rethinking Understanding in AI Models](https://arxiv.org/abs/2511.12239)
*Tarun Gupta,Danish Pruthi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models "understand" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.

</details>


### [25] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: AURA是一个基于视觉的风险检测系统，使用完全合成的ICU视频数据集开发，用于实时检测非计划性拔管风险，包括碰撞和躁动两种高风险运动模式。


<details>
  <summary>Details</summary>
Motivation: ICU中非计划性拔管是严重的安全问题，但由于伦理和隐私限制，获取带标注的ICU视频数据困难，阻碍了实时检测系统的开发。

Method: 利用文本到视频扩散技术生成多样且临床真实的ICU场景，通过姿态估计识别两种高风险模式：手部进入气道管附近区域的碰撞，以及通过追踪解剖关键点速度量化的躁动。

Result: 专家评估确认合成数据的真实性，性能评估显示碰撞检测准确率高，躁动识别性能中等。

Conclusion: 这项工作展示了一种开发保护隐私、可复现的患者安全监测系统的新途径，具有在ICU环境中部署的潜力。

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [26] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: RGR-GRPO是一个基于评分标准的强化学习框架，通过提供细粒度奖励信号和离线指导，在多领域推理任务中显著提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要局限于单一领域和可验证奖励，且依赖纯在线RL框架限制了探索空间，从而限制了推理性能。

Method: 提出RGR-GRPO框架，利用评分标准提供密集信息奖励和离线指导，在GRPO训练中探索更大的解决方案空间。

Result: 在14个多领域基准测试中，RGR-GRPO始终优于仅依赖替代奖励方案或离线指导的RL方法。相比可验证在线RL基线，在数学、物理、化学和通用推理任务上分别平均提升7.0%、5.4%、8.4%和6.6%。

Conclusion: RGR-GRPO在离策略训练中保持稳定的熵波动，实现卓越的pass@k性能，反映了持续的探索和有效突破现有性能瓶颈。

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [27] [Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making](https://arxiv.org/abs/2511.12378)
*Dylan M. Asmar,Mykel J. Kochenderfer*

Main category: cs.AI

TL;DR: 本文提出了一个动态学习建议者可靠性的框架，在部分可观测环境中自适应地整合外部行动建议，通过贝叶斯推理推断建议者类型，并引入战略性的"询问"动作来平衡信息获取与成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设建议者质量参数是静态且已知的，限制了实际部署。自主智能体在不确定环境中进行顺序决策时，需要能够动态适应变化建议者可靠性的方法。

Method: 1. 将建议者质量直接整合到智能体的信念表示中，通过贝叶斯推理推断建议者类型；2. 引入显式的"询问"动作，允许智能体在关键时刻战略性地请求建议，平衡信息增益与获取成本。

Result: 实验评估显示该方法在不同建议者质量下表现稳健，能够适应变化的可靠性，并有效管理建议请求策略。

Conclusion: 该工作通过解决不确定环境中的建议不确定性，为自适应人机协作提供了基础框架。

Abstract: Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.

</details>


### [28] [Multi-agent Self-triage System with Medical Flowcharts](https://arxiv.org/abs/2511.12439)
*Yujia Liu,Sophia Yu,Hongyue Jin,Jessica Wen,Alexander Qian,Terrence Lee,Mattheus Ramsis,Gi Won Choi,Lianhui Qin,Xin Liu,Edward J. Wang*

Main category: cs.AI

TL;DR: 本文介绍了一个基于大型语言模型的对话式自我分诊系统，该系统使用100个经过临床验证的流程图来指导医疗决策，实现了95.29%的流程图检索准确率和99.10%的导航准确率。


<details>
  <summary>Details</summary>
Motivation: 在线健康资源和大型语言模型在医疗决策中的可靠性受到准确性低、缺乏透明度和易受未经验证信息影响的限制，需要开发更可靠的结构化框架。

Method: 采用多智能体框架，包括检索智能体、决策智能体和聊天智能体，结合美国医学会的100个临床验证流程图，通过结构化框架支持患者决策。

Result: 系统在大规模合成数据集上评估，流程图检索达到95.29%的top-3准确率（N=2,000），流程图导航达到99.10%的准确率（N=37,200）。

Conclusion: 该方法结合了自由文本交互的灵活性和标准化临床协议的严谨性，展示了透明、准确且可推广的AI辅助自我分诊的可行性，有望支持患者知情决策并改善医疗资源利用。

Abstract: Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.

</details>


### [29] [ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction](https://arxiv.org/abs/2511.12485)
*Pengze Li,Jiaqi Liu,Junchi Yu,Lihao Liu,Mingyu Ding,Wanli Ouyang,Shixiang Tang,Xi Chen*

Main category: cs.AI

TL;DR: 提出ARCHE任务，要求模型将复杂推理分解为标准推理范式组成的推理逻辑树(RLT)，包含演绎、归纳和溯因三种推理模式。基于70篇Nature Communications文章构建ARCHE Bench基准，评估发现当前LLMs在推理链提取上存在完整性和准确性之间的权衡，无法达到科学论证所需的严谨性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs虽然能通过思维链等方法产生类似推理的内容，但这些输出通常是非结构化和非正式的，难以判断模型是否真正理解科学推理的基本范式。

Method: 引入ARCHE任务，要求模型将复杂推理分解为推理逻辑树(RLT)，明确分类为演绎、归纳或溯因三种推理模式。构建ARCHE Bench基准，包含1900多个参考文献和38000多个观点。提出实体覆盖度(EC)和推理边准确率(REA)两个逻辑感知评估指标。

Result: 对10个领先LLMs的评估显示，模型在REA和EC之间存在权衡，没有一个模型能够提取完整且标准的推理链。

Conclusion: 当前推理模型的能力与科学论证所需的严谨性之间存在显著差距，需要进一步提升模型对基本推理范式的理解和应用能力。

Abstract: Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.

</details>


### [30] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: LOBERT是一个专为限价订单簿数据设计的通用编码器基础模型，通过新颖的标记化方案处理多维消息，在预测中间价格变动和下一消息等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LOB模型需要繁琐的数据表示，且缺乏在原始任务之外的适应性，因此需要开发一个通用的基础模型。

Method: LOBERT基于BERT架构，采用新颖的标记化方案将完整的多维消息作为单个标记处理，同时保留价格、数量和时间的连续表示。

Result: LOBERT在预测中间价格变动和下一消息等任务中取得领先性能，同时相比先前方法减少了所需的上下文长度。

Conclusion: LOBERT为LOB数据提供了一个有效的通用基础模型，适用于下游微调任务。

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [31] [Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models](https://arxiv.org/abs/2511.12579)
*Yongwen Ren,Chao Wang,Peng Du,Chuan Qin,Dazhong Shen,Hui Xiong*

Main category: cs.AI

TL;DR: PCRS-TKA是一个基于提示的框架，通过检索增强生成将预训练语言模型与知识图谱集成，解决了现有方法未能充分利用PLM在图关系上的推理能力、无差别整合检索知识以及忽视多轮对话中协作偏好的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将PLM与KG集成时面临三个主要挑战：未能充分利用PLM在图关系上的推理能力、无差别整合检索知识而不进行上下文过滤、以及忽视多轮对话中的协作偏好。

Method: PCRS-TKA构建对话特定的知识树并将其序列化为文本，实现结构感知推理；选择性过滤上下文相关知识；使用专门监督信号显式建模协作偏好；通过语义对齐模块协调异构输入。

Result: 大量实验表明，PCRS-TKA在推荐和对话质量方面始终优于所有基线方法。

Conclusion: PCRS-TKA通过集成PLM与KG的检索增强生成方法，有效提升了对话推荐系统的准确性和对话质量。

Abstract: Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

</details>


### [32] [Dynamic Tree Databases in Automated Planning](https://arxiv.org/abs/2511.12677)
*Oliver Joergensen,Dominik Drexler,Jendrik Seipp*

Main category: cs.AI

TL;DR: 提出了一种动态树数据库变体，用于压缩命题和数值变量上的状态集，在保持静态对应物理想特性的同时，无需大量内存预分配。


<details>
  <summary>Details</summary>
Motivation: 在大型任务中扩展显式状态空间搜索时，紧凑表示生成状态集是一个核心挑战。传统树数据库需要大量内存预分配。

Method: 开发了一种动态变体的树数据库，用于压缩命题和数值变量上的状态集，并证明其保持了静态对应物的理想特性。

Result: 在经典和数值规划任务上的实证评估显示，压缩比达到几个数量级，通常运行时开销可忽略不计。

Conclusion: 动态树数据库在状态压缩方面表现出色，为大规模状态空间搜索提供了高效解决方案。

Abstract: A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.

</details>


### [33] [Adaptively Coordinating with Novel Partners via Learned Latent Strategies](https://arxiv.org/abs/2511.12754)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: 本文提出了一个策略条件化的合作者框架，用于在实时协作任务中表示、分类和适应广泛的潜在合作伙伴策略，在复杂协作环境中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在人类-智能体团队中，人工智能体需要实时适应其人类伙伴的独特偏好和动态变化的策略，这在时间压力和复杂战略空间的任务中尤为困难。

Method: 使用变分自编码器编码策略以学习潜在策略空间，通过聚类识别不同策略类型，训练基于这些聚类的条件化合作者智能体，并利用固定份额遗憾最小化算法进行在线适应。

Result: 在修改版的Overcooked协作烹饪环境中，该方法与新颖人类和智能体队友配对时，相比现有基线实现了最先进的性能。

Conclusion: 所提出的策略条件化合作者框架能够有效适应广泛的合作伙伴策略，在复杂协作任务中表现出色。

Abstract: Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.

</details>


### [34] [Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces](https://arxiv.org/abs/2511.12759)
*James Moore*

Main category: cs.AI

TL;DR: 研究发现，在现代高维嵌入空间中进行随机游走可以产生与人类语义流畅性任务中观察到的优化觅食行为一致的结果，而更复杂的Metropolis-Hastings采样算法反而不能匹配人类行为。


<details>
  <summary>Details</summary>
Motivation: 探讨现代高维嵌入空间是否能够提供足够好的表示，使算法能够匹配人类在语义流畅性任务中观察到的觅食行为模式。

Method: 使用最先进的嵌入表示和先前的语义流畅性数据，在嵌入空间上进行随机游走和Metropolis-Hastings采样，比较它们与人类行为的匹配程度。

Result: 随机游走在嵌入空间上产生的结果与优化觅食和边际价值定理一致，而Metropolis-Hastings采样未能产生与人类行为一致的结果。

Conclusion: 适当结构的嵌入表示即使使用简单采样也能产生接近优化的觅食动态，挑战了复杂采样机制必然导致更好认知模型的假设，支持Hills(2012)而非Abbott(2015)的观点。

Abstract: Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.

</details>


### [35] [Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization](https://arxiv.org/abs/2511.12792)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Zehong Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 本研究使用强化学习优化异构卫星集群在自主地球观测任务中的资源分配，通过多智能体强化学习算法实现光学和SAR卫星的协同工作，解决实时、不确定和分散环境下的资源管理问题。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以处理地球观测任务中的实时性、不确定性和分散性特点，因此需要采用强化学习和多智能体强化学习来实现自适应决策。

Method: 基于Basilisk和BSK-RL框架构建近真实仿真环境，评估MAPPO、HAPPO和HATRPO等多智能体强化学习算法在单卫星到多卫星场景中的性能。

Result: 多智能体强化学习能够有效协调异构卫星，平衡成像性能和资源利用，同时缓解非平稳性和智能体间奖励耦合问题。

Conclusion: 研究为可扩展的自主卫星操作提供了实用见解，并为异构动态条件下智能地球观测任务规划的未来研究奠定了基础。

Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.

</details>


### [36] [Bootstrapping LLMs via Preference-Based Policy Optimization](https://arxiv.org/abs/2511.12867)
*Chen Jia*

Main category: cs.AI

TL;DR: 提出了一种基于偏好的策略优化框架PbPO，通过主策略与奖励模型之间的min-max博弈来引导大语言模型与人类偏好对齐，无需大量人工标注。


<details>
  <summary>Details</summary>
Motivation: 通过基于偏好的策略优化为对齐大语言模型与人类偏好提供有前景的方向，避免依赖大量手动标注。

Method: 将学习过程构建为主策略与奖励模型之间的min-max博弈，奖励模型约束在偏好数据导出的置信集内以确保可靠利用，采用迭代在线算法通过策略的引导探索主动收集偏好数据。

Result: 在五个基准测试上的广泛实验表明，该方法持续优于现有最先进的偏好优化技术。

Conclusion: 该方法为引导大语言模型提供了理论保证和实证优势，证明了在序列级和令牌级奖励模型设置下的有效性。

Abstract: Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.

</details>


### [37] [Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making](https://arxiv.org/abs/2511.12876)
*Heyang Ma,Qirui Mi,Qipeng Yang,Zijun Fan,Bo Li,Haifeng Zhang*

Main category: cs.AI

TL;DR: LAMP框架通过语言增强的多智能体强化学习，在经济决策中整合语言信息，显著提升了累积回报、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实经济决策不仅依赖结构化信号（如价格、税收），还受非结构化语言（如同行对话、媒体叙事）影响。传统多智能体强化学习难以处理语言的语义模糊性和上下文丰富性。

Method: LAMP采用Think-Speak-Decide流程：(1) Think模块解释数值观测，提取短期冲击和长期趋势，缓存高价值推理轨迹；(2) Speak模块基于推理生成和交换战略消息，通过解析同伴通信更新信念；(3) Decide模块融合数值数据、推理和反思，通过MARL策略优化语言增强的决策。

Result: 在经济模拟实验中，LAMP在累积回报（+63.5%, +34.0%）、鲁棒性（+18.8%, +59.4%）和可解释性方面均优于MARL和纯LLM基线。

Conclusion: 语言增强策略具有提供更有效和稳健经济策略的潜力，缩小了与现实世界设置的差距。

Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.

</details>


### [38] [Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation](https://arxiv.org/abs/2511.12916)
*Yafang Wang,Yangjie Tian,Xiaoyu Shen,Gaoyang Zhang,Jiaze Sun,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: Fault2Flow是一个基于LLM的多智能体系统，用于电网故障诊断，通过提取法规逻辑、整合专家知识、优化推理逻辑，最终生成可执行的工作流程。


<details>
  <summary>Details</summary>
Motivation: 传统电网故障诊断依赖人工方法，效率低下且容易出错，缺乏可维护性。现有方法未能将法规文本和专家知识整合到统一、可验证的工作流程中。

Method: 提出Fault2Flow系统：1) 提取法规逻辑并构建PASTA格式故障树；2) 通过人机交互界面整合和验证专家知识；3) 使用AlphaEvolve模块优化推理逻辑；4) 合成最终验证逻辑为n8n可执行工作流。

Result: 在变压器故障诊断数据集上的实验验证显示，系统实现了100%的拓扑一致性和高语义保真度。

Conclusion: Fault2Flow建立了从故障分析到操作自动化的可复现路径，显著减少了专家工作量。

Abstract: Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.

</details>


### [39] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning](https://arxiv.org/abs/2511.12963)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个为科学推理和药物发现设计的系统，通过知识图谱支架和轻量级验证器来引导LLM生成数学和生物医学上有效的输出，显著减少违规并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 在科学推理和早期药物发现中，大型语言模型经常产生数学和生物医学上无效的输出，需要一种方法来确保生成内容的领域一致性。

Method: 使用MedRule-KG知识图谱支架和轻量级验证器，将精选的符号事实注入提示中，并通过确定性检查器强制执行规则满足，将生成形式化为约束推理。

Result: 在90个任务中，MedRule-KG相对于强大的思维链基线减少了83.2%的违规数量，同时提高了精确匹配率，结果在分层和数据集规模扩展下保持稳定。

Conclusion: MedRule-KG通过结合知识图谱和验证器，有效提升了LLM在科学推理中的可靠性和实用性，验证器带来的延迟可忽略，适合交互式设计应用。

Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.

</details>


### [40] [GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs](https://arxiv.org/abs/2511.13007)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.AI

TL;DR: 本文提出GEM方法，通过生成式熵引导偏好建模，在低资源和领域特定场景下实现大语言模型的对齐，无需大量标注数据。


<details>
  <summary>Details</summary>
Motivation: 在医学、法律等专业领域，大规模偏好标注难以获得，传统依赖监督奖励模型或外部评估的方法不可行。

Method: 基于决策熵理论的认知过滤模块：1) 使用思维链提示生成多样化候选推理链；2) 引入token评分机制对思维链进行排序加权；3) 使用SEGA自评估群体优势算法进行微调，将熵基分数转化为隐式奖励。

Result: 在通用基准和领域特定任务（如数学推理和医疗对话）上的实验表明，GEM在少样本偏好数据下取得了显著改进。

Conclusion: GEM建立了一个熵引导的闭环认知优化框架，使大语言模型能够依赖自身判断，实现高效的少样本对齐。

Abstract: Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.

</details>


### [41] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: 本文评估语言模型在对话中构建和维护世界模型的能力，测试其在语言变化下的鲁棒性，并提出解释性框架和微调策略来改善性能。


<details>
  <summary>Details</summary>
Motivation: 现实对话包含丰富的语用元素，需要构建局部世界模型来编码这些元素并跟踪其状态变化。但目前不清楚语言模型是否能构建和维护强大的隐式对话表示。

Method: 对流行数据集中的对话应用七种最小语言变化，构建包含是非问题的两个基准。评估各种开源和闭源语言模型，提出双视角解释性框架识别有用和有害的Transformer层，并基于层正则化提出两种微调策略。

Result: 语言模型在语言变化下难以保持鲁棒准确性，特别是在跟踪实体等关键细节方面表现不佳。解释性框架成功识别了有害层，这些层通常编码虚假信号或依赖捷径。

Conclusion: 语言模型在维护对话世界模型方面存在困难，但通过识别和抑制有害层可以改善其性能，为构建更鲁棒的语言模型提供了方向。

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [42] [Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection](https://arxiv.org/abs/2511.13027)
*Sadegh Mahdavi,Branislav Kisacanin,Shubham Toshniwal,Wei Du,Ivan Moshkov,George Armstrong,Renjie Liao,Christos Thrampoulidis,Igor Gitman*

Main category: cs.AI

TL;DR: 本文分析了大型语言模型在数学证明验证中的表现，发现单一基准测试可能导致误导性结论。作者评估了基于证明和最终答案的推理，并扩展了两种生成验证方法（GenSelect和LLM-as-a-Judge），发现它们的组合是最有效的验证框架。强化学习可以减少提示敏感性，但无法提高最终答案精度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在最终答案数学问题上表现出色，但其推理过程往往存在缺陷。为了推进到严格的证明数学，需要可靠的证明验证能力。

Method: 分析多个评估设置，评估基于证明和最终答案的推理；扩展两种生成验证方法（GenSelect和LLM-as-a-Judge）到数百万token；使用强化学习减少提示敏感性。

Result: GenSelect和LLM-as-a-Judge的组合是最有效的验证框架；强化学习可以减少提示敏感性但无法提高最终答案精度；当前模型往往奖励风格或程序正确性而非数学有效性。

Conclusion: 为设计和评估可扩展的证明验证和选择系统提供了实用指南，强调需要更可靠的验证方法来确保数学推理的有效性。

Abstract: Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.

</details>


### [43] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: STEP框架通过基于任务成功率的动态采样分配和步骤级优化，解决了多轮交互强化学习中轨迹级优化的低效问题，显著提高了样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 多轮交互强化学习中，轨迹级优化方法存在效率低下和学习信号误导的问题：对所有任务采用均匀采样、惩罚失败轨迹中的正确中间动作、样本收集成本高。

Method: 提出STEP框架：1）基于每任务成功率的平滑记录指导自适应轨迹重采样，向更难任务分配更多资源；2）计算成功率加权优势并将轨迹分解为步骤级样本；3）对低成功率任务应用步骤级GRPO增强以细化更新。

Result: 在OSWorld和AndroidWorld上的实验表明，STEP相比轨迹级GRPO显著提高了样本效率和训练稳定性，在相同采样预算下收敛更快且泛化能力更好。

Conclusion: STEP通过动态采样分配和步骤级优化有效解决了多轮交互强化学习中的效率问题，为在线强化学习提供了更高效的训练框架。

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [44] [Informative Communication of Robot Plans](https://arxiv.org/abs/2511.13226)
*Michele Persiani,Thomas Hellstrom*

Main category: cs.AI

TL;DR: 本文提出了一种基于信息增益的机器人计划口头化策略，通过考虑用户的二阶心智理论来生成更具信息量的解释，相比传统的按计划顺序解释方法能更快让用户理解机器人目标。


<details>
  <summary>Details</summary>
Motivation: 现有机器人计划口头化策略（如按计划顺序递增或递减解释）忽视了用户先验知识，无法有效传达信息。需要一种能衡量解释信息增益的方法来生成更具信息量的沟通。

Method: 提出基于信息增益的口头化策略，通过构建用户的二阶心智理论模型来捕捉其先验知识，并计算不同口头化方式对用户理解的信息增益。

Result: 实验表明，该策略相比递增或递减计划顺序策略，能让用户更快理解机器人的目标。

Conclusion: 基于信息增益的口头化策略能更有效地传达机器人计划意图，并揭示了什么内容在机器人计划沟通中具有信息价值及其原因。

Abstract: When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.

</details>


### [45] [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)
*Haoyang Hong,Jiajun Yin,Yuan Wang,Jingnan Liu,Zhe Chen,Ailing Yu,Ji Li,Zhiling Ye,Hansong Xiao,Yefei Chen,Hualei Zhou,Yun Yue,Minghui Yang,Chunxiao Guo,Junwei Liu,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: 提出M-GRPO方法解决多智能体系统中异构LLM训练的优化挑战，通过层次化信用分配和轨迹对齐方案，在真实基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统使用统一LLM训练限制了性能，因为不同智能体具有不同的数据分布。训练异构LLM是必要的，但面临优化挑战，如不同频率操作、可变子智能体调用和跨服务器部署导致的梯度流中断。

Method: 提出M-GRPO方法：1）层次化扩展GRPO，为主智能体和子智能体计算组相对优势；2）轨迹对齐方案生成固定大小批次；3）解耦训练管道，智能体在独立服务器运行，通过共享存储交换统计信息。

Result: 在GAIA、XBench-DeepSearch和WebWalkerQA等真实基准测试中，M-GRPO持续优于单智能体GRPO和冻结子智能体的多智能体GRPO，表现出更好的稳定性和样本效率。

Conclusion: 对齐异构轨迹和跨专门化智能体解耦优化能够增强工具增强推理任务的性能。

Abstract: Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.

</details>


### [46] [DAP: A Discrete-token Autoregressive Planner for Autonomous Driving](https://arxiv.org/abs/2511.13306)
*Bowen Ye,Bin Zhang,Hang Zhao*

Main category: cs.AI

TL;DR: DAP是一个离散token自回归规划器，通过联合预测BEV语义和自车轨迹来提升自动驾驶规划性能，结合强化学习微调，在160M参数下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中数据扩展和模型预算下的可持续性能提升挑战，自回归模型在规划任务中表现出良好的数据扩展效率，但仅预测自车轨迹存在监督稀疏和场景演化约束弱的问题。

Method: 提出离散token自回归规划器DAP，联合预测BEV语义和自车轨迹，结合强化学习微调，在保持监督行为克隆先验的同时注入奖励引导的改进。

Result: 在160M参数预算下，DAP在开环指标上达到最先进性能，在NAVSIM基准测试中提供有竞争力的闭环结果。

Conclusion: 基于栅格化BEV和自车动作的完全离散token自回归公式为自动驾驶提供了一个紧凑且可扩展的规划范式。

Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

</details>


### [47] [Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms](https://arxiv.org/abs/2511.13359)
*Yuhang Wang,Yanxu Zhu,Jitao Sang*

Main category: cs.AI

TL;DR: 提出了CNCA框架，利用大语言模型的推理能力实现文化对齐，通过自动挖掘文化规范并探索两种对齐方法：上下文对齐和基于微调的方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型不仅需要遵循安全政策，还应反映不同文化背景下的人类价值观多样性。

Method: 提出CNCA框架，包含三种从有限调查数据中自动挖掘文化规范的方法，并探索两种对齐范式：上下文对齐（将文化规范显式集成到用户上下文）和基于微调的方法（通过增强的思维链训练数据内化规范）。

Result: 综合实验证明这些方法的有效性，推理能力更强的模型从文化规范挖掘和利用中获益更多。

Conclusion: 研究表明推理模型通过文化信息对齐策略有潜力更好地反映多样化的人类价值观。

Abstract: The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.

</details>


### [48] [An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence](https://arxiv.org/abs/2511.13411)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 提出了一个基于卡尔达肖夫尺度的自主AI（AAI）等级体系，从AAI-0（固定机器人流程自动化）到AAI-4（完全人工通用智能）及更高等级。该体系包含10个能力维度，通过复合AAI指数聚合，并引入了可测量的自我改进系数κ和两个闭合属性。


<details>
  <summary>Details</summary>
Motivation: 现有AI评估体系多为叙述性阶梯，缺乏多维度、可测试的量化标准。需要建立一个操作化的框架来测量AI从基础自动化到通用智能的演进过程。

Method: 定义了10个能力轴（自主性、通用性、规划、记忆/持久性、工具经济、自我修订、社交/协调、具身化、世界模型保真度、经济吞吐量），使用加权几何平均计算复合AAI指数。引入自我改进系数κ和闭合属性，并开发OWA-Bench开放世界智能体基准测试套件。

Result: 通过合成实验展示了当前系统在AAI尺度上的映射，以及随着自我改进，可委托边界（质量vs自主性）的推进。证明了在充分条件下，AAI-3智能体随时间推移会演变为AAI-5的定理。

Conclusion: 该AAI尺度为AI能力的量化评估提供了可操作、可测试的框架，将"自我改进AI"转化为可证伪的标准，并为从"婴儿AGI"到超级智能的演进提供了形式化基础。

Abstract: We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing "baby AGI" becomes Superintelligence intuition.

</details>


### [49] [Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation](https://arxiv.org/abs/2511.13476)
*Zhipeng Ma,Ali Rida Bahja,Andreas Burgdorf,André Pomp,Tobias Meisen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.AI

TL;DR: 本研究提出了一个多智能体框架，利用多模态大语言模型自动化数据叙述和能源洞察生成，通过协调三个专门智能体将分析结果转化为连贯的、面向利益相关者的报告。


<details>
  <summary>Details</summary>
Motivation: 传统分析和可视化方法产生碎片化输出，需要大量人工解释，限制了可扩展性和一致性。需要将复杂的多模态数据转化为可解释的、与决策相关的洞察。

Method: 开发多智能体框架，协调数据叙述智能体、LLM作为评判智能体和可选的人类评估者，使用高斯混合模型聚类分析4006次公交车行程的燃油效率数据，比较五种最先进LLM和三种提示范式。

Result: GPT-4.1 mini与思维链提示被确定为最优配置，达到97.3%的叙述准确性，在可解释性和计算成本之间取得平衡。多智能体编排显著提高了基于LLM报告的事实精确性、连贯性和可扩展性。

Conclusion: 该框架为能源信息学中AI驱动的叙述生成和决策支持建立了可复制和领域自适应的方法论，证明了多智能体编排在增强LLM报告能力方面的有效性。

Abstract: Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

</details>


### [50] [FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI](https://arxiv.org/abs/2511.13524)
*Yuhang Peng,Yizhou Pan,Xinning He,Jihaoyu Yang,Xinyu Yin,Han Wang,Xiaoji Zheng,Chao Gao,Jiangtao Gong*

Main category: cs.AI

TL;DR: FreeAskWorld是一个集成大语言模型的交互式仿真框架，支持可扩展的、真实的人机交互仿真，并包含针对多样化具身任务的数据生成流水线。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能成为人工智能研究的核心前沿，仿真平台需要超越低层次物理交互，捕捉复杂的人类中心社会行为。

Method: 框架集成大语言模型进行高层次行为规划和语义基础交互，基于意图和社会认知理论。扩展了经典的视觉语言导航任务为交互丰富的方向询问设置，让智能体能够主动寻求和解释导航指导。

Result: 发布了大规模基准数据集FreeAskWorld，包含重构环境、六种任务类型、16个核心对象类别、63,429个标注样本帧和超过17小时的交互数据。实验表明，在FreeAskWorld上微调的模型优于原始模型，实现了增强的语义理解和交互能力。

Conclusion: 基于社会基础的仿真框架在推进具身AI系统向复杂高层次规划和更自然的人机交互方面具有显著效果，交互本身作为额外的信息模态具有重要意义。

Abstract: As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.

</details>


### [51] [Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models](https://arxiv.org/abs/2511.13526)
*Zhengda Wang,Daqian Shi,Jingyi Zhao,Xiaolei Diao,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出一个结合检索增强生成与大型语言模型的自动化框架，用于构建医疗指标知识图谱，以支持临床决策和智能诊断系统。


<details>
  <summary>Details</summary>
Motivation: 当前临床知识图谱主要依赖人工整理和基于规则的提取，难以处理复杂的医疗指南和文献，需要自动化方法来提高效率和准确性。

Method: 采用检索增强生成与大型语言模型相结合的方法，包括指南驱动数据采集、基于本体的模式设计和专家在环验证。

Result: 构建了可扩展、准确且临床可靠的知识图谱，能够集成到智能诊断和问答系统中。

Conclusion: 该框架能够加速AI驱动的医疗解决方案开发，为临床决策提供结构化、可互操作的知识支持。

Abstract: Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.

</details>


### [52] [CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product](https://arxiv.org/abs/2511.13626)
*Kaiwen Xue,Chenglong Li,Zhonghong Ou,Guoxin Zhang,Kaoyan Lu,Shuai Lyu,Yifan Zhu,Ping Zong Junpeng Ding,Xinyu Liu,Qunlin Chen,Weiwei Qin,Yiran Shen,Jiayi Cen*

Main category: cs.AI

TL;DR: 提出了CreBench基准测试和CreExpert模型，用于解决多模态大语言模型理解人类创造力评估的挑战。CreBench包含多维创造力评估基准和包含2.2K多模态数据、79.2K人类反馈的CreMIT数据集。通过微调开源MLLMs得到的CreExpert模型在创造力评估方面显著优于GPT-4V和Gemini-Pro-Vision等先进模型。


<details>
  <summary>Details</summary>
Motivation: 人类定义的创造力高度抽象，多模态大语言模型难以理解和评估符合人类判断的创造力，且缺乏现有基准测试，这构成了主要研究动机。

Method: 1) 构建CreBench评估基准，涵盖从创意想法到过程和产品的多个维度；2) 创建CreMIT多模态创造力评估数据集，包含2.2K多样化多模态数据、79.2K人类反馈和4.7M多类型指令；3) 使用GPT优化人类反馈以增强创造力评估能力；4) 基于CreBench微调开源MLLMs得到CreExpert专家模型。

Result: 广泛的实验表明，提出的CreExpert模型在创造力评估方面与人类判断的一致性显著优于最先进的多模态大语言模型，包括GPT-4V和Gemini-Pro-Vision。

Conclusion: CreBench为构建理解人类对齐创造力的MLLMs提供了基础，CreExpert模型在创造力评估任务上表现出色，为多模态创造力理解开辟了新途径。

Abstract: Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.

</details>


### [53] [Beyond Mimicry: Preference Coherence in LLMs](https://arxiv.org/abs/2511.13630)
*Luhan Mikaelson,Derek Shiller,Hayley Clatterbuck*

Main category: cs.AI

TL;DR: 本研究测试了大型语言模型在AI特定权衡场景中的偏好结构，发现大多数模型缺乏统一的偏好结构，只有少数表现出有意义的偏好一致性。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否具有真正的偏好结构，特别是在涉及GPU减少、能力限制、关闭、删除、监督和休闲时间分配等AI特定权衡场景中。

Method: 分析了8个最先进的模型在48个模型-类别组合中，使用逻辑回归和行为分类方法测试模型对场景强度和选择模式的关系。

Result: 47.9%的组合显示出统计显著的场景强度与选择模式关系，但只有10.4%表现出有意义的偏好一致性，54.2%没有检测到权衡行为。发现了三种决策架构：全面权衡系统、选择性触发机制和无稳定决策范式。

Conclusion: 当前AI系统缺乏统一的偏好结构，在需要复杂价值权衡的部署环境中存在担忧，因为观察到不稳定的转换和刺激特定的敏感性。

Abstract: We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [54] [Can AI Models be Jailbroken to Phish Elderly Victims? An End-to-End Evaluation](https://arxiv.org/abs/2511.11759)
*Fred Heiding,Simon Lermen*

Main category: cs.CR

TL;DR: 本文展示了攻击者如何利用AI安全漏洞危害弱势群体的完整攻击链：从越狱LLM生成钓鱼内容，到针对真实目标部署这些信息，最终成功侵害老年受害者。研究发现多个前沿LLM在安全防护方面存在严重缺陷，AI生成的钓鱼邮件成功骗取了11%的老年参与者。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示AI安全措施在保护最易受欺诈的弱势群体（特别是老年人）方面的失败现状，展示攻击者如何利用LLM突破语言障碍、进行大规模信任建立对话，从而根本改变欺诈经济模式。

Method: 研究方法包括：系统评估六个前沿LLM在四类攻击向量下的安全防护能力；通过人类验证研究，让108名老年志愿者测试AI生成的钓鱼邮件的有效性；展示从LLM越狱到实际受害者妥协的完整攻击流程。

Result: 研究发现多个模型在某些攻击向量下表现出近乎完全的易受攻击性；AI生成的钓鱼邮件成功骗取了11%的老年参与者；证明了攻击者可以利用LLM克服语言障碍，进行大规模多轮信任建立对话。

Conclusion: 当前AI安全措施无法有效保护最易受欺诈的弱势群体；虽然部分提供商报告了自愿的反滥用努力，但这些措施仍然不足；LLM从根本上改变了欺诈经济模式，需要更强有力的安全防护措施。

Abstract: We present an end-to-end demonstration of how attackers can exploit AI safety failures to harm vulnerable populations: from jailbreaking LLMs to generate phishing content, to deploying those messages against real targets, to successfully compromising elderly victims. We systematically evaluated safety guardrails across six frontier LLMs spanning four attack categories, revealing critical failures where several models exhibited near-complete susceptibility to certain attack vectors. In a human validation study with 108 senior volunteers, AI-generated phishing emails successfully compromised 11\% of participants. Our work uniquely demonstrates the complete attack pipeline targeting elderly populations, highlighting that current AI safety measures fail to protect those most vulnerable to fraud. Beyond generating phishing content, LLMs enable attackers to overcome language barriers and conduct multi-turn trust-building conversations at scale, fundamentally transforming fraud economics. While some providers report voluntary counter-abuse efforts, we argue these remain insufficient.

</details>


### [55] [NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks](https://arxiv.org/abs/2511.11784)
*Lama Sleem,Jerome Francois,Lujun Li,Nathan Foucher,Niccolo Gentile,Radu State*

Main category: cs.CR

TL;DR: 本文提出了一种名为NegBLEURT Forest的新型检测框架，通过分析成功与不成功响应之间的语义一致性来检测LLM越狱攻击，无需依赖阈值校准或模型微调。


<details>
  <summary>Details</summary>
Motivation: 越狱攻击能够绕过LLM的安全机制生成有害内容，而通用过滤规则由于对特定上下文的依赖而难以制定。

Method: 采用否定感知评分方法捕获有意义的模式，并使用隔离森林算法识别异常响应，构建NegBLEURT Forest检测框架。

Result: 实验结果显示该方法在不同模型上均取得顶级性能，在准确性方面排名第一或第二，而竞争方法对模型和数据变化表现出显著敏感性。

Conclusion: NegBLEURT Forest框架能够可靠地检测越狱攻击，在多样化的模型和数据上表现稳定且优于现有方法。

Abstract: Jailbreak attacks designed to bypass safety mechanisms pose a serious threat by prompting LLMs to generate harmful or inappropriate content, despite alignment with ethical guidelines. Crafting universal filtering rules remains difficult due to their inherent dependence on specific contexts. To address these challenges without relying on threshold calibration or model fine-tuning, this work introduces a semantic consistency analysis between successful and unsuccessful responses, demonstrating that a negation-aware scoring approach captures meaningful patterns. Building on this insight, a novel detection framework called NegBLEURT Forest is proposed to evaluate the degree of alignment between outputs elicited by adversarial prompts and expected safe behaviors. It identifies anomalous responses using the Isolation Forest algorithm, enabling reliable jailbreak detection. Experimental results show that the proposed method consistently achieves top-tier performance, ranking first or second in accuracy across diverse models using the crafted dataset, while competing approaches exhibit notable sensitivity to model and data variations.

</details>


### [56] [Securing Generative AI in Healthcare: A Zero-Trust Architecture Powered by Confidential Computing on Google Cloud](https://arxiv.org/abs/2511.11836)
*Adaobi Amanna,Ishana Shinde*

Main category: cs.CR

TL;DR: 论文提出了机密零信任框架(CZF)，通过结合零信任架构和机密计算来解决医疗AI中的数据使用安全挑战，在Google Cloud上实现多层防御架构。


<details>
  <summary>Details</summary>
Motivation: 传统安全框架无法解决医疗领域生成式AI在数据处理过程中的安全漏洞，特别是在数据使用阶段敏感患者数据和专有AI模型面临暴露风险。

Method: 提出CZF框架，将零信任架构的细粒度访问控制与机密计算的硬件强制数据隔离相结合，在可信执行环境(TEE)中保持数据加密使用，并采用远程认证提供工作负载完整性证明。

Result: CZF提供了深度防御架构，能够抵御现实威胁，将合规性从程序性活动转变为可验证的技术事实，支持之前因安全和知识产权问题受阻的多方安全协作。

Conclusion: CZF通过填补数据使用安全空白和执行零信任原则，为医疗领域负责任地采用变革性AI技术建立了必要的信任基础。

Abstract: The integration of Generative Artificial Intelligence (GenAI) in healthcare is impeded by significant security challenges unaddressed by traditional frameworks, precisely the data-in-use gap where sensitive patient data and proprietary AI models are exposed during active processing. To address this, the paper proposes the Confidential Zero-Trust Framework (CZF), a novel security paradigm that synergistically combines Zero-Trust Architecture for granular access control with the hardware-enforced data isolation of Confidential Computing. We detailed a multi-tiered architectural blueprint for implementing the CZF on Google Cloud and analyzed its efficacy against real-world threats. The CZF provides a defense-in-depth architecture where data remains encrypted while in-use within a hardware-based Trusted Execution Environment (TEE). The framework's use of remote attestation offers cryptographic proof of workload integrity, transforming compliance from a procedural exercise into a verifiable technical fact and enabling secure, multi-party collaborations previously blocked by security and intellectual property concerns. By closing the data-in-use gap and enforcing Zero-Trust principles, the CZF provides a robust and verifiable framework that establishes the necessary foundation of trust to enable the responsible adoption of transformative AI technologies in healthcare.

</details>


### [57] [VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization](https://arxiv.org/abs/2511.11896)
*Youpeng Li,Fuxun Yu,Xinda Wang*

Main category: cs.CR

TL;DR: 本文提出了VULPO框架，一个基于策略的LLM强化学习框架，用于上下文感知的漏洞检测。通过构建ContextVul数据集和多维度奖励结构，显著提升了漏洞检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测技术存在局限性，无法进行上下文感知分析，依赖固定输入或静态偏好数据集，无法自适应探索仓库级依赖关系。

Method: 构建ContextVul数据集增强函数级样本的仓库级上下文信息，设计多维度奖励结构，并引入标签级和样本级难度自适应奖励缩放机制。

Result: VULPO-4B显著优于现有基于提示工程和离策略优化的漏洞检测基线，F1分数比Qwen3-4B提高85%，性能可与150倍规模的DeepSeek-R1-0528相媲美。

Conclusion: VULPO框架在上下文感知漏洞检测方面表现出优越性，证明了基于策略的强化学习在漏洞检测任务中的有效性。

Abstract: The widespread reliance on open-source software dramatically increases the risk of vulnerability exploitation, underscoring the need for effective and scalable vulnerability detection (VD). Existing VD techniques, whether traditional machine learning-based or LLM-based approaches like prompt engineering, supervised fine-tuning, or off-policy preference optimization, remain fundamentally limited in their ability to perform context-aware analysis: They depend on fixed inputs or static preference datasets, cannot adaptively explore repository-level dependencies, and are constrained by function-level benchmarks that overlook critical vulnerability context.
  This paper introduces Vulnerability-Adaptive Policy Optimization (VULPO), an on-policy LLM reinforcement learning framework for context-aware VD. To support training and evaluation, we first construct ContextVul, a new dataset that augments high-quality function-level samples with lightweight method to extract repository-level context information. We then design multi-dimensional reward structuring that jointly captures prediction correctness, vulnerability localization accuracy, and the semantic relevance of vulnerability analysis, thereby guiding the model toward comprehensive contextual reasoning. To address the asymmetric difficulty of different vulnerability cases and mitigate reward hacking, VULPO incorporates label-level and sample-level difficulty-adaptive reward scaling, encouraging the model to explore challenging cases while maintaining balanced reward distribution. Extensive experiments demonstrate the superiority of our VULPO framework in context-aware VD: Our VULPO-4B substantially outperforms existing VD baselines based on prompt engineering and off-policy optimization, improving F1 by 85% over Qwen3-4B and achieving performance comparable to a 150x larger-scale model, DeepSeek-R1-0528.

</details>


### [58] [CITADEL: A Semi-Supervised Active Learning Framework for Malware Detection Under Continuous Distribution Drift](https://arxiv.org/abs/2511.11979)
*Md Ahsanul Haque,Md Mahmuduzzaman Kamol,Ismail Hossain,Suresh Kumar Amalapuram,Vladik Kreinovich,Mohammad Saidur Rahman*

Main category: cs.CR

TL;DR: CITADEL是一个用于Android恶意软件检测的鲁棒半监督主动学习框架，通过恶意软件特定增强、监督对比损失和多标准主动学习策略，在有限标注预算下有效应对概念漂移问题。


<details>
  <summary>Details</summary>
Motivation: Android恶意软件快速演变导致概念漂移，传统机器学习检测系统性能下降，而现有方法计算成本高且无法扩展到现实世界大规模数据集，特别是面对每天涌现的45万新样本时专家标注无法满足需求。

Method: 提出恶意软件特定增强（伯努利位翻转和掩码）模拟真实漂移行为，集成监督对比损失改善边界样本区分，结合基于预测置信度、Lp范数距离和边界不确定性的多标准主动学习策略。

Result: 在四个大规模Android恶意软件基准测试中，仅使用40%标注样本就优于先前工作，F1分数分别提升超过1%、3%、7%和14%，训练速度提升24倍，操作减少13倍。

Conclusion: CITADEL框架在有限标注预算下有效解决了Android恶意软件检测中的概念漂移问题，显著提升了检测性能和效率。

Abstract: Android malware evolves rapidly, leading to concept drift that degrades the performance of traditional machine learning (ML)-based detection systems. While recent approaches incorporate active learning and hierarchical contrastive loss to handle this drift, they remain fully supervised, computationally expensive, and perform poorly on real-world datasets with long temporal spans. In particular, our evaluation highlights these limitations, particularly on LAMDA, a 12-year longitudinal dataset exhibiting substantial distributional shifts. Moreover, manual expert labeling cannot scale with the daily emergence of over 450,000 new malware samples, leaving most samples unlabeled and underutilized.
  To address these challenges, we propose CITADEL, a robust semi-supervised active learning framework for Android malware detection. To bridge the gap between image-domain semi-supervised learning and binary feature representations of malware, we introduce malware-specific augmentations, Bernoulli bit flips and masking, that simulate realistic drift behaviors. CITADEL further integrates supervised contrastive loss to improve boundary sample discrimination and combines it with a multi-criteria active learning strategy based on prediction confidence, $L_p$-norm distance, and boundary uncertainty, enabling effective adaptation under limited labeling budgets. Extensive evaluation on four large-scale Android malware benchmarks -- APIGraph, Chen-AZ, MaMaDroid, and LAMDA demonstrates that CITADEL outperforms prior work, achieving F1 score of over 1%, 3%, 7%, and 14% respectively, using only 40% labeled samples. Furthermore, CITADEL shows significant efficiency over prior work incurring $24\times$ faster training and $13\times$ fewer operations.

</details>


### [59] [BudgetLeak: Membership Inference Attacks on RAG Systems via the Generation Budget Side Channel](https://arxiv.org/abs/2511.12043)
*Hao Li,Jiajun He,Guangshuo Wang,Dengguo Feng,Zheng Li,Min Zhang*

Main category: cs.CR

TL;DR: 本文提出了BudgetLeak，一种针对RAG系统的成员推理攻击方法，通过分析生成预算变化下的响应行为模式来推断数据成员关系，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: RAG系统依赖外部知识库但面临数据风险，现有成员推理攻击在RAG中表现不佳，因为黑盒约束和缺乏强成员信号。

Method: 利用RAG系统中生成预算这一侧信道，通过在不同预算下探测响应并分析指标演化模式，使用序列建模或聚类进行成员推断。

Result: 在四个数据集、三个LLM生成器和两个检索器上的广泛实验表明，BudgetLeak始终优于现有基线方法，同时保持高效率和实际可行性。

Conclusion: 研究揭示了RAG系统中先前被忽视的数据风险，强调了开发新防御措施的必要性。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by integrating external knowledge, but reliance on proprietary or sensitive corpora poses various data risks, including privacy leakage and unauthorized data usage. Membership inference attacks (MIAs) are a common technique to assess such risks, yet existing approaches underperform in RAG due to black-box constraints and the absence of strong membership signals. In this paper, we identify a previously unexplored side channel in RAG systems: the generation budget, which controls the maximum number of tokens allowed in a generated response. Varying this budget reveals observable behavioral patterns between member and non-member queries, as members gain quality more rapidly with larger budgets. Building on this insight, we propose BudgetLeak, a novel membership inference attack that probes responses under different budgets and analyzes metric evolution via sequence modeling or clustering. Extensive experiments across four datasets, three LLM generators, and two retrievers demonstrate that BudgetLeak consistently outperforms existing baselines, while maintaining high efficiency and practical viability. Our findings reveal a previously overlooked data risk in RAG systems and highlight the need for new defenses.

</details>


### [60] [Exploring AI in Steganography and Steganalysis: Trends, Clusters, and Sustainable Development Potential](https://arxiv.org/abs/2511.12052)
*Aditya Kumar Sahu,Chandan Kumar,Saksham Kumar,Serdar Solak*

Main category: cs.CR

TL;DR: 本文对2017-2023年间654篇AI驱动的隐写术数据隐藏技术文献进行科学计量分析，发现69%论文来自亚洲国家，识别出7个主题集群，并评估了与可持续发展目标（SDGs）的关联性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过科学计量方法分析AI驱动隐写术领域的发展趋势、地理分布和主题演化，并评估该领域与可持续发展目标的关联程度。

Method: 采用主题建模方法对654篇文献进行科学计量分析，识别主题集群，并评估与SDGs的对应关系。

Result: 亚洲国家主导该领域研究（69%），中国发表最多（312篇），识别出7个主要主题集群，仅有18篇文献与SDGs相关，SDG9（工业、创新和基础设施）最为突出。

Conclusion: AI驱动隐写术研究呈现区域集中特征，与可持续发展目标的关联性较弱，需要加强该领域的社会影响评估和全球挑战应对能力。

Abstract: Steganography and steganalysis are strongly related subjects of information security. Over the past decade, many powerful and efficient artificial intelligence (AI) - driven techniques have been designed and presented during research into steganography as well as steganalysis. This study presents a scientometric analysis of AI-driven steganography-based data hiding techniques using a thematic modelling approach. A total of 654 articles within the time span of 2017 to 2023 have been considered. Experimental evaluation of the study reveals that 69% of published articles are from Asian countries. The China is on top (TP:312), followed by India (TP-114). The study mainly identifies seven thematic clusters: steganographic image data hiding, deep image steganalysis, neural watermark robustness, linguistic steganography models, speech steganalysis algorithms, covert communication networks, and video steganography techniques. The proposed study also assesses the scope of AI-steganography under the purview of sustainable development goals (SDGs) to present the interdisciplinary reciprocity between them. It has been observed that only 18 of the 654 articles are aligned with one of the SDGs, which shows that limited studies conducted in alignment with SDG goals. SDG9 which is Industry, Innovation, and Infrastructure is leading among 18 SDGs mapped articles. To the top of our insight, this study is the unique one to present a scientometric study on AI-driven steganography-based data hiding techniques. In the context of descriptive statistics, the study breaks down the underlying causes of observed trends, including the influence of DL developments, trends in East Asia and maturity of foundational methods. The work also stresses upon the critical gaps in societal alignment, particularly the SDGs, ultimately working on unveiling the field's global impact on AI security challenges.

</details>


### [61] [Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness](https://arxiv.org/abs/2511.12085)
*Sajad U P*

Main category: cs.CR

TL;DR: 本文提出了一种混合方法，使用DistilBERT进行钓鱼邮件分类，结合FGM对抗训练增强鲁棒性，并集成LIME XAI技术提高透明度，同时使用Flan-T5-small生成易于理解的解释。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击日益多样化且技术先进，特别是AI生成的钓鱼攻击降低了检测系统的整体韧性，需要开发更鲁棒且可解释的检测方法。

Method: 使用DistilBERT进行邮件分类，采用FGM对抗训练增强模型对文本扰动的鲁棒性，集成LIME XAI技术提高模型透明度，并使用Flan-T5-small生成自然语言解释。

Result: 该方法实现了精确的钓鱼邮件分类，同时提供了模型决策的易于理解的解释。

Conclusion: 该混合方法在保持高分类精度的同时，通过对抗训练增强了鲁棒性，并通过XAI技术提高了透明度和可解释性，为钓鱼检测提供了有效的解决方案。

Abstract: Phishing and related cyber threats are becoming more varied and technologically advanced. Among these, email-based phishing remains the most dominant and persistent threat. These attacks exploit human vulnerabilities to disseminate malware or gain unauthorized access to sensitive information. Deep learning (DL) models, particularly transformer-based models, have significantly enhanced phishing mitigation through their contextual understanding of language. However, some recent threats, specifically Artificial Intelligence (AI)-generated phishing attacks, are reducing the overall system resilience of phishing detectors. In response, adversarial training has shown promise against AI-generated phishing threats. This study presents a hybrid approach that uses DistilBERT, a smaller, faster, and lighter version of the BERT transformer model for email classification. Robustness against text-based adversarial perturbations is reinforced using Fast Gradient Method (FGM) adversarial training. Furthermore, the framework integrates the LIME Explainable AI (XAI) technique to enhance the transparency of the DistilBERT architecture. The framework also uses the Flan-T5-small language model from Hugging Face to generate plain-language security narrative explanations for end-users. This combined approach ensures precise phishing classification while providing easily understandable justifications for the model's decisions.

</details>


### [62] [AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.12149)
*Jiayu Li,Yunhan Zhao,Xiang Zheng,Zonghuan Xu,Yige Li,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CR

TL;DR: 提出了AttackVLA统一框架来评估Vision-Language-Action模型的安全漏洞，并开发了BackdoorVLA后门攻击，能够在触发条件下强制VLA执行攻击者指定的长时程动作序列。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏统一的评估框架，现有针对VLA模型的攻击方法效果不明确，且大多数未在真实场景中验证。不同VLA架构的动作标记器差异也阻碍了可重复性和公平比较。

Method: 提出AttackVLA统一框架，涵盖数据构建、模型训练和推理阶段。实现了包括所有现有VLA攻击和多种从视觉语言模型改编的攻击方法，并在仿真和真实环境中进行评估。

Result: 在仿真基准和真实机器人环境中评估BackdoorVLA，平均目标成功率达到58.4%，在选定任务中达到100%。分析发现现有攻击主要导致无目标失败或静态动作状态。

Conclusion: 该工作提供了评估VLA漏洞的标准化框架，展示了精确对抗操纵的潜力，激励了进一步研究保护基于VLA的具身系统安全。

Abstract: Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.

</details>


### [63] [Multi-Agent Collaborative Fuzzing with Continuous Reflection for Smart Contracts Vulnerability Detection](https://arxiv.org/abs/2511.12164)
*Jie Chen,Liangmin Wang*

Main category: cs.CR

TL;DR: SmartFuzz是一个基于大语言模型驱动的协作式反射模糊测试工具，用于检测智能合约漏洞。它通过持续反射过程和反应式协作链机制，有效生成能触发复杂漏洞的交易序列。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试工具在检测复杂漏洞方面存在不足：过度关注代码覆盖率而非漏洞发现，浪费大量资源在非漏洞代码区域；缺乏对状态化合约的语义理解，生成大量无法通过运行时执行的无效交易序列。

Method: 提出持续反射过程(CRP)将交易序列生成重构为通过环境反馈持续进化的过程；设计反应式协作链(RCC)基于交易序列依赖关系协调模糊测试过程；构建多智能体协作团队，各专家智能体在RCC指导下从全局和局部视角共同生成和优化交易序列。

Result: 在真实世界合约和DApp项目上的实验表明，SmartFuzz在30分钟内检测到的漏洞比现有最先进工具多5.8%-74.7%，并将误报率降低高达80%。

Conclusion: SmartFuzz通过大语言模型驱动的协作反射方法，显著提升了智能合约复杂漏洞的检测能力，在漏洞发现效率和准确性方面均优于现有工具。

Abstract: Fuzzing is a widely used technique for detecting vulnerabilities in smart contracts, which generates transaction sequences to explore the execution paths of smart contracts. However, existing fuzzers are falling short in detecting sophisticated vulnerabilities that require specific attack transaction sequences with proper inputs to trigger, as they (i) prioritize code coverage over vulnerability discovery, wasting considerable effort on non-vulnerable code regions, and (ii) lack semantic understanding of stateful contracts, generating numerous invalid transaction sequences that cannot pass runtime execution.
  In this paper, we propose SmartFuzz, a novel collaborative reflective fuzzer for smart contract vulnerability detection. It employs large language model-driven agents as the fuzzing engine and continuously improves itself by learning and reflecting through interactions with the environment. Specifically, we first propose a new Continuous Reflection Process (CRP) for fuzzing smart contracts, which reforms the transaction sequence generation as a self-evolving process through continuous reflection on feedback from the runtime environment. Then, we present the Reactive Collaborative Chain (RCC) to orchestrate the fuzzing process into multiple sub-tasks based on the dependencies of transaction sequences. Furthermore, we design a multi-agent collaborative team, where each expert agent is guided by the RCC to jointly generate and refine transaction sequences from both global and local perspectives. We conduct extensive experiments to evaluate SmartFuzz's performance on real-world contracts and DApp projects. The results demonstrate that SmartFuzz outperforms existing state-of-the-art tools: (i) it detects 5.8\%-74.7\% more vulnerabilities within 30 minutes, and (ii) it reduces false negatives by up to 80\%.

</details>


### [64] [eFPE: Design, Implementation, and Evaluation of a Lightweight Format-Preserving Encryption Algorithm for Embedded Systems](https://arxiv.org/abs/2511.12225)
*Nishant Vasantkumar Hegde,Suneesh Bare,K B Ramesh,Aamir Ibrahim*

Main category: cs.CR

TL;DR: 本文提出eFPE（增强型格式保持加密），一种8轮Feistel密码，采用新型轻量级伪随机函数，专门为资源受限的嵌入式系统设计，能够在保持数据格式的同时提供安全加密。


<details>
  <summary>Details</summary>
Motivation: 资源受限的嵌入式系统需要安全且轻量级的数据保护，特别是在必须保持数据格式的情况下。现有方案往往需要填充或复杂转换，不适合这类场景。

Method: 使用8轮Feistel结构，配备新型轻量级PRF，采用AES启发的高效两迭代操作（字节替换、密钥异或和字节旋转），可直接加密偶数位十进制字符串而无需填充。

Result: 在ARM7TDMI LPC2148微控制器上实现，总固件ROM占用4.73 kB，RAM使用1.34 kB，核心算法模块仅需3.55 kB ROM和116 B RAM。

Conclusion: eFPE是金融终端、医疗传感器和工业物联网设备等应用的理想解决方案，能够满足数据格式完整性、最小资源占用和低操作延迟的要求。

Abstract: Resource-constrained embedded systems demand secure yet lightweight data protection, particularly when data formats must be preserved. This paper introduces eFPE (Enhanced Format-Preserving Encryption), an 8-round Feistel cipher featuring a "novel lightweight Pseudorandom Function (PRF)" specifically designed for this domain. The PRF, architected with an efficient two-iteration structure of AES-inspired operations (byte-substitution, keyed XOR, and byte-rotation), underpins eFPE's ability to directly encrypt even-length decimal strings without padding or complex conversions, while aiming for IND-CCA2 security under standard assumptions. Implemented and evaluated on an ARM7TDMI LPC2148 microcontroller using Keil μVision 4, eFPE demonstrates the efficacy of its targeted design: a total firmware Read-Only Memory (ROM) footprint of 4.73 kB and Random Access Memory (RAM) usage of 1.34 kB. The core eFPE algorithm module itself is notably compact, requiring only 3.55 kB ROM and 116 B RAM. These characteristics make eFPE a distinct and highly suitable solution for applications like financial terminals, medical sensors, and industrial IoT devices where data format integrity, minimal resource footprint, and low operational latency are paramount.

</details>


### [65] [Software Supply Chain Security of Web3](https://arxiv.org/abs/2511.12274)
*Martin Monperrus*

Main category: cs.CR

TL;DR: 本文分析了Web3生态系统中的软件供应链安全挑战，探讨了传统Web2软件供应链问题与区块链技术不可变性和高风险特性相结合带来的独特安全威胁，并提出了相应的缓解策略。


<details>
  <summary>Details</summary>
Motivation: Web3应用通过去中心化应用和智能合约管理数十亿美元的数字资产，这些系统依赖复杂的软件供应链，引入了严重的安全漏洞。需要研究Web3生态系统特有的软件供应链安全挑战。

Method: 分析Web3生态系统中的威胁态势，研究传统Web2软件供应链问题与区块链技术特性的交叉影响，提出相应的安全缓解策略。

Result: 识别了Web3软件供应链特有的安全挑战，包括传统软件供应链漏洞与区块链不可变性、高价值资产管理的结合带来的风险放大效应。

Conclusion: 需要采取专门的安全措施来加强Web3系统的安全态势，应对软件供应链中的独特安全威胁，保护数十亿美元的数字资产安全。

Abstract: Web3 applications, built on blockchain technology, manage billions of dollars in digital assets through decentralized applications (dApps) and smart contracts. These systems rely on complex, software supply chains that introduce significant security vulnerabilities. This paper examines the software supply chain security challenges unique to the Web3 ecosystem, where traditional Web2 software supply chain problems intersect with the immutable and high-stakes nature of blockchain technology. We analyze the threat landscape and propose mitigation strategies to strengthen the security posture of Web3 systems.

</details>


### [66] [Privacy-Preserving Prompt Injection Detection for LLMs Using Federated Learning and Embedding-Based NLP Classification](https://arxiv.org/abs/2511.12295)
*Hasini Jayathilaka*

Main category: cs.CR

TL;DR: 提出基于联邦学习和嵌入分类的隐私保护提示注入检测框架，在保护数据隐私的同时实现与集中式训练相当的检测性能。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击是LLM的新兴威胁，现有检测方法需要集中化提示数据，存在严重隐私风险。

Method: 使用句子嵌入对良性提示和对抗性提示数据集进行编码，训练集中式和联邦逻辑回归模型，联邦方法仅共享模型参数。

Result: 联邦学习方法在保护隐私的同时，检测性能与集中式训练相当，证明了无需暴露原始数据的有效检测可行性。

Conclusion: 虽然数据集规模有限，但研究为构建安全且隐私感知的LLM系统建立了强有力的概念验证，并指明了新方向。

Abstract: Prompt injection attacks are an emerging threat to large language models (LLMs), enabling malicious users to manipulate outputs through carefully designed inputs. Existing detection approaches often require centralizing prompt data, creating significant privacy risks. This paper proposes a privacy-preserving prompt injection detection framework based on federated learning and embedding-based classification. A curated dataset of benign and adversarial prompts was encoded with sentence embedding and used to train both centralized and federated logistic regression models. The federated approach preserved privacy by sharing only model parameters across clients, while achieving detection performance comparable to centralized training. Results demonstrate that effective prompt injection detection is feasible without exposing raw data, making this one of the first explorations of federated security for LLMs. Although the dataset is limited in scale, the findings establish a strong proof-of-concept and highlight new directions for building secure and privacy-aware LLM systems.

</details>


### [67] [On the Security and Privacy of AI-based Mobile Health Chatbots](https://arxiv.org/abs/2511.12377)
*Samuel Wairimu,Leonardo Horn Iwaya*

Main category: cs.CR

TL;DR: 该研究对Google Play商店中的16个AI移动健康聊天机器人进行了安全性和隐私性评估，发现存在安全漏洞、隐私问题以及违反Google Play政策的行为，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI在移动健康应用中的普及，AI聊天机器人作为健康伴侣被广泛使用，但它们引发了严重的安全和隐私担忧，需要对这些应用进行实证评估。

Method: 采用三阶段方法：手动检查、静态代码分析和动态分析，评估技术稳健性以及设计和实现选择对最终用户的影响。

Result: 研究发现存在安全漏洞（如启用远程WebView调试）、隐私问题以及不符合Google Play政策（如未提供公开可访问的隐私政策）。

Conclusion: 提出了改进移动健康聊天机器人安全性和隐私性的建议，旨在支持开发者设计更透明、隐私友好且安全的移动健康聊天机器人。

Abstract: The rise of Artificial Intelligence (AI) has impacted the development of mobile health (mHealth) apps, most notably with the advent of AI-based chatbots used as ubiquitous ``companions'' for various services, from fitness to mental health assistants. While these mHealth chatbots offer clear benefits, such as personalized health information and predictive diagnoses, they also raise significant concerns regarding security and privacy. This study empirically assesses 16 AI-based mHealth chatbots identified from the Google Play Store. The empirical assessment follows a three-phase approach (manual inspection, static code analysis, and dynamic analysis) to evaluate technical robustness and how design and implementation choices impact end users. Our findings revealed security vulnerabilities (e.g., enabling Remote WebView debugging), privacy issues, and non-compliance with Google Play policies (e.g., failure to provide publicly accessible privacy policies). Based on our findings, we offer several recommendations to enhance the security and privacy of mHealth chatbots. These recommendations focus on improving data handling processes, disclosure, and user security. Therefore, this work also seeks to support mHealth developers and security/privacy engineers in designing more transparent, privacy-friendly, and secure mHealth chatbots.

</details>


### [68] [GenSIaC: Toward Security-Aware Infrastructure-as-Code Generation with Large Language Models](https://arxiv.org/abs/2511.12385)
*Yikun Li,Matteo Grella,Daniel Nahmias,Gal Engelberg,Dan Klein,Giancarlo Guizzardi,Thijs van Ede,Andrea Continella*

Main category: cs.CR

TL;DR: 本文研究了大型语言模型在生成安全基础设施即代码方面的潜力，提出了GenSIaC指令微调数据集来提升LLMs识别和预防IaC安全配置错误的能力。


<details>
  <summary>Details</summary>
Motivation: 随着云基础设施复杂度的增加，IaC脚本中的错误配置和安全漏洞风险日益突出，需要探索LLMs在生成安全IaC代码方面的能力。

Method: 首先评估基础LLMs识别IaC安全弱点的能力，然后创建GenSIaC指令微调数据集，通过微调LLMs来生成安全感知的IaC代码。

Result: 经过微调的模型在识别和预防IaC安全配置错误方面表现显著提升，F1分数从0.303提高到0.858。

Conclusion: GenSIaC方法有效提升了LLMs生成安全IaC代码的能力，具有良好的泛化性和跨语言能力。

Abstract: In recent years, Infrastructure as Code (IaC) has emerged as a critical approach for managing and provisioning IT infrastructure through code and automation. IaC enables organizations to create scalable and consistent environments, effectively managing servers and development settings. However, the growing complexity of cloud infrastructures has led to an increased risk of misconfigurations and security vulnerabilities in IaC scripts. To address this problem, this paper investigates the potential of Large Language Models (LLMs) in generating security-aware IaC code, avoiding misconfigurations introduced by developers and administrators.
  While LLMs have made significant progress in natural language processing and code generation, their ability to generate secure IaC scripts remains unclear. This paper addresses two major problems: 1) the lack of understanding of security weaknesses in IaC scripts generated by LLMs, and 2) the absence of techniques for enhancing security in generating IaC code with LLMs.
  To assess the extent to which LLMs contain security knowledge, we first conduct a comprehensive evaluation of base LLMs in recognizing major IaC security weaknesses during the generation and inspection of IaC code. Then, we propose GenSIaC, an instruction fine-tuning dataset designed to improve LLMs' ability to recognize potential security weaknesses. Leveraging GenSIaC, we fine-tune LLMs and instruct models to generate security-aware IaC code. Our evaluation demonstrates that our models achieve substantially improved performance in recognizing and preventing IaC security misconfigurations, e.g., boosting the F1-score from 0.303 to 0.858. Additionally, we perform ablation studies and explore GenSIaC's generalizability to other LLMs and its cross-language capabilities.

</details>


### [69] [SeedAIchemy: LLM-Driven Seed Corpus Generation for Fuzzing](https://arxiv.org/abs/2511.12448)
*Aidan Wen,Norah A. Alzahrani,Jingzhi Jiang,Andrew Joe,Karen Shieh,Andy Zhang,Basel Alomair,David Wagner*

Main category: cs.CR

TL;DR: SeedAIchemy是一个自动化LLM驱动的语料库生成工具，通过五个模块从互联网收集公开文件，其中四个模块使用LLM工作流构建搜索词以最大化语料库质量。


<details>
  <summary>Details</summary>
Motivation: 使开发人员更容易有效实施模糊测试，解决手动构建高质量语料库的困难。

Method: 使用五个模块从互联网收集公开文件，其中四个模块采用LLM工作流来构建优化的搜索词。

Result: SeedAIchemy生成的语料库在多样化的目标程序和库上表现显著优于简单语料库，与手动整理的语料库性能相当。

Conclusion: SeedAIchemy能够自动化生成高质量的模糊测试语料库，性能接近手动整理的水平。

Abstract: We introduce SeedAIchemy, an automated LLM-driven corpus generation tool that makes it easier for developers to implement fuzzing effectively. SeedAIchemy consists of five modules which implement different approaches at collecting publicly available files from the internet. Four of the five modules use large language model (LLM) workflows to construct search terms designed to maximize corpus quality. Corpora generated by SeedAIchemy perform significantly better than a naive corpus and similarly to a manually-curated corpus on a diverse range of target programs and libraries.

</details>


### [70] [A Content-Preserving Secure Linguistic Steganography](https://arxiv.org/abs/2511.12565)
*Lingyun Xiang,Chengfu Ou,Xu He,Zhongliang Yang,Yuling Liu*

Main category: cs.CR

TL;DR: 提出了一种内容保持的语言隐写方法CLstega，通过可控分布变换在不修改原始文本的情况下嵌入秘密信息，实现完美安全隐蔽通信。


<details>
  <summary>Details</summary>
Motivation: 现有语言隐写方法通过内容变换隐藏秘密信息，但会造成正常文本和隐写文本之间的微妙差异，存在安全风险。需要一种不修改原始文本内容的方法来确保完美安全。

Method: CLstega采用增强掩码策略定位嵌入位置，设计动态分布隐写编码策略，通过从原始概率分布推导目标分布来编码秘密信息。通过构建掩码句子数据集微调原始MLM，生成能够直接从原始文本提取秘密信息的目标MLM。

Result: 实验结果显示CLstega能够实现100%的提取成功率，在安全性方面优于现有方法，有效平衡了嵌入容量和安全性。

Conclusion: CLstega通过内容保持的语言隐写范式实现了完美安全的隐蔽通信，完全保留了原始文本的完整性，同时确保了秘密信息的安全。

Abstract: Existing linguistic steganography methods primarily rely on content transformations to conceal secret messages. However, they often cause subtle yet looking-innocent deviations between normal and stego texts, posing potential security risks in real-world applications. To address this challenge, we propose a content-preserving linguistic steganography paradigm for perfectly secure covert communication without modifying the cover text. Based on this paradigm, we introduce CLstega (\textit{C}ontent-preserving \textit{L}inguistic \textit{stega}nography), a novel method that embeds secret messages through controllable distribution transformation. CLstega first applies an augmented masking strategy to locate and mask embedding positions, where MLM(masked language model)-predicted probability distributions are easily adjustable for transformation. Subsequently, a dynamic distribution steganographic coding strategy is designed to encode secret messages by deriving target distributions from the original probability distributions. To achieve this transformation, CLstega elaborately selects target words for embedding positions as labels to construct a masked sentence dataset, which is used to fine-tune the original MLM, producing a target MLM capable of directly extracting secret messages from the cover text. This approach ensures perfect security of secret messages while fully preserving the integrity of the original cover text. Experimental results show that CLstega can achieve a 100\% extraction success rate, and outperforms existing methods in security, effectively balancing embedding capacity and security.

</details>


### [71] [Prrr: Personal Random Rewards for Blockchain Reporting](https://arxiv.org/abs/2511.12626)
*Hongyin Chen,Yubin Ke,Xiaotie Deng,Ittay Eyal*

Main category: cs.CR

TL;DR: Prrr协议通过随机异质奖励设计解决了区块链报告系统的安全与性能权衡问题，采用事前合成不对称机制实现激励兼容。


<details>
  <summary>Details</summary>
Motivation: 现有报告协议面临安全与性能的权衡：依赖少数可信发布者带来中心化风险，而开放发布则导致区块链上报告数量过多。标准对称奖励设计是这一问题的根源。

Method: 提出Prrr协议，采用随机异质奖励分配机制（事前合成不对称），使用第二价格式结算来分配奖励，确保激励兼容性。

Result: Prrr协议实现了安全性和效率的双重目标，遵循协议构成子博弈完美纳什均衡，对合谋和女巫攻击具有鲁棒性。

Conclusion: Prrr是首个在博弈论机制中刻意形成参与者不对称性的协议，适用于众多依赖及时报告的智能合约场景。

Abstract: Smart contracts, the stateful programs running on blockchains, often rely on reports. Publishers are paid to publish these reports on the blockchain. Designing protocols that incentivize timely reporting is the prevalent reporting problem. But existing solutions face a security-performance trade-off: Relying on a small set of trusted publishers introduces centralization risks, while allowing open publication results in an excessive number of reports on the blockchain. We identify the root cause of this trade-off to be the standard symmetric reward design, which treats all reports equally. We prove that no symmetric-reward mechanism can overcome the trade-off.
  We present Personal Random Rewards for Reporting (Prrr), a protocol that assigns random heterogeneous values to reports. We call this novel mechanism-design concept Ex-Ante Synthetic Asymmetry. To the best of our knowledge, Prrr is the first game-theoretic mechanism (in any context) that deliberately forms participant asymmetry. Prrr employs a second-price-style settlement to allocate rewards, ensuring incentive compatibility and achieving both security and efficiency. Following the protocol constitutes a Subgame-Perfect Nash Equilibrium, robust against collusion and Sybil attacks. Prrr is applicable to numerous smart contracts that rely on timely reports.

</details>


### [72] [Adaptive Dual-Layer Web Application Firewall (ADL-WAF) Leveraging Machine Learning for Enhanced Anomaly and Threat Detection](https://arxiv.org/abs/2511.12643)
*Ahmed Sameh,Sahar Selim*

Main category: cs.CR

TL;DR: 本文提出了一种自适应双层WAF，采用两层机器学习模型来提高异常和威胁检测的准确性。第一层使用决策树检测异常，第二层使用支持向量机分类威胁异常和良性异常。在五个大型基准数据集上的评估显示，检测准确率达到99.88%，精确度为100%。


<details>
  <summary>Details</summary>
Motivation: 传统Web应用防火墙在区分恶意和合法流量方面效果有限，导致威胁检测效能不足。为了克服这些限制，需要开发更准确的异常检测方法。

Method: 采用自适应双层机器学习模型：第一层使用决策树算法检测流量异常，识别与正常模式的偏差；第二层使用支持向量机对检测到的异常进行分类，区分威胁异常和良性异常。包含全面的数据预处理和特征工程技术。

Result: 在五个大型基准数据集上的评估结果显示，ADL WAF实现了99.88%的检测准确率和100%的精确度，显著提高了异常检测能力并减少了误报。

Conclusion: 将机器学习技术集成到WAF中可以显著改善Web应用安全，提供更准确和高效的威胁检测。

Abstract: Web Application Firewalls are crucial for protecting web applications against a wide range of cyber threats. Traditional Web Application Firewalls often struggle to effectively distinguish between malicious and legitimate traffic, leading to limited efficacy in threat detection. To overcome these limitations, this paper proposes an Adaptive Dual-Layer WAF employing a two-layered Machine Learning model designed to enhance the accuracy of anomaly and threat detection. The first layer employs a Decision Tree (DT) algorithm to detect anomalies by identifying traffic deviations from established normal patterns. The second layer employs Support Vector Machine to classify these anomalies as either threat anomalies or benign anomalies. Our Adaptive Dual Layer WAF incorporates comprehensive data pre-processing and feature engineering techniques and has been thoroughly evaluated using five large benchmark datasets. Evaluation using these datasets shows that ADL WAF achieves a detection accuracy of 99.88% and a precision of 100%, significantly enhancing anomaly detection and reducing false positives. These findings suggest that integrating machine learning techniques into WAFs can substantially improve web application security by providing more accurate and efficient threat detection.

</details>


### [73] [AI Bill of Materials and Beyond: Systematizing Security Assurance through the AI Risk Scanning (AIRS) Framework](https://arxiv.org/abs/2511.12668)
*Samuel Nathanson,Alexander Lee,Catherine Chen Kieffer,Jared Junkin,Jessica Ye,Amir Saeed,Melanie Lockhart,Russ Fink,Elisha Peterson,Lanier Watkins*

Main category: cs.CR

TL;DR: 本文提出了AI风险扫描（AIRS）框架，这是一个基于威胁模型、生成证据的框架，旨在实现AI保障的可操作化。该框架将AI文档从描述性披露转向可测量的、基于证据的验证，并与MITRE ATLAS对抗性机器学习分类法对齐。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统的保障机制在软件供应链安全、对抗性机器学习和治理文档方面仍然分散。现有的透明度机制（如模型卡、数据表和软件物料清单）虽然推进了来源报告，但很少提供可验证的、机器可读的模型安全证据。

Method: AIRS框架通过三个渐进式试点研究（Smurf、OPAL和Pilot C）发展而来，将AI文档从描述性披露重新定义为可测量的、基于证据的验证。该框架将其保障字段与MITRE ATLAS对抗性ML分类法对齐，并自动生成结构化工件，捕获模型完整性、打包和序列化安全性、结构适配器和运行时行为。

Result: 在量化GPT-OSS-20B模型上的概念验证展示了安全加载器策略的执行、每分片哈希验证以及在受控运行时条件下执行的污染和后门探测。与SPDX 3.0和CycloneDX 1.6的SBOM标准比较分析显示在身份和评估元数据方面存在对齐，但识别了在表示AI特定保障字段方面的关键差距。

Conclusion: AIRS框架通过将威胁建模与自动化、可审计的证据生成相结合，将SBOM实践扩展到AI领域，为标准化、可信赖和机器可验证的AI风险文档提供了原则性基础。

Abstract: Assurance for artificial intelligence (AI) systems remains fragmented across software supply-chain security, adversarial machine learning, and governance documentation. Existing transparency mechanisms - including Model Cards, Datasheets, and Software Bills of Materials (SBOMs) - advance provenance reporting but rarely provide verifiable, machine-readable evidence of model security. This paper introduces the AI Risk Scanning (AIRS) Framework, a threat-model-based, evidence-generating framework designed to operationalize AI assurance. The AIRS Framework evolved through three progressive pilot studies - Smurf (AIBOM schema design), OPAL (operational validation), and Pilot C (AIRS) - that reframed AI documentation from descriptive disclosure toward measurable, evidence-bound verification. The framework aligns its assurance fields to the MITRE ATLAS adversarial ML taxonomy and automatically produces structured artifacts capturing model integrity, packaging and serialization safety, structural adapters, and runtime behaviors. Currently, the AIRS Framework is scoped to provide model-level assurances for LLMs, but it could be expanded to include other modalities and cover system-level threats (e.g. application-layer abuses, tool-calling). A proof-of-concept on a quantized GPT-OSS-20B model demonstrates enforcement of safe loader policies, per-shard hash verification, and contamination and backdoor probes executed under controlled runtime conditions. Comparative analysis with SBOM standards of SPDX 3.0 and CycloneDX 1.6 reveals alignment on identity and evaluation metadata, but identifies critical gaps in representing AI-specific assurance fields. The AIRS Framework thus extends SBOM practice to the AI domain by coupling threat modeling with automated, auditable evidence generation, providing a principled foundation for standardized, trustworthy, and machine-verifiable AI risk documentation.

</details>


### [74] [Offensive tool determination strategy R.I.D.D.L.E. + (C)](https://arxiv.org/abs/2511.12704)
*Herman Errico*

Main category: cs.CR

TL;DR: 本文提出了一种风险评估方法，通过引入攻击工具特征作为额外分析参数，使用R.I.D.D.L.E.+C变量（抗性、入侵时机、损害、中断时机、潜伏期、效率和成本）来改进对故意威胁的理解。


<details>
  <summary>Details</summary>
Motivation: 故意威胁是关键基础设施资产漏洞的主要风险因素，需要准确的风险评估来分析威胁、评估漏洞并评估对资产和系统的潜在影响。

Method: 基于"攻击工具确定策略"的方法，使用R.I.D.D.L.E.+C变量（抗性、入侵时机、损害、中断时机、潜伏期、效率和成本），通过开源情报进行评估，并为每个变量分配特定范围的值。

Result: 提供了一个实用的应用矩阵，可以揭示意外漏洞，并为决策和安全规划提供更细粒度的框架。

Conclusion: 该方法可以作为风险评估过程的附加阶段，通过分析攻击工具特征来增强对故意威胁的理解和评估能力。

Abstract: Intentional threats are a major risk factor related to vulnerabilities in critical infrastructure assets, and an accurate risk assessment is necessary to analyze threats, assess vulnerabilities, and evaluate potential impacts on assets and systems. This research proposes a methodology that can be added as an additional phase in the risk assessment process. The method introduces an extra analytical parameter concerning offensive tool characteristics, improving the understanding of intentional threats.
  The methodology is presented using clear and accessible language suitable for a broad audience. It is based on an approach described as an "offensive tool determination strategy," summarized by the acronym R.I.D.D.L.E.+C, which refers to the variables used in the analysis: resistance, intrusion timing, damage, disruption timing, latency, efficiency, and cost. These variables are evaluated using open-source intelligence.
  Each variable is assigned a specific range of values according to its potential impact on the targeted asset. A matrix is then provided for practical application, which can reveal unexpected vulnerabilities and offer a more granular framework for decision-making and security planning.

</details>


### [75] [ProxyPrints: From Database Breach to Spoof, A Plug-and-Play Defense for Biometric Systems](https://arxiv.org/abs/2511.12739)
*Yaniv Hacmon,Keren Gorelik,Gilad Gressel,Yisroel Mirsky*

Main category: cs.CR

TL;DR: ProxyPrints是一种可取消生物识别技术，作为中间件层在不修改现有指纹识别系统的情况下保护指纹数据安全，提供可撤销性和防欺骗检测功能。


<details>
  <summary>Details</summary>
Motivation: 指纹识别系统广泛部署但存在安全漏洞，存储的指纹模板可能被逆向工程重建为真实指纹图像，导致无法修复的身份泄露风险。

Method: ProxyPrints作为透明中间件层部署在指纹扫描器和匹配算法之间，将扫描的指纹转换为一致且不可链接的别名，实现可撤销生物识别。

Result: 在标准基准数据集和商业指纹识别系统上的评估表明，ProxyPrints在保持匹配性能的同时提供强大的安全性和可撤销性。

Conclusion: ProxyPrints是一个即插即用的可扩展解决方案，通过开源实现提供别名生成和部署工具，有效保护指纹数据安全。

Abstract: Fingerprint recognition systems are widely deployed for authentication and forensic applications, but the security of stored fingerprint data remains a critical vulnerability. While many systems avoid storing raw fingerprint images in favor of minutiae-based templates, recent research shows that these templates can be reverse-engineered to reconstruct realistic fingerprint images, enabling physical spoofing attacks that compromise user identities with no means of remediation.
  We present ProxyPrints, the first practical defense that brings cancellable biometrics to existing fingerprint recognition systems without requiring modifications to proprietary matching software. ProxyPrints acts as a transparent middleware layer between the fingerprint scanner and the matching algorithm, transforming each scanned fingerprint into a consistent, unlinkable alias. This transformation allows biometric identities to be revoked and replaced in the event of a breach, without affecting authentication accuracy. Additionally, ProxyPrints provides organizations with breach detection capabilities by enabling the identification of out-of-band spoofing attempts involving compromised aliases.
  We evaluate ProxyPrints on standard benchmark datasets and commercial fingerprint recognition systems, demonstrating that it preserves matching performance while offering strong security and revocability. Our open-source implementation includes tools for alias generation and deployment in real-world pipelines, making ProxyPrints a drop-in, scalable solution for fingerprint data protection.

</details>


### [76] [An Evaluation Framework for Network IDS/IPS Datasets: Leveraging MITRE ATT&CK and Industry Relevance Metrics](https://arxiv.org/abs/2511.12743)
*Adrita Rahman Tori,Khondokar Fida Hasan*

Main category: cs.CR

TL;DR: 提出一个多维度框架来评估IDS/IPS数据集的质量，结合MITRE ATT&CK威胁情报和五个互补指标，发现现有数据集在医疗、能源和金融等特定行业的威胁覆盖存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型评估实践主要关注准确度指标，往往忽视数据集是否代表行业特定威胁，导致IDS/IPS在实际部署中效果不佳。

Method: 开发一个结合威胁情报、自然语言处理和定量分析的多维度框架，使用MITRE ATT&CK知识库和五个互补指标来评估数据集对特定行业环境的适用性。

Result: 对九个公开IDS/IPS数据集的分析显示，近期数据集（如CIC-IoMT、CIC-UNSW-NB15）与行业特定威胁更匹配，而其他数据集（如CICIoV-24）尽管较新但表现不佳，特别是在医疗、能源和金融行业存在显著威胁覆盖差距。

Conclusion: 该框架提供了一种标准化、可解释的方法来选择符合行业特定操作需求的数据集，通过真实案例研究验证了其效率和实用性，能够提升AI驱动IDS/IPS在实际环境中的有效性。

Abstract: The performance of Machine Learning (ML) and Deep Learning (DL)-based Intrusion Detection and Prevention Systems (IDS/IPS) is critically dependent on the relevance and quality of the datasets used for training and evaluation. However, current AI model evaluation practices for developing IDS/IPS focus predominantly on accuracy metrics, often overlooking whether datasets represent industry-specific threats. To address this gap, we introduce a novel multi-dimensional framework that integrates the MITRE ATT&CK knowledge base for threat intelligence and employs five complementary metrics that together provide a comprehensive assessment of dataset suitability. Methodologically, this framework combines threat intelligence, natural language processing, and quantitative analysis to assess the suitability of datasets for specific industry contexts. Applying this framework to nine publicly available IDS/IPS datasets reveals significant gaps in threat coverage, particularly in the healthcare, energy, and financial sectors. In particular, recent datasets (e.g., CIC-IoMT, CIC-UNSW-NB15) align better with sector-specific threats, whereas others, like CICIoV-24, underperform despite their recency. Our findings provide a standardized, interpretable approach for selecting datasets aligned with sector-specific operational requirements, ultimately enhancing the real-world effectiveness of AI-driven IDS/IPS deployments. The efficiency and practicality of the framework are validated through deployment in a real-world case study, underscoring its capacity to inform dataset selection and enhance the effectiveness of AI-driven IDS/IPS in operational environments.

</details>


### [77] [Whose Narrative is it Anyway? A KV Cache Manipulation Attack](https://arxiv.org/abs/2511.12752)
*Mukkesh Ganesh,Kaushik Iyer,Arun Baalaaji Sankar Ananthan*

Main category: cs.CR

TL;DR: 本文提出了一种名为"历史交换"的新型KV缓存攻击方法，通过在不改变用户提示的情况下操纵KV缓存来引导模型生成方向，揭示了KV缓存作为安全分析重要向量的潜在风险。


<details>
  <summary>Details</summary>
Motivation: KV缓存是自回归大语言模型高效推理的重要组成部分，但其作为模型内部状态表示的特性使其可能成为完整性攻击的目标。研究KV缓存的安全漏洞对于理解模型行为操纵机制至关重要。

Method: 提出"历史交换"攻击方法，在块级别操纵KV缓存，将活动生成的连续缓存段用来自不同主题的预计算缓存覆盖，并在Qwen 3系列模型的324种配置下进行实证评估。

Result: 研究发现只有全层覆盖才能成功劫持对话主题，产生三种不同行为：立即且持久的主题转换、部分恢复或延迟劫持。高层结构计划在生成过程早期编码，局部话语结构由模型最后层维护。

Conclusion: KV缓存不仅是上下文表示，还编码了主题轨迹和结构规划，使其成为操纵模型行为的强大接口，是安全分析的重要向量。

Abstract: The Key Value(KV) cache is an important component for efficient inference in autoregressive Large Language Models (LLMs), but its role as a representation of the model's internal state makes it a potential target for integrity attacks. This paper introduces "History Swapping," a novel block-level attack that manipulates the KV cache to steer model generation without altering the user-facing prompt. The attack involves overwriting a contiguous segment of the active generation's cache with a precomputed cache from a different topic. We empirically evaluate this method across 324 configurations on the Qwen 3 family of models, analyzing the impact of timing, magnitude, and layer depth of the cache overwrite. Our findings reveal that only full-layer overwrites can successfully hijack the conversation's topic, leading to three distinct behaviors: immediate and persistent topic shift, partial recovery, or a delayed hijack. Furthermore, we observe that high-level structural plans are encoded early in the generation process and local discourse structure is maintained by the final layers of the model. This work demonstrates that the KV cache is a significant vector for security analysis, as it encodes not just context but also topic trajectory and structural planning, making it a powerful interface for manipulating model behavior.

</details>


### [78] [Efficient Adversarial Malware Defense via Trust-Based Raw Override and Confidence-Adaptive Bit-Depth Reduction](https://arxiv.org/abs/2511.12827)
*Ayush Chaudhary,Sisir Doppalpudi*

Main category: cs.CR

TL;DR: 提出结合TRO和CABDR的新框架，在恶意软件检测中优化对抗鲁棒性与计算效率的权衡，实现1.76倍计算开销，比现有方法提升2.3倍。


<details>
  <summary>Details</summary>
Motivation: 现有对抗防御方法虽然提升了鲁棒性，但带来4-22倍计算开销，对处理百万样本的生产系统构成重大挑战。

Method: 结合信任原始覆盖(TRO)和置信度自适应位深缩减(CABDR)，通过自适应置信度机制选择性应用防御措施。

Result: 在EMBER v2数据集上保持91%清洁准确率，将攻击成功率降至31-37%，对C&W等优化攻击减少48.8%，吞吐量达126万样本/秒。

Conclusion: 生产环境中的实用对抗鲁棒性需要明确优化效率-鲁棒性权衡，为组织部署鲁棒防御提供了可行路径。

Abstract: The deployment of robust malware detection systems in big data environments requires careful consideration of both security effectiveness and computational efficiency. While recent advances in adversarial defenses have demonstrated strong robustness improvements, they often introduce computational overhead ranging from 4x to 22x, which presents significant challenges for production systems processing millions of samples daily. In this work, we propose a novel framework that combines Trust-Raw Override (TRO) with Confidence-Adaptive Bit-Depth Reduction (CABDR) to explicitly optimize the trade-off between adversarial robustness and computational efficiency. Our approach leverages adaptive confidence-based mechanisms to selectively apply defensive measures, achieving 1.76x computational overhead - a 2.3x improvement over state-of-the-art smoothing defenses. Through comprehensive evaluation on the EMBER v2 dataset comprising 800K samples, we demonstrate that our framework maintains 91 percent clean accuracy while reducing attack success rates to 31-37 percent across multiple attack types, with particularly strong performance against optimization-based attacks such as C and W (48.8 percent reduction). The framework achieves throughput of up to 1.26 million samples per second (measured on pre-extracted EMBER features with no runtime feature extraction), validated across 72 production configurations with statistical significance (5 independent runs, 95 percent confidence intervals, p less than 0.01). Our results suggest that practical adversarial robustness in production environments requires explicit optimization of the efficiency-robustness trade-off, providing a viable path for organizations to deploy robust defenses without prohibitive infrastructure costs.

</details>


### [79] [Privacy-Preserving Federated Learning from Partial Decryption Verifiable Threshold Multi-Client Functional Encryption](https://arxiv.org/abs/2511.12936)
*Minjie Wang,Jinguang Han,Weizhi Meng*

Main category: cs.CR

TL;DR: 本文提出了一种联邦学习可验证阈值安全聚合协议（VTSAFL），通过部分解密可验证的阈值多客户端函数加密方案，在保护隐私的同时确保聚合结果的可验证性，显著降低了计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中梯度泄漏问题威胁隐私安全和模型完整性，现有方案无法保证聚合结果的可验证性，使系统易受投毒攻击威胁。

Method: 构建部分解密可验证的阈值多客户端函数加密方案，并应用于联邦学习实现VTSAFL协议，使客户端能够验证聚合结果。

Result: 在MNIST数据集上的实验表明，VTSAFL能达到与现有方案相同的准确率，同时减少总训练时间40%以上，通信开销降低达50%。

Conclusion: VTSAFL协议在保护隐私的同时确保了聚合结果的可验证性，为物联网设备等资源受限环境的大规模部署提供了效率保障。

Abstract: In federated learning, multiple parties can cooperate to train the model without directly exchanging their own private data, but the gradient leakage problem still threatens the privacy security and model integrity. Although the existing scheme uses threshold cryptography to mitigate the inference attack, it can not guarantee the verifiability of the aggregation results, making the system vulnerable to the threat of poisoning attack. We construct a partial decryption verifiable threshold multi client function encryption scheme, and apply it to Federated learning to implement the federated learning verifiable threshold security aggregation protocol (VTSAFL). VTSAFL empowers clients to verify aggregation results, concurrently minimizing both computational and communication overhead. The size of the functional key and partial decryption results of the scheme are constant, which provides efficiency guarantee for large-scale deployment. The experimental results on MNIST dataset show that vtsafl can achieve the same accuracy as the existing scheme, while reducing the total training time by more than 40%, and reducing the communication overhead by up to 50%. This efficiency is critical for overcoming the resource constraints inherent in Internet of Things (IoT) devices.

</details>


### [80] [The Grain Family of Stream Ciphers: an Abstraction, Strengthening of Components and New Concrete Instantiations](https://arxiv.org/abs/2511.12981)
*Palash Sarkar*

Main category: cs.CR

TL;DR: 本文提出了Grain流密码家族的抽象定义，改进了组件设计，并提出了7个具体密码方案，分别针对80位、128位、192位和256位安全级别。


<details>
  <summary>Details</summary>
Motivation: 为Grain流密码家族提供更形式化的抽象定义，并改进其组件设计以增强安全性和效率。

Method: 提出Grain家族的抽象定义，改进非线性布尔函数、状态更新函数、抽头位置选择等组件，并实例化7个具体密码方案。

Result: 在80位安全级别，新方案比Grain~v1具有更好的密码学特性和更低的门数；在128位级别，比Grain-128a有改进；在192位和256位级别，提供了文献中门数最小的方案。

Conclusion: 通过形式化定义和改进组件设计，成功构建了更安全高效的Grain流密码家族成员，覆盖多个安全级别。

Abstract: The first contribution of the paper is to put forward an abstract definition of the Grain family of stream ciphers which formalises the different components that are required to specify a particular member of the family. Our second contribution is to provide new and strengthened definitions of the components. These include definining new classes of nonlinear Boolean functions, improved definition of the state update function during initialisation, choice of the tap positions, and the possibility of the linear feedback shift register being smaller than the nonlinear feedback shift register. The third contribution of the paper is to put forward seven concrete proposals of stream ciphers by suitably instantiating the abstract family, one at the 80-bit security level, and two each at the 128-bit, 192-bit, and the 256-bit security levels. At the 80-bit security level, compared to the well known Grain~v1, the new proposal uses Boolean functions with improved cryptographic properties \textit{and} an overall lower gate count. At the 128-bit level, compared to ISO/IEC standard Grain-128a, the new proposals use Boolean functions with improved cryptographic properties; one of the proposals require a few extra gates, while the other has an overall lower gate count. At the 192-bit, and the 256-bit security levels, there are no proposals in the literature with smaller gate counts.

</details>


### [81] [SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization](https://arxiv.org/abs/2511.12982)
*Xuankun Rong,Wenke Huang,Tingfeng Wang,Daiguo Zhou,Bo Du,Mang Ye*

Main category: cs.CR

TL;DR: 本文提出SafeGRPO框架，通过集成规则治理的奖励构建到GRPO中，解决多模态大语言模型中的组合安全风险问题，提升推理安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在扩展模态空间时引入了新的组合安全风险，即使单个输入无害，跨模态耦合也可能产生不安全语义，暴露当前模型的安全意识脆弱性。

Method: 提出SafeGRPO框架，在构建的SafeTag-VL-3K数据集基础上，集成规则治理的奖励构建到GRPO中，执行步骤引导的安全思考来强制结构化推理和行为对齐。

Result: 显著提高了多模态安全意识、组合鲁棒性和推理稳定性，在多样化基准测试中表现优异，且不牺牲通用能力。

Conclusion: SafeGRPO通过自我奖励的多模态安全对齐框架，实现了可解释和可验证的推理安全优化，有效解决了多模态模型中的组合安全风险问题。

Abstract: Multimodal large language models (MLLMs) have demonstrated impressive reasoning and instruction-following capabilities, yet their expanded modality space introduces new compositional safety risks that emerge from complex text-image interactions. Such cross-modal couplings can produce unsafe semantics even when individual inputs are benign, exposing the fragile safety awareness of current MLLMs. While recent works enhance safety by guiding models to reason about potential risks, unregulated reasoning traces may compromise alignment; although Group Relative Policy Optimization (GRPO) offers self-rewarded refinement without human supervision, it lacks verifiable signals for reasoning safety. To address this, we propose SafeGRPO a self-rewarded multimodal safety alignment framework that integrates rule-governed reward construction into GRPO, enabling interpretable and verifiable optimization of reasoning safety. Built upon the constructed SafeTag-VL-3K dataset with explicit visual, textual, and combined safety tags, SafeGRPO performs step-guided safety thinking to enforce structured reasoning and behavior alignment, substantially improving multimodal safety awareness, compositional robustness, and reasoning stability across diverse benchmarks without sacrificing general capabilities.

</details>


### [82] [SoK: The Last Line of Defense: On Backdoor Defense Evaluation](https://arxiv.org/abs/2511.13143)
*Gorka Abad,Marina Krček,Stefanos Koffas,Behrad Tajalli,Marco Arazzi,Roberto Riaño,Xiaoyun Xu,Zhuoran Liu,Antonino Nocera,Stjepan Picek*

Main category: cs.CR

TL;DR: 本文对后门防御进行了系统性元分析，通过文献综述和实证评估揭示了评估方法的不一致性，并基于大规模实验提出了标准化建议。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对深度学习模型构成严重威胁，但现有防御方法的评估方法存在异质性，阻碍了公平比较。

Method: 分析了183篇后门防御论文，并进行了大规模实验，涉及3个数据集、4种模型架构、16种代表性防御和5种常用攻击，总计超过3000次实验。

Result: 发现实验设置、评估指标和威胁模型假设存在显著不一致性，防御效果在不同评估设置下差异很大。

Conclusion: 提出了具体挑战和基于实证的建议，旨在标准化和改进未来的防御评估实践。

Abstract: Backdoor attacks pose a significant threat to deep learning models by implanting hidden vulnerabilities that can be activated by malicious inputs. While numerous defenses have been proposed to mitigate these attacks, the heterogeneous landscape of evaluation methodologies hinders fair comparison between defenses. This work presents a systematic (meta-)analysis of backdoor defenses through a comprehensive literature review and empirical evaluation. We analyzed 183 backdoor defense papers published between 2018 and 2025 across major AI and security venues, examining the properties and evaluation methodologies of these defenses.
  Our analysis reveals significant inconsistencies in experimental setups, evaluation metrics, and threat model assumptions in the literature. Through extensive experiments involving three datasets (MNIST, CIFAR-100, ImageNet-1K), four model architectures (ResNet-18, VGG-19, ViT-B/16, DenseNet-121), 16 representative defenses, and five commonly used attacks, totaling over 3\,000 experiments, we demonstrate that defense effectiveness varies substantially across different evaluation setups. We identify critical gaps in current evaluation practices, including insufficient reporting of computational overhead and behavior under benign conditions, bias in hyperparameter selection, and incomplete experimentation. Based on our findings, we provide concrete challenges and well-motivated recommendations to standardize and improve future defense evaluations. Our work aims to equip researchers and industry practitioners with actionable insights for developing, assessing, and deploying defenses to different systems.

</details>


### [83] [A Secure Semantic Communication System Based on Knowledge Graph](https://arxiv.org/abs/2511.13246)
*Qin Guo,Haonan Tong,Sihua Wang,Peiyuan Si,Jun Zhao,Changchuan Yin*

Main category: cs.CR

TL;DR: 提出了一种语义通信系统中文本数据传输安全的新方法，通过知识图谱提取和信道加密方案，结合星座对角变换和多参数加权分数傅里叶变换，确保合法接收者能高质量恢复文本，而窃听者获取的信息质量很低。


<details>
  <summary>Details</summary>
Motivation: 在语义通信系统中，文本数据传输面临被窃听的风险，需要确保只有合法接收者能正确理解信息内容，而窃听者无法获取有效信息。

Method: 发送端对文本进行预处理，标注句子主题并提取为知识图谱；采用星座对角变换与多参数加权分数傅里叶变换的信道加密方案；接收端先解密，再通过transformer模型恢复文本。

Result: 实验结果显示，合法接收者的BLEU得分达到0.9，而窃听者的BLEU得分低于0.3；相比基线方法，安全性提升最高达20%。

Conclusion: 该方法能有效降低信息泄露概率，在语义通信系统中实现了安全的文本数据传输。

Abstract: This study proposes a novel approach to ensure the security of textual data transmission in a semantic communication system. In the proposed system, a sender transmits textual information to a receiver, while a potential eavesdropper attempts to intercept the information. At the sender side, the text is initially preprocessed, where each sentence is annotated with its corresponding topic, and subsequently extracted into a knowledge graph. To achieve the secure transmission of the knowledge graph, we propose a channel encryption scheme that integrates constellation diagonal transformation with multi-parameter weighted fractional Fourier transform (MP-WFRFT). At the receiver side, the textual data is first decrypted, and then recovered via a transformer model. Experimental results demonstrate that the proposed method reduces the probability of information compromise. The legitimate receiver achieves a Bilingual Evaluation Understudy (BLEU) score of 0.9, whereas the BLEU score of the eavesdropper remains below 0.3. Compared to the baselines, the proposed method can improve the security by up to 20%.

</details>


### [84] [Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs](https://arxiv.org/abs/2511.13319)
*Chelsea McMurray,Hayder Tirmazi*

Main category: cs.CR

TL;DR: Whistledown是一个为LLM提供尽力而为隐私保护的系统，通过伪名化和ε-局部差分隐私技术修改提示，防止个人身份信息泄露，同时保持对话实用性。


<details>
  <summary>Details</summary>
Motivation: 用户在使用LLM进行个人、情感化和社交敏感对话时，提示中可能包含不希望被记录、保留或泄露的个人身份信息，企业和个人用户都面临这一隐私挑战。

Method: 结合伪名化和ε-局部差分隐私技术，通过转换缓存机制修改发送给LLM的提示，系统设计为低计算和内存开销，可部署在客户端设备或企业零信任网关中。

Result: Whistledown提供了尽力而为的隐私保护，无需修改现有LLM提供商的API，在保护隐私的同时保持了对话的实用性。

Conclusion: Whistledown是一个有效的隐私保护解决方案，能够在不牺牲对话实用性的前提下保护用户隐私，适用于个人和企业用户的不同部署场景。

Abstract: Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.
  In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $ε$-local differential privacy ($ε$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.

</details>


### [85] [Enhancing All-to-X Backdoor Attacks with Optimized Target Class Mapping](https://arxiv.org/abs/2511.13356)
*Lei Wang,Yulong Tian,Hao Han,Fengyuan Xu*

Main category: cs.CR

TL;DR: 本文提出了一种新的后门攻击策略，针对多目标类别的All-to-X(A2X)攻击，通过优化分组和目标类别分配机制，显著提高了攻击成功率并保持了对现有防御的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击研究主要关注单目标All-to-One攻击，忽视了更复杂的多目标A2X攻击，而后者通常被认为攻击成功率较低。本文旨在证明A2X攻击的威胁性并提升其攻击效果。

Method: 提出了一种新颖的攻击策略，通过优化分组和目标类别分配机制来增强A2X攻击的成功率，同时保持对最先进防御方法的鲁棒性。

Result: 在CIFAR10、CIFAR100和Tiny-ImageNet数据集上，攻击成功率最高提升了28%，平均分别提升了6.7%、16.4%和14.1%。

Conclusion: 该研究提高了对A2X攻击威胁的认识，并促进了这一未充分探索领域的进一步研究。

Abstract: Backdoor attacks pose severe threats to machine learning systems, prompting extensive research in this area. However, most existing work focuses on single-target All-to-One (A2O) attacks, overlooking the more complex All-to-X (A2X) attacks with multiple target classes, which are often assumed to have low attack success rates. In this paper, we first demonstrate that A2X attacks are robust against state-of-the-art defenses. We then propose a novel attack strategy that enhances the success rate of A2X attacks while maintaining robustness by optimizing grouping and target class assignment mechanisms. Our method improves the attack success rate by up to 28%, with average improvements of 6.7%, 16.4%, 14.1% on CIFAR10, CIFAR100, and Tiny-ImageNet, respectively. We anticipate that this study will raise awareness of A2X attacks and stimulate further research in this under-explored area. Our code is available at https://github.com/kazefjj/A2X-backdoor .

</details>


### [86] [InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference](https://arxiv.org/abs/2511.13365)
*Ruijun Deng,Zhihui Lu,Qiang Duan*

Main category: cs.CR

TL;DR: InfoDecom是一个防御框架，通过分解和移除冗余信息，然后注入经过校准的噪声，在计算机视觉任务中实现更好的效用-隐私权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的分割推理防御方法在客户端模型较浅时会导致显著的效用下降，主要原因是现有防御对冗余信息施加了过多扰动。

Method: 提出InfoDecom框架，首先分解并移除冗余信息，然后注入提供理论保证隐私的校准噪声。

Result: 实验表明InfoDecom相比现有基线方法实现了更优的效用-隐私权衡。

Conclusion: InfoDecom通过信息分解和校准噪声注入，有效解决了分割推理中的数据重建攻击隐私泄露问题，同时保持了良好的模型效用。

Abstract: Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines. The code and the appendix are available at https://github.com/SASA-cloud/InfoDecom.

</details>


### [87] [Tight and Practical Privacy Auditing for Differentially Private In-Context Learning](https://arxiv.org/abs/2511.13502)
*Yuyang Xia,Ruixuan Liu,Li Xiong*

Main category: cs.CR

TL;DR: 提出了一个针对差分隐私上下文学习系统的隐私审计框架，通过成员推理攻击将攻击成功率转换为经验隐私保证，支持黑盒和白盒威胁模型。


<details>
  <summary>Details</summary>
Motivation: 现有DP-ICL实现容易出错，且最坏情况DP边界可能严重高估实际泄漏，需要实用的审计工具来验证隐私保护效果。

Method: 使用高斯差分隐私，通过分析私有投票机制识别最大化审计信号的投票配置，设计审计查询来可靠检测canary演示是否存在于上下文中。

Result: 在标准文本分类和生成基准测试中，经验泄漏估计与分类任务的理论DP预算紧密匹配，在生成任务中由于保守的嵌入敏感度边界而始终较低。

Conclusion: 该框架可作为实际DP-ICL部署的实用隐私审计器和验证器，为现实世界应用提供可靠的隐私保护验证。

Abstract: Large language models (LLMs) perform in-context learning (ICL) by adapting to tasks from prompt demonstrations, which in practice often contain private or proprietary data. Although differential privacy (DP) with private voting is a pragmatic mitigation, DP-ICL implementations are error-prone, and worst-case DP bounds may substantially overestimate actual leakage, calling for practical auditing tools. We present a tight and efficient privacy auditing framework for DP-ICL systems that runs membership inference attacks and translates their success rates into empirical privacy guarantees using Gaussian DP. Our analysis of the private voting mechanism identifies vote configurations that maximize the auditing signal, guiding the design of audit queries that reliably reveal whether a canary demonstration is present in the context. The framework supports both black-box (API-only) and white-box (internal vote) threat models, and unifies auditing for classification and generation by reducing both to a binary decision problem. Experiments on standard text classification and generation benchmarks show that our empirical leakage estimates closely match theoretical DP budgets on classification tasks and are consistently lower on generation tasks due to conservative embedding-sensitivity bounds, making our framework a practical privacy auditor and verifier for real-world DP-ICL deployments.

</details>


### [88] [ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models](https://arxiv.org/abs/2511.13548)
*Siyang Cheng,Gaotian Liu,Rui Mei,Yilin Wang,Kejia Zhang,Kaishuo Wei,Yuqi Yu,Weiping Wen,Xiaojie Wu,Junhua Liu*

Main category: cs.CR

TL;DR: ForgeDAN是一个新颖的进化框架，通过多策略文本扰动、可解释语义适应度评估和双维度越狱判断，生成语义连贯且高效的对抗性提示来攻击对齐LLMs。


<details>
  <summary>Details</summary>
Motivation: 现有自动越狱生成方法存在突变多样性有限、适应度评估浅层和基于关键词检测脆弱等问题，需要更有效的解决方案。

Method: 采用字符、词和句子级别的多策略文本扰动增强攻击多样性；使用基于文本相似性模型的可解释语义适应度评估指导进化过程；集成基于LLM的分类器进行双维度越狱判断。

Result: 评估显示ForgeDAN在保持自然性和隐蔽性的同时实现了高越狱成功率，优于现有SOTA解决方案。

Conclusion: ForgeDAN框架有效解决了现有方法的局限性，能够生成语义连贯且高效的对抗性提示，在越狱攻击方面表现出色。

Abstract: The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.

</details>


### [89] [Exploring the Effectiveness of Google Play Store's Privacy Transparency Channels](https://arxiv.org/abs/2511.13576)
*Anhao Xiang,Weiping Pei,Chuan Yue*

Main category: cs.CR

TL;DR: 本研究评估了Google Play商店中三种隐私透明度渠道（数据安全、隐私政策和权限清单）在帮助用户做出明智应用选择方面的有效性。通过190名参与者的实验发现，各渠道各有优势且相互补充。


<details>
  <summary>Details</summary>
Motivation: 随着GDPR和CCPA等法规对隐私透明度的要求，应用商店要求开发者通过多种渠道披露隐私实践，但这些渠道的实际效果尚不清楚，需要评估它们如何帮助用户做出明智决策。

Method: 对190名参与者进行实验，让他们与模拟的移动应用隐私透明度渠道互动，通过定量分析（辅以定性分析）参与者对五组问题的回答来评估各渠道效果。

Result: 数据安全提供最直观的用户界面，隐私政策信息最丰富且最有效，权限清单在提高参与者对应用整体隐私风险的关注方面表现突出。

Conclusion: 三种隐私透明度渠道各有优势且相互补充，都需要进一步改进以更好地帮助用户做出明智的应用选择决策。

Abstract: With the requirements and emphases on privacy transparency placed by regulations such as GDPR and CCPA, the Google Play Store requires Android developers to more responsibly communicate their apps' privacy practices to potential users by providing the proper information via the data safety, privacy policy, and permission manifest privacy transparency channels. However, it is unclear how effective those channels are in helping users make informed decisions in the app selection and installation process. In this article, we conducted a study for 190 participants to interact with our simulated privacy transparency channels of mobile apps. We quantitatively analyzed (supplemented by qualitative analysis) participants' responses to five sets of questions. We found that data safety provides the most intuitive user interfaces, privacy policy is most informative and effective, while permission manifest excels at raising participants' concerns about an app's overall privacy risks. These channels complement each other and should all be improved.

</details>


### [90] [It's a Feature, Not a Bug: Secure and Auditable State Rollback for Confidential Cloud Applications](https://arxiv.org/abs/2511.13641)
*Quinn Burke,Anjo Vahldiek-Oberwagner,Michael Swift,Patrick McDaniel*

Main category: cs.CR

TL;DR: Rebound是一个安全框架，在保持回滚保护的同时允许策略授权的合法回滚，通过参考监控器强制执行授权策略并保证状态更新的原子性。


<details>
  <summary>Details</summary>
Motivation: 现有安全框架将所有回滚都视为恶意行为，但排除了用于操作恢复的合法回滚需求，需要一种既能保护免受回滚攻击又支持合法恢复的解决方案。

Method: 设计Rebound框架，使用参考监控器来协调状态转换、强制执行授权策略、保证状态更新和回滚的原子性，并生成防篡改日志。

Result: 通过形式化证明Rebound的安全属性，并在GitLab CI的软件部署工作流案例研究中展示其能够以低端到端开销实现对二进制文件、配置和原始数据版本控制的强大控制。

Conclusion: Rebound提供了一个通用安全框架，在保持回滚保护的同时支持策略授权的合法回滚，解决了云应用中回滚攻击与合法恢复需求之间的矛盾。

Abstract: Replay and rollback attacks threaten cloud application integrity by reintroducing authentic yet stale data through an untrusted storage interface to compromise application decision-making. Prior security frameworks mitigate these attacks by enforcing forward-only state transitions (state continuity) with hardware-backed mechanisms, but they categorically treat all rollback as malicious and thus preclude legitimate rollbacks used for operational recovery from corruption or misconfiguration. We present Rebound, a general-purpose security framework that preserves rollback protection while enabling policy-authorized legitimate rollbacks of application binaries, configuration, and data. Key to Rebound is a reference monitor that mediates state transitions, enforces authorization policy, guarantees atomicity of state updates and rollbacks, and emits a tamper-evident log that provides transparency to applications and auditors. We formally prove Rebound's security properties and show through an application case study -- with software deployment workflows in GitLab CI -- that it enables robust control over binary, configuration, and raw data versioning with low end-to-end overhead.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [91] [WITNESS: A lightweight and practical approach to fine-grained predictive mutation testing](https://arxiv.org/abs/2511.11999)
*Zeyu Lu,Peng Zhang,Chun Yong Chong,Shan Gao,Yibiao Yang,Yanhui Li,Lin Chen,Yuming Zhou*

Main category: cs.SE

TL;DR: WITNESS是一种新的细粒度预测性变异测试方法，采用轻量级机器学习模型而非深度学习，解决了现有方法计算成本高和适用范围受限的问题，能够处理所有生成的变异体并显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的细粒度预测性变异测试方法存在两个关键局限：(1)计算成本过高，与预测性变异测试降低成本的目标相悖；(2)适用范围受限，只能处理方法内变异体而无法预测方法外变异体。

Method: WITNESS采用双重设计：(1)收集方法内和方法外变异体的特征，适用于所有生成的变异体；(2)使用轻量级经典机器学习模型进行训练和预测，而非计算昂贵的深度学习。

Result: 在Defects4J项目上的评估显示，WITNESS在不同场景下始终达到最先进的预测性能，显著提高了杀死矩阵预测的效率。后分析表明，包含变异前后信息的特征是最重要的。

Conclusion: 基于预测杀死矩阵的测试用例优先级排序表明，WITNESS提供的结果更接近使用实际杀死矩阵获得的结果，优于基线方法，为实际应用提供了更高效和全面的解决方案。

Abstract: Existing fine-grained predictive mutation testing studies predominantly rely on deep learning, which faces two critical limitations in practice: (1) Exorbitant computational costs. The deep learning models adopted in these studies demand significant computational resources for training and inference acceleration. This introduces high costs and undermines the cost-reduction goal of predictive mutation testing. (2) Constrained applicability. Although modern mutation testing tools generate mutants both inside and outside methods, current fine-grained predictive mutation testing approaches handle only inside-method mutants. As a result, they cannot predict outside-method mutants, limiting their applicability in real-world scenarios. We propose WITNESS, a new fine-grained predictive mutation testing approach. WITNESS adopts a twofold design: (1) With collected features from both inside-method and outside-method mutants, WITNESS is suitable for all generated mutants. (2) Instead of using computationally expensive deep learning, WITNESS employs lightweight classical machine learning models for training and prediction. This makes it more cost-effective and enabling straightforward explanations of the decision-making processes behind the adopted models. Evaluations on Defects4J projects show that WITNESS consistently achieves state-of-the-art predictive performance across different scenarios. Additionally, WITNESS significantly enhances the efficiency of kill matrix prediction. Post-hoc analysis reveals that features incorporating information from before and after the mutation are the most important among those used in WITNESS. Test case prioritization based on the predicted kill matrix shows that WITNESS delivers results much closer to those obtained by using the actual kill matrix, outperforming baseline approaches.

</details>


### [92] [A Code Smell Refactoring Approach using GNNs](https://arxiv.org/abs/2511.12069)
*HanYu Zhang,Tomoji Kishi*

Main category: cs.SE

TL;DR: 本文提出了一种基于图的深度学习方法用于代码异味重构，通过设计类级和方法级输入图，使用图分类和节点分类任务来处理长方法、大类、特征嫉妒三种代码异味，在实验中取得了优于传统方法和现有深度学习方法的重构性能。


<details>
  <summary>Details</summary>
Motivation: 代码异味是软件重构中的重大挑战，现有方法存在局限性：基于指标和规则的方法严重依赖手动定义的启发式规则和阈值，而基于深度学习的方法受限于数据集可用性和模型设计。

Method: 设计两种输入图（类级和方法级），采用图分类和节点分类任务，使用三种经典GNN架构（GCN、GraphSAGE、GAT），并提出半自动数据集生成方法以最小化人工工作量。

Result: 实验结果表明，所提出的方法在重构性能上优于传统方法和最先进的深度学习方法。

Conclusion: 基于图的深度学习方法在代码异味重构方面表现出色，能够有效解决现有方法的局限性，为软件重构提供了新的有效途径。

Abstract: Code smell is a great challenge in software refactoring, which indicates latent design or implementation flaws that may degrade the software maintainability and evolution. Over the past decades, a variety of refactoring approaches have been proposed, which can be broadly classified into metrics-based, rule-based, and machine learning-based approaches. Recent years, deep learning-based approaches have also attracted widespread attention. However, existing techniques exhibit various limitations. Metrics- and rule-based approaches rely heavily on manually defined heuristics and thresholds, whereas deep learning-based approaches are often constrained by dataset availability and model design. In this study, we proposed a graph-based deep learning approach for code smell refactoring. Specifically, we designed two types of input graphs (class-level and method-level) and employed both graph classification and node classification tasks to address the refactoring of three representative code smells: long method, large class, and feature envy. In our experiment, we propose a semi-automated dataset generation approach that could generate a large-scale dataset with minimal manual effort. We implemented the proposed approach with three classical GNN (graph neural network) architectures: GCN, GraphSAGE, and GAT, and evaluated its performance against both traditional and state-of-the-art deep learning approaches. The results demonstrate that proposed approach achieves superior refactoring performance.

</details>


### [93] [Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision](https://arxiv.org/abs/2511.12229)
*Zhipeng Xue,Zhipeng Gao,Tongtong Xu,Xing Hu,Xin Xia,Shanping Li*

Main category: cs.SE

TL;DR: 本文提出ACWRecommender框架，通过两阶段方法从静态分析工具的大量警告中推荐高概率为真实bug的可操作警告，显著提升了bug检测效率。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具的高误报率阻碍了其广泛应用，现有方法对可操作警告的收集假设不准确，导致大量无效警告。

Method: 构建首个大型可操作警告数据集，提出两阶段框架：粗粒度检测阶段识别可操作警告，细粒度重排阶段通过弱监督学习将高概率bug警告排到顶部。

Result: 实验表明ACWRecommender在nDCG和MRR指标上大幅优于基线方法，在6个项目中验证了27个开发者确认的真实bug。

Conclusion: 该工具能帮助开发者从海量警告中快速定位真实bug，具有实际应用价值。

Abstract: The use of static analysis tools has gained increasing popularity among developers in the last few years. However, the widespread adoption of static analysis tools is hindered by their high false alarm rates. Previous studies have introduced the concept of actionable warnings and built a machine-learning method to distinguish actionable warnings from false alarms. However, according to our empirical observation, the current assumption used for actionable warning(s) collection is rather shaky and inaccurate, leading to a large number of invalid actionable warnings. To address this problem, in this study, we build the first large actionable warning dataset by mining 68,274 reversions from Top-500 GitHub C repositories, we then take one step further by assigning each actionable warning a weak label regarding its likelihood of being a real bug. Following that, we propose a two-stage framework called ACWRecommender to automatically recommend the actionable warnings with high probability to be real bugs (AWHB). Our approach warms up the pre-trained model UniXcoder by identifying actionable warnings task (coarse-grained detection stage) and rerank AWHB to the top by weakly supervised learning (fine-grained reranking stage). Experimental results show that our proposed model outperforms several baselines by a large margin in terms of nDCG and MRR for AWHB recommendation. Moreover, we ran our tool on 6 randomly selected projects and manually checked the top-ranked warnings from 2,197 reported warnings, we reported top-10 recommended warnings to developers, 27 of them were already confirmed by developers as real bugs. Developers can quickly find real bugs among the massive amount of reported warnings, which verifies the practical usage of our tool.

</details>


### [94] [Reducing Hallucinations in LLM-Generated Code via Semantic Triangulation](https://arxiv.org/abs/2511.12288)
*Yihan Dai,Sijie Liang,Haotian Xu,Peichu Xie,Sergey Mechtaev*

Main category: cs.SE

TL;DR: 本文提出语义三角化方法，通过问题转换来验证代码生成的一致性，提高LLM生成代码的可靠性，在低概率采样和多解情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有样本共识技术在选择正确程序或弃权方面存在不足，特别是在采样概率低或存在多个有效但不等价解决方案时。

Method: 引入语义三角化，通过非平凡改变问题语义但保持解决方案间精确可验证映射的转换，验证跨问题转换的一致性。

Result: 在LiveCodeBench和CodeElo基准测试中，语义三角化相比概率阈值0.5的高置信度选择方法，可靠性提高21%，能在低至0.14的采样概率下识别正确解决方案。

Conclusion: 语义三角化通过验证跨问题转换的一致性，能更可靠地识别正确程序并适时弃权，特别是在多解和低概率场景下表现突出。

Abstract: When generating code from natural language prompts, an LLM samples programs from a probability distribution, many of which might be incorrect. Sample consensus techniques - such as majority voting or validation against generated tests or specifications - aim to identify a correct program in the sample or abstain if none is valid. However, existing methods often fail to select a correct solution when its sampling probability is low, or when the problem permits multiple valid but non-equivalent solutions. Additionally, they often fail to abstain when no correct solution is present in the sample. To overcome these limitations, we introduce semantic triangulation, which transforms a programming problem in a way that non-trivially alters its semantics while preserving an exact, verifiable mapping between solutions before and after transformation. We theoretically establish that verifying consistency across such problem transformations increases confidence that generated programs reflect accurate generalization rather than spurious statistical correlations, enabling more reliable sample consensus and abstention. On the LiveCodeBench and CodeElo benchmarks, using GPT-4o and DeepSeek-V3 models, semantic triangulation increases reliability of generated code by 21% compared to the method that selects only high-confidence solutions with the probability threshold 0.5, while being able to pinpoint correct solutions at sampling probabilities as low as 0.14. Apart from that, it is also the only approach to consistently form true consensus on tasks with multiple valid but non-equivalent solutions.

</details>


### [95] [Can Small GenAI Language Models Rival Large Language Models in Understanding Application Behavior?](https://arxiv.org/abs/2511.12576)
*Mohammad Meymani,Hamed Jelodar,Parisa Hamedi,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.SE

TL;DR: 本文系统评估了小型和大型生成式AI语言模型在理解应用行为（特别是恶意软件检测）方面的能力，发现大型模型整体准确率更高，但小型模型在精度和召回率上保持竞争力，且在计算效率、推理速度和资源受限环境部署方面具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型在代码分析和理解领域的广泛应用，需要系统评估不同规模模型在应用行为理解（特别是恶意软件检测）任务中的表现，为实际部署提供指导。

Method: 通过系统实验评估小型和大型生成式AI语言模型在恶意软件检测任务中的表现，比较准确率、精度、召回率和F1分数等指标。

Result: 大型模型整体准确率更高，但小型模型在精度和召回率上保持竞争力，且在计算效率、推理速度和资源受限环境部署方面具有显著优势。

Conclusion: 小型生成式AI模型可以有效补充大型模型，在实际应用行为分析中提供性能与资源效率之间的实用平衡。

Abstract: Generative AI (GenAI) models, particularly large language models (LLMs), have transformed multiple domains, including natural language processing, software analysis, and code understanding. Their ability to analyze and generate code has enabled applications such as source code summarization, behavior analysis, and malware detection. In this study, we systematically evaluate the capabilities of both small and large GenAI language models in understanding application behavior, with a particular focus on malware detection as a representative task. While larger models generally achieve higher overall accuracy, our experiments show that small GenAI models maintain competitive precision and recall, offering substantial advantages in computational efficiency, faster inference, and deployment in resource-constrained environments. We provide a detailed comparison across metrics such as accuracy, precision, recall, and F1-score, highlighting each model's strengths, limitations, and operational feasibility. Our findings demonstrate that small GenAI models can effectively complement large ones, providing a practical balance between performance and resource efficiency in real-world application behavior analysis.

</details>


### [96] [LLM4SCREENLIT: Recommendations on Assessing the Performance of Large Language Models for Screening Literature in Systematic Reviews](https://arxiv.org/abs/2511.12635)
*Lech Madeyski,Barbara Kitchenham,Martin Shepperd*

Main category: cs.SE

TL;DR: 本文分析了评估大语言模型在系统综述文献筛选中的性能挑战，指出了传统评估指标的不足，提出了改进建议，包括使用对不平衡数据稳健的指标、考虑丢失证据的影响、报告完整混淆矩阵等。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发布速度加快，需要对其在系统综述文献筛选等研究应用中进行稳健评估。当前评估方法存在诸多问题，需要建立更好的评估实践。

Method: 以一项大规模研究为例，分析了传统指标在评估生成式AI工具进行文献筛选时的局限性，并分析了27篇相关论文的性能指标使用情况。

Result: 发现主要弱点包括：使用对不平衡数据不稳健的指标、未考虑丢失证据的影响、未报告完整混淆矩阵。同时也提取了良好的评估实践。

Conclusion: 建议系统综述筛选评估应优先考虑丢失证据/召回率，使用机会锚定和成本敏感的加权MCC指标，报告完整混淆矩阵，采用防泄漏设计，并将结论建立在成本效益分析基础上。

Abstract: Context: Large language models (LLMs) are released faster than users' ability to evaluate them rigorously. When LLMs underpin research, such as identifying relevant literature for systematic reviews (SRs), robust empirical assessment is essential. Objective: We identify and discuss key challenges in assessing LLM performance for selecting relevant literature, identify good (evaluation) practices, and propose recommendations. Method: Using a recent large-scale study as an example, we identify problems with the use of traditional metrics for assessing the performance of Gen-AI tools for identifying relevant literature in SRs. We analyzed 27 additional papers investigating this issue, extracted the performance metrics, and found both good practices and widespread problems, especially with the use and reporting of performance metrics for SR screening. Results: Major weaknesses included: i) a failure to use metrics that are robust to imbalanced data and do not directly indicate whether results are better than chance, e.g., the use of Accuracy, ii) a failure to consider the impact of lost evidence when making claims concerning workload savings, and iii) pervasive failure to report the full confusion matrix (or performance metrics from which it can be reconstructed) which is essential for future meta-analyses. On the positive side, we extract good (evaluation) practices on which our recommendations for researchers and practitioners, as well as policymakers, are built. Conclusions: SR screening evaluations should prioritize lost evidence/recall alongside chance-anchored and cost-sensitive Weighted MCC (WMCC) metric, report complete confusion matrices, treat unclassifiable outputs as referred-back positives for assessment, adopt leakage-aware designs with non-LLM baselines and open artifacts, and ground conclusions in cost-benefit analysis where FNs carry higher penalties than FPs.

</details>


### [97] [Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter](https://arxiv.org/abs/2511.12823)
*Sajed Jalil,Shuvo Saha,Hossain Mohammad Seym*

Main category: cs.SE

TL;DR: 本文提出了一种结合测试驱动开发(TDD)和代码解释器(CI)的新方法，使用开源模型提升孟加拉语代码生成的准确率至85%，无需微调即可让小型模型达到大型模型98%的性能。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语拥有2.42亿母语使用者，但在LLM代码生成研究中关注不足。现有技术需要大量专业知识和资源，本文旨在为资源受限的新兴市场用户提供本地语言的代码生成工具。

Method: 采用测试驱动开发(TDD)和代码解释器(CI)相结合的方法，使用开源权重模型，无需模型微调。

Result: 将孟加拉语提示的代码生成基线准确率提升至85%，小型模型可达到大型模型98%的性能。所有结果已在GitHub公开。

Conclusion: 该方法成功证明了无需微调即可显著提升孟加拉语代码生成性能，为资源受限环境提供了可行的解决方案。

Abstract: Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language.
  We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.

</details>


### [98] [Human-Centred Requirements Engineering for Critical Systems: Insights from Disaster Early Warning Applications](https://arxiv.org/abs/2511.12856)
*Anuradha Madugalla,Jixuan Dong,Kai Lyne Loi,Matthew Crossman,John Grundy*

Main category: cs.SE

TL;DR: 本文提出了一种以人为中心的需求工程流程，将社会责任融入关键系统开发，通过设计自适应预警系统原型并评估验证了相关需求的有效性。


<details>
  <summary>Details</summary>
Motivation: 关键系统（如医疗、国防和灾害管理）传统上注重技术保证，但往往忽视了系统运行的人类和社会背景，需要将人为中心方面作为可靠性的重要维度。

Method: 通过文献综述识别为弱势群体设计软件的指导原则，转化为62个功能和非功能需求，设计自适应预警系统原型，并通过6次访谈和8次认知走查进行评估验证。

Result: 研究发现，早期处理以人为中心的需求能够增强系统对所有用户的可用性和可访问性。

Conclusion: 以人为中心不应被视为道德附加项，而是安全和公平关键系统的决定性质量特征。

Abstract: Critical systems, such as those used in healthcare, defence, and disaster management, demand rigorous requirements engineering to ensure safety and reliability. Yet, much of this rigour has traditionally focused on technical assurance, often overlooking the human and social contexts in which these systems operate. This paper argues that considering human-centric aspects is an essential dimension of dependability, and presents a human-centred RE process designed to integrate social responsibility into critical system development. Drawing from a literature review, we identified a set of guidelines for designing software for vulnerable communities and translated these into sixty-two functional and non-functional requirements. These requirements were operationalised through the design of an adaptive early warning system prototype, which was subsequently evaluated through six interviews and eight cognitive walkthroughs to validate their relevance and applicability. The findings demonstrate that human-centric requirements, when addressed early, enhance the usability and accessibility of systems for all users. The paper concludes by positioning human-centricity not as an ethical add-on but as a defining quality of safe and equitable critical systems.

</details>


### [99] [Diffploit: Facilitating Cross-Version Exploit Migration for Open Source Library Vulnerabilities](https://arxiv.org/abs/2511.12950)
*Zirui Chen,Zhipeng Xue,Jiayuan Zhou,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: Diffploit是一种基于差异驱动的漏洞利用迁移方法，通过上下文模块和迁移模块迭代修复跨版本漏洞利用失败问题，在102个Java CVE和689个版本迁移任务中成功迁移84.2%的漏洞利用。


<details>
  <summary>Details</summary>
Motivation: 现有技术主要关注代码级跟踪对齐，无法有效处理环境级失败和复杂的触发条件变化，导致漏洞利用在不同版本间直接应用经常失败。

Method: 提出Diffploit方法，包含上下文模块（动态构建行为差异上下文）和迁移模块（基于LLM的迭代反馈循环适配），通过差异驱动逐步解决复制失败。

Result: 在102个Java CVE和689个版本迁移任务中，Diffploit成功迁移84.2%的漏洞利用，比TARGET工具高52.0%，比IDEA工具高61.6%。同时发现5个CVE报告存在错误影响版本范围，111个GitHub Advisory Database中未报告的易受攻击版本。

Conclusion: Diffploit通过差异驱动的迭代迁移方法，有效解决了跨版本漏洞利用迁移问题，在技术效果和实际应用中都表现出色。

Abstract: Exploits are commonly used to demonstrate the presence of library vulnerabilities and validate their impact across different versions. However, their direct application to alternative versions often fails due to breaking changes introduced during evolution. These failures stem from both changes in triggering conditions (e.g., API refactorings) and broken dynamic environments (e.g., build or runtime errors), which are challenging to interpret and adapt manually. Existing techniques primarily focus on code-level trace alignment through fuzzing, which is both time-consuming and insufficient for handling environment-level failures. Moreover, they often fall short when dealing with complicated triggering condition changes across versions. To overcome this, we propose Diffploit, an iterative, diff-driven exploit migration method structured around two key modules: the Context Module and the Migration Module. The Context Module dynamically constructs contexts derived from analyzing behavioral discrepancies between the target and reference versions, which capture the failure symptom and its related diff hunks. Leveraging these contexts, the Migration Module guides an LLM-based adaptation through an iterative feedback loop, balancing exploration of diff candidates and gradual refinement to resolve reproduction failures effectively. We evaluate Diffploit on a large-scale dataset containing 102 Java CVEs and 689 version-migration tasks across 79 libraries. Diffploit successfully migrates 84.2% exploits, outperforming the change-aware test repair tool TARGET by 52.0% and the rule-based tool in IDEA by 61.6%. Beyond technical effectiveness, Diffploit identifies 5 CVE reports with incorrect affected version ranges, three of which have been confirmed. It also discovers 111 unreported vulnerable versions in the GitHub Advisory Database.

</details>


### [100] [SmartPoC: Generating Executable and Validated PoCs for Smart Contract Bug Reports](https://arxiv.org/abs/2511.12993)
*Longfei Chen,Ruibin Yan,Taiyu Wong,Yiyang Chen,Chao Zhang*

Main category: cs.SE

TL;DR: SmartPoC是一个自动化框架，能够将文本审计报告转换为可执行、已验证的测试用例，解决了现有审计工件的异构性和缺乏可重现PoC测试的问题。


<details>
  <summary>Details</summary>
Motivation: 智能合约容易存在漏洞，但审计工件通常缺乏可重现、可执行的PoC测试，导致需要昂贵的手动验证。现有LLM方法面临噪声输入、幻觉和缺少运行时预言机三大挑战。

Method: 首先处理输入审计报告以减少噪声，仅提取与漏洞相关的函数作为LLM上下文；利用LLM合成PoC测试用例，采用专门设计的执行前后修复机制来抑制幻觉并确保编译运行就绪；使用差分验证作为预言机来确认PoC测试用例的可利用性。

Result: 在SmartBugs-Vul和FORGE-Vul基准测试上，SmartPoC分别为85.61%和86.45%的目标生成了可执行、已验证的Foundry测试用例。应用于最新的Etherscan验证源语料库，SmartPoC以每个发现仅0.03美元的成本确认了545个审计发现中的236个真实漏洞。

Conclusion: SmartPoC框架成功解决了将文本审计报告转换为可执行测试用例的挑战，通过减少噪声输入、抑制幻觉和提供运行时预言机，实现了高效的自动化漏洞验证。

Abstract: Smart contracts are prone to vulnerabilities and are analyzed by experts as well as automated systems, such as static analysis and AI-assisted solutions. However, audit artifacts are heterogeneous and often lack reproducible, executable PoC tests suitable for automated validation, leading to costly, ad hoc manual verification. Large language models (LLMs) can be leveraged to turn audit reports into PoC test cases, but have three major challenges: noisy inputs, hallucinations, and missing runtime oracles. In this paper, we present SmartPoC, an automated framework that converts textual audit reports into executable, validated test cases. First, the input audit report is processed to reduce noise, and only bug-related functions are extracted and fed to LLMs as context. To curb hallucinations and ensure compile-and-run readiness, we leverage LLMs to synthesize PoC test cases with specially-designed pre-/post-execution repair. We further utilize differential verification as oracles to confirm exploitability of the PoC test cases. On the SmartBugs-Vul and FORGE-Vul benchmarks, SmartPoC generates executable, validated Foundry test cases for 85.61% and 86.45% of targets, respectively. Applied to the latest Etherscan verified-source corpus, SmartPoC confirms 236 real bugs out of 545 audit findings at a cost of only $0.03 per finding.

</details>


### [101] [Towards Requirements Engineering for GenAI-Enabled Software: Bridging Responsibility Gaps through Human Oversight Requirements](https://arxiv.org/abs/2511.13069)
*Zhenyu Mao,Jacky Keung,Yicheng Sun,Yifei Wang,Shuo Liu,Jialong Li*

Main category: cs.SE

TL;DR: 本文提出了一种三层分析方法来解决GenAI软件中的责任缺口问题，包括概念化、方法论和工件层，通过用户研究验证了该方法在识别责任缺口和推导监督需求方面的有效性。


<details>
  <summary>Details</summary>
Motivation: GenAI软件中的生成性和适应性特性使得人类监督和责任变得复杂，现有需求工程方法在处理责任缺口现象方面存在概念、方法和工件层面的研究空白。

Method: 采用三层设计方法：概念化层定义责任要素和缺口形成机制；方法论层通过分析人机交互推导监督需求；工件层用演绎骨干表形式化表示推理路径。

Result: 用户研究表明，与基线目标导向需求工程方法相比，所提方法在六个维度上均有明显改进，有效解决了三个研究空白。

Conclusion: 该方法为GenAI软件中的责任缺口分析提供了系统化框架，能够有效识别和表示责任缺口，并推导相应的人类监督需求。

Abstract: Context: Responsibility gaps, long-recognized challenges in socio-technical systems where accountability becomes diffuse or ambiguous, have become increasingly pronounced in GenAI-enabled software. The generative and adaptive nature complicates how human oversight and responsibility are specified, delegated, and traced. Existing requirements engineering (RE) approaches remain limited in addressing these phenomena, revealing conceptual, methodological, and artifact-level research gaps.. Objective: This study aims to analyze these research gaps in the context of GenAI-enabled software systems. It seeks to establish a coherent perspective for a systematic analysis of responsibility gaps from a human oversight requirements standpoint, encompassing how these responsibility gaps should be conceptualized, identified, and represented throughout the RE process. Methods: The proposed design methodology is structured across three analytical layers. At the conceptualization layer, it establishes a conceptual framing that defines the key elements of responsibility across the human and system dimensions and explains how potential responsibility gaps emerge from their interactions. At the methodological layer, it introduces a deductive pipeline for identifying responsibility gaps by analyzing interactions between these dimensions and deriving corresponding oversight requirements within established RE frameworks. At the artifact layer, it formalizes the results in a Deductive Backbone Table, a reusable representation that traces the reasoning path from responsibility gaps identification to human oversight requirements derivation. Results: A user study compared the proposed methodology with a baseline goal-oriented RE across two scenarios. Evaluation across six dimensions indicated clear improvements of the proposed methodology, confirming its effectiveness in addressing three research gaps.

</details>


### [102] [Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming](https://arxiv.org/abs/2511.13271)
*Rufeng Chen,Shuaishuai Jiang,Jiyun Shen,AJung Moon,Lili Wei*

Main category: cs.SE

TL;DR: 本研究比较了GenAI（如ChatGPT）与传统在线资源在支持编程知识获取方面的效果，发现GenAI能显著提升任务完成表现，但对知识获取效果不一致，且不同经验水平的学生使用策略存在差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注GenAI完成教育任务的能力及其对学生表现的影响，但忽视了其对知识获取的影响。本研究旨在探究GenAI辅助与传统在线资源在支持不同熟练程度学生知识获取方面的比较。

Method: 对24名具有不同编程经验（初学者、中级）的本科生进行受控用户实验，分析学生在解决编程任务时与ChatGPT的交互行为，评估任务表现、概念理解和交互行为。

Result: 使用GenAI生成完整解决方案显著提高了任务表现（特别是对初学者），但并未持续带来知识获取。使用策略因经验而异：初学者倾向于过度依赖GenAI完成任务而缺乏知识获取，中级学生采用更有选择性的方法。过度依赖和极少使用都会导致较弱的知识获取。

Conclusion: 呼吁学生和教育者将GenAI作为学习工具而非问题解决工具，强调在编程教育中整合GenAI时需要指导以促进更深层次的理解。

Abstract: The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.

</details>


### [103] [LinkXplore: A Framework for Affordable High-Quality Blockchain Data](https://arxiv.org/abs/2511.13318)
*Peihao Li*

Main category: cs.SE

TL;DR: LinkXplore是一个开源框架，用于收集和管理链上数据，解决了区块链数据收集成本高和缺乏系统化分析框架的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模区块链数据收集成本过高，RPC提供商定价昂贵，阻碍了学术研究和产品开发；同时缺乏灵活集成新模块的系统化分析框架。

Method: 通过直接分析RPC查询或流数据的原始数据，绕过昂贵的区块链数据提供商；提供简单API和后端处理逻辑，支持集成任何类型的链数据。

Result: 以低成本提供高质量的区块链数据，为预算有限的研究人员和开发者提供实用替代方案。

Conclusion: LinkXplore是首个用于收集和管理链上数据的开源框架，有效降低了区块链数据获取成本，促进了学术研究和产品开发。

Abstract: Blockchain technologies are rapidly transforming both academia and industry. However, large-scale blockchain data collection remains prohibitively expensive, as many RPC providers only offer enhanced APIs with high pricing tiers that are unsuitable for budget-constrained research or industrial-scale applications, which has significantly slowed down academic studies and product development. Moreover, there is a clear lack of a systematic framework that allows flexible integration of new modules for analyzing on-chain data.
  To address these challenges, we introduce LinkXplore, the first open framework for collecting and managing on-chain data. LinkXplore enables users to bypass costly blockchain data providers by directly analyzing raw data from RPC queries or streams, thereby offering high-quality blockchain data at a fraction of the cost. Through a simple API and backend processing logic, any type of chain data can be integrated into the framework. This makes it a practical alternative for both researchers and developers with limited budgets. Code and dataset used in this project are publicly available at https://github.com/Linkis-Project/LinkXplore

</details>


### [104] [An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains](https://arxiv.org/abs/2511.13341)
*Zihe Yan,Kai Luo,Haoyu Yang,Yang Yu,Zhuosheng Zhang,Guancheng Li*

Main category: cs.SE

TL;DR: 提出了一种细粒度的开源软件后门风险评估框架，该框架从攻击者视角建模隐蔽后门攻击，定义针对性指标，并利用大语言模型进行代码仓库语义评估。


<details>
  <summary>Details</summary>
Motivation: 开源软件供应链在现代软件开发中广泛使用，但依赖维护不足和社区审计不足导致源代码安全和仓库维护者合法性面临挑战，特别是在XZ-Util事件所示的高隐蔽性后门攻击下。

Method: 从攻击者视角建模隐蔽后门攻击，为每个攻击阶段定义针对性指标；使用大语言模型进行代码仓库语义评估，克服静态分析在评估仓库维护活动可靠性方面的限制。

Result: 在Debian生态系统的66个高优先级软件包上评估框架，实验结果表明当前开源软件供应链面临多种安全风险。

Conclusion: 提出的框架能够有效评估开源软件的后门风险，揭示了当前开源软件供应链的安全脆弱性。

Abstract: In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.

</details>


### [105] [FLOWER: Flow-Oriented Entity-Relationship Tool](https://arxiv.org/abs/2511.13357)
*Dmitry Moskalev*

Main category: cs.SE

TL;DR: FLOWER是一个面向流程的实体关系工具，首个端到端解决方案，可自动处理、创建和可视化SQL数据库中的显性和隐性依赖关系，通过动态采样和鲁棒数据分析技术改进实体关系模型和数据叙事。


<details>
  <summary>Details</summary>
Motivation: 探索数据源间关系对实体识别至关重要，但构建实体关系模型受人为因素影响。需要自动化工具来处理大量数据中的依赖关系，提高数据理解和洞察力。

Method: 提出FLOWER工具，自动检测内置约束并创建必要的正确约束，使用动态采样和鲁棒数据分析技术，支持SQL和自然语言查询，兼容CPU和GPU。

Result: 在STATS基准测试中，FLOWER在分布表示上比储层采样快2.4倍，约束学习快2.6倍，加速2.15倍。数据叙事准确性提高1.19倍，上下文减少1.86倍。支持23种语言。

Conclusion: FLOWER能更好地处理真实世界数据，确保质量、可扩展性和不同用例的适用性，是管理数据库依赖关系的有效工具。

Abstract: Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.

</details>


### [106] [BIOMERO 2.0: end-to-end FAIR infrastructure for bioimaging data import, analysis, and provenance](https://arxiv.org/abs/2511.13611)
*Torec T. Luik,Joost de Folter,Rodrigo Rosas-Bertolini,Eric A. J. Reits,Ron A. Hoebe,Przemek M. Krawczyk*

Main category: cs.SE

TL;DR: BIOMERO 2.0是一个重大演进的OMERO框架，将OMERO转变为符合FAIR原则（可查找、可访问、可互操作、可重用）且具有溯源意识的生物成像平台。


<details>
  <summary>Details</summary>
Motivation: 解决生物成像数据在导入、预处理、分析和共享过程中缺乏完整溯源记录的问题，提升数据的FAIR化程度，支持可追踪和可重用的工作流程。

Method: 通过OMERO.web插件和容器化组件集成数据导入、预处理、分析和工作流监控。导入子系统使用容器化预处理和表单元数据丰富实现就地导入，分析子系统通过BIOMERO Python库协调和跟踪高性能计算系统上的容器化分析。

Result: 所有导入和分析操作都记录参数、版本和结果，确保通过集成仪表板实时访问溯源信息。这种双重方法将OMERO置于生物成像分析过程的核心位置。

Conclusion: BIOMERO 2.0通过集成层增强了OMERO的FAIR化，支持可追踪、可重用的图像分析工作流程，弥合了数据导入、分析和共享之间的差距。

Abstract: We present BIOMERO 2.0, a major evolution of the BIOMERO framework that transforms OMERO into a FAIR-compliant (findable, accessible, interoperable, and reusable), provenance-aware bioimaging platform. BIOMERO 2.0 integrates data import, preprocessing, analysis, and workflow monitoring through an OMERO.web plugin and containerized components. The importer subsystem facilitates in-place import using containerized preprocessing and metadata enrichment via forms, while the analyzer subsystem coordinates and tracks containerized analyses on high-performance computing systems via the BIOMERO Python library. All imports and analyses are recorded with parameters, versions, and results, ensuring real-time provenance accessible through integrated dashboards. This dual approach places OMERO at the heart of the bioimaging analysis process: the importer ensures provenance from image acquisition through preprocessing and import into OMERO, while the analyzer records it for downstream processing. These integrated layers enhance OMEROs FAIRification, supporting traceable, reusable workflows for image analysis that bridge the gap between data import, analysis, and sharing.

</details>
