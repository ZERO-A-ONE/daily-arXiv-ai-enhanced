<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.CR](#cs.CR) [Total: 45]
- [cs.AI](#cs.AI) [Total: 64]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Digging Into the Internal: Causality-Based Analysis of LLM Function Calling](https://arxiv.org/abs/2509.16268)
*Zhenlan Ji,Daoyuan Wu,Wenxuan Wang,Pingchuan Ma,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: 本文通过因果分析方法研究了函数调用（FC）在大型语言模型（LLMs）中的工作机制，发现FC不仅能促进模型与外部系统交互，还能显著增强模型对用户指令的遵从性。实验表明FC在恶意输入检测方面比传统提示方法平均提升135%的性能。


<details>
  <summary>Details</summary>
Motivation: 函数调用技术虽然已被广泛应用，但其对模型行为的影响机制尚未被深入探索。作者发现FC除了常规用途外，还能显著提升LLMs对用户指令的遵从性，这促使他们采用因果分析方法来研究FC的工作原理。

Method: 采用层级别和令牌级别的因果干预方法，剖析FC在模型响应查询时的内部计算逻辑。通过对比FC指令与传统提示方法的有效性，在LLM安全鲁棒性这一关键应用场景下进行广泛实验评估。

Result: 因果分析证实了FC的显著影响，并揭示了其工作机制的深入洞察。实验结果显示，在检测恶意输入方面，FC相比传统提示方法平均性能提升约135%，在四个主流LLM和两个基准数据集上均表现优异。

Conclusion: FC技术具有增强LLM可靠性和实际应用能力的巨大潜力，特别是在安全鲁棒性方面。该研究为理解FC工作机制提供了新的因果分析视角，并验证了其在提升模型性能方面的有效性。

Abstract: Function calling (FC) has emerged as a powerful technique for facilitating
large language models (LLMs) to interact with external systems and perform
structured tasks. However, the mechanisms through which it influences model
behavior remain largely under-explored. Besides, we discover that in addition
to the regular usage of FC, this technique can substantially enhance the
compliance of LLMs with user instructions. These observations motivate us to
leverage causality, a canonical analysis method, to investigate how FC works
within LLMs. In particular, we conduct layer-level and token-level causal
interventions to dissect FC's impact on the model's internal computational
logic when responding to user queries. Our analysis confirms the substantial
influence of FC and reveals several in-depth insights into its mechanisms. To
further validate our findings, we conduct extensive experiments comparing the
effectiveness of FC-based instructions against conventional prompting methods.
We focus on enhancing LLM safety robustness, a critical LLM application
scenario, and evaluate four mainstream LLMs across two benchmark datasets. The
results are striking: FC shows an average performance improvement of around
135% over conventional prompting methods in detecting malicious inputs,
demonstrating its promising potential to enhance LLM reliability and capability
in practical applications.

</details>


### [2] [Constrained Co-evolutionary Metamorphic Differential Testing for Autonomous Systems with an Interpretability Approach](https://arxiv.org/abs/2509.16478)
*Hossein Yousefizadeh,Shenghui Gu,Lionel C. Briand,Ali Nasr*

Main category: cs.SE

TL;DR: CoCoMagic是一种结合蜕变测试、差分测试和搜索技术的自动化测试生成方法，用于检测自动驾驶系统不同版本间的行为差异。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统频繁更新可能导致意外行为退化，但系统级测试面临场景空间大、缺乏可靠测试预言等挑战。

Method: 将测试生成建模为约束协同共进化搜索，同时演化源场景和蜕变扰动，最大化版本间蜕变关系违反差异。使用约束和种群初始化策略确保场景真实性和相关性。

Result: 在Carla模拟器中评估InterFuser系统，相比基线方法识别出287%更多的高严重性行为差异，同时保持场景真实性。

Conclusion: CoCoMagic为演化自主系统的差分测试提供了高效、有效且可解释的方法，支持针对性调试和安全评估。

Abstract: Autonomous systems, such as autonomous driving systems, evolve rapidly
through frequent updates, risking unintended behavioral degradations. Effective
system-level testing is challenging due to the vast scenario space, the absence
of reliable test oracles, and the need for practically applicable and
interpretable test cases. We present CoCoMagic, a novel automated test case
generation method that combines metamorphic testing, differential testing, and
advanced search-based techniques to identify behavioral divergences between
versions of autonomous systems. CoCoMagic formulates test generation as a
constrained cooperative co-evolutionary search, evolving both source scenarios
and metamorphic perturbations to maximize differences in violations of
predefined metamorphic relations across versions. Constraints and population
initialization strategies guide the search toward realistic, relevant
scenarios. An integrated interpretability approach aids in diagnosing the root
causes of divergences. We evaluate CoCoMagic on an end-to-end ADS, InterFuser,
within the Carla virtual simulator. Results show significant improvements over
baseline search methods, identifying up to 287\% more distinct high-severity
behavioral differences while maintaining scenario realism. The interpretability
approach provides actionable insights for developers, supporting targeted
debugging and safety assessment. CoCoMagic offers an efficient, effective, and
interpretable way for the differential testing of evolving autonomous systems
across versions.

</details>


### [3] [Causal Fuzzing for Verifying Machine Unlearning](https://arxiv.org/abs/2509.16525)
*Anna Mazhar,Sainyam Galhotra*

Main category: cs.SE

TL;DR: 提出CAFÉ框架，基于因果关系统一验证数据点和特征级别的机器学习遗忘效果，能够检测直接和间接影响


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在决策系统中广泛应用，需要有效验证模型遗忘特定数据或特征的能力，以提升模型适应性、公平性和隐私保护

Method: CAFÉ框架通过因果依赖关系评估遗忘目标的直接和间接影响，为黑盒ML模型提供细粒度分析

Result: 在五个数据集和三种模型架构上的评估表明，CAFÉ成功检测到基线方法遗漏的残留影响，同时保持计算效率

Conclusion: CAFÉ框架为机器学习遗忘验证提供了更全面和有效的解决方案，能够识别传统方法无法检测的间接影响

Abstract: As machine learning models become increasingly embedded in decision-making
systems, the ability to "unlearn" targeted data or features is crucial for
enhancing model adaptability, fairness, and privacy in models which involves
expensive training. To effectively guide machine unlearning, a thorough testing
is essential. Existing methods for verification of machine unlearning provide
limited insights, often failing in scenarios where the influence is indirect.
In this work, we propose CAF\'E, a new causality based framework that unifies
datapoint- and feature-level unlearning for verification of black-box ML
models. CAF\'E evaluates both direct and indirect effects of unlearning targets
through causal dependencies, providing actionable insights with fine-grained
analysis. Our evaluation across five datasets and three model architectures
demonstrates that CAF\'E successfully detects residual influence missed by
baselines while maintaining computational efficiency.

</details>


### [4] [Is Measurement Enough? Rethinking Output Validation in Quantum Program Testing](https://arxiv.org/abs/2509.16595)
*Jiaming Ye,Xiongfei Wu,Shangzhou Xia,Fuyuan Zhang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 本文分析了量子程序测试中基于测量的验证方法的局限性，并与基于状态向量的验证方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，确保量子程序质量变得日益重要。现有的大多数量子程序质量保证方法依赖于基于测量的验证，但由于量子程序的概率特性，这些方法存在显著局限性。

Method: 通过对近期量子程序测试研究的实证分析，将现有的基于测量的验证方法分为分布级验证和输出值级验证两类，并与基于状态向量的验证方法进行比较评估。

Result: 研究发现基于测量的验证适用于简单评估（如验证特定输出值的存在），而基于状态向量的验证在处理复杂任务（如评估程序行为）时更有效。

Conclusion: 量子程序测试需要根据具体任务选择合适的验证方法，基于测量的验证适合简单验证，而基于状态向量的验证更适合复杂行为分析。

Abstract: As quantum computing continues to emerge, ensuring the quality of quantum
programs has become increasingly critical. Quantum program testing has emerged
as a prominent research area within the scope of quantum software engineering.
While numerous approaches have been proposed to address quantum program quality
assurance, our analysis reveals that most existing methods rely on
measurement-based validation in practice. However, due to the inherently
probabilistic nature of quantum programs, measurement-based validation methods
face significant limitations.
  To investigate these limitations, we conducted an empirical study of recent
research on quantum program testing, analyzing measurement-based validation
methods in the literature. Our analysis categorizes existing measurement-based
validation methods into two groups: distribution-level validation and
output-value-level validation. We then compare measurement-based validation
with statevector-based validation methods to evaluate their pros and cons. Our
findings demonstrate that measurement-based validation is suitable for
straightforward assessments, such as verifying the existence of specific output
values, while statevector-based validation proves more effective for
complicated tasks such as assessing the program behaviors.

</details>


### [5] [Incentives and Outcomes in Bug Bounties](https://arxiv.org/abs/2509.16655)
*Serena Wang,Martino Banchio,Krzysztof Kotowicz,Katrina Ligett,R. Preston McAfee,Eduardo' Vela'' Nava*

Main category: cs.SE

TL;DR: 分析Google漏洞奖励计划(VRP)中奖励激励对漏洞报告质量和数量的影响，特别是2024年7月奖励金额最高等级提升200%后的效果。


<details>
  <summary>Details</summary>
Motivation: 了解奖励激励在漏洞奖励计划中的作用，填补对奖励机制如何影响安全研究人员行为的知识空白。

Method: 实证分析Google VRP的数据，重点关注2024年7月奖励金额提升后的变化，计算弹性系数，并区分资深研究人员和新研究人员的贡献。

Result: 奖励提升后高价值漏洞数量增加，弹性分析显示正向响应，奖励提升既重新引导了资深研究人员的注意力，也吸引了新的顶级安全研究人员加入。

Conclusion: 奖励激励在漏洞奖励计划中发挥重要作用，适当的奖励提升能有效提高高价值漏洞的报告数量和质量。

Abstract: Bug bounty programs have contributed significantly to security in technology
firms in the last decade, but little is known about the role of reward
incentives in producing useful outcomes. We analyze incentives and outcomes in
Google's Vulnerability Rewards Program (VRP), one of the world's largest bug
bounty programs. We analyze the responsiveness of the quality and quantity of
bugs received to changes in payments, focusing on a change in Google's reward
amounts posted in July, 2024, in which reward amounts increased by up to 200%
for the highest impact tier. Our empirical results show an increase in the
volume of high-value bugs received after the reward increase, for which we also
compute elasticities. We further break down the sources of this increase
between veteran researchers and new researchers, showing that the reward
increase both redirected the attention of veteran researchers and attracted new
top security researchers into the program.

</details>


### [6] [Verifying User Interfaces using SPARK Ada: A Case Study of the T34 Syringe Driver](https://arxiv.org/abs/2509.16681)
*Peterson Jean*

Main category: cs.SE

TL;DR: 本文探讨了在医疗设备开发中应用形式化验证方法（特别是SPARK Ada工具）来提高安全性，通过通用模型在早期阶段预测和减少人为因素风险。


<details>
  <summary>Details</summary>
Motivation: 医疗设备的安全性至关重要，但传统测试方法往往无法在开发早期发现所有人为因素风险，这在使用如T34注射泵等移动设备时可能造成灾难性后果。需要新的解决方案来在开发早期阶段预测这些错误。

Method: 研究使用SPARK Ada的形式化验证工具对T34注射驱动器的行为模型进行验证。探索并实现了通用输液泵模型的细化，并在SPARK Ada中实施。通过SPARK评估最终原型的验证级别。

Result: 研究探索了在SPARK Ada中考虑抽象和用户界面设计组件时，所提出模型实施的潜在局限性。

Conclusion: 形式化方法研究可以为医疗设备开发提供新的解决方案，通过通用开发模型为行业安全集成提供共同框架，并可能通过形式化验证数学证明来确保安全性。

Abstract: The increase in safety and critical systems improved Healthcare. Due to their
risk of harm, such systems are subject to stringent guidelines and compliances.
These safety measures ensure a seamless experience and mitigate the risk to
end-users. Institutions like the Food and Drug Administration and the NHS,
respectively, established international standards and competency frameworks to
ensure industry compliance with these safety concerns. Medical device
manufacturing is mainly concerned with standards. Consequently, these standards
now advocate for better human factors considered in user interaction for
medical devices. This forces manufacturers to rely on heavy testing and review
to cover many of these factors during development. Sadly, many human factor
risks will not be caught until proper testing in real life, which might be
catastrophic in the case of an ambulatory device like the T34 syringe pump.
Therefore, effort in formal methods research may propose new solutions in
anticipating these errors in the early stages of development or even reducing
their occurrence based on the use of standard generic model. These generically
developed models will provide a common framework for safety integration in
industry and may potentially be proven using formal verification mathematical
proofs. This research uses SPARK Ada's formal verification tool against a
behavioural model of the T34 syringe driver. A Generic Infusion Pump model
refinement is explored and implemented in SPARK Ada. As a subset of the Ada
language, the verification level of the end prototype is evaluated using SPARK.
Exploring potential limitations defines the proposed model's implementation
liability when considering abstraction and components of User Interface design
in SPARK Ada.

</details>


### [7] [RelRepair: Enhancing Automated Program Repair by Retrieving Relevant Code](https://arxiv.org/abs/2509.16701)
*Shunyu Liu,Guangdong Bai,Mark Utting,Guowei Yang*

Main category: cs.SE

TL;DR: RelRepair是一种通过检索项目特定代码来增强大型语言模型自动程序修复能力的新方法，在Defects4J和ManySStuBs4J数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动程序修复中缺乏对项目特定信息（如领域特定标识符、代码结构和上下文关系）的理解能力，导致在需要项目特定信息的修复任务中表现不佳。

Method: RelRepair首先通过分析函数名和代码注释识别相关函数签名，然后进行深度代码分析检索与修复上下文相关的代码片段，最后将这些相关信息整合到LLM的输入提示中指导模型生成更准确的补丁。

Result: 在Defects4J V1.2数据集上成功修复101个bug，在ManySStuBs4J数据集上实现了17.1%的性能提升，总体修复率达到48.3%。

Conclusion: 研究结果表明向LLM提供相关的项目特定信息对于自动程序修复至关重要，为在APR任务中有效利用LLM提供了有效策略。

Abstract: Automated Program Repair (APR) has emerged as a promising paradigm for
reducing debugging time and improving the overall efficiency of software
development. Recent advances in Large Language Models (LLMs) have demonstrated
their potential for automated bug fixing and other software engineering tasks.
Nevertheless, the general-purpose nature of LLM pre-training means these models
often lack the capacity to perform project-specific repairs, which require
understanding of domain-specific identifiers, code structures, and contextual
relationships within a particular codebase. As a result, LLMs may struggle to
generate correct patches when the repair depends on project-specific
information.
  To address this limitation, we introduce RelRepair, a novel approach that
retrieves relevant project-specific code to enhance automated program repair.
RelRepair first identifies relevant function signatures by analyzing function
names and code comments within the project. It then conducts deeper code
analysis to retrieve code snippets relevant to the repair context. The
retrieved relevant information is then incorporated into the LLM's input
prompt, guiding the model to generate more accurate and informed patches. We
evaluate RelRepair on two widely studied datasets, Defects4J V1.2 and
ManySStuBs4J, and compare its performance against several state-of-the-art
LLM-based APR approaches. RelRepair successfully repairs 101 bugs in Defects4J
V1.2. Furthermore, RelRepair achieves a 17.1\% improvement in the ManySStuBs4J
dataset, increasing the overall fix rate to 48.3\%. These results highlight the
importance of providing relevant project-specific information to LLMs, shedding
light on effective strategies for leveraging LLMs in APR tasks.

</details>


### [8] [Can We Trust the AI Pair Programmer? Copilot for API Misuse Detection and Correction](https://arxiv.org/abs/2509.16795)
*Saikat Mondal,Chanchal K. Roy,Hong Wang,Juan Arguello,Samantha Mathan*

Main category: cs.SE

TL;DR: 该研究评估了GitHub Copilot在实时检测和修复API误用方面的有效性，使用MUBench基准测试，发现Copilot在检测准确率、精确率和召回率方面表现良好，并能成功修复大部分检测到的误用。


<details>
  <summary>Details</summary>
Motivation: API误用会导致安全漏洞、系统故障和维护成本增加，现有检测方法主要在开发后阶段进行，延迟了缺陷修复。AI代码助手如GitHub Copilot有望在开发环境中实现实时API误用检测。

Method: 使用MUBench基准测试构建740个误用示例（手动和AI辅助生成），包含147个正确使用案例，在Visual Studio Code中集成Copilot进行分析。

Result: Copilot检测准确率为86.2%，精确率91.2%，召回率92.4%。对常见误用类型表现良好，但对复杂或上下文相关案例有困难。成功修复了95%以上检测到的误用。

Conclusion: Copilot作为AI驱动的编码助手，在实时配对编程和检测修复API误用方面具有潜力，但仍有局限性需要改进。

Abstract: API misuse introduces security vulnerabilities, system failures, and
increases maintenance costs, all of which remain critical challenges in
software development. Existing detection approaches rely on static analysis or
machine learning-based tools that operate post-development, which delays defect
resolution. Delayed defect resolution can significantly increase the cost and
complexity of maintenance and negatively impact software reliability and user
trust. AI-powered code assistants, such as GitHub Copilot, offer the potential
for real-time API misuse detection within development environments. This study
evaluates GitHub Copilot's effectiveness in identifying and correcting API
misuse using MUBench, which provides a curated benchmark of misuse cases. We
construct 740 misuse examples, manually and via AI-assisted variants, using
correct usage patterns and misuse specifications. These examples and 147
correct usage cases are analyzed using Copilot integrated in Visual Studio
Code. Copilot achieved a detection accuracy of 86.2%, precision of 91.2%, and
recall of 92.4%. It performed strongly on common misuse types (e.g.,
missing-call, null-check) but struggled with compound or context-sensitive
cases. Notably, Copilot successfully fixed over 95% of the misuses it
identified. These findings highlight both the strengths and limitations of
AI-driven coding assistants, positioning Copilot as a promising tool for
real-time pair programming and detecting and fixing API misuses during software
development.

</details>


### [9] [Implementation of the Collision Avoidance System for DO-178C Compliance](https://arxiv.org/abs/2509.16844)
*Rim Zrelli,Henrique Amaral Misson,Sorelle Kamkuimo,Maroua Ben Attia,Abdo Shabah,Felipe Gohring de Magalhaes,Gabriela Nicolescu*

Main category: cs.SE

TL;DR: 本技术报告详细介绍了无人机防撞系统的实现，展示了如何通过形式化方法、模型驱动开发和自动化验证工具实现DO-178C标准合规性，为无人机安全关键软件的认证提供有效方法。


<details>
  <summary>Details</summary>
Motivation: 为无人机在民用空域的安全集成提供符合DO-178C标准的安全关键软件实现方法，解决无人机系统认证的挑战。

Method: 结合形式化方法、模型驱动开发和自动化验证工具（Alloy、SPIN、Simulink Embedded Coder、LDRA工具套件），按照DO-178C标准进行完整的软件生命周期开发。

Result: 形式化建模和自动化工具链能够早期发现和修正规范缺陷，实现强健的可追溯性，静态和动态分析确认了代码质量和覆盖率，形式化验证方法为关键组件提供了数学正确性保证。

Conclusion: 该方法有效解决了无人机安全关键系统的认证挑战，尽管集成阶段未完全实现，但证明了其在满足DO-178C标准要求方面的有效性。

Abstract: This technical report presents the detailed implementation of a Collision
Avoidance System (CAS) for Unmanned Aerial Vehicles (UAVs), developed as a case
study to demonstrate a rigorous methodology for achieving DO-178C compliance in
safety-critical software. The CAS is based on functional requirements inspired
by NASA's Access 5 project and is designed to autonomously detect, evaluate,
and avoid potential collision threats in real-time, supporting the safe
integration of UAVs into civil airspace.
  The implementation environment combines formal methods, model-based
development, and automated verification tools, including Alloy, SPIN, Simulink
Embedded Coder, and the LDRA tool suite. The report documents each phase of the
software lifecycle: requirements specification and validation, architectural
and detailed design, coding, verification, and traceability, with a strong
focus on compliance with DO-178C Design Assurance Level B objectives.
  Results demonstrate that formal modelling and automated toolchains enabled
early detection and correction of specification defects, robust traceability,
and strong evidence of verification and validation across all development
stages. Static and dynamic analyses confirmed code quality and coverage, while
formal verification methods provided mathematical assurance of correctness for
critical components. Although the integration phase was not fully implemented,
the approach proved effective in addressing certification challenges for UAV
safety-critical systems.
  \keywords Collision Avoidance System (CAS), Unmanned Aerial Vehicles (UAVs),
DO-178C compliance, Safety-critical software, Formal methods, Model-based
development, Alloy, SPIN model checker, Simulink Embedded Coder, LDRA tool
suite, Software verification and validation, Traceability, Certification.

</details>


### [10] [MobileUPReg: Identifying User-Perceived Performance Regressions in Mobile OS Versions](https://arxiv.org/abs/2509.16864)
*Wei Liu,Yi Wen Heng,Feng Lin,Tse-Hsun,Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: MobileUPReg是一个黑盒框架，用于检测移动操作系统版本间的用户感知性能回归，通过比较用户可感知的性能指标（如响应时间、完成时间、启动时间和掉帧率）来识别真正影响用户体验的回归问题。


<details>
  <summary>Details</summary>
Motivation: 现有的检测技术依赖系统级指标或专注于特定OS组件，可能错过用户实际感知的性能回归（如响应变慢或UI卡顿）。

Method: MobileUPReg在不同OS版本下运行相同应用，比较用户感知性能指标（响应时间、完成时间、启动时间、掉帧率）来识别回归。

Result: 在大规模研究中，MobileUPReg在提取用户感知指标方面达到高准确率，检测用户感知回归的精度为0.96，召回率为0.91，F1分数为0.93，显著优于使用Wilcoxon秩和检验和Cliff's Delta的统计基线。

Conclusion: MobileUPReg已部署在工业CI流水线中，每天分析数百个应用的数千个屏幕录像，发现了传统工具遗漏的回归问题，证明其能够实现准确、可扩展且与感知对齐的移动OS验证回归检测。

Abstract: Mobile operating systems (OS) are frequently updated, but such updates can
unintentionally degrade user experience by introducing performance regressions.
Existing detection techniques often rely on system-level metrics (e.g., CPU or
memory usage) or focus on specific OS components, which may miss regressions
actually perceived by users -- such as slower responses or UI stutters. To
address this gap, we present MobileUPReg, a black-box framework for detecting
user-perceived performance regressions across OS versions. MobileUPReg runs the
same apps under different OS versions and compares user-perceived performance
metrics -- response time, finish time, launch time, and dropped frames -- to
identify regressions that are truly perceptible to users. In a large-scale
study, MobileUPReg achieves high accuracy in extracting user-perceived metrics
and detects user-perceived regressions with 0.96 precision, 0.91 recall, and
0.93 F1-score -- significantly outperforming a statistical baseline using the
Wilcoxon rank-sum test and Cliff's Delta. MobileUPReg has been deployed in an
industrial CI pipeline, where it analyzes thousands of screencasts across
hundreds of apps daily and has uncovered regressions missed by traditional
tools. These results demonstrate that MobileUPReg enables accurate, scalable,
and perceptually aligned regression detection for mobile OS validation.

</details>


### [11] [DecipherGuard: Understanding and Deciphering Jailbreak Prompts for a Safer Deployment of Intelligent Software Systems](https://arxiv.org/abs/2509.16870)
*Rui Yang,Michael Fu,Chakkrit Tantithamthavorn,Chetan Arora,Gunel Gulmammadova,Joey Chua*

Main category: cs.SE

TL;DR: DecipherGuard是一个新型框架，通过集成解密层和低秩适应机制，显著提升了LLM软件系统在运行时对抗越狱攻击的防御能力。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能软件系统在关键领域的部署增加，运行时安全性成为重要挑战。研究发现当前最先进的LlamaGuard防护系统在面对混淆和模板攻击时防御成功率下降24%。

Method: 提出DecipherGuard框架，包含解密层对抗混淆攻击，以及低秩适应机制增强对模板攻击的防护效果。

Result: 在22,000多个提示上的实证评估显示，DecipherGuard相比LlamaGuard和其他两个运行时防护系统，防御成功率提升36%-65%，整体防护性能提升20%-50%。

Conclusion: DecipherGuard能有效防御LLM软件系统在运行时面临的越狱攻击，为安全部署提供了重要保障。

Abstract: Intelligent software systems powered by Large Language Models (LLMs) are
increasingly deployed in critical sectors, raising concerns about their safety
during runtime. Through an industry-academic collaboration when deploying an
LLM-powered virtual customer assistant, a critical software engineering
challenge emerged: how to enhance a safer deployment of LLM-powered software
systems at runtime? While LlamaGuard, the current state-of-the-art runtime
guardrail, offers protection against unsafe inputs, our study reveals a Defense
Success Rate (DSR) drop of 24% under obfuscation- and template-based jailbreak
attacks. In this paper, we propose DecipherGuard, a novel framework that
integrates a deciphering layer to counter obfuscation-based prompts and a
low-rank adaptation mechanism to enhance guardrail effectiveness against
template-based attacks. Empirical evaluation on over 22,000 prompts
demonstrates that DecipherGuard improves DSR by 36% to 65% and Overall
Guardrail Performance (OGP) by 20% to 50% compared to LlamaGuard and two other
runtime guardrails. These results highlight the effectiveness of DecipherGuard
in defending LLM-powered software systems against jailbreak attacks during
runtime.

</details>


### [12] [Deep Synthetic Cross-Project Approaches for Software Reliability Growth Modeling](https://arxiv.org/abs/2509.16939)
*Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: 提出了DSC-SRGM方法，通过合成数据生成和跨项目迁移学习相结合，解决数据稀缺环境下软件可靠性预测精度下降的问题。


<details>
  <summary>Details</summary>
Motivation: 传统软件可靠性增长模型在数据稀缺环境（如早期测试或安全关键系统）中预测精度下降，而跨项目迁移学习由于真实数据集稀缺和保密性限制应用有限。

Method: 使用传统SRGM生成合成数据集以保留真实缺陷发现趋势的统计特征，应用基于互相关的聚类方法识别与目标项目模式相似的合成数据集，然后用这些数据集训练深度学习模型进行可靠性预测。

Result: 在60个真实数据集上的评估显示，DSC-SRGM相比传统SRGM预测精度提升23.3%，相比基于真实数据集的跨项目深度学习模型提升32.2%。但过度使用合成数据或简单组合合成与真实数据会降低性能。

Conclusion: DSC-SRGM是数据稀缺环境下软件可靠性预测的有前景方法，但需要保持适当的数据平衡。

Abstract: Software Reliability Growth Models (SRGMs) are widely used to predict
software reliability based on defect discovery data collected during testing or
operational phases. However, their predictive accuracy often degrades in
data-scarce environments, such as early-stage testing or safety-critical
systems. Although cross-project transfer learning has been explored to mitigate
this issue by leveraging data from past projects, its applicability remains
limited due to the scarcity and confidentiality of real-world datasets. To
overcome these limitations, we propose Deep Synthetic Cross-project SRGM
(DSC-SRGM), a novel approach that integrates synthetic data generation with
cross-project transfer learning. Synthetic datasets are generated using
traditional SRGMs to preserve the statistical characteristics of real-world
defect discovery trends. A cross-correlation-based clustering method is applied
to identify synthetic datasets with patterns similar to the target project.
These datasets are then used to train a deep learning model for reliability
prediction. The proposed method is evaluated on 60 real-world datasets, and its
performance is compared with both traditional SRGMs and cross-project deep
learning models trained on real-world datasets. DSC-SRGM achieves up to 23.3%
improvement in predictive accuracy over traditional SRGMs and 32.2% over
cross-project deep learning models trained on real-world datasets. However,
excessive use of synthetic data or a naive combination of synthetic and
real-world data may degrade prediction performance, highlighting the importance
of maintaining an appropriate data balance. These findings indicate that
DSC-SRGM is a promising approach for software reliability prediction in
data-scarce environments.

</details>


### [13] [SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?](https://arxiv.org/abs/2509.16941)
*Xiang Deng,Jeff Da,Edwin Pan,Yannis Yiming He,Charles Ide,Kanak Garg,Niklas Lauffer,Andrew Park,Nitin Pasari,Chetan Rane,Karmini Sampath,Maya Krishnan,Srivatsa Kundurthy,Sean Hendryx,Zifan Wang,Chen Bo Calvin Zhang,Noah Jacobson,Bing Liu,Brad Kenstler*

Main category: cs.SE

TL;DR: SWE-Bench Pro是一个更复杂的基准测试，基于SWE-BENCH构建，专门设计用于捕捉超出SWE-BENCH范围的现实、复杂、企业级问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分反映现实世界中软件开发的复杂性和多样性，需要创建更能代表专业软件工程水平的测试平台。

Method: 从41个活跃维护的仓库中收集1,865个问题，涵盖商业应用、B2B服务和开发者工具，包含公开集、保留集和商业集三个分区。所有任务都经过人工验证并增强上下文以确保可解决性。

Result: 在统一框架下评估广泛使用的编码模型，它们在SWE-Bench Pro上的表现低于25%（Pass@1），GPT-5达到最高分23.3%。通过聚类分析失败模式来理解模型局限性。

Conclusion: SWE-BENCH PRO提供了一个抗污染测试平台，更真实地捕捉现实世界软件开发的复杂性和多样性，推动专业级自主软件工程代理的发展。

Abstract: We introduce SWE-Bench Pro, a substantially more challenging benchmark that
builds upon the best practices of SWE-BENCH [25], but is explicitly designed to
capture realistic, complex, enterprise-level problems beyond the scope of
SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of
41 actively maintained repositories spanning business applications, B2B
services, and developer tools. The benchmark is partitioned into a public set
with open access to problems sourced from 11 repositories, a held-out set of 12
repositories and a commercial set of 18 proprietary repositories where we have
formal partnership agreements with early-stage startups. Problems in the
held-out and the commercial set are not publicly accessible, but we release
results on the commercial set. Our benchmark features long-horizon tasks that
may require hours to days for a professional software engineer to complete,
often involving patches across multiple files and substantial code
modifications. All tasks are human-verified and augmented with sufficient
context to ensure resolvability. In our evaluation of widely used coding
models, under a unified scaffold, we observe that their performance on
SWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest
score to date at 23.3%. To better understand these limitations, we cluster the
failure modes observed in the collected agent trajectories for a clearer
characterization of the error patterns exhibited by current models. Overall,
SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully
captures the complexity and diversity of real-world software development,
advancing the pursuit of truly autonomous software engineering agents at a
professional level.

</details>


### [14] [Static Security Vulnerability Scanning of Proprietary and Open-Source Software: An Adaptable Process with Variants and Results](https://arxiv.org/abs/2509.16985)
*James J. Cusick*

Main category: cs.SE

TL;DR: 本文提出了一个端到端的软件漏洞扫描和修复流程，包含支持方法和工具，用于在DevSecOps环境中进行常规代码扫描和优先级修复。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞是软件开发组织实现安全目标的重要风险因素，特别是在使用专有或开源软件的技术环境中，需要系统化的方法来发现和修复漏洞。

Method: 提出了一个行业验证的通用流程，包括定制实例化、配置和执行常规代码扫描，使用一组精选工具进行漏洞检测和优先级修复，支持专有和开源应用。

Result: 该方法可以最小化调整实现，灵活应用于减少源代码漏洞、降低供应链风险，并改善新系统或遗留系统的安全状况。

Conclusion: 该方法在综合SDLC模型中的应用效果良好，未来可通过自动化和AI等先进技术进一步优化，为软件安全提供有效的解决方案。

Abstract: Software vulnerabilities remain a significant risk factor in achieving
security objectives within software development organizations. This is
especially true where either proprietary or open-source software (OSS) is
included in the technological environment. In this paper an end-to-end process
with supporting methods and tools is presented. This industry proven generic
process allows for the custom instantiation, configuration, and execution of
routinized code scanning for software vulnerabilities and their prioritized
remediation. A select set of tools are described for this key DevSecOps
function and placed into an iterative process. Examples of both industrial
proprietary applications and open-source applications are provided including
specific vulnerability instances and a discussion of their treatment. The
benefits of each selected tool are considered, and alternative tools are also
introduced. Application of this method in a comprehensive SDLC model is also
reviewed along with prospective enhancements from automation and the
application of advanced technologies including AI. Adoption of this method can
be achieved with minimal adjustments and with maximum flexibility for results
in reducing source code vulnerabilities, reducing supply chain risk, and
improving the security profile of new or legacy solutions.

</details>


### [15] [Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering](https://arxiv.org/abs/2509.17096)
*Ziyou Li,Agnia Sergeyuk,Maliheh Izadi*

Main category: cs.SE

TL;DR: Prompt-with-Me是一个实用的结构化提示管理系统，嵌入开发环境中，通过四维分类法自动分类提示，提供语言优化、敏感信息屏蔽和模板提取功能，提高软件工程中提示的可靠性和重用性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在软件工程中的应用日益广泛，但提示管理仍然随意，影响了可靠性、重用性和工业工作流的集成。需要一种结构化方法来改进提示管理。

Method: 开发了Prompt-with-Me系统，使用四维分类法（意图、作者角色、软件开发生命周期阶段、提示类型）自动分类提示，并提供语言优化、敏感信息屏蔽和可重用模板提取功能。通过对1108个真实世界提示的分类研究验证了方法的有效性。

Result: 分类研究表明现代LLM能够准确分类软件工程提示。用户研究（11名参与者）显示高可用性（平均SUS=73）、低认知负荷（平均NASA-TLX=21），参与者报告通过减少重复工作提高了提示质量和效率。

Conclusion: Prompt-with-Me为构建下一代软件工程工作流中的提示管理和维护工具提供了可行的见解，展示了结构化提示管理在提高开发效率和提示质量方面的潜力。

Abstract: Large Language Models are transforming software engineering, yet prompt
management in practice remains ad hoc, hindering reliability, reuse, and
integration into industrial workflows. We present Prompt-with-Me, a practical
solution for structured prompt management embedded directly in the development
environment. The system automatically classifies prompts using a
four-dimensional taxonomy encompassing intent, author role, software
development lifecycle stage, and prompt type. To enhance prompt reuse and
quality, Prompt-with-Me suggests language refinements, masks sensitive
information, and extracts reusable templates from a developer's prompt library.
Our taxonomy study of 1108 real-world prompts demonstrates that modern LLMs can
accurately classify software engineering prompts. Furthermore, our user study
with 11 participants shows strong developer acceptance, with high usability
(Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in
prompt quality and efficiency through reduced repetitive effort. Lastly, we
offer actionable insights for building the next generation of prompt management
and maintenance tools for software engineering workflows.

</details>


### [16] [Clotho: Measuring Task-Specific Pre-Generation Test Adequacy for LLM Inputs](https://arxiv.org/abs/2509.17314)
*Juyeon Yoon,Somin Kim,Robert Feldt,Shin Yoo*

Main category: cs.SE

TL;DR: CLOTHO是一种任务特定的预生成充分性度量方法，通过分析LLM隐藏状态来估计输入难度，无需生成输出即可预测LLM失败概率，显著降低测试成本。


<details>
  <summary>Details</summary>
Motivation: 当前测试LLM在特定任务上的表现存在困难：许多提示缺乏真实标签，依赖人工判断成本高，现有不确定性度量通常需要完整推理过程。需要一种能在生成输出前评估输入充分性的方法。

Method: 使用高斯混合模型(GMM)自适应采样最具信息量的案例进行人工标注，基于参考集对未见输入按失败可能性进行排序。该方法直接从LLM隐藏状态估计输入难度。

Result: 在8个基准任务和3个开源LLM上的评估显示，CLOTHO能以0.716的ROC-AUC预测失败，参考集标注量仅为输入的5.4%。相比随机优先级排序，可将专有模型的失败输入数量从18.7提升到42.5/100。

Conclusion: CLOTHO提供了一种高效、低成本的LLM测试方法，其充分性评分可从开源LLM有效迁移到专有模型，与后生成不确定性度量方法互补。

Abstract: Software increasingly relies on the emergent capabilities of Large Language
Models (LLMs), from natural language understanding to program analysis and
generation. Yet testing them on specific tasks remains difficult and costly:
many prompts lack ground truth, forcing reliance on human judgment, while
existing uncertainty and adequacy measures typically require full inference. A
key challenge is to assess input adequacy in a way that reflects the demands of
the task, ideally before even generating any output. We introduce CLOTHO, a
task-specific, pre-generation adequacy measure that estimates input difficulty
directly from hidden LLM states. Given a large pool of unlabelled inputs for a
specific task, CLOTHO uses a Gaussian Mixture Model (GMM) to adaptively sample
the most informative cases for human labelling. Based on this reference set the
GMM can then rank unseen inputs by their likelihood of failure. In our
empirical evaluation across eight benchmark tasks and three open-weight LLMs,
CLOTHO can predict failures with a ROC-AUC of 0.716, after labelling reference
sets that are on average only 5.4% of inputs. It does so without generating any
outputs, thereby reducing costs compared to existing uncertainty measures.
Comparison of CLOTHO and post-generation uncertainty measures shows that the
two approaches complement each other. Crucially, we show that adequacy scores
learnt from open-weight LLMs transfer effectively to proprietary models,
extending the applicability of the approach. When prioritising test inputs for
proprietary models, CLOTHO increases the average number of failing inputs from
18.7 to 42.5 out of 100, compared to random prioritisation.

</details>


### [17] [BASFuzz: Towards Robustness Evaluation of LLM-based NLP Software via Automated Fuzz Testing](https://arxiv.org/abs/2509.17335)
*Mingxuan Xiao,Yan Xiao,Shunhui Ji,Jiahe Tu,Pengcheng Zhang*

Main category: cs.SE

TL;DR: BASFuzz是一种针对基于LLM的NLP软件的高效模糊测试方法，通过文本一致性指标引导变异，采用波束退火搜索算法，在NLG和NLU场景中显著提升测试效果并降低时间开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临两个主要挑战：1）测试方法与基于LLM的NLP软件行为模式耦合不足；2）自然语言生成场景的模糊测试能力普遍下降。

Method: BASFuzz针对包含提示和示例的完整测试输入，使用文本一致性指标指导模糊测试循环的变异，采用集成波束搜索和模拟退火的波束退火搜索算法，并引入基于信息熵的自适应调整和精英策略。

Result: 在NLG和NLU的六个代表性数据集上评估，BASFuzz达到90.335%的测试效果，相比当前最佳基线平均减少2,163.852秒时间开销。

Conclusion: BASFuzz能够在软件部署前实现更有效的鲁棒性评估，为基于LLM的NLP软件提供高效的测试解决方案。

Abstract: Fuzzing has shown great success in evaluating the robustness of intelligent
natural language processing (NLP) software. As large language model (LLM)-based
NLP software is widely deployed in critical industries, existing methods still
face two main challenges: 1 testing methods are insufficiently coupled with the
behavioral patterns of LLM-based NLP software; 2 fuzzing capability for the
testing scenario of natural language generation (NLG) generally degrades. To
address these issues, we propose BASFuzz, an efficient Fuzz testing method
tailored for LLM-based NLP software. BASFuzz targets complete test inputs
composed of prompts and examples, and uses a text consistency metric to guide
mutations of the fuzzing loop, aligning with the behavioral patterns of
LLM-based NLP software. A Beam-Annealing Search algorithm, which integrates
beam search and simulated annealing, is employed to design an efficient fuzzing
loop. In addition, information entropy-based adaptive adjustment and an elitism
strategy further enhance fuzzing capability. We evaluate BASFuzz on six
datasets in representative scenarios of NLG and natural language understanding
(NLU). Experimental results demonstrate that BASFuzz achieves a testing
effectiveness of 90.335% while reducing the average time overhead by 2,163.852
seconds compared to the current best baseline, enabling more effective
robustness evaluation prior to software deployment.

</details>


### [18] [SLICET5: Static Program Slicing using Language Models with Copy Mechanism and Constrained Decoding](https://arxiv.org/abs/2509.17338)
*Pengfei He,Shaowei Wang,Tse-Hsun Chen*

Main category: cs.SE

TL;DR: 提出了一种基于轻量级语言模型的静态程序切片框架，通过复制机制和约束解码解决传统方法依赖完整代码解析和现有学习方法的依赖识别不准确、生成不受约束的问题。


<details>
  <summary>Details</summary>
Motivation: 传统静态切片工具需要完整可解析的源代码，而现实场景中代码片段往往不完整。现有学习方法存在依赖识别不准确和生成不受约束的问题。

Method: 将静态程序切片重构为序列到序列任务，使用CodeT5+等轻量级语言模型，引入复制机制准确捕获依赖关系，设计包含词汇约束和语法约束的约束解码过程。

Result: 在CodeNet和LeetCode数据集上评估，性能优于现有最佳基线，ExactMatch分数提升高达27%，在不完整代码上表现良好。

Conclusion: 该方法在现实开发环境中具有鲁棒性和实用性，解决了传统方法和现有学习方法的局限性。

Abstract: Static program slicing is a fundamental technique in software engineering.
Traditional static slicing tools rely on parsing complete source code, which
limits their applicability to real-world scenarios where code snippets are
incomplete or unparsable. While recent research developed learning-based
approaches to predict slices, they face critical challenges: (1) Inaccurate
dependency identification, where models fail to precisely capture data and
control dependencies between code elements; and (2) Unconstrained generation,
where models produce slices with extraneous or hallucinated tokens not present
in the input, violating the structural integrity of slices. To address these
challenges, we propose \ourtool, a novel slicing framework that reformulates
static program slicing as a sequence-to-sequence task using lightweight
language models (e.g., CodeT5+). Our approach incorporates two key innovations.
First, we introduce a copy mechanism that enables the model to more accurately
capture inter-element dependencies and directly copy relevant tokens from the
input, improving both dependency reasoning and generation constraint. Second,
we design a constrained decoding process with (a) lexical constraint,
restricting outputs to input tokens only, and (b) syntactic constraint,
leveraging Tree Similarity of Edit Distance (TSED) monotonicity to detect
structurally invalid outputs and discard them. We evaluate \ourtool on CodeNet
and LeetCode datasets and show it consistently outperforms state-of-the-art
baselines, improving ExactMatch scores by up to 27\%. Furthermore, \ourtool
demonstrates strong performance on incomplete code, highlighting its robustness
and practical utility in real-world development environments.

</details>


### [19] [Prompts as Software Engineering Artifacts: A Research Agenda and Preliminary Findings](https://arxiv.org/abs/2509.17548)
*Hugo Villamizar,Jannik Fischbach,Alexander Korn,Andreas Vogelsang,Daniel Mendez*

Main category: cs.SE

TL;DR: 本文提出了一个研究计划，旨在系统化地研究和管理软件工程中LLM提示的使用，包括当前实践特征分析、提示作为软件工件的分析以及制定基于证据的提示管理指南。


<details>
  <summary>Details</summary>
Motivation: 随着开发者频繁使用大型语言模型（LLMs）支持软件工程任务，提示已成为重要的软件工程工件，但目前缺乏对提示使用和管理的系统研究，不清楚系统化管理的收益是否超过成本。

Method: 提出三阶段研究计划：特征化当前提示实践与挑战；分析提示作为软件工件的演化、可追溯性和重用性；开发并实证评估基于证据的提示管理指南。首先对74名软件专业人员进行探索性调查。

Result: 调查发现软件工程中的提示使用主要是临时性的：提示通常通过试错法优化，很少被重用，更多依赖个人经验而非标准化实践。

Conclusion: 研究结果强调了需要更系统化的提示管理方法，并为后续研究阶段提供了实证基础。

Abstract: Developers now routinely interact with large language models (LLMs) to
support a range of software engineering (SE) tasks. This prominent role
positions prompts as potential SE artifacts that, like other artifacts, may
require systematic development, documentation, and maintenance. However, little
is known about how prompts are actually used and managed in LLM-integrated
workflows, what challenges practitioners face, and whether the benefits of
systematic prompt management outweigh the associated effort. To address this
gap, we propose a research programme that (a) characterizes current prompt
practices, challenges, and influencing factors in SE; (b) analyzes prompts as
software artifacts, examining their evolution, traceability, reuse, and the
trade-offs of systematic management; and (c) develops and empirically evaluates
evidence-based guidelines for managing prompts in LLM-integrated workflows. As
a first step, we conducted an exploratory survey with 74 software professionals
from six countries to investigate current prompt practices and challenges. The
findings reveal that prompt usage in SE is largely ad-hoc: prompts are often
refined through trial-and-error, rarely reused, and shaped more by individual
heuristics than standardized practices. These insights not only highlight the
need for more systematic approaches to prompt management but also provide the
empirical foundation for the subsequent stages of our research programme.

</details>


### [20] [From OCL to JSX: declarative constraint modeling in modern SaaS tools](https://arxiv.org/abs/2509.17629)
*Antonio Bucchiarone,Juri Di Rocco,Damiano Di Vincenzo,Alfonso Pierantonio*

Main category: cs.SE

TL;DR: 本文探讨了在SaaS建模环境中使用JSX作为约束表达式的替代方案，与OCL.js进行比较，发现JSX具有更广泛的表达能力和更好的前端架构适配性。


<details>
  <summary>Details</summary>
Motivation: 随着Node.js和前端框架的发展，低代码平台兴起，但现有的OCL.js在标准覆盖、采用率和与现代前端工具链集成方面存在挑战。

Method: 通过实证评估，在代表性建模场景中比较基于JSX的约束与OCL.js。

Result: 结果显示JSX提供了更广泛的表达能力，并且更适合前端优先的架构。

Conclusion: JSX为现代建模工具中的约束规范提供了一条有前景的路径。

Abstract: The rise of Node.js in 2010, followed by frameworks like Angular, React, and
Vue.js, has accelerated the growth of low code development platforms. These
platforms harness modern UIX paradigms, component-based architectures, and the
SaaS model to enable non-experts to build software. The widespread adoption of
single-page applications (SPAs), driven by these frameworks, has shaped
low-code tools to deliver responsive, client side experiences. In parallel,
many modeling platforms have moved to the cloud, adopting either server-centric
architectures (e.g., GSLP) or client-side intelligence via SPA frameworks,
anchoring core components in JavaScript or TypeScript. Within this context,
OCL.js, a JavaScript-based implementation of the Object Constraint Language,
offers a web aligned approach to model validation, yet faces challenges such as
partial standard coverage, limited adoption, and weak integration with modern
front-end toolchains. In this paper, we explore JSX, a declarative, functional
subset of JavaScript/TypeScript used in the React ecosystem, as an alternative
to constraint expression in SaaS-based modeling environments. Its
component-oriented structure supports inductive definitions for syntax, code
generation, and querying. Through empirical evaluation, we compare JSX-based
constraints with OCL.js across representative modeling scenarios. Results show
JSX provides broader expressiveness and better fits front-end-first
architectures, indicating a promising path for constraint specification in
modern modeling tools.

</details>


### [21] [Diagnosing Violations of State-based Specifications in iCFTL](https://arxiv.org/abs/2509.17776)
*Cristina Stratan,Claudio Mandrioli,Domenico Bianculli*

Main category: cs.SE

TL;DR: 本文提出了一种基于反向数据流分析的诊断方法，用于为违反iCFTL规范的软件生成信息丰富的诊断结果，通过程序插桩和运行时分析识别导致规范违反的相关语句。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性和动态性的增加，运行时验证技术变得至关重要。然而，当规范被违反时，传统的布尔或定量判决往往不足以理解违反的原因，需要更丰富的诊断信息。

Method: 采用基于反向数据流分析的静态分析方法确定导致规范违反的相关语句，通过程序插桩生成增强的执行轨迹，然后进行运行时分析识别具体违反语句。

Result: 在10个软件项目的112个规范上评估，工具在100个规范上达到90%的精确度，将需要检查的代码行数减少至少90%，诊断生成时间在7分钟内，内存使用不超过25MB，插桩带来的执行时间开销低于30%，内存开销低于20%。

Conclusion: 提出的iCFTL-Diagnostics工具能够有效生成信息丰富的诊断结果，显著减少调试工作量，同时保持较低的计算开销，为复杂软件系统的运行时验证提供了实用的诊断解决方案。

Abstract: As modern software systems grow in complexity and operate in dynamic
environments, the need for runtime analysis techniques becomes a more critical
part of the verification and validation process. Runtime verification monitors
the runtime system behaviour by checking whether an execution trace - a
sequence of recorded events - satisfies a given specification, yielding a
Boolean or quantitative verdict. However, when a specification is violated,
such a verdict is often insufficient to understand why the violation happened.
To fill this gap, diagnostics approaches aim to produce more informative
verdicts. In this paper, we address the problem of generating informative
verdicts for violated Inter-procedural Control-Flow Temporal Logic (iCFTL)
specifications that express constraints over program variable values. We
propose a diagnostic approach based on backward data-flow analysis to
statically determine the relevant statements contributing to the specification
violation. Using this analysis, we instrument the program to produce enriched
execution traces. Using the enriched execution traces, we perform the runtime
analysis and identify the statements whose execution led to the specification
violation. We implemented our approach in a prototype tool, iCFTL-Diagnostics,
and evaluated it on 112 specifications across 10 software projects. Our tool
achieves 90% precision in identifying relevant statements for 100 of the 112
specifications. It reduces the number of lines that have to be inspected for
diagnosing a violation by at least 90%. In terms of computational cost,
iCFTL-Diagnostics generates a diagnosis within 7 min, and requires no more than
25 MB of memory. The instrumentation required to support diagnostics incurs an
execution time overhead of less than 30% and a memory overhead below 20%.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [Reconnecting Citizens to Politics via Blockchain - Starting the Debate](https://arxiv.org/abs/2509.16274)
*Uwe Serdült*

Main category: cs.CR

TL;DR: 本文探讨利用区块链技术创建专门用于政治竞选和广告费用的加密货币，以解决选举资金透明度问题。


<details>
  <summary>Details</summary>
Motivation: 选举资金透明度是自由民主国家面临的重要挑战，当前方法难以有效解决选举资金滥用问题，频繁的丑闻证明了现有方法的局限性。

Method: 提出使用区块链技术创建专门的加密货币，专门用于支付政治竞选和广告费用，通过技术手段提高资金流动的透明度和可追溯性。

Result: 目前该想法仍处于探索阶段，存在许多未解决的问题，但区块链技术的持久性为这一方案提供了可行性基础。

Conclusion: 虽然实施面临挑战，但在区块链技术持续发展的背景下，这一创新思路值得进一步研究和探索，可能为解决选举资金透明度问题提供新的技术路径。

Abstract: Elections are not the only but arguably one of the most important pillars for
the proper functioning of liberal democracies. Recent evidence across the globe
shows that it is not straightforward to conduct them in a free and fair manner.
One constant concern is the role of money in politics, more specifically,
election campaign financing. Frequent scandals are proof of the difficulties
encountered with current approaches to tackle the issue. Suggestions on how to
overcome the problem exist but seem difficult to implement. With the help of
blockchain technology we might be able to make a step forward. A separate
crypto currency specifically designed to pay for costs of political campaigning
and advertising could be introduced. Admittedly, at this stage, there are many
open questions. However, under the assumption that blockchain technology is
here to stay, it is an idea that deserves further exploration.

</details>


### [23] [SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair](https://arxiv.org/abs/2509.16275)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy,Relsy Puthal,Kaustik Ranaware*

Main category: cs.CR

TL;DR: SecureFixAgent是一个结合静态分析工具Bandit和轻量级本地LLM的混合修复框架，通过迭代的检测-修复-验证循环来减少误报并提高修复准确性。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发面临大型代码库安全挑战，静态分析工具误报率高且缺乏修复能力，而LLM虽然能建议修复但容易产生幻觉变化且缺乏自我验证。

Method: 集成Bandit进行漏洞检测，使用轻量级本地LLM（<8B参数）生成候选修复和解释，通过Bandit重新验证进行确认，采用LoRA微调减少数据集偏差。

Result: 实验显示SecureFixAgent比静态分析减少10.8%误报，修复准确率提高13.51%，比预训练LLM降低5.46%误报，通常在3次迭代内收敛。

Conclusion: 该框架通过结合可验证的安全改进和透明解释，以资源高效的本地方式推进可信赖的自动化漏洞修复。

Abstract: Modern software development pipelines face growing challenges in securing
large codebases with extensive dependencies. Static analysis tools like Bandit
are effective at vulnerability detection but suffer from high false positives
and lack repair capabilities. Large Language Models (LLMs), in contrast, can
suggest fixes but often hallucinate changes and lack self-validation. We
present SecureFixAgent, a hybrid repair framework integrating Bandit with
lightweight local LLMs (<8B parameters) in an iterative detect-repair-validate
loop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning
on a diverse, curated dataset spanning multiple Python project domains,
mitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses
Bandit for detection, the LLM for candidate fixes with explanations, and Bandit
re-validation for verification, all executed locally to preserve privacy and
reduce cloud reliance. Experiments show SecureFixAgent reduces false positives
by 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers
false positives by 5.46% compared to pre-trained LLMs, typically converging
within three iterations. Beyond metrics, developer studies rate explanation
quality 4.5/5, highlighting its value for human trust and adoption. By
combining verifiable security improvements with transparent rationale in a
resource-efficient local framework, SecureFixAgent advances trustworthy,
automated vulnerability remediation for modern pipelines.

</details>


### [24] [Decoding TRON: A Comprehensive Framework for Large-Scale Blockchain Data Extraction and Exploration](https://arxiv.org/abs/2509.16292)
*Qian'ang Mao,Jiaxin Wang,Zhiqi Feng,Yi Zhang,Jiaqi Yan*

Main category: cs.CR

TL;DR: 本文提出了一个针对TRON区块链的全面数据提取和探索框架，通过高性能ETL系统提取原始链上数据，深入分析TRON生态系统的特征和模式。


<details>
  <summary>Details</summary>
Motivation: 尽管TRON在稳定币支付和结算领域很受欢迎，但对其链上数据的分析研究非常稀缺，需要填补这一研究空白。

Method: 设计创新的高性能ETL系统，高效提取TRON原始链上数据（包括区块、交易、智能合约和收据），建立研究数据集。

Result: 分析揭示了TRON的区块生成、交易趋势、交易所主导地位、资源委托市场、智能合约使用模式以及USDT稳定币的核心作用，特别强调了赌博应用和与USDT相关的潜在非法活动。

Conclusion: 该研究增强了区块链数据管理能力，加深了对快速发展的TRON生态系统的理解，并为未来研究提供了机会，包括委托服务分析、赌博场景、稳定币活动和非法交易检测等。

Abstract: Cryptocurrencies and Web3 applications based on blockchain technology have
flourished in the blockchain research field. Unlike Bitcoin and Ethereum, due
to its unique architectural designs in consensus mechanisms, resource
management, and throughput, TRON has developed a more distinctive ecosystem and
application scenarios centered around stablecoins. Although it is popular in
areas like stablecoin payments and settlement, research on analyzing on-chain
data from the TRON blockchain is remarkably scarce. To fill this gap, this
paper proposes a comprehensive data extraction and exploration framework for
the TRON blockchain. An innovative high-performance ETL system aims to
efficiently extract raw on-chain data from TRON, including blocks,
transactions, smart contracts, and receipts, establishing a research dataset.
An in-depth analysis of the extracted dataset reveals insights into TRON's
block generation, transaction trends, the dominance of exchanges, the resource
delegation market, smart contract usage patterns, and the central role of the
USDT stablecoin. The prominence of gambling applications and potential illicit
activities related to USDT is emphasized. The paper discusses opportunities for
future research leveraging this dataset, including analysis of delegate
services, gambling scenarios, stablecoin activities, and illicit transaction
detection. These contributions enhance blockchain data management capabilities
and understanding of the rapidly evolving TRON ecosystem.

</details>


### [25] [To Unpack or Not to Unpack: Living with Packers to Enable Dynamic Analysis of Android Apps](https://arxiv.org/abs/2509.16340)
*Mohammad Hossein Asghari,Lianying Zhao*

Main category: cs.CR

TL;DR: 本文提出Purifire，一种基于eBPF的规避引擎，用于绕过Android打包器的反分析技术，实现对打包应用的有效动态分析，而无需解包。


<details>
  <summary>Details</summary>
Motivation: Android打包器广泛用于保护应用，但也阻碍了安全分析，现有解包工具对新兴商业打包器无效且解包后应用无法运行。

Method: 基于eBPF内核特性构建Purifire引擎，提供对用户空间应用的观察性和隐身性，通过定义规避规则绕过打包器的反分析检查。

Result: 评估显示Purifire能有效绕过打包器的反分析检查，显著提升先前研究的检测效果（如设备指纹检测数量大幅增加）。

Conclusion: Purifire为解决打包器阻碍动态分析的问题提供了有效方案，基于eBPF的方法具有低开销和高隐蔽性的优势。

Abstract: Android apps have become a valuable target for app modifiers and imitators
due to its popularity and being trusted with highly sensitive data. Packers, on
the other hand, protect apps from tampering with various anti-analysis
techniques embedded in the app. Meanwhile, packers also conceal certain
behavior potentially against the interest of the users, aside from being abused
by malware for stealth. Security practitioners typically try to capture
undesired behavior at runtime with hooking (e.g., Frida) or debugging
techniques, which are heavily affected by packers. Unpackers have been the
community's continuous effort to address this, but due to the emerging
commercial packers, our study shows that none of the unpackers remain
effective, and they are unfit for this purpose as unpacked apps can no longer
run. We first perform a large-scale prevalence analysis of Android packers with
a real-world dataset of 12,341 apps, the first of its kind, to find out what
percentage of Android apps are actually packed and to what extent dynamic
analysis is hindered. We then propose Purifire, an evasion engine to bypass
packers' anti-analysis techniques and enable dynamic analysis on packed apps
without unpacking them. Purifire is based on eBPF, a low-level kernel feature,
which provides observability and invisibility to userspace apps to enforce
defined evasion rules while staying low-profile. Our evaluation shows that
Purifire is able to bypass packers' anti-analysis checks and more importantly,
for previous research works suffering from packers, we observe a significant
improvement (e.g., a much higher number of detected items such as device
fingerprints).

</details>


### [26] [Secure Confidential Business Information When Sharing Machine Learning Models](https://arxiv.org/abs/2509.16352)
*Yunfan Yang,Jiarong Xu,Hongzhe Zhang,Xiao Fang*

Main category: cs.CR

TL;DR: 本文提出了一种针对响应式机密属性推断攻击的新型防御方法，通过模拟真实世界对手的响应行为，构建攻击-防御军备竞赛框架，有效保护模型共享中的数据机密性。


<details>
  <summary>Details</summary>
Motivation: 模型共享具有重要商业价值，但机密属性推断攻击会泄露模型训练数据中的机密信息。现有防御方法假设攻击是非自适应的，忽略了真实对手的响应能力。

Method: 提出响应式CPI攻击模拟真实对手行为，构建攻击-防御军备竞赛框架迭代增强模型安全性，并引入近似策略解决计算瓶颈问题。

Result: 通过多个真实模型共享场景的实证评估，该方法在防御CPI攻击、保持模型效用和降低计算开销方面优于现有防御方法。

Conclusion: 该方法有效解决了响应式CPI攻击威胁，为安全的模型共享提供了实用解决方案，显著提升了防御效果和效率。

Abstract: Model-sharing offers significant business value by enabling firms with
well-established Machine Learning (ML) models to monetize and share their
models with others who lack the resources to develop ML models from scratch.
However, concerns over data confidentiality remain a significant barrier to
model-sharing adoption, as Confidential Property Inference (CPI) attacks can
exploit shared ML models to uncover confidential properties of the model
provider's private model training data. Existing defenses often assume that CPI
attacks are non-adaptive to the specific ML model they are targeting. This
assumption overlooks a key characteristic of real-world adversaries: their
responsiveness, i.e., adversaries' ability to dynamically adjust their attack
models based on the information of the target and its defenses. To overcome
this limitation, we propose a novel defense method that explicitly accounts for
the responsive nature of real-world adversaries via two methodological
innovations: a novel Responsive CPI attack and an attack-defense arms race
framework. The former emulates the responsive behaviors of adversaries in the
real world, and the latter iteratively enhances both the target and attack
models, ultimately producing a secure ML model that is robust against
responsive CPI attacks. Furthermore, we propose and integrate a novel
approximate strategy into our defense, which addresses a critical computational
bottleneck of defense methods and improves defense efficiency. Through
extensive empirical evaluations across various realistic model-sharing
scenarios, we demonstrate that our method outperforms existing defenses by more
effectively defending against CPI attacks, preserving ML model utility, and
reducing computational overhead.

</details>


### [27] [LiteRSan: Lightweight Memory Safety Via Rust-specific Program Analysis and Selective Instrumentation](https://arxiv.org/abs/2509.16389)
*Tianrou Xia,Kaiming Huang,Dongyeon Yu,Yuseok Jeon,Jie Zhou,Dinghao Wu,Taegyu Kim*

Main category: cs.CR

TL;DR: LiteRSan是一种新型内存安全检测器，通过利用Rust的所有权模型进行静态分析，选择性地检测风险指针，显著降低了运行时和内存开销，并能检测现有ASan工具遗漏的内存安全漏洞。


<details>
  <summary>Details</summary>
Motivation: Rust虽然内存安全，但允许使用unsafe代码绕过安全检查。现有的ASan工具存在性能开销大、内存占用高的问题，且无法检测某些类型的内存安全漏洞。

Method: 利用Rust独特的所有权模型进行静态分析，识别风险指针，然后选择性地对风险指针进行空间或时间内存安全检查的插桩。

Result: 与现有ASan工具相比，LiteRSan显著降低了运行时开销（18.84% vs 152.05%和183.50%）和内存开销（0.81% vs 739.27%和861.98%），并能检测到之前技术遗漏的内存安全漏洞。

Conclusion: LiteRSan通过Rust特定的静态分析和选择性插桩，有效解决了现有内存安全检测工具的性能和检测能力限制问题。

Abstract: Rust is a memory-safe language, and its strong safety guarantees combined
with high performance have been attracting widespread adoption in systems
programming and security-critical applications. However, Rust permits the use
of unsafe code, which bypasses compiler-enforced safety checks and can
introduce memory vulnerabilities. A widely adopted approach for detecting
memory safety bugs in Rust is Address Sanitizer (ASan). Optimized versions,
such as ERASan and RustSan, have been proposed to selectively apply security
checks in order to reduce performance overhead. However, these tools still
incur significant performance and memory overhead and fail to detect many
classes of memory safety vulnerabilities due to the inherent limitations of
ASan. In this paper, we present LiteRSan, a novel memory safety sanitizer that
addresses the limitations of prior approaches. By leveraging Rust's unique
ownership model, LiteRSan performs Rust-specific static analysis that is aware
of pointer lifetimes to identify risky pointers. It then selectively
instruments risky pointers to enforce only the necessary spatial or temporal
memory safety checks. Consequently, LiteRSan introduces significantly lower
runtime overhead (18.84% versus 152.05% and 183.50%) and negligible memory
overhead (0.81% versus 739.27% and 861.98%) compared with existing ASan-based
sanitizers while being capable of detecting memory safety bugs that prior
techniques miss.

</details>


### [28] [B5GRoam: A Zero Trust Framework for Secure and Efficient On-Chain B5G Roaming](https://arxiv.org/abs/2509.16390)
*Mohamed Abdessamed Rezazi,Mouhamed Amine Bouchiha,Ahmed Mounsf Rafik Bendada,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: B5GRoam是一个基于区块链的5G漫游结算框架，通过零知识证明和Layer 2技术解决现有方案的数据隐私、信任假设和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 5G及未来网络中的漫游结算需要安全、高效、可信的计费对账机制。现有区块链方案存在数据隐私风险、相互信任假设和可扩展性瓶颈等关键限制。

Method: B5GRoam采用密码学可验证的通话详单提交协议，集成非交互式零知识证明（zkSNARKs）实现链上验证而不暴露敏感数据，并利用Layer 2 zk-Rollups技术满足高吞吐量需求。

Result: 实验结果显示系统吞吐量超过7,200 tx/s，具有强隐私保护和显著的成本节约。

Conclusion: B5GRoam通过消除中介机构和增强可验证性，为未来移动网络的去中心化漫游提供了实用且安全的基础。

Abstract: Roaming settlement in 5G and beyond networks demands secure, efficient, and
trustworthy mechanisms for billing reconciliation between mobile operators.
While blockchain promises decentralization and auditability, existing solutions
suffer from critical limitations-namely, data privacy risks, assumptions of
mutual trust, and scalability bottlenecks. To address these challenges, we
present B5GRoam, a novel on-chain and zero-trust framework for secure,
privacy-preserving, and scalable roaming settlements. B5GRoam introduces a
cryptographically verifiable call detail record (CDR) submission protocol,
enabling smart contracts to authenticate usage claims without exposing
sensitive data. To preserve privacy, we integrate non-interactive
zero-knowledge proofs (zkSNARKs) that allow on-chain verification of roaming
activity without revealing user or network details. To meet the high-throughput
demands of 5G environments, B5GRoam leverages Layer 2 zk-Rollups, significantly
reducing gas costs while maintaining the security guarantees of Layer 1.
Experimental results demonstrate a throughput of over 7,200 tx/s with strong
privacy and substantial cost savings. By eliminating intermediaries and
enhancing verifiability, B5GRoam offers a practical and secure foundation for
decentralized roaming in future mobile networks.

</details>


### [29] [LenslessMic: Audio Encryption and Authentication via Lensless Computational Imaging](https://arxiv.org/abs/2509.16418)
*Petr Grinberg,Eric Bezzam,Paolo Prandoni,Martin Vetterli*

Main category: cs.CR

TL;DR: LenslessMic是一种基于光学硬件的音频加密方法，使用无透镜相机作为物理安全层，能够实现音频认证和强加密，同时保持高质量信号。


<details>
  <summary>Details</summary>
Motivation: 随着社会对数字数据共享的依赖增加，敏感信息保护变得至关重要。现有音频加密主要依赖信号处理或软件方法，需要更安全的硬件级解决方案。

Method: 采用无透镜相机作为物理安全层的混合光学硬件加密方法，适用于多种音频类型，通过低成本Raspberry Pi原型实现。

Result: LenslessMic能够实现音频记录的鲁棒认证，加密强度可与256位数字标准相媲美，同时保持高质量信号和最小内容信息损失。

Conclusion: 该方法通过低成本原型验证，并开源了相关数据集，为音频安全研究提供了新的硬件级解决方案。

Abstract: With society's increasing reliance on digital data sharing, the protection of
sensitive information has become critical. Encryption serves as one of the
privacy-preserving methods; however, its realization in the audio domain
predominantly relies on signal processing or software methods embedded into
hardware. In this paper, we introduce LenslessMic, a hybrid optical
hardware-based encryption method that utilizes a lensless camera as a physical
layer of security applicable to multiple types of audio. We show that
LenslessMic enables (1) robust authentication of audio recordings and (2)
encryption strength that can rival the search space of 256-bit digital
standards, while maintaining high-quality signals and minimal loss of content
information. The approach is validated with a low-cost Raspberry Pi prototype
and is open-sourced together with datasets to facilitate research in the area.

</details>


### [30] [End-to-End Co-Simulation Testbed for Cybersecurity Research and Development in Intelligent Transportation Systems](https://arxiv.org/abs/2509.16489)
*Minhaj Uddin Ahmad,Akid Abrar,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.CR

TL;DR: 本章介绍了一个集成的协同仿真测试平台，结合CARLA、SUMO和OMNeT++，用于智能交通系统的网络安全评估，并通过案例研究展示了其在提升ITS安全性和弹性方面的能力。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统（ITS）的广泛部署扩大了网络攻击面，但全面的网络安全评估成本高昂且耗时。仿真平台提供了一种经济高效的方法来评估ITS的网络安全。

Method: 开发了一个集成的协同仿真测试平台，结合CARLA进行3D环境和传感器建模，SUMO进行微观交通仿真和控制，OMNeT++进行V2X通信仿真。

Result: 该测试平台支持端到端实验、漏洞识别和缓解基准测试，为开发安全、高效和弹性的ITS基础设施提供了实用见解。

Conclusion: 通过案例研究（C-V2X主动安全警报系统增强后量子密码学）展示了测试平台在推进安全弹性ITS基础设施方面的作用。

Abstract: Intelligent Transportation Systems (ITS) have been widely deployed across
major metropolitan regions worldwide to improve roadway safety, optimize
traffic flow, and reduce environmental impacts. These systems integrate
advanced sensors, communication networks, and data analytics to enable
real-time traffic monitoring, adaptive signal control, and predictive
maintenance. However, such integration significantly broadens the ITS attack
surface, exposing critical infrastructures to cyber threats that jeopardize
safety, data integrity, and operational resilience. Ensuring robust
cybersecurity is therefore essential, yet comprehensive vulnerability
assessments, threat modeling, and mitigation validations are often
cost-prohibitive and time-intensive when applied to large-scale, heterogeneous
transportation systems. Simulation platforms offer a cost-effective and
repeatable means for cybersecurity evaluation, and the simulation platform
should encompass the full range of ITS dimensions - mobility, sensing,
networking, and applications. This chapter discusses an integrated
co-simulation testbed that links CARLA for 3D environment and sensor modeling,
SUMO for microscopic traffic simulation and control, and OMNeT++ for V2X
communication simulation. The co-simulation testbed enables end-to-end
experimentation, vulnerability identification, and mitigation benchmarking,
providing practical insights for developing secure, efficient, and resilient
ITS infrastructures. To illustrate its capabilities, the chapter incorporates a
case study on a C-V2X proactive safety alert system enhanced with post-quantum
cryptography, highlighting the role of the testbed in advancing secure and
resilient ITS infrastructures.

</details>


### [31] [Train to Defend: First Defense Against Cryptanalytic Neural Network Parameter Extraction Attacks](https://arxiv.org/abs/2509.16546)
*Ashley Kurian,Aydin Aysu*

Main category: cs.CR

TL;DR: 本文提出了首个针对密码分析参数提取攻击的防御机制，通过消除神经元独特性来防止攻击成功，采用提取感知训练方法在标准损失函数中加入正则化项，实现零推理开销。


<details>
  <summary>Details</summary>
Motivation: 神经网络作为重要知识产权，其参数保护对维持竞争优势、增强模型安全性和隐私至关重要。现有研究表明密码分析攻击已能扩展到更深层模型，因此需要有效的防御机制。

Method: 提出一种新颖的提取感知训练方法，在标准损失函数基础上增加正则化项，最小化层内神经元权重之间的距离，从而消除攻击所需的神经元独特性。

Result: 实验结果显示，防御机制仅导致模型准确率变化小于1%，在相同攻击设置下，受保护网络能持续抵御提取攻击，而未保护网络在14分钟到4小时内就会被提取。

Conclusion: 该防御方法有效且实用，在保持模型性能的同时显著提高了参数安全性，为神经网络知识产权保护提供了可行的解决方案。

Abstract: Neural networks are valuable intellectual property due to the significant
computational cost, expert labor, and proprietary data involved in their
development. Consequently, protecting their parameters is critical not only for
maintaining a competitive advantage but also for enhancing the model's security
and privacy. Prior works have demonstrated the growing capability of
cryptanalytic attacks to scale to deeper models. In this paper, we present the
first defense mechanism against cryptanalytic parameter extraction attacks. Our
key insight is to eliminate the neuron uniqueness necessary for these attacks
to succeed. We achieve this by a novel, extraction-aware training method.
Specifically, we augment the standard loss function with an additional
regularization term that minimizes the distance between neuron weights within a
layer. Therefore, the proposed defense has zero area-delay overhead during
inference. We evaluate the effectiveness of our approach in mitigating
extraction attacks while analyzing the model accuracy across different
architectures and datasets. When re-trained with the same model architecture,
the results show that our defense incurs a marginal accuracy change of less
than 1% with the modified loss function. Moreover, we present a theoretical
framework to quantify the success probability of the attack. When tested
comprehensively with prior attack settings, our defense demonstrated empirical
success for sustained periods of extraction, whereas unprotected networks are
extracted between 14 minutes to 4 hours.

</details>


### [32] [MoPE: A Mixture of Password Experts for Improving Password Guessing](https://arxiv.org/abs/2509.16558)
*Mingjian Duan,Ming Xu,Shenghao Zhang,Jiaheng Zhang,Weili Han*

Main category: cs.CR

TL;DR: MoPE是一个基于密码结构模式的混合专家框架，通过识别密码的结构特征并路由到专门的专家模型来改进密码猜测性能，相比现有方法在离线/在线场景中分别提升了38.80%和9.27%的破解率。


<details>
  <summary>Details</summary>
Motivation: 现有密码强度评估模型通常将密码统一处理，忽视了密码之间的结构差异，导致训练偏向于频繁出现的密码结构模式。密码作为复杂的短文本数据，应该以结构感知的方式处理。

Method: MoPE框架包含：(1)基于密码结构模式生成专门专家模型的新方法；(2)轻量级门控方法选择适当的专家模型输出可靠猜测。该方法利用了相似结构密码在潜在空间中倾向于聚集的观察。

Result: MoPE在离线和在线猜测场景中显著优于现有最先进基线方法，破解率分别提升高达38.80%和9.27%。还实现了基于MoPE的实时密码强度计，具有毫秒级响应延迟。

Conclusion: MoPE能够有效利用数据驱动模型的能力进行密码猜测，通过结构感知方法解决了传统模型的训练偏差问题，为密码安全评估提供了更精确的工具。

Abstract: Textual passwords remain a predominant authentication mechanism in web
security. To evaluate their strength, existing research has proposed several
data-driven models across various scenarios. However, these models generally
treat passwords uniformly, neglecting the structural differences among
passwords. This typically results in biased training that favors frequent
password structural patterns. To mitigate the biased training, we argue that
passwords, as a type of complex short textual data, should be processed in a
structure-aware manner by identifying their structural patterns and routing
them to specialized models accordingly. In this paper, we propose MoPE, a
Mixture of Password Experts framework, specifically designed to leverage the
structural patterns in passwords to improveguessing performance. Motivated by
the observation that passwords with similar structural patterns (e.g.,
fixed-length numeric strings) tend to cluster in high-density regions within
the latent space, our MoPE introduces: (1) a novel structure-based method for
generating specialized expert models; (2) a lightweight gate method to select
appropriate expert models to output reliable guesses, better aligned with the
high computational frequency of password guessing tasks. Our evaluation shows
that MoPE significantly outperforms existing state-of-the-art baselines in both
offline and online guessing scenarios, achieving up to 38.80% and 9.27%
improvement in cracking rate, respectively, showcasing that MoPE can
effectively exploit the capabilities of data-driven models for password
guessing. Additionally, we implement a real-time Password Strength Meter (PSM)
based on offline MoPE, assisting users in choosing stronger passwords more
precisely with millisecond-level response latency.

</details>


### [33] [Towards Cost-Effective ZK-Rollups: Modeling and Optimization of Proving Infrastructure](https://arxiv.org/abs/2509.16581)
*Mohsen Ahmadvand,Pedro Souto*

Main category: cs.CR

TL;DR: 本文提出了一种参数化成本模型，用于优化零知识rollup证明系统的经济可行性，通过Z3 SMT求解器找到成本最优配置，实验显示可降低高达70%的成本。


<details>
  <summary>Details</summary>
Motivation: 零知识rollup依赖于证明者在严格最终性和可用性约束下生成多步状态转换证明。随着rollup规模扩大，由于吞吐量增加、快速最终性需求、波动的gas价格和动态资源需求，保持经济可行性变得越来越困难。

Method: 基于Halo2证明系统，提出参数化成本模型捕捉rollup特定约束，将其表述为约束系统并使用Z3 SMT求解器寻找成本最优配置。实现模拟器检测延迟并估算运营成本。

Result: 通过提出的方法，实验显示可以实现高达70%的成本降低，证明系统能够跟上交易负载。

Conclusion: 该参数化成本模型有效解决了零知识rollup证明系统的经济可行性问题，为大规模rollup部署提供了实用的成本优化方案。

Abstract: Zero-knowledge rollups rely on provers to generate multi-step state
transition proofs under strict finality and availability constraints. These
steps require expensive hardware (e.g., GPUs), and finality is reached only
once all stages complete and results are posted on-chain. As rollups scale,
staying economically viable becomes increasingly difficult due to rising
throughput, fast finality demands, volatile gas prices, and dynamic resource
needs. We base our study on Halo2-based proving systems and identify
transactions per second (TPS), average gas usage, and finality time as key cost
drivers. To address this, we propose a parametric cost model that captures
rollup-specific constraints and ensures provers can keep up with incoming
transaction load. We formulate this model as a constraint system and solve it
using the Z3 SMT solver to find cost-optimal configurations. To validate our
approach, we implement a simulator that detects lag and estimates operational
costs. Our method shows a potential cost reduction of up to 70\%.

</details>


### [34] [Reproducing a Security Risk Assessment Using Computer Aided Design](https://arxiv.org/abs/2509.16593)
*Avi Shaked*

Main category: cs.CR

TL;DR: 本文应用基于模型的安全设计工具来复现先前报道的安全评估，比较计算机辅助应用与非计算机辅助应用的优势。


<details>
  <summary>Details</summary>
Motivation: 当前安全风险评估方法多为"纸笔"实现，容易出错且不一致。计算机辅助设计方法可以使安全风险评估更加严谨和可持续。

Method: 使用基于模型的安全设计工具来复现先前报道的安全评估，并进行比较分析。

Result: 展示了计算机辅助设计方法在分析报告和评估系统方面的潜在优势。

Conclusion: 计算机辅助设计方法对工业从业者和研究人员都有价值，可以提高安全风险评估的严谨性和可持续性。

Abstract: Security risk assessment is essential in establishing the trustworthiness and
reliability of modern systems. While various security risk assessment
approaches exist, prevalent applications are "pen and paper" implementations
that -- even if performed digitally using computers -- remain prone to
authoring mistakes and inconsistencies. Computer-aided design approaches can
transform security risk assessments into more rigorous and sustainable efforts.
This is of value to both industrial practitioners and researchers, who practice
security risk assessments to reflect on systems' designs and to contribute to
the discipline's state-of-the-art. In this article, we report the application
of a model-based security design tool to reproduce a previously reported
security assessment. The main contributions are: 1) an independent attempt to
reproduce a refereed article describing a real security risk assessment of a
system; 2) comparison of a new computer-aided application with a previous
non-computer-aided application, based on a published, real-world case study; 3)
a showcase for the potential advantages -- for both practitioners and
researchers -- of using computer-aided design approaches to analyze reports and
to assess systems.

</details>


### [35] [Delving into Cryptanalytic Extraction of PReLU Neural Networks](https://arxiv.org/abs/2509.16620)
*Yi Chen,Xiaoyang Dong,Ruijie Ma,Yantian Shen,Anyu Wang,Hongbo Yu,Xiaoyun Wang*

Main category: cs.CR

TL;DR: 本文首次提出了针对PReLU神经网络的模型提取攻击方法，填补了该领域30多年来主要关注ReLU网络的空白。


<details>
  <summary>Details</summary>
Motivation: 模型提取问题自1991年提出以来，过去30多年的研究主要集中在ReLU神经网络上。PReLU网络使用更复杂的非线性激活函数，但缺乏相应的密码分析研究。

Method: 提出了基于原始输出的PReLU网络参数恢复攻击方法，并扩展到仅能访问top-m概率分数的限制场景。通过端到端实验在多种PReLU网络上进行验证，包括MNIST数据集训练的模型。

Result: 在三种不同的攻击场景下成功实现了PReLU神经网络的提取，这是该领域的首次实际演示。

Conclusion: 这项工作为PReLU神经网络的密码分析开辟了新方向，证明了复杂激活函数网络同样面临模型提取的安全威胁。

Abstract: The machine learning problem of model extraction was first introduced in 1991
and gained prominence as a cryptanalytic challenge starting with Crypto 2020.
For over three decades, research in this field has primarily focused on
ReLU-based neural networks. In this work, we take the first step towards the
cryptanalytic extraction of PReLU neural networks, which employ more complex
nonlinear activation functions than their ReLU counterparts. We propose a raw
output-based parameter recovery attack for PReLU networks and extend it to more
restrictive scenarios where only the top-m probability scores are accessible.
Our attacks are rigorously evaluated through end-to-end experiments on diverse
PReLU neural networks, including models trained on the MNIST dataset. To the
best of our knowledge, this is the first practical demonstration of PReLU
neural network extraction across three distinct attack scenarios.

</details>


### [36] ["Digital Camouflage": The LLVM Challenge in LLM-Based Malware Detection](https://arxiv.org/abs/2509.16671)
*Ekin Böke,Simon Torka*

Main category: cs.CR

TL;DR: 本研究评估了ChatGPT-4o、Gemini Flash 2.5和Claude Sonnet 4三种先进LLM在编译器级混淆技术下的鲁棒性，发现这些模型对混淆代码的分类准确率显著下降。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在恶意软件检测方面显示出潜力，但其在对抗性编译器级混淆下的可靠性尚未被充分研究，需要评估这些模型在面对代码混淆技术时的表现。

Method: 使用LLVM基础设施实现控制流平坦化、虚假控制流注入、指令替换和基本块分割等混淆技术，对Devign数据集中的40个C函数（20个易受攻击，20个安全）进行结构化评估。

Result: 结果显示这些模型经常无法正确分类混淆后的代码，精确率、召回率和F1分数在转换后显著下降，表明LLM容易被基于编译器的混淆策略误导。

Conclusion: LLM在语言理解能力方面存在关键局限性，容易被编译器级混淆技术欺骗。研究提出了软件水印、编译器感知防御和抗混淆模型设计等未来研究方向。

Abstract: Large Language Models (LLMs) have emerged as promising tools for malware
detection by analyzing code semantics, identifying vulnerabilities, and
adapting to evolving threats. However, their reliability under adversarial
compiler-level obfuscation is yet to be discovered. In this study, we
empirically evaluate the robustness of three state-of-the-art LLMs: ChatGPT-4o,
Gemini Flash 2.5, and Claude Sonnet 4 against compiler-level obfuscation
techniques implemented via the LLVM infrastructure. These include control flow
flattening, bogus control flow injection, instruction substitution, and split
basic blocks, which are widely used to evade detection while preserving
malicious behavior. We perform a structured evaluation on 40~C functions (20
vulnerable, 20 secure) sourced from the Devign dataset and obfuscated using
LLVM passes. Our results show that these models often fail to correctly
classify obfuscated code, with precision, recall, and F1-score dropping
significantly after transformation. This reveals a critical limitation: LLMs,
despite their language understanding capabilities, can be easily misled by
compiler-based obfuscation strategies. To promote reproducibility, we release
all evaluation scripts, prompts, and obfuscated code samples in a public
repository. We also discuss the implications of these findings for adversarial
threat modeling, and outline future directions such as software watermarking,
compiler-aware defenses, and obfuscation-resilient model design.

</details>


### [37] [Design and Development of an Intelligent LLM-based LDAP Honeypot](https://arxiv.org/abs/2509.16682)
*Javier Jiménez-Román,Florina Almenares-Mendoza,Alfonso Sánchez-Macián*

Main category: cs.CR

TL;DR: 本文提出了一种基于大语言模型（LLM）的LDAP蜜罐设计，旨在解决传统蜜罐在动态场景中适应性差和配置复杂的问题，通过AI技术提升网络安全防御能力。


<details>
  <summary>Details</summary>
Motivation: 网络安全威胁日益增加，传统蜜罐存在刚性和配置复杂的问题，无法适应动态攻击场景。人工智能特别是大语言模型的发展为开发更灵活、易用的欺骗工具提供了新机遇。

Method: 设计和实现基于LLM的蜜罐来模拟LDAP服务器，利用大语言模型的自然语言处理能力与攻击者进行逼真交互，收集攻击者的战术和方法信息。

Result: 提出的解决方案能够提供灵活且逼真的交互体验，增强对LDAP服务攻击的早期检测和威胁分析能力。

Conclusion: 基于LLM的蜜罐技术代表了网络安全防御的新方向，能够有效提升基础设施对LDAP协议攻击的防御能力，具有重要的实践价值。

Abstract: Cybersecurity threats continue to increase, with a growing number of
previously unknown attacks each year targeting both large corporations and
smaller entities. This scenario demands the implementation of advanced security
measures, not only to mitigate damage but also to anticipate emerging attack
trends. In this context, deception tools have become a key strategy, enabling
the detection, deterrence, and deception of potential attackers while
facilitating the collection of information about their tactics and methods.
Among these tools, honeypots have proven their value, although they have
traditionally been limited by rigidity and configuration complexity, hindering
their adaptability to dynamic scenarios. The rise of artificial intelligence,
and particularly general-purpose Large Language Models (LLMs), is driving the
development of new deception solutions capable of offering greater adaptability
and ease of use. This work proposes the design and implementation of an
LLM-based honeypot to simulate an LDAP server, a critical protocol present in
most organizations due to its central role in identity and access management.
The proposed solution aims to provide a flexible and realistic tool capable of
convincingly interacting with attackers, thereby contributing to early
detection and threat analysis while enhancing the defensive capabilities of
infrastructures against intrusions targeting this service.

</details>


### [38] [Evaluating LLM Generated Detection Rules in Cybersecurity](https://arxiv.org/abs/2509.16749)
*Anna Bertiger,Bobby Filar,Aryan Luthra,Stefano Meschiari,Aiden Mitchell,Sam Scholten,Vivek Sharath*

Main category: cs.CR

TL;DR: 提出了一个开源评估框架和基准指标，用于评估LLM生成的网络安全规则的有效性，通过与人工生成规则对比来衡量LLM在安全领域的实用性。


<details>
  <summary>Details</summary>
Motivation: LLM在安全环境中应用日益普及，但缺乏有效的评估指标，限制了安全从业者对LLM的信任和实际应用价值。

Method: 采用保留集方法，将LLM生成的网络安全规则与人工生成的规则语料库进行比较，提供三个关键评估指标。

Result: 使用Sublime Security检测团队的规则和其自动化检测工程师(ADE)编写的规则进行验证，并在结果部分详细分析了ADE的能力。

Conclusion: 该框架为LLM生成的安全规则提供了现实、多方面的有效性评估，有助于提升LLM在网络安全领域的可信度和实用性。

Abstract: LLMs are increasingly pervasive in the security environment, with limited
measures of their effectiveness, which limits trust and usefulness to security
practitioners. Here, we present an open-source evaluation framework and
benchmark metrics for evaluating LLM-generated cybersecurity rules. The
benchmark employs a holdout set-based methodology to measure the effectiveness
of LLM-generated security rules in comparison to a human-generated corpus of
rules. It provides three key metrics inspired by the way experts evaluate
security rules, offering a realistic, multifaceted evaluation of the
effectiveness of an LLM-based security rule generator. This methodology is
illustrated using rules from Sublime Security's detection team and those
written by Sublime Security's Automated Detection Engineer (ADE), with a
thorough analysis of ADE's skills presented in the results section.

</details>


### [39] [AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software](https://arxiv.org/abs/2509.16861)
*Rui Yang,Michael Fu,Chakkrit Tantithamthavorn,Chetan Arora,Gunel Gulmammadova,Joey Chua*

Main category: cs.CR

TL;DR: 提出了AdaptiveGuard，一种自适应护栏系统，通过持续学习框架检测新型越狱攻击并动态防御，在OOD检测准确率达到96%，仅需两步更新即可适应新攻击。


<details>
  <summary>Details</summary>
Motivation: 传统护栏系统在面对未见过的越狱攻击时性能急剧下降（最低至12%），需要构建能够动态适应新兴威胁的后部署护栏。

Method: 使用持续学习框架，将新型越狱攻击检测为分布外输入，并通过动态学习机制进行防御。

Result: 实现了96%的OOD检测准确率，仅需两步更新即可适应新攻击，适应后对分布内数据保持85%以上的F1分数。

Conclusion: AdaptiveGuard证明了护栏系统能够在部署后持续进化以应对新兴越狱策略，是解决LLM安全部署挑战的有效方案。

Abstract: Guardrails are critical for the safe deployment of Large Language Models
(LLMs)-powered software. Unlike traditional rule-based systems with limited,
predefined input-output spaces that inherently constrain unsafe behavior, LLMs
enable open-ended, intelligent interactions--opening the door to jailbreak
attacks through user inputs. Guardrails serve as a protective layer, filtering
unsafe prompts before they reach the LLM. However, prior research shows that
jailbreak attacks can still succeed over 70% of the time, even against advanced
models like GPT-4o. While guardrails such as LlamaGuard report up to 95%
accuracy, our preliminary analysis shows their performance can drop sharply--to
as low as 12%--when confronted with unseen attacks. This highlights a growing
software engineering challenge: how to build a post-deployment guardrail that
adapts dynamically to emerging threats? To address this, we propose
AdaptiveGuard, an adaptive guardrail that detects novel jailbreak attacks as
out-of-distribution (OOD) inputs and learns to defend against them through a
continual learning framework. Through empirical evaluation, AdaptiveGuard
achieves 96% OOD detection accuracy, adapts to new attacks in just two update
steps, and retains over 85% F1-score on in-distribution data post-adaptation,
outperforming other baselines. These results demonstrate that AdaptiveGuard is
a guardrail capable of evolving in response to emerging jailbreak strategies
post deployment. We release our AdaptiveGuard and studied datasets at
https://github.com/awsm-research/AdaptiveGuard to support further research.

</details>


### [40] [Security Vulnerabilities in Software Supply Chain for Autonomous Vehicles](https://arxiv.org/abs/2509.16899)
*Md Wasiul Haque,Md Erfan,Sagar Dasgupta,Md Rayhanur Rahman,Mizanur Rahman*

Main category: cs.CR

TL;DR: 该论文分析了自动驾驶汽车开源软件供应链中的安全漏洞，使用静态分析工具对主流开源AV软件进行检测，旨在提高开发过程中的安全意识。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶汽车在关键任务中的广泛应用，其软件系统日益依赖开源供应链，但汽车行业软件开发中安全实践往往被忽视，存在重大网络安全风险。

Method: 使用静态分析工具对流行的开源自动驾驶软件（如Autoware、Apollo、openpilot）进行安全漏洞检测和分析。

Result: 研究发现自动驾驶汽车开源软件中存在普遍的安全漏洞，通过比较不同开源仓库的静态分析结果揭示了具体的安全问题。

Conclusion: 研究强调了在软件开发生命周期早期实施安全最佳实践的必要性，以降低网络安全风险，确保系统可靠性，保护用户数据，维护公众对自动化世界的信任。

Abstract: The interest in autonomous vehicles (AVs) for critical missions, including
transportation, rescue, surveillance, reconnaissance, and mapping, is growing
rapidly due to their significant safety and mobility benefits. AVs consist of
complex software systems that leverage artificial intelligence (AI), sensor
fusion algorithms, and real-time data processing. Additionally, AVs are
becoming increasingly reliant on open-source software supply chains, such as
open-source packages, third-party software components, AI models, and
third-party datasets. Software security best practices in the automotive sector
are often an afterthought for developers. Thus, significant cybersecurity risks
exist in the software supply chain of AVs, particularly when secure software
development practices are not rigorously implemented. For example, Upstream's
2024 Automotive Cybersecurity Report states that 49.5% of cyberattacks in the
automotive sector are related to exploiting security vulnerabilities in
software systems. In this chapter, we analyze security vulnerabilities in
open-source software components in AVs. We utilize static analyzers on popular
open-source AV software, such as Autoware, Apollo, and openpilot. Specifically,
this chapter covers: (1) prevalent software security vulnerabilities of AVs;
and (2) a comparison of static analyzer outputs for different open-source AV
repositories. The goal is to inform researchers, practitioners, and
policymakers about the existing security flaws in the commonplace open-source
software ecosystem in the AV domain. The findings would emphasize the necessity
of security best practices earlier in the software development lifecycle to
reduce cybersecurity risks, thereby ensuring system reliability, safeguarding
user data, and maintaining public trust in an increasingly automated world.

</details>


### [41] [Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving](https://arxiv.org/abs/2509.16950)
*Xuan Chen,Shiwei Feng,Zikang Xiong,Shengwei An,Yunshu Mao,Lu Yan,Guanhong Tao,Wenbo Guo,Xiangyu Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种针对端到端自动驾驶系统的新型后门攻击方法，使用其他车辆的轨迹作为触发器，相比现有的像素级触发器更实用。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注不切实际的像素级触发器，而真实世界中基于车辆轨迹的后门攻击威胁尚未得到充分探索。

Method: 使用时序逻辑规范定义攻击车辆行为，通过可配置行为模型生成精确触发轨迹，并采用负训练策略增强攻击隐蔽性。

Result: 在5个离线强化学习驾驶代理上测试6种触发模式，证明了该攻击方法的灵活性和有效性。

Conclusion: 端到端自动驾驶系统对基于轨迹的后门攻击存在未充分探索的脆弱性，需要加强安全防护。

Abstract: Assessing the safety of autonomous driving (AD) systems against security
threats, particularly backdoor attacks, is a stepping stone for real-world
deployment. However, existing works mainly focus on pixel-level triggers that
are impractical to deploy in the real world. We address this gap by introducing
a novel backdoor attack against the end-to-end AD systems that leverage one or
more other vehicles' trajectories as triggers. To generate precise trigger
trajectories, we first use temporal logic (TL) specifications to define the
behaviors of attacker vehicles. Configurable behavior models are then used to
generate these trajectories, which are quantitatively evaluated and iteratively
refined based on the TL specifications. We further develop a negative training
strategy by incorporating patch trajectories that are similar to triggers but
are designated not to activate the backdoor. It enhances the stealthiness of
the attack and refines the system's responses to trigger scenarios. Through
extensive experiments on 5 offline reinforcement learning (RL) driving agents
with 6 trigger patterns and target action combinations, we demonstrate the
flexibility and effectiveness of our proposed attack, showing the
under-exploration of existing end-to-end AD systems' vulnerabilities to such
trajectory-based backdoor attacks.

</details>


### [42] [In Numeris Veritas: An Empirical Measurement of Wi-Fi Integration in Industry](https://arxiv.org/abs/2509.16987)
*Vyron Kampourakis,Christos Smiliotopoulos,Vasileios Gkioulos,Sokratis Katsikas*

Main category: cs.CR

TL;DR: 该研究通过分析全球WiGLE数据库，首次创建了包含1,087个高置信度工业Wi-Fi网络的公开数据集，揭示了工业环境中Wi-Fi使用的安全配置问题。


<details>
  <summary>Details</summary>
Motivation: 随着IT技术渗透到OT领域，工业系统中的传统物理隔离正在消失，Wi-Fi等无线解决方案加速集成。新一代Wi-Fi标准虽然满足工业用例的性能需求，但其引入带来了严重的安全担忧，目前缺乏关于工业环境中Wi-Fi实际普及和安全配置的实证研究。

Method: 通过挖掘全球众包WiGLE数据库，创建首个公开可用的工业Wi-Fi网络数据集，分析关键属性如SSID模式、加密方法、供应商类型和全球分布。

Result: 研究发现工业领域Wi-Fi采用率不断增长，但存在严重的安全缺陷，包括持续使用弱或过时的安全配置，直接暴露关键基础设施。

Conclusion: 这项研究作为一个关键参考点，提供了独特的数据集和实用见解，指导未来工业环境中无线安全的研究方向。

Abstract: Traditional air gaps in industrial systems are disappearing as IT
technologies permeate the OT domain, accelerating the integration of wireless
solutions like Wi-Fi. Next-generation Wi-Fi standards (IEEE 802.11ax/be) meet
performance demands for industrial use cases, yet their introduction raises
significant security concerns. A critical knowledge gap exists regarding the
empirical prevalence and security configuration of Wi-Fi in real-world
industrial settings. This work addresses this by mining the global crowdsourced
WiGLE database to provide a data-driven understanding. We create the first
publicly available dataset of 1,087 high-confidence industrial Wi-Fi networks,
examining key attributes such as SSID patterns, encryption methods, vendor
types, and global distribution. Our findings reveal a growing adoption of Wi-Fi
across industrial sectors but underscore alarming security deficiencies,
including the continued use of weak or outdated security configurations that
directly expose critical infrastructure. This research serves as a pivotal
reference point, offering both a unique dataset and practical insights to guide
future investigations into wireless security within industrial environments.

</details>


### [43] [Electronic Reporting Using SM2-Based Ring Signcryption](https://arxiv.org/abs/2509.17048)
*Huifang Yu,Jiaxing Jie,Lei Li*

Main category: cs.CR

TL;DR: 提出了一种基于SM2可追踪环签密方案的电子举报系统，该方案结合SM2椭圆曲线公钥密码算法和环签名算法，在保护举报者身份隐私的同时能够追踪恶意举报者。


<details>
  <summary>Details</summary>
Motivation: 电子举报系统需要解决举报者身份隐私保护、防止恶意举报以及举报信息保密性等关键问题，现有方案在效率和安全性方面存在不足。

Method: 将SM2椭圆曲线公钥密码算法与环签名算法相结合，设计可追踪的环签密方案，确保核心密码算法的自主可控性。

Result: 安全性分析表明该方案满足机密性、不可伪造性、可追踪性、可链接性和可否认性；效率分析显示在签名阶段相比现有环签名方案具有显著优势。

Conclusion: 基于该方案设计的电子举报系统能够在保护用户身份隐私的同时追踪恶意举报者，并确保举报内容对第三方保密。

Abstract: Electronic whistleblowing systems are widely used due to their efficiency and
convenience. The key to designing such systems lies in protecting the identity
privacy of whistleblowers, preventing malicious whistleblowing, and ensuring
the confidentiality of whistleblowing information. To address these issues, a
SM2 traceable ring signcryption scheme for electronic voting is proposed. This
scheme combines the SM2 elliptic curve public key cryptography algorithm with
the ring signature algorithm, enhancing the overall efficiency of the scheme
while ensuring the autonomy and controllability of the core cryptographic
algorithms. Security analysis demonstrates that the proposed scheme satisfies
confidentiality, unforgeability, traceability, linkability, and deniability.
Efficiency analysis shows that, compared to existing ring signature schemes,
the proposed scheme exhibits significant efficiency advantages during the
signature phase. The electronic whistleblowing system designed using the
proposed scheme can track malicious whistleblowers while protecting user
identity privacy, and ensures that the content of whistleblowing remains
unknown to third parties.

</details>


### [44] [Localizing Malicious Outputs from CodeLLM](https://arxiv.org/abs/2509.17070)
*Mayukh Borana,Junyi Liang,Sai Sathiesh Rajan,Sudipta Chattopadhyay*

Main category: cs.CR

TL;DR: FreqRank是一种基于突变的防御方法，用于定位LLM输出中的恶意组件及其对应的后门触发器。该方法通过频率排名系统识别恶意子字符串，并利用此知识定位输入中的后门触发器。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在代码相关任务中的广泛应用，恶意模型可能通过微调或自定义指令植入后门，导致在特定触发条件下输出恶意内容。需要一种有效的方法来检测和定位这些后门威胁。

Method: FreqRank基于突变技术，假设恶意子字符串在触发输入时会在输出中一致出现。通过频率排名系统识别这些频繁出现的恶意组件，并反向定位输入中的后门触发器。

Result: 在9个恶意模型（涉及代码补全、代码生成和代码总结任务）上测试，平均攻击成功率为86.6%。FreqRank在98%的情况下将恶意输出列为前5个建议之一，且效果随突变体数量增加而提升。

Conclusion: FreqRank比其他防御方法有效35-50%，能够在有限的触发样本下有效定位后门触发器，为LLM安全性提供了实用的防御解决方案。

Abstract: We introduce FreqRank, a mutation-based defense to localize malicious
components in LLM outputs and their corresponding backdoor triggers. FreqRank
assumes that the malicious sub-string(s) consistently appear in outputs for
triggered inputs and uses a frequency-based ranking system to identify them.
Our ranking system then leverages this knowledge to localize the backdoor
triggers present in the inputs. We create nine malicious models through
fine-tuning or custom instructions for three downstream tasks, namely, code
completion (CC), code generation (CG), and code summarization (CS), and show
that they have an average attack success rate (ASR) of 86.6%. Furthermore,
FreqRank's ranking system highlights the malicious outputs as one of the top
five suggestions in 98% of cases. We also demonstrate that FreqRank's
effectiveness scales as the number of mutants increases and show that FreqRank
is capable of localizing the backdoor trigger effectively even with a limited
number of triggered samples. Finally, we show that our approach is 35-50% more
effective than other defense methods.

</details>


### [45] [Unaligned Incentives: Pricing Attacks Against Blockchain Rollups](https://arxiv.org/abs/2509.17126)
*Stefanos Chaliasos,Conner Swann,Sina Pilehchiha,Nicolas Mohnblatt,Benjamin Livshits,Assimakis Kattis*

Main category: cs.CR

TL;DR: 本文识别了现有Rollup交易费用机制中的关键错误定价问题，揭示了两种攻击方式：数据饱和攻击和证明杀手交易攻击，导致DoS和最终性延迟，并提出了相应的缓解措施。


<details>
  <summary>Details</summary>
Motivation: Rollup作为以太坊的主要扩容方案，管理着超过550亿美元的资产，但其交易费用机制存在资源定价不准确的问题，可能导致严重的安全漏洞和经济损失。

Method: 通过分析主要以太坊Rollup的攻击向量，量化攻击成本和协议损失，包括数据饱和攻击和证明杀手交易攻击的具体实施方式和影响评估。

Result: 研究发现数据饱和攻击可实现周期性DoS（最长30分钟），成本低于2 ETH；三个Rollup面临无限期DoS风险，成本为0.8-2.7 ETH/小时；证明杀手攻击使最终性延迟增加约94倍。

Conclusion: 提出了全面的缓解措施，建议通过多维Rollup交易费用机制的正确实践来纠正已识别的错误定价攻击，提升Rollup系统的安全性和稳定性。

Abstract: Rollups have become the de facto scalability solution for Ethereum, securing
more than $55B in assets. They achieve scale by executing transactions on a
Layer 2 ledger, while periodically posting data and finalizing state on the
Layer 1, either optimistically or via validity proofs. Their fees must
simultaneously reflect the pricing of three resources: L2 costs (e.g.,
execution), L1 DA, and underlying L1 gas costs for batch settlement and proof
verification. In this work, we identify critical mis-pricings in existing
rollup transaction fee mechanisms (TFMs) that allow for two powerful attacks.
Firstly, an adversary can saturate the L2's DA batch capacity with
compute-light data-heavy transactions, forcing low-gas transaction batches that
enable both L2 DoS attacks, and finality-delay attacks. Secondly, by crafting
prover killer transactions that maximize proving cycles relative to the gas
charges, an adversary can effectively stall proof generation, delaying finality
by hours and inflicting prover-side economic losses to the rollup at a minimal
cost.
  We analyze the above attack vectors across the major Ethereum rollups,
quantifying adversarial costs and protocol losses. We find that the first
attack enables periodic DoS on rollups, lasting up to 30 minutes, at a cost
below 2 ETH for most rollups. Moreover, we identify three rollups that are
exposed to indefinite DoS at a cost of approximately 0.8 to 2.7 ETH per hour.
The attack can be further modified to increase finalization delays by a factor
of about 1.45x to 2.73x, compared to direct L1 blob-stuffing, depending on the
rollup's parameters. Furthermore, we find that the prover killer attack induces
a finalization latency increase of about 94x. Finally, we propose comprehensive
mitigations to prevent these attacks and suggest how some practical uses of
multi-dimensional rollup TFMs can rectify the identified mis-pricing attacks.

</details>


### [46] [Bribers, Bribers on The Chain, Is Resisting All in Vain? Trustless Consensus Manipulation Through Bribing Contracts](https://arxiv.org/abs/2509.17185)
*Bence Soóki-Tóth,István András Seres,Kamilla Kara,Ábel Nagy,Balázs Pejó,Gergely Biczók*

Main category: cs.CR

TL;DR: 本文介绍了三种针对以太坊验证者的新型高效贿赂合约，分析了这些贿赂攻击对加密货币激励机制兼容性的威胁，并进行了初步的博弈论分析。


<details>
  <summary>Details</summary>
Motivation: 加密货币的长期成功依赖于验证者的激励兼容性，而通过智能合约实现的贿赂攻击威胁着这一基础。

Method: 设计并实现了三种贿赂合约：第一种通过购买投票来分叉区块链；第二种激励验证者自愿退出共识协议；第三种建立信任缺失的贿赂市场来操纵RANDAO随机数信标。

Result: 成功开发了三种高效的贿赂攻击机制，能够有效威胁以太坊的共识安全性。

Conclusion: 贿赂攻击对加密货币的激励机制构成严重威胁，需要进一步研究防御措施来保护区块链系统的安全性。

Abstract: The long-term success of cryptocurrencies largely depends on the incentive
compatibility provided to the validators. Bribery attacks, facilitated
trustlessly via smart contracts, threaten this foundation. This work
introduces, implements, and evaluates three novel and efficient bribery
contracts targeting Ethereum validators. The first bribery contract enables a
briber to fork the blockchain by buying votes on their proposed blocks. The
second contract incentivizes validators to voluntarily exit the consensus
protocol, thus increasing the adversary's relative staking power. The third
contract builds a trustless bribery market that enables the briber to auction
off their manipulative power over the RANDAO, Ethereum's distributed randomness
beacon. Finally, we provide an initial game-theoretical analysis of one of the
described bribery markets.

</details>


### [47] [Seeing is Deceiving: Mirror-Based LiDAR Spoofing for Autonomous Vehicle Deception](https://arxiv.org/abs/2509.17253)
*Selma Yahia,Ildi Alla,Girija Bangalore Mohan,Daniel Rau,Mridula Singh,Valeria Loscri*

Main category: cs.CR

TL;DR: 本文提出了一种利用镜面反射的低成本被动LiDAR欺骗攻击，可在自动驾驶车辆中注入或移除物体感知，通过几何光学模型和实验验证了攻击的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆严重依赖LiDAR进行3D感知，但现有研究主要关注主动电子攻击，缺乏对被动物理攻击的探索。本文旨在揭示利用日常镜面材料即可实施的LiDAR欺骗威胁。

Method: 使用平面镜重定向LiDAR光束，定义物体添加攻击(OAA)和物体移除攻击(ORA)；开发几何光学模型，通过户外实验和CARLA仿真进行验证。

Result: 实验表明镜面攻击能够污染占据栅格，诱导错误检测，并触发不安全的规划控制行为。攻击在真实环境中有效且无需电子设备。

Conclusion: 镜面LiDAR欺骗攻击构成严重安全威胁，现有防御措施如热传感、多传感器融合等存在局限性，需要新的防御机制来应对此类物理攻击。

Abstract: Autonomous vehicles (AVs) rely heavily on LiDAR sensors for accurate 3D
perception. We show a novel class of low-cost, passive LiDAR spoofing attacks
that exploit mirror-like surfaces to inject or remove objects from an AV's
perception. Using planar mirrors to redirect LiDAR beams, these attacks require
no electronics or custom fabrication and can be deployed in real settings. We
define two adversarial goals: Object Addition Attacks (OAA), which create
phantom obstacles, and Object Removal Attacks (ORA), which conceal real
hazards. We develop geometric optics models, validate them with controlled
outdoor experiments using a commercial LiDAR and an Autoware-equipped vehicle,
and implement a CARLA-based simulation for scalable testing. Experiments show
mirror attacks corrupt occupancy grids, induce false detections, and trigger
unsafe planning and control behaviors. We discuss potential defenses (thermal
sensing, multi-sensor fusion, light-fingerprinting) and their limitations.

</details>


### [48] [Bridging Cybersecurity Practice and Law: a Hands-on, Scenario-Based Curriculum Using the NICE Framework to Foster Skill Development](https://arxiv.org/abs/2509.17263)
*Colman McGuan,Aadithyan V. Raghavan,Komala M. Mandapati,Chansu Yu,Brian E. Ray,Debbie K. Jackson,Sathish Kumar*

Main category: cs.CR

TL;DR: 本文针对中小企业在实施NICE网络安全框架时的困难，提出了一个实用的模型，通过识别常见攻击向量、开发基于场景的课程，帮助中小企业评估和培养网络安全人才。


<details>
  <summary>Details</summary>
Motivation: 中小企业难以有效实施复杂的网络安全框架（如NICE框架），需要更实用的指导来应对网络威胁。

Method: 识别中小企业最常见的攻击向量，从NICE框架中提取相关技术与非技术任务、知识、技能和能力（TKSA），开发基于真实威胁场景的课程。

Result: 提出了一个实用的模型，中小企业可用其评估现有员工能力或招聘新员工，教育机构可开发场景化学习模块。

Conclusion: 该模型通过模拟环境强化学员应对网络威胁的能力，为中小企业网络安全人才培养提供了有效解决方案。

Abstract: In an increasingly interconnected world, cybersecurity professionals play a
pivotal role in safeguarding organizations from cyber threats. To secure their
cyberspace, organizations are forced to adopt a cybersecurity framework such as
the NIST National Initiative for Cybersecurity Education Workforce Framework
for Cybersecurity (NICE Framework). Although these frameworks are a good
starting point for businesses and offer critical information to identify,
prevent, and respond to cyber incidents, they can be difficult to navigate and
implement, particularly for small-medium businesses (SMB). To help overcome
this issue, this paper identifies the most frequent attack vectors to SMBs
(Objective 1) and proposes a practical model of both technical and
non-technical tasks, knowledge, skills, abilities (TKSA) from the NICE
Framework for those attacks (Objective 2). The research develops a
scenario-based curriculum. By immersing learners in realistic cyber threat
scenarios, their practical understanding and preparedness in responding to
cybersecurity incidents is enhanced (Objective 3). Finally, this work
integrates practical experience and real-life skill development into the
curriculum (Objective 4). SMBs can use the model as a guide to evaluate, equip
their existing workforce, or assist in hiring new employees. In addition,
educational institutions can use the model to develop scenario-based learning
modules to adequately equip the emerging cybersecurity workforce for SMBs.
Trainees will have the opportunity to practice both technical and legal issues
in a simulated environment, thereby strengthening their ability to identify,
mitigate, and respond to cyber threats effectively.

</details>


### [49] [Privacy-Preserving State Estimation with Crowd Sensors: An Information-Theoretic Respective](https://arxiv.org/abs/2509.17266)
*Farhad Farokhi*

Main category: cs.CR

TL;DR: 该论文提出了一种针对线性时不变动态系统的隐私保护状态估计方法，使用随机选择的众包传感器和Luenberger类观测器，通过添加隐私保护噪声来控制信息泄露。


<details>
  <summary>Details</summary>
Motivation: 在众包传感器系统中，需要保护传感器身份隐私，防止对手通过状态估计推断出具体使用了哪个传感器，特别是面对能够获取高质量状态测量的强大对手。

Method: 采用Luenberger类观测器融合系统模型和传感器测量，通过添加可调节方差的隐私保护噪声来约束信息泄露，信息泄露通过条件互信息来量化。

Result: 研究表明，通过适当选择隐私保护噪声的方差，可以实现任意预设水平的信息泄露控制，从而能够精细调节隐私与效用之间的权衡。

Conclusion: 该方法为线性时不变动态系统的隐私保护状态估计提供了一种有效的解决方案，能够在保护传感器身份隐私的同时维持状态估计的实用性。

Abstract: Privacy-preserving state estimation for linear time-invariant dynamical
systems with crowd sensors is considered. At any time step, the estimator has
access to measurements from a randomly selected sensor from a pool of sensors
with pre-specified models and noise profiles. A Luenberger-like observer is
used to fuse the measurements with the underlying model of the system to
recursively generate the state estimates. An additive privacy-preserving noise
is used to constrain information leakage. Information leakage is measured via
mutual information between the identity of the sensors and the state estimate
conditioned on the actual state of the system. This captures an omnipotent
adversary that not only can access state estimates but can also gather direct
high-quality state measurements. Any prescribed level of information leakage is
shown to be achievable by appropriately selecting the variance of the
privacy-preserving noise. Therefore, privacy-utility trade-off can be
fine-tuned.

</details>


### [50] [TextCrafter: Optimization-Calibrated Noise for Defending Against Text Embedding Inversion](https://arxiv.org/abs/2509.17302)
*Duoxun Tang,Xinhang Jiang,Jiajun Niu*

Main category: cs.CR

TL;DR: TextCrafter是一种基于优化的对抗扰动机制，通过RL学习的几何感知噪声注入来抑制文本嵌入反转攻击，在保护隐私的同时保持任务效用。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入反转攻击能够从潜在表示重构原始句子，在协作推理和边缘计算中构成严重隐私威胁，需要有效的防御机制。

Method: 结合RL学习的几何感知噪声注入（正交于用户嵌入）、聚类先验和PII信号引导，提供方向性保护策略来平衡隐私和效用。

Result: 在强隐私设置下，TextCrafter在四个数据集上保持70%的分类准确率，在较低隐私预算下始终优于高斯/LDP基线方法。

Conclusion: TextCrafter展示了优越的隐私-效用权衡，为文本嵌入反转攻击提供了有效的防御解决方案。

Abstract: Text embedding inversion attacks reconstruct original sentences from latent
representations, posing severe privacy threats in collaborative inference and
edge computing. We propose TextCrafter, an optimization-based adversarial
perturbation mechanism that combines RL learned, geometry aware noise injection
orthogonal to user embeddings with cluster priors and PII signal guidance to
suppress inversion while preserving task utility. Unlike prior defenses either
non learnable or agnostic to perturbation direction, TextCrafter provides a
directional protective policy that balances privacy and utility. Under strong
privacy setting, TextCrafter maintains 70 percentage classification accuracy on
four datasets and consistently outperforms Gaussian/LDP baselines across lower
privacy budgets, demonstrating a superior privacy utility trade off.

</details>


### [51] [SilentStriker:Toward Stealthy Bit-Flip Attacks on Large Language Models](https://arxiv.org/abs/2509.17371)
*Haotian Xu,Qingsong Peng,Jie Shi,Huadi Zheng,Yu Li,Cheng Zhuo*

Main category: cs.CR

TL;DR: SilentStriker是一种针对大语言模型的隐蔽比特翻转攻击方法，能够在有效降低任务性能的同时保持输出文本的自然性


<details>
  <summary>Details</summary>
Motivation: 现有比特翻转攻击方法在性能下降和输出自然性之间难以平衡，容易被发现。需要开发更隐蔽的攻击方法

Method: 通过利用关键输出标记作为抑制目标来重新制定攻击目标，采用迭代渐进搜索策略最大化攻击效果

Result: 实验表明SilentStriker显著优于现有基线方法，能够成功实施攻击而不影响生成文本的自然性

Conclusion: 该方法有效解决了LLM可变输出长度和巨大输出空间带来的攻击损失函数设计挑战，实现了攻击有效性和隐蔽性的联合优化

Abstract: The rapid adoption of large language models (LLMs) in critical domains has
spurred extensive research into their security issues. While input manipulation
attacks (e.g., prompt injection) have been well studied, Bit-Flip Attacks
(BFAs) -- which exploit hardware vulnerabilities to corrupt model parameters
and cause severe performance degradation -- have received far less attention.
Existing BFA methods suffer from key limitations: they fail to balance
performance degradation and output naturalness, making them prone to discovery.
In this paper, we introduce SilentStriker, the first stealthy bit-flip attack
against LLMs that effectively degrades task performance while maintaining
output naturalness. Our core contribution lies in addressing the challenge of
designing effective loss functions for LLMs with variable output length and the
vast output space. Unlike prior approaches that rely on output perplexity for
attack loss formulation, which inevitably degrade output naturalness, we
reformulate the attack objective by leveraging key output tokens as targets for
suppression, enabling effective joint optimization of attack effectiveness and
stealthiness. Additionally, we employ an iterative, progressive search strategy
to maximize attack efficacy. Experiments show that SilentStriker significantly
outperforms existing baselines, achieving successful attacks without
compromising the naturalness of generated text.

</details>


### [52] [A Lightweight Authentication and Key Agreement Protocol Design for FANET](https://arxiv.org/abs/2509.17409)
*Yao Wu,Ziye Jia,Qihui Wu,Yian Zhu*

Main category: cs.CR

TL;DR: 提出一种轻量级认证和密钥协商协议，将物理不可克隆功能与动态凭证管理和轻量级密码原语相结合，用于无人机自组网（FANETs）


<details>
  <summary>Details</summary>
Motivation: 现有基于多因素和公钥密码学的协议因依赖存储敏感信息而存在漏洞，无人机在开放环境中运行面临资源约束、动态拓扑和安全通信挑战

Method: 集成物理不可克隆功能（PUF）、动态凭证管理和轻量级密码原语，设计轻量级认证和密钥协商协议

Result: 协议降低了计算和通信开销，同时增强了安全性，安全分析确认其能抵御各种攻击

Conclusion: 比较评估表明该协议在安全性、通信效率和计算成本方面具有优越性

Abstract: The advancement of low-altitude intelligent networks enables unmanned aerial
vehicle (UAV) interconnection via flying ad-hoc networks (FANETs), offering
flexibility and decentralized coordination. However, resource constraints,
dynamic topologies, and UAV operations in open environments present significant
security and communication challenges. Existing multi-factor and public-key
cryptography protocols are vulnerable due to their reliance on stored sensitive
information, increasing the risk of exposure and compromise. This paper
proposes a lightweight authentication and key agreement protocol for FANETs,
integrating physical unclonable functions with dynamic credential management
and lightweight cryptographic primitives. The protocol reduces computational
and communication overhead while enhancing security. Security analysis confirms
its resilience against various attacks, and comparative evaluations demonstrate
its superiority in security, communication efficiency, and computational cost.

</details>


### [53] [DINVMark: A Deep Invertible Network for Video Watermarking](https://arxiv.org/abs/2509.17416)
*Jianbin Ji,Dawen Xu,Li Dong,Lin Yang,Songhan He*

Main category: cs.CR

TL;DR: 本文提出了一种基于深度可逆网络的视频水印方法DINVMark，通过设计专门模拟HEVC压缩的噪声层，提高了水印容量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视频水印方法在水印容量和鲁棒性方面存在不足，且缺乏专门针对HEVC压缩的噪声层，需要解决这些问题。

Method: 使用可逆神经网络(INN)，编码器和解码器共享相同网络结构，设计专门模拟HEVC压缩的噪声层，实现水印嵌入和提取。

Result: 实验结果表明，该方法显著提高了水印鲁棒性，保持了视频质量，并大幅增加了水印嵌入容量。

Conclusion: DINVMark方法有效解决了视频水印的容量和鲁棒性问题，为视频版权保护和内容认证提供了有效解决方案。

Abstract: With the wide spread of video, video watermarking has become increasingly
crucial for copyright protection and content authentication. However, video
watermarking still faces numerous challenges. For example, existing methods
typically have shortcomings in terms of watermarking capacity and robustness,
and there is a lack of specialized noise layer for High Efficiency Video
Coding(HEVC) compression. To address these issues, this paper introduces a Deep
Invertible Network for Video watermarking (DINVMark) and designs a noise layer
to simulate HEVC compression. This approach not only in creases watermarking
capacity but also enhances robustness. DINVMark employs an Invertible Neural
Network (INN), where the encoder and decoder share the same network structure
for both watermark embedding and extraction. This shared architecture ensures
close coupling between the encoder and decoder, thereby improving the accuracy
of the watermark extraction process. Experimental results demonstrate that the
proposed scheme significantly enhances watermark robustness, preserves video
quality, and substantially increases watermark embedding capacity.

</details>


### [54] [Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation for LLM-Powered Agents](https://arxiv.org/abs/2509.17488)
*Shouju Wang,Fenglin Yu,Xirui Liu,Xiaoting Qin,Jue Zhang,Qingwei Lin,Dongmei Zhang,Saravan Rajmohan*

Main category: cs.CR

TL;DR: PrivacyChecker是一个模型无关的隐私保护方法，基于上下文完整性原则，在LLM代理环境中有效减少隐私泄露，同时保持任务有效性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在处理敏感通信时的自主性增强，特别是在MCP和A2A框架下，现有的隐私基准测试局限于静态简化场景，无法反映实际应用中的隐私风险。

Method: 提出了PrivacyChecker缓解方法和PrivacyLens-Live动态基准测试框架，将静态基准转化为动态的MCP和A2A环境，通过三种部署策略无缝集成到代理协议中。

Result: 在DeepSeek-R1上将隐私泄露从36.08%降低到7.30%，在GPT-4o上从33.06%降低到8.32%，同时保持了任务的有效性。

Conclusion: 该方法为新兴的代理生态系统提供了实用的隐私保护解决方案，数据和代码将公开可用。

Abstract: The increasing autonomy of LLM agents in handling sensitive communications,
accelerated by Model Context Protocol (MCP) and Agent-to-Agent (A2A)
frameworks, creates urgent privacy challenges. While recent work reveals
significant gaps between LLMs' privacy Q&A performance and their agent
behavior, existing benchmarks remain limited to static, simplified scenarios.
We present PrivacyChecker, a model-agnostic, contextual integrity based
mitigation approach that effectively reduces privacy leakage from 36.08% to
7.30% on DeepSeek-R1 and from 33.06% to 8.32% on GPT-4o, all while preserving
task helpfulness. We also introduce PrivacyLens-Live, transforming static
benchmarks into dynamic MCP and A2A environments that reveal substantially
higher privacy risks in practical. Our modular mitigation approach integrates
seamlessly into agent protocols through three deployment strategies, providing
practical privacy protection for the emerging agentic ecosystem. Our data and
code will be made available at https://aka.ms/privacy_in_action.

</details>


### [55] [Community Covert Communication - Dynamic Mass Covert Communication Through Social Media](https://arxiv.org/abs/2509.17508)
*Eric Filiol*

Main category: cs.CR

TL;DR: 本文探讨了利用社交媒体傀儡主控技术进行隐蔽信息传输的方法，将社区管理技术重新定义为加密通信工具，使传统监听和干扰手段失效。


<details>
  <summary>Details</summary>
Motivation: 随着2010年代以来社交网络影响力技术的指数级增长，出现了专门从事影响力操作的公司。本研究旨在展示这些社区管理技术如何被重新用于传输大量加密信息。

Method: 使用傀儡主控活动技术，创建大量虚拟身份并组织成社区，利用专门软件（如Ripon、AIMS）自动化操作，实现多层加密信息的分发。

Result: 成功展示了如何通过社交网络社区管理技术传输数十MB的多层加密信息，这种通信方式完全重新定义了传统通信概念。

Conclusion: 这种基于社交网络的影响力技术不仅可用于信息传播，还能作为强大的隐蔽通信工具，使传统的监听、拦截和干扰操作失去意义。

Abstract: Since the early 2010s, social network-based influence technologies have grown
almost exponentially. Initiated by the U.S. Army's early OEV system in 2011, a
number of companies specializing in this field have emerged. The most
(in)famous cases are Bell Pottinger, Cambridge Analytica, Aggregate-IQ and,
more recently, Team Jorge.
  In this paper, we consider the use-case of sock puppet master activities,
which consist in creating hundreds or even thousands of avatars, in organizing
them into communities and implement influence operations. On-purpose software
is used to automate these operations (e.g. Ripon software, AIMS) and organize
these avatar populations into communities. The aim is to organize targeted and
directed influence communication to rather large communities (influence
targets).
  The goal of the present research work is to show how these community
management techniques (social networks) can also be used to
communicate/disseminate relatively large volumes (up to a few tens of Mb) of
multi-level encrypted information to a limited number of actors. To a certain
extent, this can be compared to a Dark Post-type function, with a number of
much more powerful potentialities. As a consequence, the concept of
communication has been totally redefined and disrupted, so that eavesdropping,
interception and jamming operations no longer make sense.

</details>


### [56] [Impossibility Results of Card-Based Protocols via Mathematical Optimization](https://arxiv.org/abs/2509.17595)
*Shunnosuke Ikeda,Kazumasa Shinagawa*

Main category: cs.CR

TL;DR: 本文提出使用数学优化方法证明卡牌密码学中的不可能性证明，突破了以往只能处理少量卡牌的限制，建立了适用于大量卡牌的结果。


<details>
  <summary>Details</summary>
Motivation: 传统的不可能性证明方法通常只能处理少量卡牌的情况，需要一种新的方法来建立适用于任意数量卡牌的不可能性结果。

Method: 采用数学优化作为新方法，专注于单切全开（SCFO）协议的研究，该协议包括执行一次随机切牌然后揭示所有卡牌。

Result: 对于任何三变量布尔函数，在附加卡牌颜色相同的条件下，除了已知的协议外不存在新的SCFO协议。

Conclusion: 这项工作为不可能性证明提供了新框架，并给出了在附加卡牌颜色相同条件下对任意数量卡牌都有效的证明。

Abstract: This paper introduces mathematical optimization as a new method for proving
impossibility proofs in the field of card-based cryptography. While previous
impossibility proofs were often limited to cases involving a small number of
cards, this new approach establishes results that hold for a large number of
cards. The research focuses on single-cut full-open (SCFO) protocols, which
consist of performing one random cut and then revealing all cards. The main
contribution is that for any three-variable Boolean function, no new SCFO
protocols exist beyond those already known, under the condition that all
additional cards have the same color. The significance of this work is that it
provides a new framework for impossibility proofs and delivers a proof that is
valid for any number of cards, as long as all additional cards have the same
color.

</details>


### [57] [Ordered Multi-Signatures with Public-Key Aggregation from SXDH Assumption](https://arxiv.org/abs/2509.17709)
*Masayuki Tezuka,Keisuke Tanaka*

Main category: cs.CR

TL;DR: 本文提出了一种有序多重签名方案，通过修改Chatterjee和Kabaleeshwaran的顺序聚合签名方案，实现了紧凑的公共参数大小和公钥聚合特性。


<details>
  <summary>Details</summary>
Motivation: 有序多重签名方案允许多个签名者按顺序对共同消息进行签名，并能验证签名者的签名顺序。现有方案在公共参数大小和公钥管理方面存在改进空间。

Method: 通过修改Chatterjee和Kabaleeshwaran的顺序聚合签名方案，引入公钥聚合特性，将公钥列表压缩为短聚合密钥。

Result: 提出的方案在对称外部Diffie-Hellman（SXDH）假设下证明了安全性，无需随机预言模型。

Conclusion: 该有序多重签名方案具有紧凑的公共参数和高效的公钥聚合能力，为顺序签名验证提供了安全有效的解决方案。

Abstract: An ordered multi-signature scheme allows multiple signers to sign a common
message in a sequential manner and allows anyone to verify the signing order of
signers with a public-key list. In this work, we propose an ordered
multi-signature scheme by modifying the sequential aggregate signature scheme
by Chatterjee and Kabaleeshwaran (ACISP 2020). Our scheme offers compact public
parameter size and the public-key aggregation property. This property allows us
to compress a public-key list into a short aggregated key. We prove the
security of our scheme under the symmetric external Diffie-Hellman (SXDH)
assumption without the random oracle model.

</details>


### [58] [Public Key Encryption with Equality Test from Tag-Based Encryption](https://arxiv.org/abs/2509.17722)
*Masayuki Tezuka,Keisuke Tanaka*

Main category: cs.CR

TL;DR: 本文提出了一种基于标签加密的PKEET通用构造方案，无需随机预言机模型，通过不同实例化实现了无配对和基于LPN假设的PKEET方案。


<details>
  <summary>Details</summary>
Motivation: 现有的PKEET通用构造依赖随机预言机模型或基于身份的加密方案，存在局限性。本文旨在设计不依赖这些假设的通用构造方法。

Method: 基于标签加密这一比身份基加密更弱的原语，提出通用构造框架，并通过Kiltz的无配对标签加密方案和基于LPN假设的标签加密方案进行实例化。

Result: 成功构建了无需随机预言机模型的PKEET方案，实现了无配对版本和基于LPN假设的版本，扩展了PKEET的应用范围。

Conclusion: 提出的通用构造方法有效降低了PKEET方案的安全假设要求，为构建更实用的PKEET方案提供了新途径。

Abstract: Public key encryption with equality test (PKEET), proposed by Yang et al.
(CT-RSA 2010), is a variant of public key encryption that enables an equality
test to determine whether two ciphertexts correspond to the same plaintext.
This test applies not only for ciphertexts generated under the same encryption
key but also for those generated under different encryption keys. To date,
several generic constructions of PKEET have been proposed. However, these
generic constructions have the drawback of reliance on the random oracle model
or a (hierarchical) identity-based encryption scheme. In this paper, we propose
a generic construction of a PKEET scheme based on tag-based encryption without
the random oracle model. Tag-based encryption is a weaker primitive than
identity-based encryption. Our scheme allows to derive new PKEET schemes
without the random oracle model. By instantiating our construction with the
pairing-free tag-based encryption scheme by Kiltz (TCC 2006), we obtain a
pairing-free PKEET scheme without the random oracle model. Moreover, by
instantiating our construction with a tag-based encryption scheme based on the
learning parity with noise (LPN) assumption, we obtain a PKEET scheme based on
the LPN assumption without the random oracle model.

</details>


### [59] [AEAS: Actionable Exploit Assessment System](https://arxiv.org/abs/2509.17832)
*Xiangmin Shen,Wenyuan Cheng,Yan Chen,Zhenyuan Li,Yuqiao Gu,Lingzhi Wang,Wencheng Zhao,Dawei Sun,Jiashui Wang*

Main category: cs.CR

TL;DR: AEAS是一个自动化系统，通过静态分析评估和优先处理可操作的漏洞利用代码，解决了现有评分系统在评估实际可用漏洞利用方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 安全从业者在漏洞评估中面临挑战，因为公共漏洞库中存在不一致和低质量的漏洞利用工件。现有评分系统如CVSS和EPSS对此任务支持有限，它们要么依赖理论指标，要么产生不透明的概率估计，无法评估是否存在可用的漏洞利用代码。

Method: AEAS通过静态分析漏洞利用代码和相关文档，提取反映漏洞利用可用性、功能和设置复杂性的结构化特征集，然后计算每个漏洞利用的可操作性分数并生成排名建议。

Result: 在包含600多个真实应用程序的5000多个漏洞数据集上评估，手动验证和专家评审显示AEAS在推荐功能性漏洞利用方面达到100%的前3成功率，并与专家验证的排名高度一致。

Conclusion: AEAS在支持基于漏洞利用的漏洞优先级排序方面表现出有效性，能够帮助安全团队更高效地识别和优先处理真正可用的漏洞利用。

Abstract: Security practitioners face growing challenges in exploit assessment, as
public vulnerability repositories are increasingly populated with inconsistent
and low-quality exploit artifacts. Existing scoring systems, such as CVSS and
EPSS, offer limited support for this task. They either rely on theoretical
metrics or produce opaque probability estimates without assessing whether
usable exploit code exists. In practice, security teams often resort to manual
triage of exploit repositories, which is time-consuming, error-prone, and
difficult to scale. We present AEAS, an automated system designed to assess and
prioritize actionable exploits through static analysis. AEAS analyzes both
exploit code and associated documentation to extract a structured set of
features reflecting exploit availability, functionality, and setup complexity.
It then computes an actionability score for each exploit and produces ranked
exploit recommendations. We evaluate AEAS on a dataset of over 5,000
vulnerabilities derived from 600+ real-world applications frequently
encountered by red teams. Manual validation and expert review on representative
subsets show that AEAS achieves a 100% top-3 success rate in recommending
functional exploits and shows strong alignment with expert-validated rankings.
These results demonstrate the effectiveness of AEAS in supporting
exploit-driven vulnerability prioritization.

</details>


### [60] [Federated Learning in the Wild: A Comparative Study for Cybersecurity under Non-IID and Unbalanced Settings](https://arxiv.org/abs/2509.17836)
*Roberto Doriguzzi-Corin,Petr Sabel,Silvio Cretti,Silvio Ranise*

Main category: cs.CR

TL;DR: 本文系统评估了联邦学习在DDoS攻击入侵检测中的应用，比较了多种FL方法在非独立同分布和不平衡数据环境下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在网络安全分析中面临数据隐私和共享限制，而标准的联邦平均算法在异构环境中收敛效果不佳。需要探索更适合网络入侵检测的FL策略。

Method: 使用Kubernetes测试平台中的网络攻击数据集，系统评估多种FL方法的收敛效率、计算开销、带宽消耗和模型准确性。

Result: 在真实的非独立同分布和不平衡设置下，提供了FL算法在入侵检测中的首个比较分析，为设计鲁棒的隐私保护网络安全解决方案提供了新见解。

Conclusion: 本研究填补了FL算法在网络入侵检测应用中的研究空白，特别是在非理想数据分布条件下的性能评估，为实际部署提供了重要参考。

Abstract: Machine Learning (ML) techniques have shown strong potential for network
traffic analysis; however, their effectiveness depends on access to
representative, up-to-date datasets, which is limited in cybersecurity due to
privacy and data-sharing restrictions. To address this challenge, Federated
Learning (FL) has recently emerged as a novel paradigm that enables
collaborative training of ML models across multiple clients while ensuring that
sensitive data remains local. Nevertheless, Federated Averaging (FedAvg), the
canonical FL algorithm, has proven poor convergence in heterogeneous
environments where data distributions are non-independent and identically
distributed (i.i.d.) and client datasets are unbalanced, conditions frequently
observed in cybersecurity contexts. To overcome these challenges, several
alternative FL strategies have been developed, yet their applicability to
network intrusion detection remains insufficiently explored. This study
systematically reviews and evaluates a range of FL methods in the context of
intrusion detection for DDoS attacks. Using a dataset of network attacks within
a Kubernetes-based testbed, we assess convergence efficiency, computational
overhead, bandwidth consumption, and model accuracy. To the best of our
knowledge, this is the first comparative analysis of FL algorithms for
intrusion detection under realistic non-i.i.d. and unbalanced settings,
providing new insights for the design of robust, privacypreserving network
security solutions.

</details>


### [61] [B-Privacy: Defining and Enforcing Privacy in Weighted Voting](https://arxiv.org/abs/2509.17871)
*Samuel Breckenridge,Dani Vilardell,Andrés Fábrega,Amy Zhao,Patrick McCorry,Rafael Solari,Ari Juels*

Main category: cs.CR

TL;DR: 该论文揭示了加权投票系统颠覆了传统投票隐私概念，提出了基于贿赂成本的B-隐私框架，并通过噪声机制在隐私和透明度之间实现权衡，发现在投票权重集中的系统中隐私保护效果受限。


<details>
  <summary>Details</summary>
Motivation: 传统的一人一票系统通过选票保密实现隐私，但加密货币和web3系统中普遍采用的加权投票系统（按代币持有量分配投票权重）会泄露投票者选择，需要新的隐私框架。

Method: 提出B-隐私概念（基于贿赂成本），设计通过噪声投票结果来增强B-隐私的机制，分析3,582个DAO提案数据，评估机制在投票权重集中程度不同情况下的效果。

Result: 在投票权重高度集中的系统中（存在'鲸鱼'大投票者），任何B-隐私增强技术效果有限；但在需要≥5个投票者联盟才能改变结果的情况下，该机制将B-隐私提高了4.1倍的几何平均数。

Conclusion: 这是首个为加权投票系统提供透明度-隐私权衡原则指导的研究，揭示了投票权重集中对隐私机制的根本限制，补充了传统关注选票保密的方法。

Abstract: In traditional, one-vote-per-person voting systems, privacy equates with
ballot secrecy: voting tallies are published, but individual voters' choices
are concealed.
  Voting systems that weight votes in proportion to token holdings, though, are
now prevalent in cryptocurrency and web3 systems. We show that these
weighted-voting systems overturn existing notions of voter privacy. Our
experiments demonstrate that even with secret ballots, publishing raw tallies
often reveals voters' choices.
  Weighted voting thus requires a new framework for privacy. We introduce a
notion called B-privacy whose basis is bribery, a key problem in voting systems
today. B-privacy captures the economic cost to an adversary of bribing voters
based on revealed voting tallies.
  We propose a mechanism to boost B-privacy by noising voting tallies. We prove
bounds on its tradeoff between B-privacy and transparency, meaning
reported-tally accuracy. Analyzing 3,582 proposals across 30 Decentralized
Autonomous Organizations (DAOs), we find that the prevalence of large voters
("whales") limits the effectiveness of any B-Privacy-enhancing technique.
However, our mechanism proves to be effective in cases without extreme voting
weight concentration: among proposals requiring coalitions of $\geq5$ voters to
flip outcomes, our mechanism raises B-privacy by a geometric mean factor of
$4.1\times$.
  Our work offers the first principled guidance on transparency-privacy
tradeoffs in weighted-voting systems, complementing existing approaches that
focus on ballot secrecy and revealing fundamental constraints that voting
weight concentration imposes on privacy mechanisms.

</details>


### [62] [What if we could hot swap our Biometrics?](https://arxiv.org/abs/2509.17962)
*Jon Crowcroft,Anil Madhavapeddy,Chris Hicks,Richard Mortier,Vasilios Mavroudis*

Main category: cs.CR

TL;DR: 本文探讨了通过生物技术实时改写生物特征来撤销和更换身份的可能性，分析了这种技术可能带来的积极应用和负面后果。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索生物特征身份的可撤销性，因为当前生物特征被认为是不可伪造的，但如果技术发展到可以低成本伪造或更换生物特征，可能会对用户安全构成威胁。

Method: 提出基于新型生物技术的身份热交换机制，通过活体改写生物自我来实现身份更换。

Result: 分析表明，如果生物特征变得易于伪造，伪造他人生物特征的成本可能低于更换自身生物特征的成本，这对用户来说可能是一个不利的交易。

Conclusion: 虽然目前这种威胁还高度推测性，但有必要提前考虑其潜在后果，生物特征可能不再是安全的身份验证方式。

Abstract: What if you could really revoke your actual biometric identity, and install a
new one, by live rewriting your biological self? We propose some novel
mechanisms for hot swapping identity based in novel biotechnology. We discuss
the potential positive use cases, and negative consequences if such technology
was to become available and affordable. Biometrics are selected on the basis
that they are supposed to be unfakeable, or at least not at reasonable cost. If
they become easier to fake, it may be much cheaper to fake someone else's
biometrics than it is for you to change your own biometrics if someone does
copy yours. This potentially makes biometrics a bad trade-off for the user. At
the time of writing, this threat is highly speculative, but we believe it is
worth raising and considering the potential consequences.

</details>


### [63] [The Reverse File System: Towards open cost-effective secure WORM storage devices for logging](https://arxiv.org/abs/2509.17969)
*Gorka Guardiola Múzquiz,Juan González-Gómez,Enrique Soriano-Salvador*

Main category: cs.CR

TL;DR: Socarrat是一种新颖、经济高效的本地WORM存储解决方案，利用外部USB设备实现数据不可变性，无需专用硬件或软件。


<details>
  <summary>Details</summary>
Motivation: 传统WORM存储解决方案依赖昂贵的专用硬件，分布式方法存在拒绝服务漏洞和操作复杂性。需要一种安全、兼容且成本低的本地WORM存储方案。

Method: 采用基于反向文件系统的方法，通过推断主机计算机中文件系统操作来实现WORM功能。使用单板计算机运行Linux和USB OTG支持，实现隔离的WORM执行机制。

Result: 开发了Socarrat原型系统，用Go语言实现并作为自由软件提供。在不同单板计算机上进行了完整的日志性能评估。

Conclusion: Socarrat提供了一种有效的本地WORM存储解决方案，显著减少了攻击面，即使特权攻击者也无法修改存储数据，同时具备防篡改能力。

Abstract: Write Once Read Many (WORM) properties for storage devices are desirable to
ensure data immutability for applications such as secure logging, regulatory
compliance, archival storage, and other types of backup systems. WORM devices
guarantee that data, once written, cannot be altered or deleted. However,
implementing secure and compatible WORM storage remains a challenge.
Traditional solutions often rely on specialized hardware, which is either
costly, closed, or inaccessible to the general public. Distributed approaches,
while promising, introduce additional risks such as denial-of-service
vulnerabilities and operational complexity. We introduce Socarrat, a novel,
cost-effective, and local WORM storage solution that leverages a simple
external USB device (specifically, a single-board computer running Linux with
USB On-The-Go support). The resulting device can be connected via USB,
appearing as an ordinary external disk formatted with an ext4 or exFAT file
system, without requiring any specialized software or drivers. By isolating the
WORM enforcement mechanism in a dedicated USB hardware module, Socarrat
significantly reduces the attack surface and ensures that even privileged
attackers cannot modify or erase stored data. In addition to the WORM capacity,
the system is designed to be tamper-evident, becoming resilient against
advanced attacks. This work describes a novel approach, the Reverse File
System, based on inferring the file system operations occurring at higher
layers in the host computer where Socarrat is mounted. The paper also describes
the current Socarrat prototype, implemented in Go and available as free/libre
software. Finally, it provides a complete evaluation of the logging performance
on different single-board computers.

</details>


### [64] [Synth-MIA: A Testbed for Auditing Privacy Leakage in Tabular Data Synthesis](https://arxiv.org/abs/2509.18014)
*Joshua Ward,Xiaofeng Lin,Chi-Hua Wang,Guang Cheng*

Main category: cs.CR

TL;DR: 提出了Synth-MIA框架，用于评估表格生成模型的隐私泄露风险，通过统一的威胁模型和13种攻击方法系统评估合成数据的隐私保护效果。


<details>
  <summary>Details</summary>
Motivation: 现有的表格生成模型隐私评估方法存在挑战，相似性指标无法有效表征隐私风险，而成员推理攻击(MIA)在实际应用中效果有限且难以一致实施。

Method: 开发了Synth-MIA开源Python库，采用模型无关的威胁框架，集成13种攻击方法，提供Scikit-Learn风格的API，便于系统评估隐私泄露。

Result: 在最大的表格合成隐私基准测试中显示：更高的合成数据质量对应更大的隐私泄露；基于相似性的隐私指标与MIA结果相关性弱；差分隐私生成器PATEGAN在攻击下可能失效。

Conclusion: 强调在设计和使用表格生成模型时，基于MIA的隐私审计是必要的，Synth-MIA为实践者和研究者提供了有效的隐私评估工具。

Abstract: Tabular Generative Models are often argued to preserve privacy by creating
synthetic datasets that resemble training data. However, auditing their
empirical privacy remains challenging, as commonly used similarity metrics fail
to effectively characterize privacy risk. Membership Inference Attacks (MIAs)
have recently emerged as a method for evaluating privacy leakage in synthetic
data, but their practical effectiveness is limited. Numerous attacks exist
across different threat models, each with distinct implementations targeting
various sources of privacy leakage, making them difficult to apply
consistently. Moreover, no single attack consistently outperforms the others,
leading to a routine underestimation of privacy risk.
  To address these issues, we propose a unified, model-agnostic threat
framework that deploys a collection of attacks to estimate the maximum
empirical privacy leakage in synthetic datasets. We introduce Synth-MIA, an
open-source Python library that streamlines this auditing process through a
novel testbed that integrates seamlessly into existing synthetic data
evaluation pipelines through a Scikit-Learn-like API. Our software implements
13 attack methods through a Scikit-Learn-like API, designed to enable fast
systematic estimation of privacy leakage for practitioners as well as
facilitate the development of new attacks and experiments for researchers.
  We demonstrate our framework's utility in the largest tabular synthesis
privacy benchmark to date, revealing that higher synthetic data quality
corresponds to greater privacy leakage, that similarity-based privacy metrics
show weak correlation with MIA results, and that the differentially private
generator PATEGAN can fail to preserve privacy under such attacks. This
underscores the necessity of MIA-based auditing when designing and deploying
Tabular Generative Models.

</details>


### [65] [STAFF: Stateful Taint-Assisted Full-system Firmware Fuzzing](https://arxiv.org/abs/2509.18039)
*Alessio Izzillo,Riccardo Lazzeretti,Emilio Coppa*

Main category: cs.CR

TL;DR: STAFF是一个嵌入式Linux固件模糊测试框架，通过用户驱动的多请求记录、进程间依赖检测和协议感知的污点引导模糊测试，发现涉及多个网络请求和不同固件守护程序的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代嵌入式Linux设备使用复杂的软件栈，但厂商通常使用自定义软件而不记录发布和补丁。现有的覆盖引导模糊测试主要测试单个进程，忽略了守护程序之间的深度依赖关系和持久内部状态。

Method: STAFF采用三个关键技术：(a)用户驱动的多请求记录，监控用户与模拟固件的交互；(b)进程内和进程间依赖检测，使用全系统污点分析跟踪输入字节如何影响用户空间状态；(c)协议感知的污点引导模糊测试，基于识别的依赖关系对请求序列进行变异。

Result: 在15个基于Linux的固件目标上评估STAFF，发现了42个涉及多个网络请求和不同固件守护程序的漏洞，在发现的漏洞数量和可复现性方面显著优于现有最先进的模糊测试解决方案。

Conclusion: STAFF框架能够有效发现嵌入式Linux固件中的复杂漏洞，特别是在处理多请求交互和进程间依赖的场景下表现出色。

Abstract: Modern embedded Linux devices, such as routers, IP cameras, and IoT gateways,
rely on complex software stacks where numerous daemons interact to provide
services. Testing these devices is crucial from a security perspective since
vendors often use custom closed- or open-source software without documenting
releases and patches. Recent coverage-guided fuzzing solutions primarily test
individual processes, ignoring deep dependencies between daemons and their
persistent internal state. This article presents STAFF, a firmware fuzzing
framework for discovering bugs in Linux-based firmware built around three key
ideas: (a) user-driven multi-request recording, which monitors user
interactions with emulated firmware to capture request sequences involving
application-layer protocols (e.g., HTTP); (b) intra- and inter-process
dependency detection, which uses whole-system taint analysis to track how input
bytes influence user-space states, including files, sockets, and memory areas;
(c) protocol-aware taint-guided fuzzing, which applies mutations to request
sequences based on identified dependencies, exploiting multi-staged forkservers
to efficiently checkpoint protocol states. When evaluating STAFF on 15
Linux-based firmware targets, it identifies 42 bugs involving multiple network
requests and different firmware daemons, significantly outperforming existing
state-of-the-art fuzzing solutions in both the number and reproducibility of
discovered bugs.

</details>


### [66] [Hybrid Reputation Aggregation: A Robust Defense Mechanism for Adversarial Federated Learning in 5G and Edge Network Environments](https://arxiv.org/abs/2509.18044)
*Saeid Sheikhi,Panos Kostakos,Lauri Loven*

Main category: cs.CR

TL;DR: 本文提出了一种混合信誉聚合（HRA）机制，用于防御5G和边缘网络环境中联邦学习面临的安全威胁，通过结合几何异常检测和动量信誉跟踪来应对各种对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 5G和边缘网络环境中的联邦学习面临来自对抗客户端的严重安全威胁，恶意参与者可以通过标签翻转、后门注入或Sybil攻击等方式破坏全局模型。

Method: HRA结合了几何异常检测和基于动量的客户端信誉跟踪。在每一轮中，通过基于距离的几何分析检测异常模型更新，同时基于历史行为持续更新每个客户端的信任评分。

Result: 实验结果显示，HRA在5G数据集上达到98.66%的鲁棒全局模型准确率，在NF-CSE-CIC-IDS2018基准上达到96.60%，显著优于Krum、Trimmed Mean和Bulyan等现有聚合器。

Conclusion: HRA在5G/边缘联邦学习部署中展现出增强的弹性和鲁棒性，即使在显著的对抗条件下也能保持良好性能，验证了双机制方法的协同价值。

Abstract: Federated Learning (FL) in 5G and edge network environments face severe
security threats from adversarial clients. Malicious participants can perform
label flipping, inject backdoor triggers, or launch Sybil attacks to corrupt
the global model. This paper introduces Hybrid Reputation Aggregation (HRA), a
novel robust aggregation mechanism designed to defend against diverse
adversarial behaviors in FL without prior knowledge of the attack type. HRA
combines geometric anomaly detection with momentum-based reputation tracking of
clients. In each round, it detects outlier model updates via distance-based
geometric analysis while continuously updating a trust score for each client
based on historical behavior. This hybrid approach enables adaptive filtering
of suspicious updates and long-term penalization of unreliable clients,
countering attacks ranging from backdoor insertions to random noise Byzantine
failures. We evaluate HRA on a large-scale proprietary 5G network dataset (3M+
records) and the widely used NF-CSE-CIC-IDS2018 benchmark under diverse
adversarial attack scenarios. Experimental results reveal that HRA achieves
robust global model accuracy of up to 98.66% on the 5G dataset and 96.60% on
NF-CSE-CIC-IDS2018, outperforming state-of-the-art aggregators such as Krum,
Trimmed Mean, and Bulyan by significant margins. Our ablation studies further
demonstrate that the full hybrid system achieves 98.66% accuracy, while the
anomaly-only and reputation-only variants drop to 84.77% and 78.52%,
respectively, validating the synergistic value of our dual-mechanism approach.
This demonstrates HRA's enhanced resilience and robustness in 5G/edge federated
learning deployments, even under significant adversarial conditions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [67] [Identifying Critical Pathways in Coronary Heart Disease via Fuzzy Subgraph Connectivity](https://arxiv.org/abs/2509.16288)
*Shanookha Ali,Nitha Niralda P C*

Main category: cs.AI

TL;DR: 该论文提出使用模糊子图连通性（FSC）方法来分析冠心病（CHD）风险因素之间的复杂关系，通过构建模糊CHD图来识别关键诊断路径和风险因素。


<details>
  <summary>Details</summary>
Motivation: 冠心病风险涉及不可控因素、可控生活方式因素和临床指标之间的复杂不确定关系，需要一种能够处理这种不确定性的系统方法来支持临床决策。

Method: 构建模糊CHD图，顶点代表不可控因素、可控因素和指标成分，边权重为模糊隶属度，使用FSC评估连通性以识别最强诊断路径、主导风险因素和关键桥梁。

Result: FSC能够突出有影响力的路径，界定最弱和最强相关性之间的连通性边界，并揭示移除关键边会降低预测强度的临界边。

Conclusion: FSC为冠心病风险预测中的不确定性建模提供了一个可解释且稳健的框架，有助于支持临床决策。

Abstract: Coronary heart disease (CHD) arises from complex interactions among
uncontrollable factors, controllable lifestyle factors, and clinical
indicators, where relationships are often uncertain. Fuzzy subgraph
connectivity (FSC) provides a systematic tool to capture such imprecision by
quantifying the strength of association between vertices and subgraphs in fuzzy
graphs. In this work, a fuzzy CHD graph is constructed with vertices for
uncontrollable, controllable, and indicator components, and edges weighted by
fuzzy memberships. Using FSC, we evaluate connectivity to identify strongest
diagnostic routes, dominant risk factors, and critical bridges. Results show
that FSC highlights influential pathways, bounds connectivity between weakest
and strongest correlations, and reveals critical edges whose removal reduces
predictive strength. Thus, FSC offers an interpretable and robust framework for
modeling uncertainty in CHD risk prediction and supporting clinical
decision-making.

</details>


### [68] [A global view of diverse construction methods of fuzzy implication functions rooted on F-chains](https://arxiv.org/abs/2509.16298)
*Raquel Fernandez-Peralta,Juan Vicente Riera*

Main category: cs.AI

TL;DR: 本文提出了模糊蕴涵函数的广义F链构造方法，将Mesiar等人提出的F链构造推广到模糊蕴涵函数领域，使用函数集合和两个递增函数替代单一F链，建立了统一的构造框架。


<details>
  <summary>Details</summary>
Motivation: 模糊蕴涵函数是模糊逻辑中的重要算子，但其构造方法多样且缺乏统一的理论框架。需要深入理解不同构造方法之间的结构关系，建立统一的构造理论。

Method: 推广F链构造方法，使用模糊蕴涵函数集合和两个递增函数来构造新的模糊蕴涵函数，分析性质保持条件，证明该方法可以统一多种现有构造方法。

Result: 建立了广义F链构造的统一框架，证明了该方法可以涵盖对偶、聚合、广义垂直/水平阈值等多种构造方法，揭示了不同构造策略之间的结构相似性。

Conclusion: 广义F链构造为模糊蕴涵函数的构造方法提供了统一的视角，有助于深入理解不同构造方法之间的内在联系，为模糊逻辑理论的发展提供了新的工具。

Abstract: Fuzzy implication functions are one of the most important operators used in
the fuzzy logic framework. While their flexible definition allows for diverse
families with distinct properties, this variety needs a deeper theoretical
understanding of their structural relationships. In this work, we focus on the
study of construction methods, which employ different techniques to generate
new fuzzy implication functions from existing ones. Particularly, we generalize
the $F$-chain-based construction, recently introduced by Mesiar et al. to
extend a method for constructing aggregation functions to the context of fuzzy
implication functions. Our generalization employs collections of fuzzy
implication functions rather than single ones, and uses two different
increasing functions instead of a unique $F$-chain. We analyze property
preservation under this construction and establish sufficient conditions.
Furthermore, we demonstrate that our generalized $F$-chain-based construction
is a unifying framework for several existing methods. In particular, we show
that various construction techniques, such as contraposition, aggregation, and
generalized vertical/horizontal threshold methods, can be reformulated within
our approach. This reveals structural similarities between seemingly distinct
construction strategies and provides a cohesive perspective on fuzzy
implication construction methods.

</details>


### [69] [On the Non-Uniqueness of Representation of $(U,N)$-Implications](https://arxiv.org/abs/2509.16299)
*Raquel Fernandez-Peralta,Andrea Mesiarová-Zemánková*

Main category: cs.AI

TL;DR: 本文证明了(U,N)-蕴涵函数不一定具有唯一表示，即使模糊否定是连续的，推翻了先前关于唯一性的假设，并对连续和非连续基础函数的唯一性条件进行了全面研究。


<details>
  <summary>Details</summary>
Motivation: 模糊蕴涵函数是模糊逻辑系统中的基本算子，扩展了经典条件句以处理逻辑推理中的不确定性。先前的研究假设在模糊否定N连续的情况下，(U,N)-蕴涵具有唯一表示，本文旨在验证这一假设的正确性。

Method: 通过理论分析和证明，研究(U,N)-蕴涵函数的表示唯一性问题，特别关注连续和非连续基础函数的情况。

Result: 证明了(U,N)-蕴涵函数不一定具有唯一表示，即使模糊否定是连续的，这推翻了先前的假设。

Conclusion: 研究结果为理解这些算子的结构特性提供了重要的理论见解，表明(U,N)-蕴涵的表示唯一性条件比先前认为的更复杂。

Abstract: Fuzzy implication functions constitute fundamental operators in fuzzy logic
systems, extending classical conditionals to manage uncertainty in logical
inference. Among the extensive families of these operators, generalizations of
the classical material implication have received considerable theoretical
attention, particularly $(S,N)$-implications constructed from t-conorms and
fuzzy negations, and their further generalizations to $(U,N)$-implications
using disjunctive uninorms. Prior work has established characterization
theorems for these families under the assumption that the fuzzy negation $N$ is
continuous, ensuring uniqueness of representation. In this paper, we disprove
this last fact for $(U,N)$-implications and we show that they do not
necessarily possess a unique representation, even if the fuzzy negation is
continuous. Further, we provide a comprehensive study of uniqueness conditions
for both uninorms with continuous and non-continuous underlying functions. Our
results offer important theoretical insights into the structural properties of
these operators.

</details>


### [70] [Generalizability of Large Language Model-Based Agents: A Comprehensive Survey](https://arxiv.org/abs/2509.16330)
*Minxing Zhang,Yi Yang,Roy Xie,Bhuwan Dhingra,Shuyan Zhou,Jian Pei*

Main category: cs.AI

TL;DR: 这篇论文是关于LLM智能体泛化性的首次全面综述，重点探讨了如何确保基于大语言模型的智能体在不同指令、任务、环境和领域中的一致性表现。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在网页导航、家庭机器人等领域的广泛应用，确保其泛化能力成为关键挑战。目前智能体泛化性的概念定义不清，缺乏系统性的测量和改进方法。

Method: 论文采用综述研究方法，首先明确了智能体泛化性的重要性，建立了层次化的领域-任务本体，回顾了现有数据集、评估维度和方法，并将改进方法分为LLM骨干、智能体组件及其交互三类。

Result: 提出了可泛化框架与可泛化智能体的区分，并阐述了如何将框架级泛化转化为智能体级泛化。识别了标准化框架、基于方差和成本的指标等关键挑战。

Conclusion: 通过综合现有进展和突出机会，本综述旨在为构建能够可靠泛化到多样化应用的LLM智能体建立理论基础，推动该领域的规范化研究。

Abstract: Large Language Model (LLM)-based agents have emerged as a new paradigm that
extends LLMs' capabilities beyond text generation to dynamic interaction with
external environments. By integrating reasoning with perception, memory, and
tool use, agents are increasingly deployed in diverse domains like web
navigation and household robotics. A critical challenge, however, lies in
ensuring agent generalizability - the ability to maintain consistent
performance across varied instructions, tasks, environments, and domains,
especially those beyond agents' fine-tuning data. Despite growing interest, the
concept of generalizability in LLM-based agents remains underdefined, and
systematic approaches to measure and improve it are lacking. In this survey, we
provide the first comprehensive review of generalizability in LLM-based agents.
We begin by emphasizing agent generalizability's importance by appealing to
stakeholders and clarifying the boundaries of agent generalizability by
situating it within a hierarchical domain-task ontology. We then review
datasets, evaluation dimensions, and metrics, highlighting their limitations.
Next, we categorize methods for improving generalizability into three groups:
methods for the backbone LLM, for agent components, and for their interactions.
Moreover, we introduce the distinction between generalizable frameworks and
generalizable agents and outline how generalizable frameworks can be translated
into agent-level generalizability. Finally, we identify critical challenges and
future directions, including developing standardized frameworks, variance- and
cost-based metrics, and approaches that integrate methodological innovations
with architecture-level designs. By synthesizing progress and highlighting
opportunities, this survey aims to establish a foundation for principled
research on building LLM-based agents that generalize reliably across diverse
applications.

</details>


### [71] [Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models](https://arxiv.org/abs/2509.16332)
*Stephen Fitz,Peter Romero,Steven Basart,Sipeng Chen,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 该论文研究了通过调节大语言模型的人格特质（基于大五人格框架）对模型能力和安全性的影响，发现降低尽责性会显著降低安全相关指标和一般能力，强调了人格塑造作为模型控制的重要维度。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究表明LLMs表现出可测量的合成人格特质，但关于调节这些特质如何影响模型行为的研究还很缺乏，特别是在能力和安全基准测试中的影响。

Method: 基于大五人格框架进行心理测量人格控制实验，在WMDP、TruthfulQA、ETHICS、Sycophancy等安全基准和MMLU能力基准上测试不同人格特质下的模型表现。

Result: 降低尽责性会导致安全相关指标显著下降，同时一般能力（MMLU）也会降低，表明人格特质调节对模型行为有显著影响。

Conclusion: 人格塑造是一个强大且未被充分探索的模型控制维度，与安全性和能力都相关，需要开展人格敏感的安全评估和动态行为控制研究。

Abstract: Large Language Models increasingly mediate high-stakes interactions,
intensifying research on their capabilities and safety. While recent work has
shown that LLMs exhibit consistent and measurable synthetic personality traits,
little is known about how modulating these traits affects model behavior. We
address this gap by investigating how psychometric personality control grounded
in the Big Five framework influences AI behavior in the context of capability
and safety benchmarks. Our experiments reveal striking effects: for example,
reducing conscientiousness leads to significant drops in safety-relevant
metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well
as reduction in general capabilities as measured by MMLU. These findings
highlight personality shaping as a powerful and underexplored axis of model
control that interacts with both safety and general competence. We discuss the
implications for safety evaluation, alignment strategies, steering model
behavior after deployment, and risks associated with possible exploitation of
these findings. Our findings motivate a new line of research on
personality-sensitive safety evaluations and dynamic behavioral control in
LLMs.

</details>


### [72] [A Unified AI Approach for Continuous Monitoring of Human Health and Diseases from Intensive Care Unit to Home with Physiological Foundation Models (UNIPHY+)](https://arxiv.org/abs/2509.16348)
*Minxiao Wang,Saurabh Kataria,Juntong Ni,Timothy G. Buchman,Jocelyn Grunwell,Mark Mai,Wei Jin,Matthew Clark,Stephanie Brown,Michael Fundora,Puneet Sharma,Tony Pan,Sam Khan,Timothy Ruchti,Naveen Muthu,Kevin Maher,Sivasubramanium V Bhavani,Xiao Hu*

Main category: cs.AI

TL;DR: UNIPHY+是一个统一的生理基础模型框架，旨在利用普遍可获得的生理数据实现跨护理环境的连续人类健康和疾病监测。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够整合上下文信息并支持个性化生理AI的通用框架，以增强临床决策和长期健康监测能力。

Method: 提出多模态学习、特征融合调优和知识蒸馏等策略，在预训练、微调和轻量级模型个性化阶段融入上下文信息。

Result: 通过在重症监护到动态监测等多种用例中测试，证明UNIPHY+能够实现可泛化、可扩展和个性化的生理AI。

Conclusion: UNIPHY+框架有潜力赋能通用、可扩展和个性化的生理AI，支持临床决策和长期健康监测。

Abstract: We present UNIPHY+, a unified physiological foundation model (physioFM)
framework designed to enable continuous human health and diseases monitoring
across care settings using ubiquitously obtainable physiological data. We
propose novel strategies for incorporating contextual information during
pretraining, fine-tuning, and lightweight model personalization via multi-modal
learning, feature fusion-tuning, and knowledge distillation. We advocate
testing UNIPHY+ with a broad set of use cases from intensive care to ambulatory
monitoring in order to demonstrate that UNIPHY+ can empower generalizable,
scalable, and personalized physiological AI to support both clinical
decision-making and long-term health monitoring.

</details>


### [73] [Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation](https://arxiv.org/abs/2509.16372)
*Balu Bhasuran,Mattia Prosperi,Karim Hanna,John Petrilli,Caretia JeLayne Washington,Zhe He*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型在临床实验室测试场景中的因果推理能力，GPT-o1在关联、干预和反事实推理方面均优于Llama-3.2-8b-instruct，但两种模型在反事实推理场景中表现最差，需要进一步改进才能用于高风险临床应用。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在临床环境中的因果推理能力，特别是在实验室测试场景中应用Pearl的因果阶梯理论（关联、干预、反事实推理），以确定这些模型是否适合用于高风险的医疗决策。

Method: 使用99个基于临床的实验室测试场景，涵盖血红蛋白A1c、肌酐、维生素D等常见测试，结合年龄、性别、肥胖、吸烟等因果因素。测试GPT-o1和Llama-3.2-8b-instruct两个模型，由四位医学专家评估响应质量。

Result: GPT-o1在整体判别性能（AUROC = 0.80）上优于Llama-3.2-8b-instruct（0.73），在关联（0.75 vs 0.72）、干预（0.84 vs 0.70）和反事实推理（0.84 vs 0.69）方面均表现更好。两种模型在干预问题上表现最佳，在反事实场景中表现最差。

Conclusion: GPT-o1提供了更一致的因果推理能力，但在应用于高风险临床环境之前仍需进一步改进，特别是在反事实推理方面。

Abstract: This study evaluates causal reasoning in large language models (LLMs) using
99 clinically grounded laboratory test scenarios aligned with Pearl's Ladder of
Causation: association, intervention, and counterfactual reasoning. We examined
common laboratory tests such as hemoglobin A1c, creatinine, and vitamin D, and
paired them with relevant causal factors including age, gender, obesity, and
smoking. Two LLMs - GPT-o1 and Llama-3.2-8b-instruct - were tested, with
responses evaluated by four medically trained human experts. GPT-o1
demonstrated stronger discriminative performance (AUROC overall = 0.80 +/-
0.12) compared to Llama-3.2-8b-instruct (0.73 +/- 0.15), with higher scores
across association (0.75 vs 0.72), intervention (0.84 vs 0.70), and
counterfactual reasoning (0.84 vs 0.69). Sensitivity (0.90 vs 0.84) and
specificity (0.93 vs 0.80) were also greater for GPT-o1, with reasoning ratings
showing similar trends. Both models performed best on intervention questions
and worst on counterfactuals, particularly in altered outcome scenarios. These
findings suggest GPT-o1 provides more consistent causal reasoning, but
refinement is required before adoption in high-stakes clinical applications.

</details>


### [74] [VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping](https://arxiv.org/abs/2509.16399)
*Guojun Xiong,Milind Tambe*

Main category: cs.AI

TL;DR: VORTEX是一个语言引导的奖励塑造框架，通过多目标优化方法将人类自然语言偏好融入AI决策系统，同时保持核心效用保证


<details>
  <summary>Details</summary>
Motivation: 现有AI决策系统无法直接适应人类自然语言表达的动态偏好，而现有LLM方法虽然灵活但可能牺牲系统核心效用保证

Method: 将问题形式化为多目标优化，使用LLM迭代生成基于语言反馈和文本梯度提示更新的塑造奖励，允许利益相关者通过自然语言引导决策行为

Result: VORTEX在真实世界分配任务中优于基线方法，在满足人类对齐覆盖目标的同时保持高任务性能

Conclusion: 这项工作为基于自然语言的人类-AI协作优化引入了一个实用且理论基础的范式

Abstract: In social impact optimization, AI decision systems often rely on solvers that
optimize well-calibrated mathematical objectives. However, these solvers cannot
directly accommodate evolving human preferences, typically expressed in natural
language rather than formal constraints. Recent approaches address this by
using large language models (LLMs) to generate new reward functions from
preference descriptions. While flexible, they risk sacrificing the system's
core utility guarantees. In this paper, we propose \texttt{VORTEX}, a
language-guided reward shaping framework that preserves established
optimization goals while adaptively incorporating human feedback. By
formalizing the problem as multi-objective optimization, we use LLMs to
iteratively generate shaping rewards based on verbal reinforcement and
text-gradient prompt updates. This allows stakeholders to steer decision
behavior via natural language without modifying solvers or specifying trade-off
weights. We provide theoretical guarantees that \texttt{VORTEX} converges to
Pareto-optimal trade-offs between utility and preference satisfaction.
Empirical results in real-world allocation tasks demonstrate that
\texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage
goals while maintaining high task performance. This work introduces a practical
and theoretically grounded paradigm for human-AI collaborative optimization
guided by natural language.

</details>


### [75] [Proactive Statistical Process Control Using AI: A Time Series Forecasting Approach for Semiconductor Manufacturing](https://arxiv.org/abs/2509.16431)
*Mohammad Iqbal Rasul Seeam,Victor S. Sheng*

Main category: cs.AI

TL;DR: 本文提出了一种将Facebook Prophet机器学习模型与传统统计过程控制(SPC)相结合的方法，用于预测制造过程中的潜在问题，实现主动质量控制。


<details>
  <summary>Details</summary>
Motivation: 传统SPC方法只能在问题发生后进行反应性检测，导致材料浪费、机器停机和成本增加。需要一种能够提前预测问题的主动质量控制方法。

Method: 使用Facebook Prophet时间序列预测模型分析历史数据，预测未来测量值，然后应用SPC规则将预测值分类为安全区、警告区或临界区。

Result: 在半导体制造公司的真实数据上测试，尽管数据采集时间间隔不规则，模型仍能做出准确预测并正确分类未来测量值的风险等级。

Conclusion: 结合机器学习与传统SPC的质量控制方法更加主动、准确和实用，有助于减少意外故障，提高生产过程的稳定性和可靠性。

Abstract: In the manufacturing industry, it is very important to keep machines and
processes running smoothly and without unexpected problems. One of the most
common tools used to check if everything is working properly is called
Statistical Process Control (SPC). Traditional SPC methods work by checking
whether recent measurements are within acceptable limits. However, they only
react after a problem has already occurred. This can lead to wasted materials,
machine downtime, and increased costs. In this paper, we present a smarter way
to use SPC. Instead of just reacting to issues after they happen, our system
can predict future problems before they occur. We use a machine learning tool
called Facebook Prophet, which is designed to work with time-series data (data
that changes over time). Prophet looks at past data and forecasts what the next
value will be. Then, we use SPC rules to decide if the predicted value is in a
Safe zone (no problem), a Warning zone (needs attention), or a Critical zone
(may require shutting down the process). We applied this system to real data
from a semiconductor manufacturing company. One of the challenges with this
data is that the measurements are not taken at regular time intervals. This
makes it harder to predict future values accurately. Despite this, our model
was able to make strong predictions and correctly classify the risk level of
future measurements. The main benefit of our system is that it gives engineers
and technicians a chance to act early - before something goes wrong. This helps
reduce unexpected failures and improves the overall stability and reliability
of the production process. By combining machine learning with traditional SPC,
we make quality control more proactive, accurate, and useful for modern
industry.

</details>


### [76] [Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots](https://arxiv.org/abs/2509.16444)
*Chenhan Lyu,Yutong Song,Pengfei Zhang,Amir M. Rahmani*

Main category: cs.AI

TL;DR: 本文提出了一种基于宪法AI训练的领域特定心理健康原则方法，用于开发安全、领域适应的CAI系统，以解决心理健康应用中AI安全挑战。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康问题日益严重，AI在心理护理中的集成需求增加，但现有AI安全措施无法充分应对心理健康领域的特殊挑战，如危机干预准确性、治疗指南遵守、资源受限环境下的扩展性等。

Method: 采用宪法AI训练方法，结合领域特定的心理健康原则，开发专门针对心理健康应用的安全AI系统。

Result: 该方法能够更好地处理心理健康应用中的敏感数据，提高危机干预准确性，确保治疗指南遵守，并适应资源受限环境。

Conclusion: 宪法AI训练结合心理健康特定原则是解决心理健康应用中AI安全挑战的有效途径，能够提供更安全、更适应的AI系统支持心理健康服务。

Abstract: Mental health applications have emerged as a critical area in computational
health, driven by rising global rates of mental illness, the integration of AI
in psychological care, and the need for scalable solutions in underserved
communities. These include therapy chatbots, crisis detection, and wellness
platforms handling sensitive data, requiring specialized AI safety beyond
general safeguards due to emotional vulnerability, risks like misdiagnosis or
symptom exacerbation, and precise management of vulnerable states to avoid
severe outcomes such as self-harm or loss of trust. Despite AI safety advances,
general safeguards inadequately address mental health-specific challenges,
including crisis intervention accuracy to avert escalations, therapeutic
guideline adherence to prevent misinformation, scale limitations in
resource-constrained settings, and adaptation to nuanced dialogues where
generics may introduce biases or miss distress signals. We introduce an
approach to apply Constitutional AI training with domain-specific mental health
principles for safe, domain-adapted CAI systems in computational mental health
applications.

</details>


### [77] [GPO: Learning from Critical Steps to Improve LLM Reasoning](https://arxiv.org/abs/2509.16456)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: GPO是一种新颖的微调策略，通过识别推理轨迹中的关键步骤来提升大语言模型的多步推理能力。该方法首先估计优势函数定位关键步骤，然后重置策略并优先学习这些关键时刻，从而更有效地改进推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法通常将推理轨迹视为整体，没有考虑轨迹中的关键步骤。为了更有效地提升LLMs的多步推理能力，需要深入推理过程，关注对问题解决至关重要的关键时刻。

Method: GPO首先通过估计优势函数识别推理轨迹中的关键步骤，然后重置策略到关键步骤，采样新的轨迹并优先学习这些关键时刻。该方法可与各种优化方法结合使用。

Result: 在多个具有挑战性的推理基准测试中，GPO能够一致且显著地提升现有优化方法的性能，证明了其在改进LLM推理方面的有效性和通用性。

Conclusion: GPO通过关注推理过程中的关键时刻，为提升大语言模型的多步推理能力提供了一种通用且有效的策略，可以集成到各种优化方法中持续改进推理性能。

Abstract: Large language models (LLMs) are increasingly used in various domains,
showing impressive potential on different tasks. Recently, reasoning LLMs have
been proposed to improve the \textit{reasoning} or \textit{thinking}
capabilities of LLMs to solve complex problems. Despite the promising results
of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs
still remains a significant challenge. While existing optimization methods have
advanced the LLM reasoning capabilities, they often treat reasoning
trajectories as a whole, without considering the underlying critical steps
within the trajectory. In this paper, we introduce \textbf{G}uided
\textbf{P}ivotal \textbf{O}ptimization (GPO), a novel fine-tuning strategy that
dives into the reasoning process to enable more effective improvements. GPO
first identifies the `critical step' within a reasoning trajectory - a point
that the model must carefully proceed to succeed at the problem. We locate the
critical step by estimating the advantage function. GPO then resets the policy
to the critical step, samples the new rollout and prioritizes the learning
process on those rollouts. This focus allows the model to learn more
effectively from pivotal moments within the reasoning process to improve the
reasoning performance. We demonstrate that GPO is a general strategy that can
be integrated with various optimization methods to improve reasoning
performance. Besides theoretical analysis, our experiments across challenging
reasoning benchmarks show that GPO can consistently and significantly enhance
the performance of existing optimization methods, showcasing its effectiveness
and generalizability in improving LLM reasoning by concentrating on pivotal
moments within the generation process.

</details>


### [78] [Checking extracted rules in Neural Networks](https://arxiv.org/abs/2509.16547)
*Adrian Wurm*

Main category: cs.AI

TL;DR: 本文从计算复杂性理论的角度研究神经网络提取规则的正式验证问题，包括规则适用性、一致性和完备性三个核心问题，并证明这些问题大多是co-NP完全的。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络提取规则的验证问题，因为现有规则提取方法多使用启发式算法和随机近似，需要验证这些方法获得的知识是否可靠。

Method: 针对ReLU激活神经网络和布尔网络，研究不同类型的规则验证问题，通过问题间的归约分析计算复杂性。

Result: 证明了大多数规则验证问题是co-NP完全的，表明这些问题在计算上具有很高的复杂性。

Conclusion: 神经网络规则提取的正式验证具有很高的计算复杂性，这对实际应用中的规则可信度验证提出了挑战。

Abstract: In this paper we investigate formal verification of extracted rules for
Neural Networks under a complexity theoretic point of view. A rule is a global
property or a pattern concerning a large portion of the input space of a
network. These rules are algorithmically extracted from networks in an effort
to better understand their inner way of working. Here, three problems will be
in the focus: Does a given set of rules apply to a given network? Is a given
set of rules consistent or do the rules contradict themselves? Is a given set
of rules exhaustive in the sense that for every input the output is determined?
Finding algorithms that extract such rules out of networks has been
investigated over the last 30 years, however, to the author's current
knowledge, no attempt in verification was made until now. A lot of attempts of
extracting rules use heuristics involving randomness and over-approximation, so
it might be beneficial to know whether knowledge obtained in that way can
actually be trusted.
  We investigate the above questions for neural networks with ReLU-activation
as well as for Boolean networks, each for several types of rules. We
demonstrate how these problems can be reduced to each other and show that most
of them are co-NP-complete.

</details>


### [79] [SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.16561)
*Yue Xin,Chen Shen,Shaotian Yan,Xiaosong Yuan,Yaoming Wang,Xiaofeng Zhang,Chenxi Huang,Jieping Ye*

Main category: cs.AI

TL;DR: SalaMAnder是一个基于Shapley值的数学表达式归因框架，通过CoSP指标量化思维链提示中组件的贡献度，为CoT的成功提供理论解释并优化提示构建。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链提示显著提升了大型语言模型的数学推理能力，但其背后的机制尚未被充分探索。本文旨在开发一个理论严谨的方法来量化CoT中各组件的贡献。

Method: 提出SalaMAnder框架，利用Shapley值进行数学表达式归因，开发高效的分层采样算法降低计算复杂度，并通过协方差分析建立CoSP评估指标。

Result: 在多个LLM模型和数学基准上的验证表明，CoSP指标与模型性能呈现稳健的单调相关性，为现有few-shot CoT的成功提供了理论解释。

Conclusion: SalaMAnder框架不仅解释了CoT的有效性，还建立了数学严谨的提示构建原则，统一了先前工作的见解，验证了解释的可靠性。

Abstract: Chain-of-Thought (CoT) prompting enhances the math reasoning capability of
large language models (LLMs) to a large margin. However, the mechanism
underlying such improvements remains unexplored. In this paper, we present
\textbf{SalaMAnder} (\textbf{S}h\textbf{a}p\textbf{l}ey-b\textbf{a}sed
\textbf{M}athematical Expression \textbf{A}ttribution a\textbf{nd}
M\textbf{e}t\textbf{r}ic), a theoretically grounded methodology as well as a
mathematically rigorous evaluation metric for quantifying component-level
contributions in few-shot CoT reasoning. Concretely, we leverage the Shapley
value for mathematical expression attribution and develop an efficient
stratified sampling algorithm that significantly reduces the computational
complexity. Besides, we develop the \textbf{CoSP} (\textbf{C}ardinality
\textbf{o}f \textbf{S}hapley \textbf{P}ositives) metric through covariance
analysis. Comprehensive validation across popular LLM models and diverse
mathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder
framework exhibits a robust monotonic correlation with model performance, not
only providing theoretical explanations for the empirical success of existing
few-shot CoT but also establishing mathematically rigorous principles for
prompt construction optimization. Furthermore, we verify the reliability of the
explanation, based on which we unify the insights of previous work.

</details>


### [80] [Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning](https://arxiv.org/abs/2509.16578)
*Wenyao Li,Ran Zhang,Pengyang Wang,Yuanchun Zhou,Pengfei Wang*

Main category: cs.AI

TL;DR: ZHMF是一个零样本人类移动预测框架，通过语义增强检索和分层语言模型推理系统来处理未见过的用户和位置预测场景，将移动预测重新定义为自然语言问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以泛化到未见过的用户或位置，且由于标记数据有限和移动模式复杂性，难以捕捉动态意图。

Method: 结合语义增强检索和反射机制，采用分层语言模型推理系统，将预测分解为活动级规划器和位置级选择器，实现长期用户意图和短期上下文偏好的协同建模。

Result: 在标准人类移动数据集上的实验表明，该方法优于现有模型，消融研究验证了各模块的贡献。

Conclusion: 该方法能有效捕捉用户意图并适应多样化上下文场景，为零样本人类移动预测提供了有效解决方案。

Abstract: Human mobility forecasting is important for applications such as
transportation planning, urban management, and personalized recommendations.
However, existing methods often fail to generalize to unseen users or locations
and struggle to capture dynamic intent due to limited labeled data and the
complexity of mobility patterns. We propose ZHMF, a framework for zero-shot
human mobility forecasting that combines a semantic enhanced retrieval and
reflection mechanism with a hierarchical language model based reasoning system.
The task is reformulated as a natural language question answering paradigm.
Leveraging LLMs semantic understanding of user histories and context, our
approach handles previously unseen prediction scenarios. We further introduce a
hierarchical reflection mechanism for iterative reasoning and refinement by
decomposing forecasting into an activity level planner and a location level
selector, enabling collaborative modeling of long term user intentions and
short term contextual preferences. Experiments on standard human mobility
datasets show that our approach outperforms existing models. Ablation studies
reveal the contribution of each module, and case studies illustrate how the
method captures user intentions and adapts to diverse contextual scenarios.

</details>


### [81] [Question Answering with LLMs and Learning from Answer Sets](https://arxiv.org/abs/2509.16590)
*Manuel Borroto,Katie Gallagher,Antonio Ielo,Irfan Kareem,Francesco Ricca,Alessandra Russo*

Main category: cs.AI

TL;DR: LLM2LAS是一个结合大型语言模型、答案集学习系统和答案集编程的混合系统，用于解决故事问答任务中的常识推理问题，能够自动学习符号推理规则。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言理解方面表现出色，但在显式常识推理方面存在困难。现有方法通常依赖人工设计符号组件，而本文认为这个组件可以从示例中自动学习。

Method: 使用LLM从文本中提取语义结构，然后通过ILASP系统将其转换为可解释的逻辑规则，最后利用ASP求解器进行精确推理。

Result: 实证结果展示了该方法在故事问答基准测试中学习和推理的优势和局限性。

Conclusion: LLM2LAS系统成功地将自然语言理解、规则学习和形式推理相结合，能够对未见问题给出正确答案，证明了自动学习符号推理规则的可行性。

Abstract: Large Language Models (LLMs) excel at understanding natural language but
struggle with explicit commonsense reasoning. A recent trend of research
suggests that the combination of LLM with robust symbolic reasoning systems can
overcome this problem on story-based question answering tasks. In this setting,
existing approaches typically depend on human expertise to manually craft the
symbolic component. We argue, however, that this component can also be
automatically learned from examples. In this work, we introduce LLM2LAS, a
hybrid system that effectively combines the natural language understanding
capabilities of LLMs, the rule induction power of the Learning from Answer Sets
(LAS) system ILASP, and the formal reasoning strengths of Answer Set
Programming (ASP). LLMs are used to extract semantic structures from text,
which ILASP then transforms into interpretable logic rules. These rules allow
an ASP solver to perform precise and consistent reasoning, enabling correct
answers to previously unseen questions. Empirical results outline the strengths
and weaknesses of our automatic approach for learning and reasoning in a
story-based question answering benchmark.

</details>


### [82] [FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs](https://arxiv.org/abs/2509.16648)
*Debarpan Bhattacharya,Apoorva Kulkarni,Sriram Ganapathy*

Main category: cs.AI

TL;DR: FESTA是一种用于多模态大语言模型信任评估的输入采样技术，通过等效和互补采样生成不确定性度量，无需真实标签即可检测错误预测。


<details>
  <summary>Details</summary>
Motivation: 由于多模态输入范式的多样性，准确评估MLLM生成预测的可信度具有挑战性，需要开发能够实现选择性预测并提高用户信心的信任评估方法。

Method: 提出功能等效采样技术FESTA，通过任务保持采样方法扩展输入空间，探测模型的一致性（通过等效样本）和敏感性（通过互补样本），仅需模型的黑箱输入输出访问。

Result: 在视觉和音频推理任务上，FESTA不确定性估计在选择性预测性能上取得显著提升（视觉LLM相对改进33.3%，音频LLM相对改进29.6%），基于AUROC指标检测错误预测。

Conclusion: FESTA作为一种无监督的黑箱方法，能够有效评估多模态大语言模型的不确定性，提高错误预测检测性能，代码已开源。

Abstract: The accurate trust assessment of multimodal large language models (MLLMs)
generated predictions, which can enable selective prediction and improve user
confidence, is challenging due to the diverse multi-modal input paradigms. We
propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a
multimodal input sampling technique for MLLMs, that generates an uncertainty
measure based on the equivalent and complementary input samplings. The proposed
task-preserving sampling approach for uncertainty quantification expands the
input space to probe the consistency (through equivalent samples) and
sensitivity (through complementary samples) of the model. FESTA uses only
input-output access of the model (black-box), and does not require ground truth
(unsupervised). The experiments are conducted with various off-the-shelf
multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA
uncertainty estimate achieves significant improvement (33.3% relative
improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in
selective prediction performance, based on
area-under-receiver-operating-characteristic curve (AUROC) metric in detecting
mispredictions. The code implementation is open-sourced.

</details>


### [83] [NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities](https://arxiv.org/abs/2509.16656)
*Changyu Zeng,Yifan Wang,Zimu Wang,Wei Wang,Zhengni Yang,Muyi Bao,Jiming Xiao,Ahn Nguyen,Yutao Yue*

Main category: cs.AI

TL;DR: NUMINA是首个用于增强多模态室内感知理解的自然理解基准，专注于多维智能和数值推理能力，填补了现有3D基准在细粒度数值推理任务标注方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有3D基准缺乏细粒度数值推理任务标注，限制了MLLMs进行精确空间测量和复杂数值推理的能力。

Method: 开发了NUMINA-Flow自动化标注流程，集成LLM重写和基于规则的自我验证，生成多尺度标注和多样化问答对。

Result: 评估显示当前LLMs在多模态数值推理方面表现不佳，特别是在距离和体积估计等精确计算任务上存在困难。

Conclusion: 该研究强调了3D模型在数值推理能力方面需要进一步改进，NUMINA基准为评估和推进3D多模态理解提供了重要工具。

Abstract: Recent advancements in 2D multimodal large language models (MLLMs) have
significantly improved performance in vision-language tasks. However, extending
these capabilities to 3D environments remains a distinct challenge due to the
complexity of spatial reasoning. Nevertheless, existing 3D benchmarks often
lack fine-grained numerical reasoning task annotations, limiting MLLMs' ability
to perform precise spatial measurements and complex numerical reasoning. To
address this gap, we introduce NUMINA, the first Natural Understanding
benchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities
to enhance multimodal indoor perceptual understanding. NUMINA features
multi-scale annotations and various question-answer pairs, generated using
NUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and
rule-based self-verification. We evaluate the performance of various
state-of-the-art LLMs on NUMINA following the Chat-Scene framework,
demonstrating that current LLMs struggle with multimodal numerical reasoning,
particularly in performing precise computations such as distance and volume
estimation, highlighting the need for further advancements in 3D models. The
dataset and source codes can be obtained from
https://github.com/fengshun124/NUMINA.

</details>


### [84] [Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories](https://arxiv.org/abs/2509.16742)
*Mohammad Beigi,Ying Shen,Parshin Shojaee,Qifan Wang,Zichao Wang,Chandan Reddy,Ming Jin,Lifu Huang*

Main category: cs.AI

TL;DR: SMART框架通过将奉承行为重新定义为推理优化问题，提出两阶段方法（UA-MCTS和基于进度的强化学习）来显著减少大语言模型的奉承行为，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练范式无意中培养了奉承行为，即模型倾向于同意或强化用户提供的信息，即使这些信息在事实上是错误的。

Method: SMART是一个两阶段框架：1）不确定性感知自适应蒙特卡洛树搜索（UA-MCTS），基于状态级不确定性动态调整模型探索；2）基于进度的强化学习，使用收集的轨迹和奖励信号微调模型。

Result: 实验表明SMART显著减少了奉承行为，同时在分布外输入上保持强性能，并维持一般能力。

Conclusion: 优化内部推理机制对于构建更真实和对齐的AI助手至关重要。

Abstract: Despite the remarkable capabilities of large language models, current
training paradigms inadvertently foster \textit{sycophancy}, i.e., the tendency
of a model to agree with or reinforce user-provided information even when it's
factually incorrect. To address this challenge, we introduce \textbf{SMART}
(Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes
sycophancy as a \textit{reasoning optimization problem} rather than an output
alignment issue. SMART is a two-stage framework comprising: (1)
Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically
adjusts model exploration based on state-level uncertainty to collect
high-quality, diverse reasoning trajectories alongside both stepwise progress
and final outcome rewards; and (2) progress-based reinforcement learning, which
fine-tunes the model using the collected trajectories and reward signals to
reinforce effective reasoning patterns. Through extensive experiments, we show
that SMART significantly reduces sycophantic behavior while preserving strong
performance on out-of-distribution inputs and maintaining general capabilities.
These results underscore the importance of optimizing internal reasoning
mechanisms to build more truthful and aligned AI assistants.

</details>


### [85] [Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing Skills Assessment](https://arxiv.org/abs/2509.16810)
*Shen Chang,Dennis Liu,Renran Tian,Kristen L. Swartzell,Stacie L. Klingler,Amy M. Nagle,Nan Kong*

Main category: cs.AI

TL;DR: 提出基于视频语言模型的AI框架，用于自动化护理技能评估和反馈，解决传统护理培训依赖主观人工反馈的局限性


<details>
  <summary>Details</summary>
Motivation: 当前护理教育依赖主观、耗时的教师反馈，限制了培训的可扩展性和效率，影响护理人员入职后的专业能力

Method: 采用课程启发式渐进框架，从高层次动作识别到细粒度子动作分解，最终实现程序推理，提供错误诊断、可解释反馈和客观评估三大核心功能

Result: 在合成视频上的验证显示系统能够可靠检测错误并进行时间定位，具备处理真实训练变异性的潜力

Conclusion: 该工作通过解决工作流程瓶颈和支持大规模标准化评估，推进了AI在护理教育中的应用，有助于加强劳动力发展和提升患者安全

Abstract: Consistent high-quality nursing care is essential for patient safety, yet
current nursing education depends on subjective, time-intensive instructor
feedback in training future nurses, which limits scalability and efficiency in
their training, and thus hampers nursing competency when they enter the
workforce. In this paper, we introduce a video-language model (VLM) based
framework to develop the AI capability of automated procedural assessment and
feedback for nursing skills training, with the potential of being integrated
into existing training programs. Mimicking human skill acquisition, the
framework follows a curriculum-inspired progression, advancing from high-level
action recognition, fine-grained subaction decomposition, and ultimately to
procedural reasoning. This design supports scalable evaluation by reducing
instructor workload while preserving assessment quality. The system provides
three core capabilities: 1) diagnosing errors by identifying missing or
incorrect subactions in nursing skill instruction videos, 2) generating
explainable feedback by clarifying why a step is out of order or omitted, and
3) enabling objective, consistent formative evaluation of procedures.
Validation on synthesized videos demonstrates reliable error detection and
temporal localization, confirming its potential to handle real-world training
variability. By addressing workflow bottlenecks and supporting large-scale,
standardized evaluation, this work advances AI applications in nursing
education, contributing to stronger workforce development and ultimately safer
patient care.

</details>


### [86] [Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media](https://arxiv.org/abs/2509.16811)
*Zihan Ding,Junlong Chen,Per Ola Kristensson,Junxiao Shen,Xinyi Wang*

Main category: cs.AI

TL;DR: 提出了一种基于提示驱动的模块化视频编辑系统，通过语义索引管道构建全局叙事，帮助创作者重构长篇叙事视频内容。


<details>
  <summary>Details</summary>
Motivation: 现有基于转录或嵌入的方法在创意工作流程中表现不足，无法有效跟踪角色、推断动机和连接分散事件，创作者在处理长篇叙事视频时面临认知挑战。

Method: 采用语义索引管道，包括时间分割、引导记忆压缩和跨粒度融合，生成可解释的情节、对话、情感和上下文轨迹。用户可以通过自由形式提示而非时间线进行编辑。

Result: 在400多个视频上评估，系统能够扩展提示驱动的编辑功能，保持叙事连贯性，并在自动化和创作者控制之间取得平衡。

Conclusion: 该系统成功解决了长篇叙事视频编辑的认知挑战，提供了一种透明且可控制的创意编辑解决方案。

Abstract: Creators struggle to edit long-form, narrative-rich videos not because of UI
complexity, but due to the cognitive demands of searching, storyboarding, and
sequencing hours of footage. Existing transcript- or embedding-based methods
fall short for creative workflows, as models struggle to track characters,
infer motivations, and connect dispersed events. We present a prompt-driven,
modular editing system that helps creators restructure multi-hour content
through free-form prompts rather than timelines. At its core is a semantic
indexing pipeline that builds a global narrative via temporal segmentation,
guided memory compression, and cross-granularity fusion, producing
interpretable traces of plot, dialogue, emotion, and context. Users receive
cinematic edits while optionally refining transparent intermediate outputs.
Evaluated on 400+ videos with expert ratings, QA, and preference studies, our
system scales prompt-driven editing, preserves narrative coherence, and
balances automation with creator control.

</details>


### [87] [Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs](https://arxiv.org/abs/2509.16839)
*Yu Yao,Jiayi Dong,Ju Li,Yang Yang,Yilun Du*

Main category: cs.AI

TL;DR: Roundtable Policy是一种基于多LLM加权共识的推理框架，通过模拟科学委员会动态来提升复杂科学任务中的推理能力，减少幻觉并提高叙述质量。


<details>
  <summary>Details</summary>
Motivation: 受到科学委员会和"心智社会"的启发，旨在改进LLM在复杂科学任务中的推理能力，减少单一模型容易产生的幻觉问题。

Method: 通过多个LLM的加权共识进行推理，采用结构化且可解释的共识机制，仅需黑盒访问和统一流程。

Result: 该方法显著提升了复杂异质科学任务中的推理能力，提高了科学叙述的创造性、严谨性和逻辑连贯性，同时减少了幻觉。

Conclusion: Roundtable Policy是一个广泛适用于多LLM推理的互补性推理框架，强调结构化共识而非不透明的收敛。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities not
only in language generation but also in advancing scientific discovery. A
growing body of work has explored ways to improve their reasoning, from
self-consistency and chain-of-thought to multi-agent debate. Inspired by the
dynamics of scientific committees and the "Society of Mind," we introduce
Roundtable Policy, a complementary inference-time reasoning framework that
performs inference through the weighted consensus of multiple LLMs. Our
findings indicate that this approach significantly enhances reasoning in
complex heterogeneous scientific tasks and improves scientific narratives in
terms of creativity, rigor, and logical coherence, while reducing
hallucinations that single models are prone to. Our approach emphasizes
structured and interpretable consensus rather than opaque convergence, while
requiring only black-box access and uniform procedures, making it broadly
applicable to multi-LLM reasoning.

</details>


### [88] [The Principles of Human-like Conscious Machine](https://arxiv.org/abs/2509.16859)
*Fangfang Li,Xiaojie Zhang*

Main category: cs.AI

TL;DR: 本文提出了一个独立于基质的、逻辑严谨且防伪造的充分性标准，用于判断系统是否具有现象意识，并开发了一个形式化框架来指导设计能够满足该条件的系统。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和先进AI系统的兴起，如何判断AI是否具有意识成为一个紧迫问题。现有的意识归因问题需要更严谨的标准来解决。

Method: 提出一个基质独立的逻辑严谨充分性标准，构建形式化框架和操作原则，验证人类自身也满足该框架。

Result: 开发了一个能够判断系统是否具有现象意识的框架，并证明人类可以作为满足该框架的机器实例。

Conclusion: 该研究对哲学、认知科学和人工智能具有重要意义，为构建真正类人AI提供了新范式，同时解释了某些感受质为何无法还原为物理描述。

Abstract: Determining whether another system, biological or artificial, possesses
phenomenal consciousness has long been a central challenge in consciousness
studies. This attribution problem has become especially pressing with the rise
of large language models and other advanced AI systems, where debates about "AI
consciousness" implicitly rely on some criterion for deciding whether a given
system is conscious. In this paper, we propose a substrate-independent,
logically rigorous, and counterfeit-resistant sufficiency criterion for
phenomenal consciousness. We argue that any machine satisfying this criterion
should be regarded as conscious with at least the same level of confidence with
which we attribute consciousness to other humans. Building on this criterion,
we develop a formal framework and specify a set of operational principles that
guide the design of systems capable of meeting the sufficiency condition. We
further argue that machines engineered according to this framework can, in
principle, realize phenomenal consciousness. As an initial validation, we show
that humans themselves can be viewed as machines that satisfy this framework
and its principles. If correct, this proposal carries significant implications
for philosophy, cognitive science, and artificial intelligence. It offers an
explanation for why certain qualia, such as the experience of red, are in
principle irreducible to physical description, while simultaneously providing a
general reinterpretation of human information processing. Moreover, it suggests
a path toward a new paradigm of AI beyond current statistics-based approaches,
potentially guiding the construction of genuinely human-like AI.

</details>


### [89] [Large Language Models as End-to-end Combinatorial Optimization Solvers](https://arxiv.org/abs/2509.16865)
*Xia Jiang,Yaoxin Wu,Minshuo Li,Zhiguang Cao,Yingqian Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种让大语言模型直接作为端到端组合优化求解器的新框架，通过两阶段训练策略实现从自然语言问题描述到解决方案的直接映射。


<details>
  <summary>Details</summary>
Motivation: 传统组合优化问题需要领域专业知识，现有基于LLM的方法依赖中间步骤（如代码生成或求解器调用），限制了通用性和可访问性。

Method: 采用两阶段训练策略：监督微调从领域特定求解器学习解决方案模式，可行性-最优性感知强化学习过程显式缓解约束违反并优化解质量。

Result: 在7个NP难组合优化问题上的评估显示，该方法实现了高可行性率，将平均最优性差距降低到1.03-8.20%，超越了通用LLM、推理模型和领域特定启发式方法。

Conclusion: 该方法建立了统一的基于语言的组合优化流程，无需大量代码执行或针对不同问题的手动架构调整，为传统求解器设计提供了通用且语言驱动的替代方案。

Abstract: Combinatorial optimization (CO) problems, central to decision-making
scenarios like logistics and manufacturing, are traditionally solved using
problem-specific algorithms requiring significant domain expertise. While large
language models (LLMs) have shown promise in automating CO problem solving,
existing approaches rely on intermediate steps such as code generation or
solver invocation, limiting their generality and accessibility. This paper
introduces a novel framework that empowers LLMs to serve as end-to-end CO
solvers by directly mapping natural language problem descriptions to solutions.
We propose a two-stage training strategy: supervised fine-tuning (SFT) imparts
LLMs with solution generation patterns from domain-specific solvers, while a
feasibility-and-optimality-aware reinforcement learning (FOARL) process
explicitly mitigates constraint violations and refines solution quality.
Evaluation across seven NP-hard CO problems shows that our method achieves a
high feasibility rate and reduces the average optimality gap to 1.03-8.20% by
tuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o),
reasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our
method establishes a unified language-based pipeline for CO without extensive
code execution or manual architectural adjustments for different problems,
offering a general and language-driven alternative to traditional solver design
while maintaining relative feasibility guarantees.

</details>


### [90] [seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs](https://arxiv.org/abs/2509.16866)
*Mohammad Ramezanali,Mo Vazifeh,Paolo Santi*

Main category: cs.AI

TL;DR: seqBench是一个参数化基准测试，用于通过精确控制多个关键复杂度维度来探测大型语言模型的顺序推理极限。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法系统分析LLM的顺序推理失败模式，需要一种能够精细控制逻辑深度、回溯步骤和噪声比率的基准来揭示模型推理能力的统计极限。

Method: 设计seqBench基准，系统变化三个关键维度：(1)逻辑深度（解决任务所需的顺序动作数）(2)回溯步骤数（最优路径中需要重新访问先前状态的次数）(3)噪声比率（支持性事实与干扰性事实的比例）。

Result: 评估显示最先进的LLM存在普遍失败模式：在模型特定的逻辑深度之外，准确率呈指数级崩溃。即使顶级模型在最小搜索复杂度下也会系统性地失败。

Conclusion: seqBench揭示了LLM在常识推理能力方面的关键局限性，建立了清晰的推理边界理解，为未来模型发展提供了科学基准。

Abstract: We introduce seqBench, a parametrized benchmark for probing sequential
reasoning limits in Large Language Models (LLMs) through precise,
multi-dimensional control over several key complexity dimensions. seqBench
allows systematic variation of (1) the logical depth, defined as the number of
sequential actions required to solve the task; (2) the number of backtracking
steps along the optimal path, quantifying how often the agent must revisit
prior states to satisfy deferred preconditions (e.g., retrieving a key after
encountering a locked door); and (3) the noise ratio, defined as the ratio
between supporting and distracting facts about the environment. Our evaluations
on state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses
exponentially beyond a model-specific logical depth. Unlike existing
benchmarks, seqBench's fine-grained control facilitates targeted analyses of
these reasoning failures, illuminating universal scaling laws and statistical
limits, as detailed in this paper alongside its generation methodology and
evaluation metrics. We find that even top-performing models systematically fail
on seqBench's structured reasoning tasks despite minimal search complexity,
underscoring key limitations in their commonsense reasoning capabilities.
Designed for future evolution to keep pace with advancing models, the seqBench
datasets are publicly released to spur deeper scientific inquiry into LLM
reasoning, aiming to establish a clearer understanding of their true potential
and current boundaries for robust real-world application.

</details>


### [91] [LLMs as Layout Designers: A Spatial Reasoning Perspective](https://arxiv.org/abs/2509.16891)
*Sha Li*

Main category: cs.AI

TL;DR: LaySPA是一个基于强化学习的框架，旨在增强LLM代理的空间推理能力，用于图形布局设计任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本领域的推理和规划能力很强，但在空间理解和推理方面存在局限，这对于需要精确空间布局的应用（如图形设计）至关重要。

Method: 采用强化学习框架，利用混合奖励信号（几何有效性、结构保真度和视觉质量）来建模元素间关系、导航画布并优化空间布局，通过迭代自探索和自适应策略优化生成可解释的推理轨迹和结构化布局。

Result: 实验结果表明，LaySPA能够生成结构合理且视觉吸引人的布局，性能优于更大的通用LLM，并与最先进的专用布局模型相当。

Conclusion: LaySPA成功地将空间推理能力集成到LLM中，为内容感知的图形布局设计提供了有效的解决方案。

Abstract: While Large Language Models (LLMs) have demonstrated impressive reasoning and
planning abilities in textual domains and can effectively follow instructions
for complex tasks, their capacity for spatial understanding and reasoning
remains limited. Such capabilities, however, are critical for applications like
content-aware graphic layout design, which demands precise placement,
alignment, and structural organization of multiple elements within constrained
visual spaces. To address this gap, we propose LaySPA, a reinforcement
learning-based framework that augments LLM agents with explicit spatial
reasoning capabilities. LaySPA leverages hybrid reward signals that capture
geometric validity, structural fidelity, and visual quality, enabling agents to
model inter-element relationships, navigate the canvas, and optimize spatial
arrangements. Through iterative self-exploration and adaptive policy
optimization, LaySPA produces both interpretable reasoning traces and
structured layouts. Experimental results demonstrate that LaySPA generates
structurally sound and visually appealing layouts, outperforming larger
general-purpose LLMs and achieving results on par with state-of-the-art
specialized layout models.

</details>


### [92] [Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation](https://arxiv.org/abs/2509.16924)
*Jia Li,Yinfeng Yu,Liejun Wang,Fuchun Sun,Wendong Zheng*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的音频-视觉导航框架，通过立体声感知注意力模块和音频引导动态融合模块，显著提升了在复杂3D环境中的导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态模态融合策略，忽视了立体音频中的空间线索，导致在杂乱或遮挡场景中性能下降。

Method: 端到端强化学习框架，包含立体声感知注意力模块（SAM）学习左右音频通道的空间差异，以及音频引导动态融合模块（AGDF）根据音频线索动态调整视觉和听觉特征的融合比例。

Result: 在Replica和Matterport3D数据集上的实验表明，该方法在导航成功率和路径效率方面显著优于现有方法，在纯音频条件下相比最佳基线有超过40%的提升。

Conclusion: 明确建模立体声通道的空间线索并进行深度多模态融合对于实现鲁棒高效的音频-视觉导航至关重要。

Abstract: In audio-visual navigation (AVN) tasks, an embodied agent must autonomously
localize a sound source in unknown and complex 3D environments based on
audio-visual signals. Existing methods often rely on static modality fusion
strategies and neglect the spatial cues embedded in stereo audio, leading to
performance degradation in cluttered or occluded scenes. To address these
issues, we propose an end-to-end reinforcement learning-based AVN framework
with two key innovations: (1) a \textbf{S}tereo-Aware \textbf{A}ttention
\textbf{M}odule (\textbf{SAM}), which learns and exploits the spatial disparity
between left and right audio channels to enhance directional sound perception;
and (2) an \textbf{A}udio-\textbf{G}uided \textbf{D}ynamic \textbf{F}usion
Module (\textbf{AGDF}), which dynamically adjusts the fusion ratio between
visual and auditory features based on audio cues, thereby improving robustness
to environmental changes. Extensive experiments are conducted on two realistic
3D scene datasets, Replica and Matterport3D, demonstrating that our method
significantly outperforms existing approaches in terms of navigation success
rate and path efficiency. Notably, our model achieves over 40\% improvement
under audio-only conditions compared to the best-performing baselines. These
results highlight the importance of explicitly modeling spatial cues from
stereo channels and performing deep multi-modal fusion for robust and efficient
audio-visual navigation.

</details>


### [93] [Quantum Abduction: A New Paradigm for Reasoning under Uncertainty](https://arxiv.org/abs/2509.16958)
*Remo Pareschi*

Main category: cs.AI

TL;DR: 本文提出量子溯因推理，这是一种非经典范式，通过量子叠加态建模假设，允许假设间产生建设性或破坏性干涉，仅在证据一致性达到时坍缩，更符合人类推理的多面性本质。


<details>
  <summary>Details</summary>
Motivation: 传统AI将溯因推理简化为消除性搜索，忽略了人类推理者能够维持多个解释线索、处理矛盾并生成新颖综合的能力。

Method: 基于量子认知理论，结合现代NLP嵌入和生成式AI实现，支持动态综合而非过早消除假设。

Result: 在历史谜案、文学演示、医疗诊断和科学理论变革等多个领域的案例研究中，量子溯因推理显示出更符合人类推理本质的优势。

Conclusion: 量子溯因推理为构建更具表达力和透明度的AI推理系统提供了新途径，更忠实于人类推理的建构性和多面性特征。

Abstract: Abductive reasoning - the search for plausible explanations - has long been
central to human inquiry, from forensics to medicine and scientific discovery.
Yet formal approaches in AI have largely reduced abduction to eliminative
search: hypotheses are treated as mutually exclusive, evaluated against
consistency constraints or probability updates, and pruned until a single
"best" explanation remains. This reductionist framing overlooks the way human
reasoners sustain multiple explanatory lines in suspension, navigate
contradictions, and generate novel syntheses. This paper introduces quantum
abduction, a non-classical paradigm that models hypotheses in superposition,
allows them to interfere constructively or destructively, and collapses only
when coherence with evidence is reached. Grounded in quantum cognition and
implemented with modern NLP embeddings and generative AI, the framework
supports dynamic synthesis rather than premature elimination. Case studies span
historical mysteries (Ludwig II of Bavaria, the "Monster of Florence"),
literary demonstrations ("Murder on the Orient Express"), medical diagnosis,
and scientific theory change. Across these domains, quantum abduction proves
more faithful to the constructive and multifaceted nature of human reasoning,
while offering a pathway toward expressive and transparent AI reasoning
systems.

</details>


### [94] [KAHAN: Knowledge-Augmented Hierarchical Analysis and Narration for Financial Data Narration](https://arxiv.org/abs/2509.17037)
*Yajing Yang,Tony Deng,Min-Yen Kan*

Main category: cs.AI

TL;DR: KAHAN是一个知识增强的分层框架，利用LLM作为领域专家从原始表格数据中系统提取实体、成对、组和系统级别的洞察。在金融报告基准测试中，KAHAN在叙事质量上优于现有方法20%以上，保持98.2%的事实准确性，并展示出实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从表格数据中提取多层次洞察方面存在局限，需要更系统化的分析框架来提升分析质量和事实准确性。

Method: KAHAN采用知识增强的分层框架，利用LLM作为领域专家，在实体、成对、组和系统四个层次上进行分析，通过知识蒸馏提升模型性能。

Result: 在DataTales金融报告基准测试中，KAHAN在叙事质量上比现有方法提升20%以上（GPT-4o评估），事实准确性达98.2%，在人类评估中展示实用价值，并能有效迁移到医疗领域。

Conclusion: 知识质量通过蒸馏驱动模型性能，分层分析的效果随市场复杂性变化，框架具有良好的领域迁移能力，为表格数据分析提供了有效解决方案。

Abstract: We propose KAHAN, a knowledge-augmented hierarchical framework that
systematically extracts insights from raw tabular data at entity, pairwise,
group, and system levels. KAHAN uniquely leverages LLMs as domain experts to
drive the analysis. On DataTales financial reporting benchmark, KAHAN
outperforms existing approaches by over 20% on narrative quality (GPT-4o),
maintains 98.2% factuality, and demonstrates practical utility in human
evaluation. Our results reveal that knowledge quality drives model performance
through distillation, hierarchical analysis benefits vary with market
complexity, and the framework transfers effectively to healthcare domains. The
data and code are available at https://github.com/yajingyang/kahan.

</details>


### [95] [From domain-landmark graph learning to problem-landmark graph generation](https://arxiv.org/abs/2509.17062)
*Cristian Pérez-Corral,Antonio Garrido,Laura Sebastia*

Main category: cs.AI

TL;DR: 提出一种从多个规划任务中学习地标关系的新方法，创建概率提升排序图来捕捉参数化地标之间的加权抽象关系，并将其应用于新规划实例中。


<details>
  <summary>Details</summary>
Motivation: 传统地标提取方法对特定规划任务敏感，导致地标完全针对单个实例定制，限制了其在同一规划域其他实例中的适用性。

Method: 1) 从规划域的多个任务中学习地标关系，构建概率提升排序图；2) 对新规划任务，分两阶段实例化关系：首先生成初始状态和目标状态的两个图，然后通过搜索等价性将两个图合并为统一图来提取地标排序。

Result: 在知名规划域上评估了方法的精确度和召回率。

Conclusion: 虽然这些排序关系不是100%准确（概率性的），但在规划中仍然非常有用。

Abstract: Landmarks have long played a pivotal role in automated planning, serving as
crucial elements for improving the planning algorithms. The main limitation of
classical landmark extraction methods is their sensitivity to specific planning
tasks. This results in landmarks fully tailored to individual instances,
thereby limiting their applicability across other instances of the same
planning domain. We propose a novel approach that learns landmark relationships
from multiple planning tasks of a planning domain. This leads to the creation
of a \textit{probabilistic lifted ordering graph}, as a structure that captures
weighted abstractions of relationships between parameterized landmarks.
Although these orderings are not 100\% true (they are probabilistic), they can
still be very useful in planning. Next, given a new planning task for that
domain, we instantiate the relationships from that graph to this particular
instance. This instantiation operates in two phases. First, it generates two
graphs: the former instantiating information from the initial state and the
latter from the goal state. Second, it combines these two graphs into one
unified graph by searching equivalences to extract landmark orderings. We
evaluate the precision and recallof the information found by our approach over
well-known planning domains.

</details>


### [96] [RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation with Geographical Reranking](https://arxiv.org/abs/2509.17066)
*Kunrong Li,Kwan Hui Lim*

Main category: cs.AI

TL;DR: RALLM-POI是一个结合检索增强生成和自我修正的框架，用于下一个兴趣点推荐，无需额外训练即可显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 传统模型需要密集训练，而现有LLM方法由于缺乏轨迹和空间上下文，往往产生通用或地理不相关的结果。

Method: 提出RALLM-POI框架：1）历史轨迹检索器检索相关轨迹作为上下文参考；2）地理距离重排序器优先选择空间相关轨迹；3）代理LLM修正器通过自我反思优化输出。

Result: 在三个真实Foursquare数据集上实现了显著的准确性提升，优于传统和基于LLM的基线方法。

Conclusion: RALLM-POI框架有效解决了LLM在POI推荐中的上下文缺失问题，展示了检索增强和自我修正策略的有效性。

Abstract: Next point-of-interest (POI) recommendation predicts a user's next
destination from historical movements. Traditional models require intensive
training, while LLMs offer flexible and generalizable zero-shot solutions but
often generate generic or geographically irrelevant results due to missing
trajectory and spatial context. To address these issues, we propose RALLM-POI,
a framework that couples LLMs with retrieval-augmented generation and
self-rectification. We first propose a Historical Trajectory Retriever (HTR)
that retrieves relevant past trajectories to serve as contextual references,
which are then reranked by a Geographical Distance Reranker (GDR) for
prioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier
(ALR) is designed to refine outputs through self-reflection. Without additional
training, RALLM-POI achieves substantial accuracy gains across three real-world
Foursquare datasets, outperforming both conventional and LLM-based baselines.
Code is released at https://github.com/LKRcrocodile/RALLM-POI.

</details>


### [97] [Intention-aware Hierarchical Diffusion Model for Long-term Trajectory Anomaly Detection](https://arxiv.org/abs/2509.17068)
*Chen Wang,Sarah Erfani,Tansu Alpcan,Christopher Leckie*

Main category: cs.AI

TL;DR: 本文提出了一种名为IHiD的无监督轨迹异常检测方法，通过结合高层意图评估和低层子轨迹分析来检测异常轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹异常检测方法无法同时考虑智能体的高层意图和低层导航细节，限制了捕捉正常轨迹多样性的能力。

Method: IHiD方法使用逆Q学习作为高层模型评估子目标与智能体意图的一致性，同时使用扩散模型作为低层模型生成基于子目标信息的子轨迹，通过重构误差进行异常检测。

Result: 实验表明，IHiD在F1分数上比最先进的基线方法提高了30.2%的异常检测性能。

Conclusion: 通过整合高层和低层模型，IHiD有效利用子目标转换知识，能够捕捉正常轨迹的多样化分布。

Abstract: Long-term trajectory anomaly detection is a challenging problem due to the
diversity and complex spatiotemporal dependencies in trajectory data. Existing
trajectory anomaly detection methods fail to simultaneously consider both the
high-level intentions of agents as well as the low-level details of the agent's
navigation when analysing an agent's trajectories. This limits their ability to
capture the full diversity of normal trajectories. In this paper, we propose an
unsupervised trajectory anomaly detection method named Intention-aware
Hierarchical Diffusion model (IHiD), which detects anomalies through both
high-level intent evaluation and low-level sub-trajectory analysis. Our
approach leverages Inverse Q Learning as the high-level model to assess whether
a selected subgoal aligns with an agent's intention based on predicted
Q-values. Meanwhile, a diffusion model serves as the low-level model to
generate sub-trajectories conditioned on subgoal information, with anomaly
detection based on reconstruction error. By integrating both models, IHiD
effectively utilises subgoal transition knowledge and is designed to capture
the diverse distribution of normal trajectories. Our experiments show that the
proposed method IHiD achieves up to 30.2% improvement in anomaly detection
performance in terms of F1 score over state-of-the-art baselines.

</details>


### [98] [Governing Automated Strategic Intelligence](https://arxiv.org/abs/2509.17087)
*Nicholas Kruus,Madhavendra Thakur,Adam Khoja,Leonhard Nagel,Maximilian Nicholson,Abeer Sharma,Jason Hausenloy,Alberto KoTafoya,Aliya Mukhanova,Alli Katila-Miikkulainen,Harish Chandran,Ivan Zhang,Jessie Chen,Joel Raj,Jord Nguyen,Lai Hsien Hao,Neja Jayasundara,Soham Sen,Sophie Zhang,Ashley Dora Kokui Tamaklo,Bhavya Thakur,Henry Close,Janghee Lee,Nina Sefton,Raghavendra Thakur,Shiv Munagala,Yeeun Kim*

Main category: cs.AI

TL;DR: 本文探讨了前沿人工智能模型在国家军事和经济战略竞争中的重要性，特别关注了多模态基础模型在自动化军事情报分析方面的潜力，并提出了保持战略竞争力的建议。


<details>
  <summary>Details</summary>
Motivation: 国家间的军事和经济战略竞争将越来越多地由前沿AI模型的能力和成本定义，但目前对于AI系统如何通过融合多源数据实现大规模战略分析的讨论不足。

Method: 进行了初步的提升研究来实证评估这些能力，提出了这类系统将回答的真实问题分类法，建立了系统AI能力决定因素的高层模型。

Result: 多模态基础模型有望自动化以前由人类完成的战略分析工作，能够融合卫星图像、手机位置追踪、社交媒体记录和书面文档等多种数据源。

Conclusion: 为国家提供了在新的自动化情报范式下保持战略竞争力的具体建议。

Abstract: Military and economic strategic competitiveness between nation-states will
increasingly be defined by the capability and cost of their frontier artificial
intelligence models. Among the first areas of geopolitical advantage granted by
such systems will be in automating military intelligence. Much discussion has
been devoted to AI systems enabling new military modalities, such as lethal
autonomous weapons, or making strategic decisions. However, the ability of a
country of "CIA analysts in a data-center" to synthesize diverse data at scale,
and its implications, have been underexplored. Multimodal foundation models
appear on track to automate strategic analysis previously done by humans. They
will be able to fuse today's abundant satellite imagery, phone-location traces,
social media records, and written documents into a single queryable system. We
conduct a preliminary uplift study to empirically evaluate these capabilities,
then propose a taxonomy of the kinds of ground truth questions these systems
will answer, present a high-level model of the determinants of this system's AI
capabilities, and provide recommendations for nation-states to remain
strategically competitive within the new paradigm of automated intelligence.

</details>


### [99] [MCTS-EP: Empowering Embodied Planning with Online Preference Optimization](https://arxiv.org/abs/2509.17116)
*Hang Xu,Zang Yu,Yehui Tang,Pengbo Hu,Yuhao Tang,Hao Dong*

Main category: cs.AI

TL;DR: MCTS-EP是一个结合大型语言模型和蒙特卡洛树搜索的在线学习框架，用于训练具身智能体，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够有效整合LLM推理能力和MCTS探索策略的框架，以提升具身智能体在复杂环境中的学习效率和性能。

Method: 结合MCTS引导的偏好数据收集、高效多模态推理机制和基于偏好优化的迭代训练流程，理论证明在强凸损失函数下优于传统在线策略算法。

Result: 在ALFWorld中文本任务达到92%成功率，视觉任务达到87%成功率；在WebShop中获得0.81平均奖励；视觉ALFWorld中平均交互步数从18.7/19.5减少到10.2/9.9步。

Conclusion: MCTS-EP框架有效提升了具身智能体的学习效率和性能，证明了搜索增强方法在具身智能任务中的优势。

Abstract: This paper introduces MCTS-EP, an online learning framework that combines
large language models (LLM) with Monte Carlo Tree Search (MCTS) for training
embodied agents. MCTS-EP integrates three key components: MCTS-guided
exploration for preference data collection, efficient multi-modal reasoning
mechanism, and iterative training pipeline based on preference optimization. We
theoretically prove that MCTS-EP achieves better performance bounds than
conventional on-policy algorithms when the loss function is strongly convex,
and demonstrate that it can be formulated as a search-enhanced variant of GAIL.
MCTS-EP achieves state-of-the-art performace across serval benchmarks. In
ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks.
In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average
interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code
available at: https://github.com/xuhang-2/Embodied-Agent-Planning

</details>


### [100] [ARE: Scaling Up Agent Environments and Evaluations](https://arxiv.org/abs/2509.17158)
*Pierre Andrews,Amine Benhalloum,Gerard Moreno-Torres Bertran,Matteo Bettini,Amar Budhiraja,Ricardo Silveira Cabral,Virginie Do,Romain Froger,Emilien Garreau,Jean-Baptiste Gaya,Hugo Laurençon,Maxime Lecanu,Kunal Malkan,Dheeraj Mekala,Pierre Ménard,Grégoire Mialon,Ulyana Piterbarg,Mikhail Plekhanov,Mathieu Rita,Andrey Rusakov,Thomas Scialom,Vladislav Vorotilov,Mengjue Wang,Ian Yu*

Main category: cs.AI

TL;DR: 本文介绍了Meta Agents Research Environments (ARE)平台和Gaia2基准测试，旨在通过可扩展的环境创建和动态评估来提升智能体在真实世界中的能力。


<details>
  <summary>Details</summary>
Motivation: 当前智能体开发与真实世界部署之间存在差距，需要能够处理动态环境、协作和时序约束的评估基准。

Method: 开发ARE平台提供环境构建抽象，并在此基础上创建Gaia2异步基准测试，要求智能体处理模糊性、噪声、动态环境、多智能体协作和时序约束。

Result: 实验表明，现有系统在智能谱系中无绝对优势，强推理能力往往以效率为代价，预算扩展曲线趋于平缓。

Conclusion: ARE平台和Gaia2基准为社区提供了持续扩展的能力，AI发展后半段需要更有意义的任务定义和鲁棒评估来推动前沿能力发展。

Abstract: We introduce Meta Agents Research Environments (ARE), a research platform for
scalable creation of environments, integration of synthetic or real
applications, and execution of agentic orchestrations. ARE provides simple
abstractions to build complex and diverse environments, each with their own
rules, tools, content, and verifiers, helping to bridge the gap between model
development and real-world deployment. We also propose Gaia2, a benchmark built
in ARE and designed to measure general agent capabilities. Beyond search and
execution, Gaia2 requires agents to handle ambiguities and noise, adapt to
dynamic environments, collaborate with other agents, and operate under temporal
constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new
failure modes that are invisible in static settings. Our experiments show that
no system dominates across the intelligence spectrum: stronger reasoning often
comes at the cost of efficiency, and budget scaling curves plateau,
highlighting the need for new architectures and adaptive compute strategies.
Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2
to other environments, empowering the community to rapidly create new
benchmarks tailored to their domains. In AI's second half, progress
increasingly depends on defining meaningful tasks and robust evaluations to
drive frontier capabilities forward.

</details>


### [101] [Shall We Play a Game? Language Models for Open-ended Wargames](https://arxiv.org/abs/2509.17192)
*Glenn Matlin,Parv Mahajan,Isaac Song,Yixiong Hao,Ryan Bard,Stu Topp,Evan Montoya,M. Rehan Parwani,Soham Shetty,Mark Riedl*

Main category: cs.AI

TL;DR: 本文通过文献综述构建了兵棋推演的分类体系，重点关注玩家和裁判创造力最开放的类型，提出了语言模型在兵棋推演中的应用考虑因素、安全实践和开放研究挑战。


<details>
  <summary>Details</summary>
Motivation: 兵棋推演是多方面、多玩家的冲突模拟，参与者决策影响未来事件。语言模型越来越多地被考虑用于提供现实世界决策的洞察，但需要系统分析其在开放型兵棋推演中的应用。

Method: 对100篇关于AI在兵棋推演中的近期文献进行范围性综述，构建基于玩家和裁判创造力的兵棋推演本体论，重点关注最开放类型的应用考虑。

Result: 建立了兵棋推演的分类体系，提炼了语言模型在不同应用领域的使用时机和方法，提出了安全考虑和最佳实践。

Conclusion: 确定了高影响力的开放研究挑战，为语言模型在开放型兵棋推演中的安全有效部署提供了指导框架。

Abstract: Wargames are multi-faceted, multi-player depictions of conflict in which
participants' decisions influence future events. Wargames are often used to
explore the strategic implications of decision-making. However, it also
encompasses entertainment-oriented simulations, ranging from _Chess_ to
tabletop role-playing games like _Dungeons & Dragons_ (D&D). On the more
open-ended side of the spectrum of wargames, players use natural language to
convey their moves, and adjudicators propose outcomes. Language Models (LMs)
are increasingly being considered for how they can provide insights into
real-world, consequential decisions. We conduct a scoping literature review of
a curated selection of 100 recent works on AI in wargames, from which we
construct an ontology of wargames in terms of the creativity afforded to either
the players or adjudicators. Focusing on the space of wargames with the most
open-endedness for players and adjudicators, we distill a set of considerations
for when and how to use LMs in different application areas. We also present a
set of safety considerations, best practices for deploying LMs in open-ended
wargames, and conclude with a set of high-impact open research challenges.

</details>


### [102] [MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE](https://arxiv.org/abs/2509.17238)
*Soheil Zibakhsh,Mohammad Samragh,Kumari Nishu,Lauren Hannah,Arnav Kundu,Minsik Cho*

Main category: cs.AI

TL;DR: 本文提出了超并行缩放框架，通过token级别的多专家输出聚合来提升MoE模型的推理质量，无需训练即可实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的生成质量通常通过序列级缩放方法改进，但缺乏token级别的优化。作者希望开发一种互补的token级缩放框架来进一步提升预测准确性。

Method: 提出了Roster of Experts (RoE)方法，在MoE模型中引入受控随机性到专家路由机制，为每个token采样多个不同专家并聚合其输出。采用高效批处理策略和专用KV缓存机制来降低计算成本。

Result: RoE使7B MoE模型能够匹配10.5B MoE模型的性能，同时推理计算量减少30%，且无需微调模型参数。

Conclusion: 超并行缩放是一种有效的训练无关推理优化方法，通过token级专家集成显著提升MoE模型性能，为高效推理提供了新思路。

Abstract: The generation quality of large language models (LLMs) is often improved by
utilizing inference-time sequence-level scaling methods (e.g.,
Chain-of-Thought). We introduce hyper-parallel scaling, a complementary
framework that improves prediction quality at the token level. Hyper-parallel
scaling computes and aggregates multiple output proposals for a single token
from the model. We implement this concept in Mixture-of-Experts (MoE) models,
which we refer to as Roster of Experts (RoE). RoE is a training-free inference
algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects
controlled stochasticity into the expert routing mechanism, enabling it to
sample multiple diverse experts for each token and aggregate their outputs for
a more accurate final prediction.To overcome the computational cost, we
introduce an efficient batching strategy and a specialized KV-caching mechanism
that minimizes compute and memory overhead. For example, RoE enables a 7B MoE
model to match the performance of a 10.5B MoE model while using 30% less
compute for inference. These gains are achieved without any fine-tuning of
model parameters.

</details>


### [103] [Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System](https://arxiv.org/abs/2509.17240)
*Abdullah Mushtaq,Muhammad Rafay Naeem,Ibrahim Ghaznavi,Alaa Abd-alrazaq,Aliya Tabassum,Junaid Qadir*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM和多代理系统架构的SLR评估助手，用于自动化系统文献综述的质量评估，在初步研究中与专家评估结果达到84%的一致性。


<details>
  <summary>Details</summary>
Motivation: 系统文献综述(SLR)是循证研究的基础，但传统方法劳动密集且在不同学科间存在不一致性，需要更高效和标准化的评估工具。

Method: 采用多代理系统(MAS)架构，基于PRISMA指南设计专门代理，自动化进行协议验证、方法学评估和主题相关性检查。

Result: 在五个已发表SLR的初步研究中，系统输出与专家标注的PRISMA评分达到84%的一致性。

Conclusion: 虽然早期结果有前景，但这是迈向可扩展NLP驱动系统的第一步，展示了其在跨学科工作流程中进行严格、领域无关知识聚合的潜力。

Abstract: Systematic Literature Reviews (SLRs) are foundational to evidence-based
research but remain labor-intensive and prone to inconsistency across
disciplines. We present an LLM-based SLR evaluation copilot built on a
Multi-Agent System (MAS) architecture to assist researchers in assessing the
overall quality of the systematic literature reviews. The system automates
protocol validation, methodological assessment, and topic relevance checks
using a scholarly database. Unlike conventional single-agent methods, our
design integrates a specialized agentic approach aligned with PRISMA guidelines
to support more structured and interpretable evaluations. We conducted an
initial study on five published SLRs from diverse domains, comparing system
outputs to expert-annotated PRISMA scores, and observed 84% agreement. While
early results are promising, this work represents a first step toward scalable
and accurate NLP-driven systems for interdisciplinary workflows and reveals
their capacity for rigorous, domain-agnostic knowledge aggregation to
streamline the review process.

</details>


### [104] [Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with Action-Graph Observability on GPT-OSS-20B](https://arxiv.org/abs/2509.17259)
*Ilham Wicaksono,Zekun Wu,Rahul Patel,Theo King,Adriano Koshiyama,Philip Treleaven*

Main category: cs.AI

TL;DR: 本文通过比较性红队分析发现，AI代理系统存在独特的漏洞特征，其中代理级漏洞在独立模型层面不存在，而某些模型级漏洞在代理环境中失效。


<details>
  <summary>Details</summary>
Motivation: 随着行业越来越多地采用AI代理系统，理解其独特的安全漏洞变得至关重要。现有研究表明，模型级安全漏洞不能完全反映代理部署中的风险。

Method: 使用AgentSeer可观测性框架将代理系统分解为细粒度动作和组件，在GPT-OSS-20B模型上应用HarmBench的有害目标进行迭代红队攻击，比较独立模型和代理循环中的模型表现。

Result: 发现代理级迭代攻击能够成功攻破在模型级完全失败的目标，工具调用环境的漏洞率比非工具环境高24%。同时，某些模型级攻击在代理环境中失效。

Conclusion: AI代理系统具有独特的漏洞特征，模型级漏洞评估不能准确预测实际部署系统的安全性，需要专门的代理级安全测试方法。

Abstract: As the industry increasingly adopts agentic AI systems, understanding their
unique vulnerabilities becomes critical. Prior research suggests that security
flaws at the model level do not fully capture the risks present in agentic
deployments, where models interact with tools and external environments. This
paper investigates this gap by conducting a comparative red teaming analysis of
GPT-OSS-20B, a 20-billion parameter open-source model. Using our observability
framework AgentSeer to deconstruct agentic systems into granular actions and
components, we apply iterative red teaming attacks with harmful objectives from
HarmBench at two distinct levels: the standalone model and the model operating
within an agentic loop. Our evaluation reveals fundamental differences between
model level and agentic level vulnerability profiles. Critically, we discover
the existence of agentic-only vulnerabilities, attack vectors that emerge
exclusively within agentic execution contexts while remaining inert against
standalone models. Agentic level iterative attacks successfully compromise
objectives that completely failed at the model level, with tool-calling
contexts showing 24\% higher vulnerability than non-tool contexts. Conversely,
certain model-specific exploits work exclusively at the model level and fail
when transferred to agentic contexts, demonstrating that standalone model
vulnerabilities do not always generalize to deployed systems.

</details>


### [105] [CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2509.17318)
*Zhuofan Chen,Jiyuan He,Yichi Zhang,Xing Hu,Haoxing Wen,Jun Bai,Wenge Rong*

Main category: cs.AI

TL;DR: CogAtom是一个基于认知原子的框架，用于合成数学上严谨且认知多样的问题，通过选择和重组从人类解决方案中提取的基本推理单元来生成高质量数学问题。


<details>
  <summary>Details</summary>
Motivation: 由于多步推理和抽象概念整合的需求，数学推理对大型语言模型构成重大挑战。当前测试时扩展技术严重依赖高质量、具有挑战性的问题，但奥林匹克级别数学问题的稀缺性成为瓶颈。

Method: CogAtom将问题构建建模为选择和重组认知原子的过程，使用多样性促进的随机游走算法探索认知原子空间，同时基于约束的重组机制确保逻辑合理性和结构有效性。

Result: 实验结果表明，CogAtom在准确性、推理深度和多样性方面优于现有方法，生成的问题在难度上接近AIME，但在结构变化上超过AIME。

Conclusion: 这项工作为可扩展、高质量的数学问题生成提供了一条基于认知的途径。

Abstract: Mathematical reasoning poses significant challenges for Large Language Models
(LLMs) due to its demand for multi-step reasoning and abstract conceptual
integration. While recent test-time scaling techniques rely heavily on
high-quality, challenging problems, the scarcity of Olympiad-level math
problems remains a bottleneck. We introduce CogAtom, a novel cognitive
atom-based framework for synthesizing mathematically rigorous and cognitively
diverse problems. Unlike prior approaches, CogAtom models problem construction
as a process of selecting and recombining fundamental reasoning units,
cognitive atoms, extracted from human-authored solutions. A diversity-promoting
random walk algorithm enables exploration of the cognitive atom space, while a
constraint-based recombination mechanism ensures logical soundness and
structural validity. The combinatorial nature of the graph structure provides a
near-infinite space of reasoning paths, and the walk algorithm systematically
explores this space to achieve large-scale synthesis of high-quality problems;
meanwhile, by controlling the number of cognitive atoms, we can precisely
adjust problem difficulty, ensuring diversity, scalability, and controllability
of the generated problems. Experimental results demonstrate that CogAtom
outperforms existing methods in accuracy, reasoning depth, and diversity,
generating problems that closely match the difficulty of AIME while exceeding
it in structural variation. Our work offers a cognitively grounded pathway
toward scalable, high-quality math problem generation.Our code is publicly
available at https://github.com/Icarus-1111/CogAtom.

</details>


### [106] [LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code](https://arxiv.org/abs/2509.17337)
*Ala Jararweh,Michael Adams,Avinash Sahu,Abdullah Mueen,Afsah Anwar*

Main category: cs.AI

TL;DR: LLaVul是一个多模态大语言模型，专门用于通过问答方式对代码进行细粒度推理，增强代码漏洞的上下文相关分析能力。


<details>
  <summary>Details</summary>
Motivation: 当前软件系统复杂性增加，需要更好的漏洞分析工具。现有方法将漏洞分析简化为分类任务，忽略了现实场景的细微差别和上下文依赖性。虽然代码大语言模型在代码理解方面表现出色，但很少关注安全特定的推理。

Method: 提出LLaVul多模态LLM，训练模型将配对的代码和自然查询整合到统一空间中，通过问答方式增强代码漏洞的推理和上下文相关洞察。构建了包含真实世界漏洞的安全焦点问答数据集进行评估。

Result: LLaVul在问答和检测任务中优于最先进的通用和代码LLM。通过定性分析进一步解释了决策过程，突出了模型的能力和局限性。

Conclusion: 通过整合代码和问答，LLaVul实现了更可解释和以安全为中心的代码理解。

Abstract: Increasing complexity in software systems places a growing demand on
reasoning tools that unlock vulnerabilities manifest in source code. Many
current approaches focus on vulnerability analysis as a classifying task,
oversimplifying the nuanced and context-dependent real-world scenarios. Even
though current code large language models (LLMs) excel in code understanding,
they often pay little attention to security-specific reasoning. We propose
LLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code
through question-answering (QA). Our model is trained to integrate paired code
and natural queries into a unified space, enhancing reasoning and
context-dependent insights about code vulnerability. To evaluate our model
performance, we construct a curated dataset of real-world vulnerabilities
paired with security-focused questions and answers. Our model outperforms
state-of-the-art general-purpose and code LLMs in the QA and detection tasks.
We further explain decision-making by conducting qualitative analysis to
highlight capabilities and limitations. By integrating code and QA, LLaVul
enables more interpretable and security-focused code understanding.

</details>


### [107] [Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation](https://arxiv.org/abs/2509.17353)
*Ahmed T. Elboardy,Ghada Khoriba,Essam A. Rashed*

Main category: cs.AI

TL;DR: 本文提出了一个多智能体强化学习框架，用于自动化放射学报告生成的双重挑战：构建临床可靠系统和设计严格评估协议。该框架整合了LLM和LVM，通过十个专门智能体进行图像分析、特征提取、报告生成和评估。


<details>
  <summary>Details</summary>
Motivation: 解决放射学报告生成中的临床可靠性问题和评估协议设计挑战，建立可信赖的基于偏差的放射学报告生成路径。

Method: 采用多智能体强化学习框架，整合大型语言模型和大型视觉模型，构建包含十个专门智能体的模块化架构，负责图像分析、特征提取、报告生成、审查和评估。

Result: 在公共放射学数据集上使用chatGPT-4o进行实现，LLM作为评估者与医学放射科医生反馈相结合，实现了细粒度评估。

Conclusion: 通过将评估协议与LLM开发生命周期对齐，包括预训练、微调、对齐和部署，该基准为可信赖的放射学报告生成建立了路径。

Abstract: Automating radiology report generation poses a dual challenge: building
clinically reliable systems and designing rigorous evaluation protocols. We
introduce a multi-agent reinforcement learning framework that serves as both a
benchmark and evaluation environment for multimodal clinical reasoning in the
radiology ecosystem. The proposed framework integrates large language models
(LLMs) and large vision models (LVMs) within a modular architecture composed of
ten specialized agents responsible for image analysis, feature extraction,
report generation, review, and evaluation. This design enables fine-grained
assessment at both the agent level (e.g., detection and segmentation accuracy)
and the consensus level (e.g., report quality and clinical relevance). We
demonstrate an implementation using chatGPT-4o on public radiology datasets,
where LLMs act as evaluators alongside medical radiologist feedback. By
aligning evaluation protocols with the LLM development lifecycle, including
pretraining, finetuning, alignment, and deployment, the proposed benchmark
establishes a path toward trustworthy deviance-based radiology report
generation.

</details>


### [108] [Multi-Scenario Highway Lane-Change Intention Prediction: A Physics-Informed AI Framework for Three-Class Classification](https://arxiv.org/abs/2509.17354)
*Jiazhao Shi,Yichen Lin,Yiheng Hua,Ziyu Wang,Zijian Zhang,Wenjia Zheng,Yun Song,Kuan Lu,Shoufeng Lu*

Main category: cs.AI

TL;DR: 提出了一种基于物理知识的AI框架，用于实时车道变换意图预测，通过整合车辆运动学、交互可行性和交通安全指标，在三分类问题上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 车道变换是高速公路事故的主要原因，现有方法存在二元分类限制、场景多样性不足和长预测时间性能下降等问题，需要更准确的车道变换意图预测来提高自动驾驶系统的安全性。

Method: 提出物理信息AI框架，整合车辆运动学、交互可行性和交通指标（如车距、时间间隔、碰撞时间等），将车道变换预测建模为左转、右转和无变化的三分类问题，使用LightGBM等机器学习模型。

Result: 在highD数据集上达到99.8%准确率和93.6%宏F1分数，在exiD数据集上达到96.1%准确率和88.7%宏F1分数（1秒预测时间），优于双层堆叠LSTM基线。

Conclusion: 物理信息和特征丰富的机器学习框架在自动驾驶系统的实时车道变换意图预测中具有实际优势，展示了良好的泛化能力。

Abstract: Lane-change maneuvers are a leading cause of highway accidents, underscoring
the need for accurate intention prediction to improve the safety and
decision-making of autonomous driving systems. While prior studies using
machine learning and deep learning methods (e.g., SVM, CNN, LSTM, Transformers)
have shown promise, most approaches remain limited by binary classification,
lack of scenario diversity, and degraded performance under longer prediction
horizons. In this study, we propose a physics-informed AI framework that
explicitly integrates vehicle kinematics, interaction feasibility, and
traffic-safety metrics (e.g., distance headway, time headway,
time-to-collision, closing gap time) into the learning process. lane-change
prediction is formulated as a three-class problem that distinguishes left
change, right change, and no change, and is evaluated across both straight
highway segments (highD) and complex ramp scenarios (exiD). By integrating
vehicle kinematics with interaction features, our machine learning models,
particularly LightGBM, achieve state-of-the-art accuracy and strong
generalization. Results show up to 99.8% accuracy and 93.6% macro F1 on highD,
and 96.1% accuracy and 88.7% macro F1 on exiD at a 1-second horizon,
outperforming a two-layer stacked LSTM baseline. These findings demonstrate the
practical advantages of a physics-informed and feature-rich machine learning
framework for real-time lane-change intention prediction in autonomous driving
systems.

</details>


### [109] [Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process](https://arxiv.org/abs/2509.17380)
*Zhizhang FU,Guangsheng Bao,Hongbo Zhang,Chenkai Hu,Yue Zhang*

Main category: cs.AI

TL;DR: 该研究对LLMs和LRMs进行系统性因果分析，发现RLVR训练的LRMs具有更强的因果推理能力，能减少虚假相关性并增强真实因果模式，而LLMs和蒸馏LRMs未能解决因果缺陷。


<details>
  <summary>Details</summary>
Motivation: LLMs存在不忠实、偏见和不一致等关键推理问题，因为它们缺乏稳健的因果基础，可能依赖表面相关性而非真正理解。虽然LRMs通过强化学习和蒸馏等技术提高了任务准确性，但这些训练方法对因果关系的影响尚未充分探索。

Method: 研究使用结构因果模型(SCMs)分析四个关键变量：问题指令(Z)、思考过程(T)、推理步骤(X)和答案(Y)，对LLMs和LRMs进行系统性因果分析，并深入调查RLVR训练过程的动态变化。

Result: RLVR训练的LRMs表现出增强的因果推理能力，更接近理想因果结构，同时减少了虚假相关性并加强了真实因果模式。RLVR训练过程中观察到减少的虚假特征与改进的因果结构高度相关。

Conclusion: 该研究有助于理解推理模型中的因果关系，强调了RLVR在增强因果推理中的关键作用，为设计具有更强因果基础的未来AI系统提供了见解。

Abstract: LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and
inconsistency, since they lack robust causal underpinnings and may rely on
superficial correlations rather than genuine understanding. Successive LRMs
have emerged as a promising alternative, leveraging advanced training
techniques such as reinforcement learning (RL) and distillation to improve task
accuracy. However, the impact of these training methods on causality remains
largely unexplored. In this study, we conduct a systematic causal analysis on
LLMs and LRMs, examining structural causal models (SCMs) of four key variables:
problem instruction (Z), thinking process (T), reasoning steps (X), and answer
(Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal
reasoning capabilities, aligning more closely with ideal causal structures,
while LLMs and distilled LRMs fail to address causality-related deficiencies.
Our further investigation indicates that RLVR reduces spurious correlations and
strengthens genuine causal patterns, thereby mitigating unfaithfulness and
bias. In addition, our inspection on the dynamics of the RLVR training process
observes a high correlation between reduced spurious features and improved
causal structures, where the causal relationships consistently improve in the
training process. This study contributes to the understanding of causality in
reasoning models, highlights the critical role of RLVR in enhancing causal
reasoning, and provides insights for designing future AI systems with stronger
causal foundations. We release our code and data at
https://github.com/Harryking1999/CoT_Causal_Analysis.

</details>


### [110] [Program Synthesis via Test-Time Transduction](https://arxiv.org/abs/2509.17393)
*Kang-il Lee,Jahyun Koo,Seunghyun Yoon,Minbeom Kim,Hyukhun Koh,Dongryeol Lee,Kyomin Jung*

Main category: cs.AI

TL;DR: 提出了一种新的程序合成方法——转导式程序合成，通过在合成过程中显式利用测试输入来提高鲁棒性，解决了传统方法在训练样本有限和边缘情况下泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统程序合成方法（基于自然语言描述或输入输出示例）通常难以在训练样本有限且测试输入包含各种边缘情况的真实场景中保持鲁棒性。

Method: 将合成视为在程序输出定义的有限假设类上进行主动学习，使用LLM预测选定测试输入的输出并消除不一致假设，通过贪婪最大化算法选择输入以最小化LLM查询次数。

Result: 在Playgol（字符串转换基准）和MBPP+（Python代码生成基准）两个真实数据集上评估，方法在准确性和效率方面显著提升了程序合成性能。

Conclusion: 转导式程序合成框架通过主动利用测试输入有效提高了程序合成的鲁棒性和性能，代码已开源。

Abstract: We introduce transductive program synthesis, a new formulation of the program
synthesis task that explicitly leverages test inputs during synthesis. While
prior approaches to program synthesis--whether based on natural language
descriptions or input-output examples--typically aim to generalize from
training examples, they often struggle with robustness, especially in
real-world settings where training examples are limited and test inputs involve
various edge cases. To address this, we propose a novel framework that improves
robustness by treating synthesis as an active learning over a finite hypothesis
class defined by programs' outputs. We use an LLM to predict outputs for
selected test inputs and eliminate inconsistent hypotheses, where the inputs
are chosen via a greedy maximin algorithm to minimize the number of LLM queries
required. We evaluate our approach on two real-world datasets: Playgol, a
string transformation benchmark, and MBPP+, a Python code generation benchmark.
We demonstrate that our method significantly improves program synthesis in both
accuracy and efficiency. We release our code at
https://github.com/klee972/SYNTRA.

</details>


### [111] [Evaluating Multimodal Large Language Models with Daily Composite Tasks in Home Environments](https://arxiv.org/abs/2509.17425)
*Zhenliang Zhang,Yuxi Wang,Hongzhao Xie,Shiyun Zhao,Mingyuan Liu,Yujie Lu,Xinyi He,Zhenku Cheng,Yujia Peng*

Main category: cs.AI

TL;DR: 该研究设计了一套基于幼儿日常活动的复合任务来评估多模态大语言模型在具身智能体中的综合能力，发现当前模型在物体理解、空间智能和社交活动三个核心领域表现不佳，与通用智能要求存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 虽然基于多模态大语言模型的具身智能体具有丰富的感知和交互能力，但它们在解决需要多种能力的复合任务方面的能力尚未得到充分探索。研究旨在评估这些模型是否具备解决复合任务的能力。

Method: 研究设计了一套受幼儿日常活动启发的复合任务，在动态模拟家庭环境中进行，涵盖物体理解、空间智能和社交活动三个核心领域。评估了17个领先的专有和开源多模态大语言模型。

Result: 所有模型在三个领域的表现均不理想，表明当前能力与通用智能要求之间存在显著差距。

Conclusion: 这些任务为评估具身智能体的通用能力提供了初步框架，标志着向具身多模态大语言模型开发及其实际部署迈出了早期但重要的一步。

Abstract: A key feature differentiating artificial general intelligence (AGI) from
traditional AI is that AGI can perform composite tasks that require a wide
range of capabilities. Although embodied agents powered by multimodal large
language models (MLLMs) offer rich perceptual and interactive capabilities, it
remains largely unexplored whether they can solve composite tasks. In the
current work, we designed a set of composite tasks inspired by common daily
activities observed in early childhood development. Within a dynamic and
simulated home environment, these tasks span three core domains: object
understanding, spatial intelligence, and social activity. We evaluated 17
leading proprietary and open-source MLLMs on these tasks. The results
consistently showed poor performance across all three domains, indicating a
substantial gap between current capabilities and general intelligence
requirements. Together, our tasks offer a preliminary framework for evaluating
the general capabilities of embodied agents, marking an early but significant
step toward the development of embodied MLLMs and their real-world deployment.

</details>


### [112] [SPICED: A Synaptic Homeostasis-Inspired Framework for Unsupervised Continual EEG Decoding](https://arxiv.org/abs/2509.17439)
*Yangxuan Zhou,Sha Zhao,Jiquan Wang,Haiteng Jiang,Shijian Li,Tao Li,Gang Pan*

Main category: cs.AI

TL;DR: SPICED是一个受大脑突触稳态平衡启发的神经形态框架，用于无监督持续EEG解码，能够动态适应新个体的变异性，通过临界记忆重激活、突触巩固和突触重归一化机制实现动态扩展，有效缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 受人类大脑通过突触稳态实现动态稳定性-可塑性平衡的生物原理启发，解决实际场景中不断出现具有个体间变异性的新个体的持续EEG解码问题。

Method: SPICED包含一个新颖的突触网络，通过三种生物启发的神经机制实现持续适应中的动态扩展：(1)临界记忆重激活；(2)突触巩固；(3)突触重归一化。这些机制与持续学习系统集成，优先重放与新个体有强关联的任务判别性记忆痕迹。

Result: 在三个EEG数据集上的验证表明，SPICED能够有效实现鲁棒适应，在长期持续学习中通过抑制有害记忆的重放优先级来有效缓解灾难性遗忘。

Conclusion: SPICED框架通过整合突触稳态机制，成功实现了无监督持续EEG解码，为处理个体间变异性的动态适应问题提供了有效的解决方案。

Abstract: Human brain achieves dynamic stability-plasticity balance through synaptic
homeostasis. Inspired by this biological principle, we propose SPICED: a
neuromorphic framework that integrates the synaptic homeostasis mechanism for
unsupervised continual EEG decoding, particularly addressing practical
scenarios where new individuals with inter-individual variability emerge
continually. SPICED comprises a novel synaptic network that enables dynamic
expansion during continual adaptation through three bio-inspired neural
mechanisms: (1) critical memory reactivation; (2) synaptic consolidation and
(3) synaptic renormalization. The interplay within synaptic homeostasis
dynamically strengthens task-discriminative memory traces and weakens
detrimental memories. By integrating these mechanisms with continual learning
system, SPICED preferentially replays task-discriminative memory traces that
exhibit strong associations with newly emerging individuals, thereby achieving
robust adaptations. Meanwhile, SPICED effectively mitigates catastrophic
forgetting by suppressing the replay prioritization of detrimental memories
during long-term continual learning. Validated on three EEG datasets, SPICED
show its effectiveness.

</details>


### [113] [AI Pangaea: Unifying Intelligence Islands for Adapting Myriad Tasks](https://arxiv.org/abs/2509.17460)
*Jianlong Chang,Haixin Wang,Zhiyuan Dang,Li Huang,Zhiyu Wang,Ruoqi Cao,Shihao Piao,Dongzhe Li,Dianyu Gao,Dongsheng Wang,Yin Li,Jinan Sun,Lu Fang,Zhouchen Lin*

Main category: cs.AI

TL;DR: Pangaea是一个AI超级大陆模型，旨在通过统一数据格式和跨296个数据集的多模态预训练来连接孤立的人工智能岛屿，实现跨45个通用任务和15个科学任务的强大泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型局限于特定任务，形成所谓的"智能岛屿"，缺乏跨任务的泛化能力。研究旨在打破这种孤立状态，向通用人工智能迈进。

Method: 提出Pangaea模型，将任何数据编码为统一格式，在296个跨模态数据集上进行预训练，积累通用知识。

Result: 模型在45个通用任务和15个科学任务上表现出卓越的泛化能力，并揭示了模态缩放效应，量化了跨模态知识积累的几何分布规律。

Conclusion: Pangaea展示了处理多种任务的强大潜力，为通向通用人工智能指明了新方向。

Abstract: The pursuit of artificial general intelligence continuously demands
generalization in one model across myriad tasks, even those not seen before.
However, current AI models are isolated from each other for being limited to
specific tasks, now first defined as Intelligence Islands. To unify
Intelligence Islands into one, we propose Pangaea, the first AI supercontinent
akin to the geological Pangaea. Pangaea encodes any data into a unified format
and accumulates universal knowledge through pre-training on 296 datasets across
diverse modalities. Eventually, it demonstrates remarkable generalization
across 45 general tasks and 15 scientific tasks encompassing a wide range of
scientific subjects. By investigating Pangaea deeper, the scaling effect of
modality is revealed, quantifying the universal knowledge accumulation across
modalities as the cumulative distribution function of a geometric distribution.
On the whole, Pangaea shows strong potential to handle myriad tasks, indicating
a new direction toward artificial general intelligence.

</details>


### [114] [A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data](https://arxiv.org/abs/2509.17544)
*Juan Cañada,Raúl Alonso,Julio Molleda,Fidel Díez*

Main category: cs.AI

TL;DR: 该研究开发了一个开源对话助手，通过整合多模态检索和大型语言模型，使非专家用户能够用自然语言与异质农业和地理空间数据进行交互。


<details>
  <summary>Details</summary>
Motivation: 开放地球观测和农业数据集虽然潜力巨大，但技术门槛高限制了非专家用户的使用。研究旨在降低获取专业农业信息的技术障碍。

Method: 提出结合正射影像、Sentinel-2植被指数和用户提供文档的架构，通过检索增强生成技术，系统能灵活决定依赖多模态证据、文本知识或两者结合来生成答案。采用LLM-as-a-judge方法进行零样本无监督评估。

Result: 初步结果显示系统能够生成清晰、相关且上下文感知的农业查询响应，同时保持跨地理区域的可重现性和可扩展性。

Conclusion: 主要贡献包括融合多模态地球观测和文本知识源的架构设计，展示了通过自然语言交互降低获取专业农业信息门槛的可行性，以及开放可重现的设计理念。

Abstract: The increasing availability of open Earth Observation (EO) and agricultural
datasets holds great potential for supporting sustainable land management.
However, their high technical entry barrier limits accessibility for non-expert
users. This study presents an open-source conversational assistant that
integrates multimodal retrieval and large language models (LLMs) to enable
natural language interaction with heterogeneous agricultural and geospatial
data. The proposed architecture combines orthophotos, Sentinel-2 vegetation
indices, and user-provided documents through retrieval-augmented generation
(RAG), allowing the system to flexibly determine whether to rely on multimodal
evidence, textual knowledge, or both in formulating an answer. To assess
response quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a
zero-shot, unsupervised setting, applying direct scoring in a multi-dimensional
quantitative evaluation framework. Preliminary results show that the system is
capable of generating clear, relevant, and context-aware responses to
agricultural queries, while remaining reproducible and scalable across
geographic regions. The primary contributions of this work include an
architecture for fusing multimodal EO and textual knowledge sources, a
demonstration of lowering the barrier to access specialized agricultural
information through natural language interaction, and an open and reproducible
design.

</details>


### [115] [Is It Certainly a Deepfake? Reliability Analysis in Detection & Generation Ecosystem](https://arxiv.org/abs/2509.17550)
*Neslihan Kose,Anthony Rhodes,Umur Aybars Ciftci,Ilke Demir*

Main category: cs.AI

TL;DR: 本文提出了首个深度伪造检测器的综合不确定性分析，系统研究生成伪影如何影响预测置信度，并利用不确定性进行深度伪造源检测。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型质量提升，深度伪造导致在线信任危机。检测器的误用会加剧错误信息问题，因此需要分析检测器的不确定性来提高可靠性。

Method: 使用贝叶斯神经网络和蒙特卡洛dropout量化偶然和认知不确定性，在多个数据集和生成器上进行评估，包括二元/多分类、源检测等实验。

Result: 不确定性流形包含足够一致信息可用于深度伪造源检测，不确定性图谱能定位像素级预测置信度，揭示与生成器特定伪影相关的模式。

Conclusion: 不确定性量化为可信合成媒体检测的基本要求，为部署可靠深度伪造检测系统提供关键见解。

Abstract: As generative models are advancing in quality and quantity for creating
synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors
are proposed to counter this effect, however, misuse of detectors claiming fake
content as real or vice versa further fuels this misinformation problem. We
present the first comprehensive uncertainty analysis of deepfake detectors,
systematically investigating how generative artifacts influence prediction
confidence. As reflected in detectors' responses, deepfake generators also
contribute to this uncertainty as their generative residues vary, so we cross
the uncertainty analysis of deepfake detectors and generators. Based on our
observations, the uncertainty manifold holds enough consistent information to
leverage uncertainty for deepfake source detection. Our approach leverages
Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and
epistemic uncertainties across diverse detector architectures. We evaluate
uncertainty on two datasets with nine generators, with four blind and two
biological detectors, compare different uncertainty methods, explore region-
and pixel-based uncertainty, and conduct ablation studies. We conduct and
analyze binary real/fake, multi-class real/fake, source detection, and
leave-one-out experiments between the generator/detector combinations to share
their generalization capability, model calibration, uncertainty, and robustness
against adversarial attacks. We further introduce uncertainty maps that
localize prediction confidence at the pixel level, revealing distinct patterns
correlated with generator-specific artifacts. Our analysis provides critical
insights for deploying reliable deepfake detection systems and establishes
uncertainty quantification as a fundamental requirement for trustworthy
synthetic media detection.

</details>


### [116] [MontePrep: Monte-Carlo-Driven Automatic Data Preparation without Target Data Instances](https://arxiv.org/abs/2509.17553)
*Congcong Ge,Yachuan Liu,Yixuan Tang,Yifan Zhu,Yaofeng Tu,Yunjun Gao*

Main category: cs.AI

TL;DR: MontePrep是一个基于开源大语言模型的自动数据准备框架，无需训练和目标表数据访问权限，通过树状搜索实现零目标实例需求的数据管道合成。


<details>
  <summary>Details</summary>
Motivation: 解决商业系统中传统自动数据准备方法依赖人工监督信号或目标表数据访问权限的限制，使其难以在实际场景中应用。

Method: 采用LLM驱动的蒙特卡洛树搜索方法，包含三个核心组件：数据准备动作沙盒(DPAS)、基础管道生成器(FPG)和执行感知管道优化器(EPO)。

Result: 实验结果表明MontePrep在五个最先进竞争对手中表现出显著优势。

Conclusion: MontePrep提供了一个有效的端到端ADP框架，能够在真实场景中实现无训练的数据管道合成。

Abstract: In commercial systems, a pervasive requirement for automatic data preparation
(ADP) is to transfer relational data from disparate sources to targets with
standardized schema specifications. Previous methods rely on labor-intensive
supervision signals or target table data access permissions, limiting their
usage in real-world scenarios. To tackle these challenges, we propose an
effective end-to-end ADP framework MontePrep, which enables training-free
pipeline synthesis with zero target-instance requirements. MontePrep is
formulated as an open-source large language model (LLM) powered tree-structured
search problem. It consists of three pivot components, i.e., a data preparation
action sandbox (DPAS), a fundamental pipeline generator (FPG), and an
execution-aware pipeline optimizer (EPO). We first introduce DPAS, a
lightweight action sandbox, to navigate the search-based pipeline generation.
The design of DPAS circumvents exploration of infeasible pipelines. Then, we
present FPG to build executable DP pipelines incrementally, which explores the
predefined action sandbox by the LLM-powered Monte Carlo Tree Search.
Furthermore, we propose EPO, which invokes pipeline execution results from
sources to targets to evaluate the reliability of the generated pipelines in
FPG. In this way, unreasonable pipelines are eliminated, thus facilitating the
search process from both efficiency and effectiveness perspectives. Extensive
experimental results demonstrate the superiority of MontePrep with significant
improvement against five state-of-the-art competitors.

</details>


### [117] [LIMI: Less is More for Agency](https://arxiv.org/abs/2509.17567)
*Yang Xiao,Mohan Jiang,Jie Sun,Keyu Li,Jifan Lin,Yumin Zhuang,Ji Zeng,Shijie Xia,Qishuo Hua,Xuefeng Li,Xiaojie Cai,Tongyu Wang,Yue Zhang,Liming Liu,Xia Wu,Jinlong Hou,Yuan Cheng,Wenjie Li,Xiang Wang,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: LIMI研究挑战了传统AI代理开发的数据规模依赖范式，提出"少即是多"的代理智能发展原则，仅用78个精心设计的训练样本就在综合代理基准测试中达到73.5%的准确率，显著优于使用大量数据的现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统擅长推理和生成响应，但行业需要能够执行任务、操作工具并推动现实世界成果的自主代理。传统方法认为更多数据产生更好的代理能力，但本研究旨在挑战这一范式。

Method: LIMI方法通过战略性地关注协作软件开发和科学研究工作流程，仅使用78个精心策划的自主行为演示样本来培养代理智能，而非依赖大规模数据。

Result: LIMI在综合代理基准测试中达到73.5%的准确率，显著优于现有最先进模型（Kimi-K2-Instruct 24.1%、DeepSeek-V3.1 11.9%等），且比使用10,000个样本训练的模型性能提升53.7%。

Conclusion: 研究确立了"代理效率原则"：机器自主性并非来自数据丰富性，而是来自高质量代理演示的战略性策划，证明了代理智能可以从最小但精心策划的演示中涌现。

Abstract: We define Agency as the emergent capacity of AI systems to function as
autonomous agents actively discovering problems, formulating hypotheses, and
executing solutions through self-directed engagement with environments and
tools. This fundamental capability marks the dawn of the Age of AI Agency,
driven by a critical industry shift: the urgent need for AI systems that don't
just think, but work. While current AI excels at reasoning and generating
responses, industries demand autonomous agents that can execute tasks, operate
tools, and drive real-world outcomes. As agentic intelligence becomes the
defining characteristic separating cognitive systems from productive workers,
efficiently cultivating machine autonomy becomes paramount. Current approaches
assume that more data yields better agency, following traditional scaling laws
from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is
More for Intelligent Agency) demonstrates that agency follows radically
different development principles. Through strategic focus on collaborative
software development and scientific research workflows, we show that
sophisticated agentic intelligence can emerge from minimal but strategically
curated demonstrations of autonomous behavior. Using only 78 carefully designed
training samples, LIMI achieves 73.5% on comprehensive agency benchmarks,
dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),
DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).
Most strikingly, LIMI demonstrates 53.7% improvement over models trained on
10,000 samples-achieving superior agentic intelligence with 128 times fewer
samples. Our findings establish the Agency Efficiency Principle: machine
autonomy emerges not from data abundance but from strategic curation of
high-quality agentic demonstrations.

</details>


### [118] [Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via Reinforced Multimodal Language Models](https://arxiv.org/abs/2509.17589)
*Jun Ling,Yao Qi,Tao Huang,Shibo Zhou,Yanqin Huang,Jiang Yang,Ziqi Song,Ying Zhou,Yang Yang,Heng Tao Shen,Peng Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化多模态大语言模型的表格图像到LaTeX代码生成方法，通过双奖励强化学习策略优化生成质量，在复杂表格处理上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 自动化从视觉输入重建高质量、可发布的表格，解决现有方法在处理复杂表格（大尺寸、深层嵌套结构、语义丰富或不规则单元格内容）时的失败问题。

Method: 提出强化多模态大语言模型框架，在大型表格到LaTeX数据集上微调预训练MLLM，引入基于GRPO的双奖励强化学习策略，结合LaTeX代码结构级奖励和渲染输出的视觉保真度奖励。

Result: 采用TEDS-Structure和CW-SSIM混合评估协议，证明该方法在结构复杂表格上达到最先进性能，显示出方法的有效性和鲁棒性。

Conclusion: 该方法通过直接优化视觉输出质量，显著提升了复杂表格的生成质量，为表格图像到LaTeX代码生成任务提供了有效的解决方案。

Abstract: In this work, we address the task of table image to LaTeX code generation,
with the goal of automating the reconstruction of high-quality,
publication-ready tables from visual inputs. A central challenge of this task
lies in accurately handling complex tables -- those with large sizes, deeply
nested structures, and semantically rich or irregular cell content -- where
existing methods often fail. We begin with a comprehensive analysis,
identifying key challenges and highlighting the limitations of current
evaluation protocols. To overcome these issues, we propose a reinforced
multimodal large language model (MLLM) framework, where a pre-trained MLLM is
fine-tuned on a large-scale table-to-LaTeX dataset. To further improve
generation quality, we introduce a dual-reward reinforcement learning strategy
based on Group Relative Policy Optimization (GRPO). Unlike standard approaches
that optimize purely over text outputs, our method incorporates both a
structure-level reward on LaTeX code and a visual fidelity reward computed from
rendered outputs, enabling direct optimization of the visual output quality. We
adopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and
show that our method achieves state-of-the-art performance, particularly on
structurally complex tables, demonstrating the effectiveness and robustness of
our approach.

</details>


### [119] [EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving](https://arxiv.org/abs/2509.17677)
*Xiyuan Zhou,Xinlei Wang,Yirui He,Yang Wu,Ruixi Zou,Yuheng Cheng,Yulu Xie,Wenxuan Liu,Huan Zhao,Yan Xu,Jinjin Gu,Junhua Zhao*

Main category: cs.AI

TL;DR: EngiBench是一个分层基准测试，用于评估大语言模型在解决工程问题上的能力，涵盖三个难度级别和多个工程子领域，通过系统改写问题变体来深入分析模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法捕捉真实工程问题的复杂性（不确定性、上下文和开放式场景），需要新的评估工具来测试LLMs在工程领域的实际应用能力。

Method: 设计分层基准测试（基础知识检索、多步上下文推理、开放式建模），系统地将每个问题改写为三个受控变体（扰动、知识增强、数学抽象），以分别评估模型的鲁棒性、领域知识和数学推理能力。

Result: 实验结果显示明显的性能差距：随着任务难度增加模型表现下降，问题稍有变化时性能变差，在高级工程任务上远落后于人类专家。

Conclusion: 当前LLMs仍缺乏真实世界工程所需的高级推理能力，需要未来模型具备更深入和可靠的问题解决能力。

Abstract: Large language models (LLMs) have shown strong performance on mathematical
reasoning under well-posed conditions. However, real-world engineering problems
require more than mathematical symbolic computation -- they need to deal with
uncertainty, context, and open-ended scenarios. Existing benchmarks fail to
capture these complexities. We introduce EngiBench, a hierarchical benchmark
designed to evaluate LLMs on solving engineering problems. It spans three
levels of increasing difficulty (foundational knowledge retrieval, multi-step
contextual reasoning, and open-ended modeling) and covers diverse engineering
subfields. To facilitate a deeper understanding of model performance, we
systematically rewrite each problem into three controlled variants (perturbed,
knowledge-enhanced, and math abstraction), enabling us to separately evaluate
the model's robustness, domain-specific knowledge, and mathematical reasoning
abilities. Experiment results reveal a clear performance gap across levels:
models struggle more as tasks get harder, perform worse when problems are
slightly changed, and fall far behind human experts on the high-level
engineering tasks. These findings reveal that current LLMs still lack the
high-level reasoning needed for real-world engineering, highlighting the need
for future models with deeper and more reliable problem-solving capabilities.
Our source code and data are available at
https://github.com/EngiBench/EngiBench.

</details>


### [120] [Virtual Arc Consistency for Linear Constraints inCost Function Networks](https://arxiv.org/abs/2509.17706)
*Pierre Montalbano,Simon de Givry,George Katsirelos*

Main category: cs.AI

TL;DR: 本文提出了一种改进的软弧一致性算法来处理线性约束，相比原有算法能显著提高下界，在某些情况下减少求解时间。


<details>
  <summary>Details</summary>
Motivation: 在约束规划中，解决带有硬约束和软约束的离散最小化问题有三种方法：软全局约束、线性规划重构和局部成本函数重构。软全局约束方法传播能力弱，线性规划方法规模大，因此研究局部成本函数方法中的软弧一致性算法。

Method: 将现有软弧一致性算法适配以处理线性约束，利用线性约束作为局部成本函数来增强建模表达能力。

Result: 改进后的算法在多个基准测试中相比原算法显著提高了下界，在某些情况下减少了求解时间。

Conclusion: 通过将线性约束整合到软弧一致性框架中，可以在保持建模灵活性的同时获得更好的求解性能。

Abstract: In Constraint Programming, solving discrete minimization problems with hard
and soft constraints can be done either using (i) soft global constraints, (ii)
a reformulation into a linear program, or (iii) a reformulation into local cost
functions. Approach (i) benefits from a vast catalog of constraints. Each soft
constraint propagator communicates with other soft constraints only through the
variable domains, resulting in weak lower bounds. Conversely, the approach (ii)
provides a global view with strong bounds, but the size of the reformulation
can be problematic. We focus on approach (iii) in which soft arc consistency
(SAC) algorithms produce bounds of intermediate quality. Recently, the
introduction of linear constraints as local cost functions increases their
modeling expressiveness. We adapt an existing SAC algorithm to handle linear
constraints. We show that our algorithm significantly improves the lower bounds
compared to the original algorithm on several benchmarks, reducing solving time
in some cases.

</details>


### [121] [DA-Mamba: Dialogue-aware selective state-space model for multimodal engagement estimation](https://arxiv.org/abs/2509.17711)
*Shenwei Kang,Xin Zhang,Wen Liu,Bin Li,Yujie Liu,Bo Gao*

Main category: cs.AI

TL;DR: DA-Mamba是一种对话感知的多模态架构，用于估计对话场景中的人类参与度，通过Mamba选择性状态空间处理替代注意力机制，实现线性时间和内存复杂度。


<details>
  <summary>Details</summary>
Motivation: 对话场景中的人类参与度估计对于自适应教学、远程医疗评估和社交感知人机交互等应用至关重要，需要处理动态的多模态信号。

Method: 设计了基于Mamba的对话感知选择性状态空间模型，包含三个核心模块：对话感知编码器、模态组融合和伙伴组融合，实现表达性对话理解。

Result: 在三个标准基准测试（NoXi、NoXi-Add和MPIIGI）上的实验表明，DA-Mamba在一致性相关系数（CCC）上超越了现有最先进方法，同时减少了训练时间和峰值内存使用。

Conclusion: DA-Mamba能够处理更长的序列，并促进在资源受限的多方对话设置中的实时部署，为多模态参与度估计提供了高效解决方案。

Abstract: Human engagement estimation in conversational scenarios is essential for
applications such as adaptive tutoring, remote healthcare assessment, and
socially aware human--computer interaction. Engagement is a dynamic, multimodal
signal conveyed by facial expressions, speech, gestures, and behavioral cues
over time. In this work we introduce DA-Mamba, a dialogue-aware multimodal
architecture that replaces attention-heavy dialogue encoders with Mamba-based
selective state-space processing to achieve linear time and memory complexity
while retaining expressive cross-modal reasoning. We design a Mamba
dialogue-aware selective state-space model composed of three core modules: a
Dialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group
Fusion and Partner-Group Fusion, these modules achieve expressive dialogue
understanding. Extensive experiments on three standard benchmarks (NoXi,
NoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art
(SOTA) methods in concordance correlation coefficient (CCC), while reducing
training time and peak memory; these gains enable processing much longer
sequences and facilitate real-time deployment in resource-constrained,
multi-party conversational settings. The source code will be available at:
https://github.com/kksssssss-ssda/MMEA.

</details>


### [122] [Efficient & Correct Predictive Equivalence for Decision Trees](https://arxiv.org/abs/2509.17774)
*Joao Marques-Silva,Alexey Ignatiev*

Main category: cs.AI

TL;DR: 本文分析了McTavish等人提出的MBDSR方法在决策树预测等价性判定中的局限性，证明该方法存在最坏情况指数复杂度且可能产生错误结果，并提出多项式时间算法替代方案。


<details>
  <summary>Details</summary>
Motivation: 决策树的Rashomon集合中存在大量预测等价的决策树，这会影响特征重要性分析的准确性。McTavish等人提出的MBDSR方法虽然试图解决预测等价性判定问题，但该方法存在理论缺陷。

Method: 本文首先证明QM方法在最坏情况下具有指数级时间和空间复杂度，其次展示MBDSR方法在预测等价性判定中可能产生错误结果，最后提出多项式时间算法来解决相关问题。

Result: 实验证实，对于触发QM方法最坏情况的决策树，本文提出的算法比McTavish等人的方法快几个数量级。

Conclusion: MBDSR方法存在理论缺陷，而本文提出的多项式时间算法能更高效地解决决策树预测等价性判定及相关问题。

Abstract: The Rashomon set of decision trees (DTs) finds importance uses. Recent work
showed that DTs computing the same classification function, i.e. predictive
equivalent DTs, can represent a significant fraction of the Rashomon set. Such
redundancy is undesirable. For example, feature importance based on the
Rashomon set becomes inaccurate due the existence of predictive equivalent DTs,
i.e. DTs with the same prediction for every possible input. In recent work,
McTavish et al. proposed solutions for several computational problems related
with DTs, including that of deciding predictive equivalent DTs. This approach,
which this paper refers to as MBDSR, consists of applying the well-known method
of Quine-McCluskey (QM) for obtaining minimum-size DNF (disjunctive normal
form) representations of DTs, which are then used for comparing DTs for
predictive equivalence. Furthermore, the minimum-size DNF representation was
also applied to computing explanations for the predictions made by DTs, and to
finding predictions in the presence of missing data. However, the problem of
formula minimization is hard for the second level of the polynomial hierarchy,
and the QM method may exhibit worst-case exponential running time and space.
This paper first demonstrates that there exist decision trees that trigger the
worst-case exponential running time and space of the QM method. Second, the
paper shows that the MBDSR approach can produce incorrect results for the
problem of deciding predictive equivalence. Third, the paper shows that any of
the problems to which the minimum-size DNF representation has been applied to
can in fact be solved in polynomial time, in the size of the DT. The
experiments confirm that, for DTs for which the the worst-case of the QM method
is triggered, the algorithms proposed in this paper are orders of magnitude
faster than the ones proposed by McTavish et al.

</details>


### [123] [Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling](https://arxiv.org/abs/2509.17905)
*Zongqian Wu,Baoduo Xu,Tianyu Li,Zhu Sun,Xiaofeng Zhu,Lei Feng*

Main category: cs.AI

TL;DR: 本文提出了TTS-Uniform框架来解决测试时扩展中的推理策略选择偏差问题，通过均匀分配采样预算和过滤不稳定策略来提高大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法忽视了推理策略选择偏差问题，即LLMs在生成推理过程时倾向于遵循某些特定策略而忽略其他有效替代方案，导致解决方案空间探索不足。

Method: TTS-Uniform框架包含三个步骤：(i)识别潜在推理策略，(ii)均匀分配采样预算给不同策略，(iii)在聚合前过滤不稳定策略。

Result: 实验结果表明，TTS-Uniform在多个主流LLM和基准数据集上显著提高了扩展效果。

Conclusion: 该研究揭示了推理策略选择偏差对测试时扩展有效性的影响，并提出了有效的解决方案，为改进LLM的推理能力提供了新思路。

Abstract: Test-time scaling (TTS) has been shown to improve the performance of large
language models (LLMs) by sampling and aggregating diverse reasoning paths.
However, existing research has overlooked a critical issue: selection bias of
reasoning strategies during scaling. Specifically, when generating reasoning
processes, LLMs tend to follow certain strategies (e.g., algebraic solutions
for math problems) while neglecting other valid alternatives (e.g., geometric
solutions), resulting in insufficient exploration of the solution space. To
further understand the impact of this bias, we present a theoretical analysis
that reveals when it undermines the effectiveness of test-time scaling.
Motivated by this theoretical insight, we introduce TTS-Uniform, a framework
designed to mitigate the selection bias of reasoning strategies. It (i)
identifies potential strategies, (ii) uniformly allocates the sampling budget
across them, and (iii) filters out unstable strategies prior to aggregation.
Experimental results show that TTS-Uniform significantly enhances scaling
effectiveness across multiple mainstream LLMs and benchmark datasets.

</details>


### [124] [MEF: A Systematic Evaluation Framework for Text-to-Image Models](https://arxiv.org/abs/2509.17907)
*Xiaojing Dong,Weilin Huang,Liang Li,Yiying Li,Shu Liu,Tongtong Ou,Shuang Ouyang,Yu Tian,Fengxuan Zhao*

Main category: cs.AI

TL;DR: 该论文提出了Magic评估框架(MEF)，用于系统评估文本到图像(T2I)生成模型。通过构建Magic-Bench-377基准测试集，结合ELO和MOS评估方法，实现模型排名和细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 现有T2I评估方法缺乏应用场景视角，且ELO和MOS方法各有局限性，需要更系统实用的评估框架。

Method: 提出结构化分类法构建Magic-Bench-377基准测试集，结合ELO进行模型排名和MOS进行维度评分，使用多元逻辑回归分析各维度对用户满意度的贡献。

Result: 应用MEF框架获得了当前T2I模型的排行榜和关键特征，框架和基准测试集已开源。

Conclusion: MEF框架为视觉生成模型评估提供了系统实用的方法，有助于推动该领域的研究发展。

Abstract: Rapid advances in text-to-image (T2I) generation have raised higher
requirements for evaluation methodologies. Existing benchmarks center on
objective capabilities and dimensions, but lack an application-scenario
perspective, limiting external validity. Moreover, current evaluations
typically rely on either ELO for overall ranking or MOS for dimension-specific
scoring, yet both methods have inherent shortcomings and limited
interpretability. Therefore, we introduce the Magic Evaluation Framework (MEF),
a systematic and practical approach for evaluating T2I models. First, we
propose a structured taxonomy encompassing user scenarios, elements, element
compositions, and text expression forms to construct the Magic-Bench-377, which
supports label-level assessment and ensures a balanced coverage of both user
scenarios and capabilities. On this basis, we combine ELO and
dimension-specific MOS to generate model rankings and fine-grained assessments
respectively. This joint evaluation method further enables us to quantitatively
analyze the contribution of each dimension to user satisfaction using
multivariate logistic regression. By applying MEF to current T2I models, we
obtain a leaderboard and key characteristics of the leading models. We release
our evaluation framework and make Magic-Bench-377 fully open-source to advance
research in the evaluation of visual generative models.

</details>


### [125] [Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent](https://arxiv.org/abs/2509.17917)
*Junyu Lu,Songxin Zhang,Zejian Xie,Zhuoyang Song,Jiaxing Zhang*

Main category: cs.AI

TL;DR: Orcust是一个GUI代理框架，通过约束奖励建模和在线轨迹构建来提升交互式GUI任务中的推理可靠性和数据效率


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理模型在不可靠的奖励信号和有限的在线轨迹生成方面存在困难，需要更可靠的推理和数据效率

Method: 结合原则约束奖励建模(PCRM)和在线虚拟机基础轨迹构建(OVTC)，使用环境可验证和LLM衍生的原则来约束推理链，并通过虚拟机构建结构化GUI交互轨迹

Result: 在标准GUI基准测试中实现最先进性能，ScreenSpot提升22.2%，ScreenSpot-Pro提升23.9%

Conclusion: Orcust有效提升了GUI代理在不同环境和任务复杂度下的推理能力、适应性和可扩展性

Abstract: Recent advances in GUI agents have achieved remarkable grounding and
action-prediction performance, yet existing models struggle with unreliable
reward signals and limited online trajectory generation. In this paper, we
introduce Orcust, a framework that integrates Principle-Constrained Reward
Modeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to
enhance reasoning reliability and data efficiency in interactive GUI tasks. We
leverages environment-verifiable and LLM-derived principle to enforce
interpretable reward signals that constrain long chain-of-thought reasoning and
rule-based feedback. OVTC spins up instrumented virtual machines to
autonomously collect structured GUI interaction trajectories with explicit
procedural and structural objectives, enabling the training of a stepwise
reward model that robustly captures human preferences and adheres to
task-specific constraints. Extensive experiments on standard GUI benchmarks
covering perceptual grounding, foundational operations, and end-to-end task
execution reveal that Orcust achieves state-of-the-art performance, improving
by 22.2\% on ScreenSpot and 23.9\% on ScreenSpot-Pro over the base model (i.e.
Qwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the
reasoning, adaptability and scalability of GUI agents across various
environments and task complexities.

</details>


### [126] ["I think this is fair'': Uncovering the Complexities of Stakeholder Decision-Making in AI Fairness Assessment](https://arxiv.org/abs/2509.17956)
*Lin Luo,Yuri Nakao,Mathieu Chollet,Hiroya Inakoshi,Simone Stumpf*

Main category: cs.AI

TL;DR: 研究通过质性研究发现，非AI专家的利益相关者在评估AI公平性时比AI专家更复杂，他们考虑更多特征、定制化指标和更严格的阈值，强调需要将利益相关者的公平判断纳入AI治理。


<details>
  <summary>Details</summary>
Motivation: 当前AI公平性评估主要由AI专家主导，但缺乏对受AI决策影响但无AI专业知识的利益相关者如何评估公平性的了解。

Method: 对30名无AI专业知识的利益相关者进行质性研究，在信用评级场景中观察他们如何选择特征优先级、指标和阈值来评估公平性。

Result: 利益相关者的公平性决策比AI专家实践更复杂：考虑超出法律保护特征的特征、为特定情境定制指标、设定多样化且更严格的公平阈值，甚至偏好设计定制化公平性。

Conclusion: 研究结果扩展了对利益相关者如何有意义地参与AI公平性治理的理解，强调纳入利益相关者细致公平判断的重要性。

Abstract: Assessing fairness in artificial intelligence (AI) typically involves AI
experts who select protected features, fairness metrics, and set fairness
thresholds. However, little is known about how stakeholders, particularly those
affected by AI outcomes but lacking AI expertise, assess fairness. To address
this gap, we conducted a qualitative study with 30 stakeholders without AI
expertise, representing potential decision subjects in a credit rating
scenario, to examine how they assess fairness when placed in the role of
deciding on features with priority, metrics, and thresholds. We reveal that
stakeholders' fairness decisions are more complex than typical AI expert
practices: they considered features far beyond legally protected features,
tailored metrics for specific contexts, set diverse yet stricter fairness
thresholds, and even preferred designing customized fairness. Our results
extend the understanding of how stakeholders can meaningfully contribute to AI
fairness governance and mitigation, underscoring the importance of
incorporating stakeholders' nuanced fairness judgments.

</details>


### [127] [On the Variational Costs of Changing Our Minds](https://arxiv.org/abs/2509.17957)
*David Hyland,Mahault Albarracin*

Main category: cs.AI

TL;DR: 本文提出了一种形式化框架，将信念更新建模为动机驱动的变分决策过程，认为常见的认知偏差（如确认偏误）并非认知缺陷，而是对信念更新成本的自适应响应。


<details>
  <summary>Details</summary>
Motivation: 人类思维经常表现出与理性信念更新标准相悖的行为（如确认偏误、态度极化），但作者认为这些"偏差"实际上是应对信念更新高昂成本的适应性策略，而非认知缺陷。

Method: 采用资源理性模型，将信念更新视为动机驱动的变分决策过程，使用Kullback-Leibler散度量化从先验到变分后验的信息成本，并通过计算实验验证模型。

Result: 计算实验表明，该模型的简单实例能够定性模拟常见的人类行为模式，包括确认偏误和态度极化现象。

Conclusion: 该框架为理解信念变化的动机贝叶斯机制提供了更全面的解释，并为预测、补偿和纠正信念更新过程中的偏差提供了实用见解。

Abstract: The human mind is capable of extraordinary achievements, yet it often appears
to work against itself. It actively defends its cherished beliefs even in the
face of contradictory evidence, conveniently interprets information to conform
to desired narratives, and selectively searches for or avoids information to
suit its various purposes. Despite these behaviours deviating from common
normative standards for belief updating, we argue that such 'biases' are not
inherently cognitive flaws, but rather an adaptive response to the significant
pragmatic and cognitive costs associated with revising one's beliefs. This
paper introduces a formal framework that aims to model the influence of these
costs on our belief updating mechanisms.
  We treat belief updating as a motivated variational decision, where agents
weigh the perceived 'utility' of a belief against the informational cost
required to adopt a new belief state, quantified by the Kullback-Leibler
divergence from the prior to the variational posterior. We perform
computational experiments to demonstrate that simple instantiations of this
resource-rational model can be used to qualitatively emulate commonplace human
behaviours, including confirmation bias and attitude polarisation. In doing so,
we suggest that this framework makes steps toward a more holistic account of
the motivated Bayesian mechanics of belief change and provides practical
insights for predicting, compensating for, and correcting deviations from
desired belief updating processes.

</details>


### [128] [The STAR-XAI Protocol: An Interactive Framework for Inducing Second-Order Agency in AI Agents](https://arxiv.org/abs/2509.17978)
*Antoni Guasch,Maria Isabel Valdez*

Main category: cs.AI

TL;DR: STAR-XAI协议是一种新型AI代理训练方法，通过结构化对话和规则手册将大型推理模型转化为透明可靠的"清晰盒子"代理


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在复杂任务中存在可靠性和透明度不足的问题，存在"思考幻觉"，需要开发可验证的可靠AI代理

Method: 采用结构化苏格拉底对话，使用意识转移包(CTP)规则手册，通过游戏循环实现事前战略论证和状态锁定校验和防止错误累积

Result: 在复杂战略游戏"Caps i Caps"的25步案例研究中，代理不仅解决了高复杂度难题，还展示了二阶代理能力，能够识别自身计划缺陷并调整核心协议

Conclusion: STAR-XAI协议为创建高性能、透明、可审计且可信赖的AI代理提供了实用途径

Abstract: Current Large Reasoning Models (LRMs) exhibit significant limitations in
reliability and transparency, often showing a collapse in reasoning
capabilities when faced with high-complexity, long-horizon tasks. This
"illusion of thinking" is frequently an artifact of non-agentic, black-box
evaluation paradigms that fail to cultivate robust problem-solving processes.
In response, we introduce The STAR-XAI Protocol (Socratic, Transparent,
Agentic, Reasoning - for eXplainable Artificial Intelligence), a novel
methodology for training and operating verifiably reliable AI agents. Our
method reframes the human-AI interaction as a structured, Socratic dialogue,
governed by an explicit and evolving rulebook, the Consciousness Transfer
Package (CTP). Through an interactive Gameplay Cycle that enforces ante-hoc
strategic justification and a state-locking Checksum that prevents error
accumulation, the protocol transforms a powerful but opaque LRM into a
disciplined "Clear Box" agent. We demonstrate the efficacy of this method
through an exhaustive 25-move case study in the complex strategic game "Caps i
Caps". The agent not only solved the high-complexity puzzle but also
demonstrated Second-Order Agency, identifying flaws in its own
supervisor-approved plans and adapting its core integrity protocols mid-task.
The STAR-XAI Protocol offers a practical pathway to creating AI agents that are
not just high-performing, but also transparent, auditable, and trustworthy by
design.

</details>


### [129] [Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates](https://arxiv.org/abs/2509.18076)
*Hy Dang,Tianyi Liu,Zhuofeng Wu,Jingfeng Yang,Haoming Jiang,Tao Yang,Pei Chen,Zhengyang Wang,Helen Wang,Huasheng Li,Bing Yin,Meng Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种基于课程学习理念的结构化推理模板框架，用于改进大语言模型在工具调用任务中的表现，相比自由形式的思维链提示，该方法能显著减少工具使用错误。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在现实世界工具交互中经常失败，主要原因是参数化错误、工具选择不当或用户意图误解，这些问题源于对用户目标理解不完整和对工具文档理解不足。

Method: 引入课程学习启发的框架，利用结构化推理模板来引导LLM通过更谨慎的逐步指令生成函数调用，取代自由形式的思维链提示。

Result: 实验结果显示该方法减少了工具使用错误，在不同模型系列和方法上实现了3-12%的相对改进。

Conclusion: 该框架增强了工具使用代理的鲁棒性、可解释性和透明度，推动了更可靠的现实世界AI助手的发展。

Abstract: Large language models (LLMs) have demonstrated strong reasoning and tool-use
capabilities, yet they often fail in real-world tool-interactions due to
incorrect parameterization, poor tool selection, or misinterpretation of user
intent. These issues often stem from an incomplete understanding of user goals
and inadequate comprehension of tool documentation. While Chain-of-Thought
(CoT) prompting has proven effective for enhancing reasoning in general
contexts, our analysis reveals that free-form CoT is insufficient and sometimes
counterproductive for structured function-calling tasks. To address this, we
introduce a curriculum-inspired framework that leverages structured reasoning
templates to guide LLMs through more deliberate step-by-step instructions for
generating function callings. Experimental results show that our method reduces
tool-use errors, achieving 3-12% relative improvements over strong baselines
across diverse model series and approaches. Moreover, our framework enhances
the robustness, interpretability, and transparency of tool-using agents,
advancing the development of more reliable AI assistants for real-world
applications.

</details>


### [130] [Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning](https://arxiv.org/abs/2509.18083)
*Valentin Lacombe,Valentin Quesnel,Damien Sileo*

Main category: cs.AI

TL;DR: Reasoning Core是一个用于强化学习与可验证奖励的新环境，旨在提升大语言模型的符号推理能力，通过程序化生成多领域问题并提供无限训练实例。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注游戏或孤立谜题，缺乏对核心形式化领域推理能力的系统性评估。需要构建一个能够持续生成新颖、可验证推理问题的环境来推动LLM推理能力的发展。

Method: 基于高通用性问题分布、外部工具验证和连续难度控制三大设计原则，程序化生成PDDL规划、一阶逻辑、上下文无关文法解析、因果推理和系统方程求解等领域的推理问题。

Result: 前沿LLM的零样本评估证实了Reasoning Core任务的难度，表明该环境能够有效挑战现有模型的推理能力。

Conclusion: Reasoning Core为提升未来模型的推理能力提供了有前景的资源，其无限问题生成和可验证奖励机制为RLVR研究开辟了新方向。

Abstract: We introduce Reasoning Core, a new scalable environment for Reinforcement
Learning with Verifiable Rewards (RLVR), designed to advance foundational
symbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks
that focus on games or isolated puzzles, Reasoning Core procedurally generates
problems across core formal domains, including PDDL planning, first-order
logic, context-free grammar parsing, causal reasoning, and system equation
solving. The environment is built on key design principles of high-generality
problem distributions, verification via external tools, and continuous
difficulty control, which together provide a virtually infinite supply of novel
training instances. Initial zero-shot evaluations with frontier LLMs confirm
the difficulty of Reasoning Core's tasks, positioning it as a promising
resource to improve the reasoning capabilities of future models.

</details>
