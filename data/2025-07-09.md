<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.CR](#cs.CR) [Total: 22]
- [cs.AI](#cs.AI) [Total: 41]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks](https://arxiv.org/abs/2507.05269)
*Danning Xie,Mingwei Zheng,Xuwei Liu,Jiannan Wang,Chengpeng Wang,Lin Tan,Xiangyu Zhang*

Main category: cs.SE

TL;DR: CoRe是一个高质量、人工验证的基准测试，用于评估大语言模型在静态分析任务中的表现，发现现有模型在深层语义理解和多步推理方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注端到端结果，忽略了对程序语义推理能力的评估，因此需要更全面的测试工具。

Method: 提出CoRe基准测试，包含12,553个任务实例，涵盖数据依赖、控制依赖和信息流，采用语义感知的多样性采样策略。

Result: 评估10种主流大语言模型，发现其在依赖识别上表现良好，但在深层语义和多步推理任务中表现不佳。

Conclusion: 研究揭示了模型在复杂控制结构和反向依赖模式上的挑战，为提升代码推理能力提供了方向。

Abstract: Large language models (LLMs) have been widely adopted across diverse software
engineering domains, such as code generation, program repair, and vulnerability
detection. These applications require understanding beyond surface-level code
patterns: value propagation, control flow, and interdependence between program
elements. However, existing benchmarks primarily evaluate end-to-end outcomes,
such as whether code is correctly repaired or generated, leaving the models
ability for program semantic reasoning underexplored. This work presents CoRe,
a high-quality, human-verified benchmark designed to evaluate LLMs on
fundamental static analysis tasks. CoRe includes 12,553 task instances spanning
data dependency, control dependency, and information flow across programs
written in C/C++, Java, and Python. To ensure semantic diversity and reasoning
complexity, we propose a semantics-aware diverse sampling strategy that selects
targets and task instances based on structural coverage and dependency depth.
We evaluate 10 mainstream LLMs and show that, while they perform well at
identifying dependencies, models still struggle with tasks that require deeper
semantic understanding and multi-step reasoning. We further conduct qualitative
analyses to uncover key challenges, such as complex control structures and
backward dependency patterns, offering insights into improving LLMs code
reasoning capabilities.

</details>


### [2] [Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management](https://arxiv.org/abs/2507.05270)
*Boyuan Li,Chengwei Liu,Lingling Fan,Sen Chen,Zhenlin Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: 论文探讨了第三方软件组件集成中的许可证风险，通过系统文献综述分类现有研究，并提出未来方向和实践建议。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发中集成第三方组件虽高效但伴随许可证风险，现有解决方案仍有局限，需系统性管理。

Method: 对80篇开源软件许可证相关论文进行系统文献综述，分类为许可证识别、风险评估和风险缓解。

Result: 揭示了现有解决方案的挑战，提出了未来研究方向和实践建议。

Conclusion: 希望填补学术界与工业界间的鸿沟，加速软件工程社区对合法软件风险的系统治理。

Abstract: Integrating third-party software components is a common practice in modern
software development, offering significant advantages in terms of efficiency
and innovation. However, this practice is fraught with risks related to
software licensing. A lack of understanding may lead to disputes, which can
pose serious legal and operational challenges. To these ends, both academia and
industry have conducted various investigations and proposed solutions and tools
to deal with these challenges. However, significant limitations still remain.
Moreover, the rapid evolution of open-source software (OSS) licenses, as well
as the rapidly incorporated generative software engineering techniques, such as
large language models for code (CodeLLMs), are placing greater demands on the
systematic management of software license risks. To unveil the severe
challenges and explore possible future directions, we conduct the first
systematic literature review (SLR) on 80 carefully selected OSS license-related
papers, classifying existing research into three key categories, i.e., license
identification, license risk assessment, and license risk mitigation. Based on
these, we discuss challenges in existing solutions, conclude the opportunities
to shed light on future research directions and offer practical recommendations
for practitioners. We hope this thorough review will help bridge the gaps
between academia and industry and accelerate the ecosystem-wide governance of
legitimate software risks within the software engineering community.

</details>


### [3] [FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing](https://arxiv.org/abs/2507.05272)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.SE

TL;DR: 结合大型语言模型（LLMs）和模糊测试生成最弱前置条件（WP），通过模糊指导（FG）利用执行反馈优化LLMs的输出。


<details>
  <summary>Details</summary>
Motivation: 生成最弱前置条件（WP）在验证和运行时错误检查等领域有重要应用，但传统方法效率有限。

Method: 提出模糊指导（FG），利用模糊测试验证候选WP的弱性和正确性，并将反馈用于优化LLMs的输出。

Result: 在Java确定性数组程序基准测试中，LLMs能生成可行的候选WP，且FG能显著提升其效果。

Conclusion: LLMs结合FG能有效生成WP，为程序验证提供新思路。

Abstract: The weakest precondition (WP) of a program describes the largest set of
initial states from which all terminating executions of the program satisfy a
given postcondition. The generation of WPs is an important task with practical
applications in areas ranging from verification to run-time error checking.
  This paper proposes the combination of Large Language Models (LLMs) and fuzz
testing for generating WPs. In pursuit of this goal, we introduce Fuzzing
Guidance (FG); FG acts as a means of directing LLMs towards correct WPs using
program execution feedback. FG utilises fuzz testing for approximately checking
the validity and weakness of candidate WPs, this information is then fed back
to the LLM as a means of context refinement.
  We demonstrate the effectiveness of our approach on a comprehensive benchmark
set of deterministic array programs in Java. Our experiments indicate that LLMs
are capable of producing viable candidate WPs, and that this ability can be
practically enhanced through FG.

</details>


### [4] [ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy](https://arxiv.org/abs/2507.05279)
*Virgile Boraud,Yannis Bendi-Ouis,Paul Bernard,Xavier Hinaut*

Main category: cs.SE

TL;DR: 介绍了一个工具，通过结合RAG和知识图谱提升LLMs在ReservoirPy库代码开发和Reservoir Computing领域问答中的能力，减少幻觉并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 提升LLMs在代码开发和专业领域问答中的表现，减少错误信息。

Method: 结合Retrieval-Augmented Generation (RAG)和知识图谱，提供交互式体验。

Result: 在代码任务上优于ChatGPT-4o和NotebookLM，显著超越基础模型Codestral-22B。

Conclusion: 该工具在专业领域和代码任务中表现出色，具有实际应用潜力。

Abstract: We introduce a tool designed to improve the capabilities of Large Language
Models (LLMs) in assisting with code development using the ReservoirPy library,
as well as in answering complex questions in the field of Reservoir Computing.
By incorporating external knowledge through Retrieval-Augmented Generation
(RAG) and knowledge graphs, our approach aims to reduce hallucinations and
increase the factual accuracy of generated responses. The system provides an
interactive experience similar to ChatGPT, tailored specifically for
ReservoirPy, enabling users to write, debug, and understand Python code while
accessing reliable domain-specific insights. In our evaluation, while
proprietary models such as ChatGPT-4o and NotebookLM performed slightly better
on general knowledge questions, our model outperformed them on coding tasks and
showed a significant improvement over its base model, Codestral-22B.

</details>


### [5] [CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark](https://arxiv.org/abs/2507.05281)
*Lingyue Fu,Hao Guan,Bolun Zhang,Haowei Yuan,Yaoming Zhu,Jun Xu,Zongyu Wang,Lin Qiu,Xunliang Cai,Xuezhi Cao,Weiwen Liu,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: CorePipe和CoreCodeBench提出了一种自动化管道和可配置的多场景仓库级基准测试，用于评估LLM在真实工程环境中的代码处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有仓库级基准测试局限于单一场景，无法全面反映真实软件工程的多样性和复杂性，且存在可控性和可靠性问题。

Method: CorePipe将仓库转化为综合测试用例，生成三种原子问题（开发、修复、测试驱动开发），并组合为复合问题，通过超参数调整难度。CoreCodeBench作为基准测试工具。

Result: 实验评估了16种LLM，揭示了其在不同场景下的能力差异，提供了多维度的性能洞察。

Conclusion: CoreCodeBench为研究LLM在真实工程项目的适用性提供了全面且广泛的基准测试工具。

Abstract: As Large Language Models (LLMs) demonstrate increasingly sophisticated code
processing capabilities, evaluating their performance on engineering-level code
remains challenging. Existing repository-level benchmarks primarily focus on
single scenarios, such as code generation or bug fixing, without adequately
capturing the diversity and complexity of real-world software or project
engineering workflows. Furthermore, these benchmarks suffer from limited
controllability in question positioning and reliability issues in their
generated test cases. To address these limitations, we present CorePipe, a
fully automated pipeline that converts repositories into comprehensive test
cases, and introduce CoreCodeBench, a configurable multi-scenario
repository-level benchmark. To simulate real engineering scenarios, CorePipe
generates three types of atomic questions (Development, BugFix, and Test-Driven
Development) specifically targeting core code segments. These atomic questions
are further combined into three types of composite questions, with difficulty
levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides
a comprehensive and extensive repository-level benchmark to investigate the
applicability of LLMs in real-world engineering projects. Experiments with 16
LLMs across diverse scenarios reveal varying capabilities and offer
multi-dimensional insights into LLM performance in engineering contexts. The
code for CorePipe is available at
https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for
CoreCodeBench can be accessed at
https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.

</details>


### [6] [Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models](https://arxiv.org/abs/2507.05289)
*Igor Regis da Silva Simoes,Elaine Venson*

Main category: cs.SE

TL;DR: 论文探讨了使用大型语言模型（LLMs）标准化评估代码可读性，通过实验验证了LLMs对代码干预的敏感性和语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 代码可读性是代码质量的重要方面，但现有评估方法存在主观性和不一致性，LLMs可能提供标准化解决方案。

Method: 通过准实验研究，测试9种LLMs对三种代码干预（删除注释、替换标识符名称、重构消除代码异味）的反应，并与参考模型对比。

Result: LLMs对所有干预敏感，与参考分类器在原始和重构代码场景中一致性高，且表现出语义敏感性。响应存在一定变异性，但结果仍具统计显著性。

Conclusion: LLMs在评估代码语义质量方面具有潜力，尤其在标识符名称、注释与代码目的的连贯性上表现突出。

Abstract: Code readability is one of the main aspects of code quality, influenced by
various properties like identifier names, comments, code structure, and
adherence to standards. However, measuring this attribute poses challenges in
both industry and academia. While static analysis tools assess attributes such
as code smells and comment percentage, code reviews introduce an element of
subjectivity. This paper explores using Large Language Models (LLMs) to
evaluate code quality attributes related to its readability in a standardized,
reproducible, and consistent manner. We conducted a quasi-experiment study to
measure the effects of code changes on Large Language Model (LLM)s
interpretation regarding its readability quality attribute. Nine LLMs were
tested, undergoing three interventions: removing comments, replacing identifier
names with obscure names, and refactoring to remove code smells. Each
intervention involved 10 batch analyses per LLM, collecting data on response
variability. We compared the results with a known reference model and tool. The
results showed that all LLMs were sensitive to the interventions, with
agreement with the reference classifier being high for the original and
refactored code scenarios. The LLMs demonstrated a strong semantic sensitivity
that the reference model did not fully capture. A thematic analysis of the LLMs
reasoning confirmed their evaluations directly reflected the nature of each
intervention. The models also exhibited response variability, with 9.37% to
14.58% of executions showing a standard deviation greater than zero, indicating
response oscillation, though this did not always compromise the statistical
significance of the results. LLMs demonstrated potential for evaluating
semantic quality aspects, such as coherence between identifier names, comments,
and documentation with code purpose.

</details>


### [7] [zkSDK: Streamlining zero-knowledge proof development through automated trace-driven ZK-backend selection](https://arxiv.org/abs/2507.05294)
*William Law*

Main category: cs.SE

TL;DR: zkSDK是一个模块化框架，通过抽象后端复杂性简化ZK应用开发，使用Presto语言动态选择最优ZK后端。


<details>
  <summary>Details</summary>
Motivation: ZK开发者面临多种后端选择，学习曲线陡峭且体验碎片化，zkSDK旨在解决这一问题。

Method: zkSDK基于Presto语言分析程序计算负载，结合用户标准动态选择最优ZK后端。

Result: zkSDK能有效选择最适合的后端，提供无缝的开发体验。

Conclusion: zkSDK为ZK开发提供了统一且高效的解决方案。

Abstract: The rapid advancement of creating Zero-Knowledge (ZK) programs has led to the
development of numerous tools designed to support developers. Popular options
include being able to write in general-purpose programming languages like Rust
from Risc Zero. Other languages exist like Circom, Lib-snark, and Cairo.
However, developers entering the ZK space are faced with many different ZK
backends to choose from, leading to a steep learning curve and a fragmented
developer experience across different platforms. As a result, many developers
tend to select a single ZK backend and remain tied to it. This thesis
introduces zkSDK, a modular framework that streamlines ZK application
development by abstracting the backend complexities. At the core of zkSDK is
Presto, a custom Python-like programming language that enables the profiling
and analysis of a program to assess its computational workload intensity.
Combined with user-defined criteria, zkSDK employs a dynamic selection
algorithm to automatically choose the optimal ZK-proving backend. Through an
in-depth analysis and evaluation of real-world workloads, we demonstrate that
zkSDK effectively selects the best-suited backend from a set of supported ZK
backends, delivering a seamless and user-friendly development experience.

</details>


### [8] [ASSURE: Metamorphic Testing for AI-powered Browser Extensions](https://arxiv.org/abs/2507.05307)
*Xuanqi Gao,Juan Zhai,Shiqing Ma,Siyi Xie,Chao Shen*

Main category: cs.SE

TL;DR: ASSURE是一个专为AI浏览器扩展设计的自动化测试框架，解决了传统方法无法应对的非确定性行为和上下文敏感性等问题。


<details>
  <summary>Details</summary>
Motivation: AI浏览器扩展的测试和可靠性保障面临前所未有的挑战，传统方法和现有LLM测试方法均无法有效应对。

Method: ASSURE包含模块化测试用例生成引擎、自动化执行框架和可配置验证管道，系统评估行为一致性和安全性。

Result: 在六个广泛使用的AI浏览器扩展中，ASSURE识别了531个问题，测试吞吐量比手动方法提高了6.4倍。

Conclusion: ASSURE为AI浏览器扩展的测试提供了高效、全面的解决方案，适合集成到开发流程中。

Abstract: The integration of Large Language Models (LLMs) into browser extensions has
revolutionized web browsing, enabling sophisticated functionalities like
content summarization, intelligent translation, and context-aware writing
assistance. However, these AI-powered extensions introduce unprecedented
challenges in testing and reliability assurance. Traditional browser extension
testing approaches fail to address the non-deterministic behavior,
context-sensitivity, and complex web environment integration inherent to
LLM-powered extensions. Similarly, existing LLM testing methodologies operate
in isolation from browser-specific contexts, creating a critical gap in
effective evaluation frameworks. To bridge this gap, we present ASSURE, a
modular automated testing framework specifically designed for AI-powered
browser extensions. ASSURE comprises three principal components: (1) a modular
test case generation engine that supports plugin-based extension of testing
scenarios, (2) an automated execution framework that orchestrates the complex
interactions between web content, extension processing, and AI model behavior,
and (3) a configurable validation pipeline that systematically evaluates
behavioral consistency and security invariants rather than relying on exact
output matching. Our evaluation across six widely-used AI browser extensions
demonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning
security vulnerabilities, metamorphic relation violations, and content
alignment problems. ASSURE achieves 6.4x improved testing throughput compared
to manual approaches, detecting critical security vulnerabilities within 12.4
minutes on average. This efficiency makes ASSURE practical for integration into
development pipelines, offering a comprehensive solution to the unique
challenges of testing AI-powered browser extensions.

</details>


### [9] [OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models](https://arxiv.org/abs/2507.05316)
*Koren Lazar,Matan Vetzler,Kiran Kate,Jason Tsay,David Boaz Himanshu Gupta,Avraham Shinnar,Rohith D Vallam,David Amid Esther Goldbraich,Guy Uziel,Jim Laredo,Ateret Anaby Tavor*

Main category: cs.SE

TL;DR: OASBuilder是一个框架，将非结构化的API文档转换为机器可读的OpenAPI规范，结合了大型语言模型和基于规则的算法。


<details>
  <summary>Details</summary>
Motivation: 在线API文档多为非结构化HTML，需要手动转换为结构化格式，耗时费力。

Method: 通过结合大型语言模型和基于规则的算法，利用文档网页结构知识，构建转换管道。

Result: 实验表明，OASBuilder能处理数百种API，生成有效的OpenAPI规范，涵盖大部分原始文档信息。

Conclusion: OASBuilder在企业环境中成功应用，节省大量手动时间，使复杂API更易于LLM使用。

Abstract: AI agents and business automation tools interacting with external web
services require standardized, machine-readable information about their APIs in
the form of API specifications. However, the information about APIs available
online is often presented as unstructured, free-form HTML documentation,
requiring external users to spend significant time manually converting it into
a structured format. To address this, we introduce OASBuilder, a novel
framework that transforms long and diverse API documentation pages into
consistent, machine-readable API specifications. This is achieved through a
carefully crafted pipeline that integrates large language models and rule-based
algorithms which are guided by domain knowledge of the structure of
documentation webpages. Our experiments demonstrate that OASBuilder generalizes
well across hundreds of APIs, and produces valid OpenAPI specifications that
encapsulate most of the information from the original documentation. OASBuilder
has been successfully implemented in an enterprise environment, saving
thousands of hours of manual effort and making hundreds of complex enterprise
APIs accessible as tools for LLMs.

</details>


### [10] [Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners' Perspectives](https://arxiv.org/abs/2507.05325)
*Lidiany Cerqueira,João Pedro Bastos,Danilo Neves,Glauco Carneiro,Rodrigo Spínola,Sávio Freire,José Amancio Macedo Santos,Manoel Mendonça*

Main category: cs.SE

TL;DR: 研究探讨了软件工程中的共情，定义了其含义，识别了障碍，提出了克服方法，并构建了一个概念框架。


<details>
  <summary>Details</summary>
Motivation: 共情是软件工程中未被充分研究的关键社交技能，对沟通和协作至关重要。

Method: 通过分析55篇DEV和Medium的文章，并结合专家调查进行定性内容分析。

Result: 提出了共情的定义，识别了障碍（如毒性文化和过度技术导向），提出了团队共情实践，并展示了积极效果（如改善协作和减少压力）。

Conclusion: 框架被认为清晰且有价值，未来将进一步探索共情在软件工程中的广泛影响。

Abstract: Context. Empathy, a key social skill, is essential for communication and
collaboration in SE but remains an under-researched topic. Aims. This study
investigates empathy in SE from practitioners' perspectives, aiming to
characterize its meaning, identify barriers, discuss practices to overcome
them, and explore its effects. Method. A qualitative content analysis was
conducted on 55 web articles from DEV and Medium, two communities widely used
by practitioners. To strengthen our findings, we conducted a follow-up survey
with empathy experts. Results. The study proposes a definition of empathy in
SE, identifies barriers such as toxic culture and excessive technical focus,
practices to foster empathy in teams, and outcomes, including improved
collaboration, communication, and reduced anxiety, frustration, and stress.
These findings are synthesized into a conceptual framework. Conclusion. Survey
results indicate the framework is clear, valuable, and raises empathy
awareness, with suggestions for improvements and integration into training.
This study paves the way for improving team dynamics by addressing barriers and
offering strategies to cultivate empathy. Future work will explore empathy's
broader implications in SE practice.

</details>


### [11] [Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs](https://arxiv.org/abs/2507.05504)
*Alex Kleijwegt,Sinem Getir Yaman,Radu Calinescu*

Main category: cs.SE

TL;DR: SLEEC-LLM利用大语言模型（LLMs）为SLEEC规则不一致性提供自然语言解释，提升规范需求获取和一致性分析的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 规范需求（SLEEC）的识别和管理复杂且易出错，现有方法的技术性结果对非技术用户不友好，导致效率低下。

Method: 引入SLEEC-LLM工具，利用LLMs将模型检查的反例转化为自然语言解释。

Result: 通过两个真实案例研究验证了SLEEC-LLM的有效性。

Conclusion: SLEEC-LLM显著提升了规范需求获取和一致性分析的效率和可解释性。

Abstract: Normative requirements specify social, legal, ethical, empathetic, and
cultural (SLEEC) norms that must be observed by a system. To support the
identification of SLEEC requirements, numerous standards and regulations have
been developed. These requirements are typically defined by stakeholders in the
non-technical system with diverse expertise (e.g., ethicists, lawyers, social
scientists). Hence, ensuring their consistency and managing the requirement
elicitation process are complex and error-prone tasks. Recent research has
addressed this challenge using domain-specific languages to specify normative
requirements as rules, whose consistency can then be analyzed with formal
methods. Nevertheless, these approaches often present the results from formal
verification tools in a way that is inaccessible to non-technical users. This
hinders understanding and makes the iterative process of eliciting and
validating these requirements inefficient in terms of both time and effort. To
address this problem, we introduce SLEEC-LLM, a tool that uses large language
models (LLMs) to provide natural-language interpretations for model-checking
counterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves
the efficiency and explainability of normative requirements elicitation and
consistency analysis. To demonstrate its effectiveness, we summarise its use in
two real-world case studies involving non-technical stakeholders.

</details>


### [12] [Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models](https://arxiv.org/abs/2507.05565)
*Sangwon Hyun,Shaukat Ali,M. Ali Babar*

Main category: cs.SE

TL;DR: 本文提出了一种基于搜索的方法，优化大型语言模型（LLM）的稳健性测试中的变形关系（MR）选择，以最大化故障检测并最小化执行成本。


<details>
  <summary>Details</summary>
Motivation: 现有研究在LLM稳健性测试中主要关注自动生成测试用例（MRs），但缺乏对MR选择的优化，且测试空间有限。本文旨在解决这一问题。

Method: 提出了一种搜索过程，并实现了四种搜索算法（Single-GA、NSGA-II、SPEA2、MOEA/D）来解决MR选择问题，同时考虑了组合扰动。

Result: 实验表明，MOEA/D算法在优化LLM稳健性测试的MR空间方面表现最佳，并发现了具有显著混淆能力的“银弹MRs”。

Conclusion: 本研究为LLM稳健性测试中的优化问题提供了搜索解决方案，并扩展了测试空间。

Abstract: Assessing the trustworthiness of Large Language Models (LLMs), such as
robustness, has garnered significant attention. Recently, metamorphic testing
that defines Metamorphic Relations (MRs) has been widely applied to evaluate
the robustness of LLM executions. However, the MR-based robustness testing
still requires a scalable number of MRs, thereby necessitating the optimization
of selecting MRs. Most extant LLM testing studies are limited to automatically
generating test cases (i.e., MRs) to enhance failure detection. Additionally,
most studies only considered a limited test space of single perturbation MRs in
their evaluation of LLMs. In contrast, our paper proposes a search-based
approach for optimizing the MR groups to maximize failure detection and
minimize the LLM execution cost. Moreover, our approach covers the
combinatorial perturbations in MRs, facilitating the expansion of test space in
the robustness assessment. We have developed a search process and implemented
four search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel
encoding to solve the MR selection problem in the LLM robustness testing. We
conducted comparative experiments on the four search algorithms along with a
random search, using two major LLMs with primary Text-to-Text tasks. Our
statistical and empirical investigation revealed two key findings: (1) the
MOEA/D algorithm performed the best in optimizing the MR space for LLM
robustness testing, and (2) we identified silver bullet MRs for the LLM
robustness testing, which demonstrated dominant capabilities in confusing LLMs
across different Text-to-Text tasks. In LLM robustness assessment, our research
sheds light on the fundamental problem for optimized testing and provides
insights into search-based solutions.

</details>


### [13] [TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems](https://arxiv.org/abs/2507.05932)
*You Lu,Dingji Wang,Kaifeng Huang,Bihuan Chen,Xin Peng*

Main category: cs.SE

TL;DR: 论文提出TigAug方法，通过自动增强标记的交通灯图像来测试和改进自动驾驶系统中的交通灯检测模型。


<details>
  <summary>Details</summary>
Motivation: 当前交通灯检测模型的测试依赖人工收集和标注数据，效率低且难以覆盖多样化的驾驶环境。

Method: 构建了两类蜕变关系和三类图像变换，基于天气、相机和交通灯特性生成增强图像，用于测试和模型改进。

Result: 实验证明TigAug能有效测试模型、高效合成图像，且生成图像具有可接受的自然度。

Conclusion: TigAug为自动驾驶系统中交通灯检测模型的测试和性能提升提供了自动化解决方案。

Abstract: Autonomous vehicle technology has been developed in the last decades with
recent advances in sensing and computing technology. There is an urgent need to
ensure the reliability and robustness of autonomous driving systems (ADSs).
Despite the recent achievements in testing various ADS modules, little
attention has been paid on the automated testing of traffic light detection
models in ADSs. A common practice is to manually collect and label traffic
light data. However, it is labor-intensive, and even impossible to collect
diverse data under different driving environments.
  To address these problems, we propose and implement TigAug to automatically
augment labeled traffic light images for testing traffic light detection models
in ADSs. We construct two families of metamorphic relations and three families
of transformations based on a systematic understanding of weather environments,
camera properties, and traffic light properties. We use augmented images to
detect erroneous behaviors of traffic light detection models by
transformation-specific metamorphic relations, and to improve the performance
of traffic light detection models by retraining. Large-scale experiments with
four state-of-the-art traffic light detection models and two traffic light
datasets have demonstrated that i) TigAug is effective in testing traffic light
detection models, ii) TigAug is efficient in synthesizing traffic light images,
and iii) TigAug generates traffic light images with acceptable naturalness.

</details>


### [14] [Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models](https://arxiv.org/abs/2507.05981)
*Marc Oriol,Quim Motger,Jordi Marco,Xavier Franch*

Main category: cs.SE

TL;DR: 论文探讨了通过多智能体辩论（MAD）策略提升大型语言模型（LLM）在需求工程（RE）任务中的准确性，提出了一种初步框架并验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将LLM视为孤立黑箱，缺乏迭代优化和协作，限制了鲁棒性和适应性。受人类辩论启发，研究MAD策略是否能提升RE性能。

Method: 系统研究现有MAD策略，提出分类法，并初步实现MAD框架用于RE分类任务。

Result: 研究总结了MAD策略的核心特征，初步评估表明MAD在RE分类中具有可行性。

Conclusion: MAD为提升LLM在RE任务中的准确性提供了新思路，为未来研究奠定了基础。

Abstract: Context: Large Language Model (LLM) agents are becoming widely used for
various Requirements Engineering (RE) tasks. Research on improving their
accuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval
augmented generation. However, these methods often treat models as isolated
black boxes - relying on single-pass outputs without iterative refinement or
collaboration, limiting robustness and adaptability. Objective: We propose
that, just as human debates enhance accuracy and reduce bias in RE tasks by
incorporating diverse perspectives, different LLM agents debating and
collaborating may achieve similar improvements. Our goal is to investigate
whether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:
We conducted a systematic study of existing MAD strategies across various
domains to identify their key characteristics. To assess their applicability in
RE, we implemented and tested a preliminary MAD-based framework for RE
classification. Results: Our study identified and categorized several MAD
strategies, leading to a taxonomy outlining their core attributes. Our
preliminary evaluation demonstrated the feasibility of applying MAD to RE
classification. Conclusions: MAD presents a promising approach for improving
LLM accuracy in RE tasks. This study provides a foundational understanding of
MAD strategies, offering insights for future research and refinements in RE
applications.

</details>


### [15] [PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning](https://arxiv.org/abs/2507.05995)
*Pengzhou Chen,Tao Chen*

Main category: cs.SE

TL;DR: PromiseTune通过因果推断纯化的规则指导配置调优，有效平衡探索与利用，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统的高可配置性使配置调优成为确保性能的关键步骤，但现有调优方法因难以平衡探索与利用而效果不佳。

Method: 提出PromiseTune，通过因果推断纯化规则，近似反映有潜力的配置区域，并以此指导调优。

Result: 在12个系统和不同预算下，PromiseTune显著优于11种现有方法，排名优势达42%。

Conclusion: PromiseTune不仅性能优越，还能提供更丰富的解释性信息，揭示系统隐藏特性。

Abstract: The high configurability of modern software systems has made configuration
tuning a crucial step for assuring system performance, e.g., latency or
throughput. However, given the expensive measurements, large configuration
space, and rugged configuration landscape, existing tuners suffer
ineffectiveness due to the difficult balance of budget utilization between
exploring uncertain regions (for escaping from local optima) and exploiting
guidance of known good configurations (for fast convergence). The root cause is
that we lack knowledge of where the promising regions lay, which also causes
challenges in the explainability of the results.
  In this paper, we propose PromiseTune that tunes configuration guided by
causally purified rules. PromiseTune is unique in the sense that we learn
rules, which reflect certain regions in the configuration landscape, and purify
them with causal inference. The remaining rules serve as approximated
reflections of the promising regions, bounding the tuning to emphasize these
places in the landscape. This, as we demonstrate, can effectively mitigate the
impact of the exploration and exploitation trade-off. Those purified regions
can then be paired with the measured configurations to provide spatial
explainability at the landscape level. Comparing with 11 state-of-the-art
tuners on 12 systems and varying budgets, we show that PromiseTune performs
significantly better than the others with $42\%$ superior rank to the overall
second best while providing richer information to explain the hidden system
characteristics.

</details>


### [16] [Model Cards Revisited: Bridging the Gap Between Theory and Practice for Ethical AI Requirements](https://arxiv.org/abs/2507.06014)
*Tim Puhlfürß,Julia Butzke,Walid Maalej*

Main category: cs.SE

TL;DR: 论文分析了AI模型文档（模型卡）的现状，发现开发者主要关注模型能力和可靠性，而忽略了其他伦理要求（如可解释性、用户自主性和公平性）。通过分析26份伦理指南、3个文档框架和10个实际模型卡，提出了包含43个伦理要求的分类法，为改进模型卡框架提供了基础。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型文档实践不足，未能满足伦理要求，需要系统性分析以填补这一差距。

Method: 对26份伦理指南、3个AI文档框架、3项定量研究和10个实际模型卡进行主题分析，提取并分类伦理要求。

Result: 提出了包含43个伦理要求的分类法，发现开发者主要关注模型能力和可靠性，忽视了其他伦理方面。

Conclusion: 需要改进模型卡框架以全面满足伦理要求，提出的分类法为此提供了基础。

Abstract: Model cards are the primary documentation framework for developers of
artificial intelligence (AI) models to communicate critical information to
their users. Those users are often developers themselves looking for relevant
documentation to ensure that their AI systems comply with the ethical
requirements of existing laws, guidelines, and standards. Recent studies
indicate inadequate model documentation practices, suggesting a gap between AI
requirements and current practices in model documentation. To understand this
gap and provide actionable guidance to bridge it, we conducted a thematic
analysis of 26 guidelines on ethics and AI, three AI documentation frameworks,
three quantitative studies of model cards, and ten actual model cards. We
identified a total of 43 ethical requirements relevant to model documentation
and organized them into a taxonomy featuring four themes and twelve sub-themes
representing ethical principles. Our findings indicate that model developers
predominantly emphasize model capabilities and reliability in the documentation
while overlooking other ethical aspects, such as explainability, user autonomy,
and fairness. This underscores the need for enhanced support in documenting
ethical AI considerations. Our taxonomy serves as a foundation for a revised
model card framework that holistically addresses ethical AI requirements.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [Layered, Overlapping, and Inconsistent: A Large-Scale Analysis of the Multiple Privacy Policies and Controls of U.S. Banks](https://arxiv.org/abs/2507.05415)
*Lu Xian,Van Tran,Lauren Lee,Meera Kumar,Yichen Zhang,Florian Schaub*

Main category: cs.CR

TL;DR: 研究发现美国银行在隐私政策实施中存在不一致性，可能削弱透明度目标。


<details>
  <summary>Details</summary>
Motivation: 分析美国银行如何执行隐私政策，尤其是涉及第三方数据共享的隐私侵入性实践。

Method: 收集并分析2067家美国最大银行的隐私政策，关注披露和控制的一致性。

Result: 发现政策间存在频繁不一致，如GLBA通知与实际披露不符，可能引发消费者困惑。

Conclusion: 质疑当前政策要求是否有效，提出改革和协调联邦与州法律的隐私政策建议。

Abstract: Privacy policies are often complex. An exception is the two-page standardized
notice that U.S. financial institutions must provide under the
Gramm-Leach-Bliley Act (GLBA). However, banks now operate websites, mobile
apps, and other services that involve complex data sharing practices that
require additional privacy notices and do-not-sell opt-outs. We conducted a
large-scale analysis of how U.S. banks implement privacy policies and controls
in response to GLBA; other federal privacy policy requirements; and the
California Consumer Privacy Act (CCPA), a key example for U.S. state privacy
laws. We focused on the disclosure and control of a set of especially
privacy-invasive practices: third-party data sharing for marketing-related
purposes. We collected privacy policies for the 2,067 largest U.S. banks,
45.3\% of which provided multiple policies. Across disclosures and controls
within the \textit{same} bank, we identified frequent, concerning
inconsistencies -- such as banks indicating in GLBA notices that they do not
share with third parties but disclosing sharing elsewhere, or using third-party
marketing/advertising cookies without disclosure. This multiplicity of
policies, with the inconsistencies it causes, may create consumer confusion and
undermine the transparency goals of the very laws that require them. Our
findings call into question whether current policy requirements, such as the
GLBA notice, are achieving their intended goals in today's online banking
landscape. We discuss potential avenues for reforming and harmonizing privacy
policies and control requirements across federal and state laws.

</details>


### [18] [FrameShift: Learning to Resize Fuzzer Inputs Without Breaking Them](https://arxiv.org/abs/2507.05421)
*Harrison Green,Claire Le Goues,Fraser Brown*

Main category: cs.CR

TL;DR: FrameShift是一种轻量级技术，通过检测和使用关系字段在突变中保留输入结构，避免破坏性突变，提升模糊测试性能。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试工具因缺乏输入格式知识，容易生成破坏性突变，导致无效输入。

Method: 提出FrameShift技术，利用关系字段保持输入结构，无需额外工具。

Result: 在AFL++和LibAFL中实现，12+ CPU年测试显示覆盖率提升超50%，适用于多种语言。

Conclusion: FrameShift简单高效，显著提升模糊测试性能，适用于多种格式和语言。

Abstract: Coverage-guided fuzzers are powerful automated bug-finding tools. They mutate
program inputs, observe coverage, and save any input that hits an unexplored
path for future mutation. Unfortunately, without knowledge of input
formats--for example, the relationship between formats' data fields and
sizes--fuzzers are prone to generate destructive frameshift mutations. These
time-wasting mutations yield malformed inputs that are rejected by the target
program. To avoid such breaking mutations, this paper proposes a novel,
lightweight technique that preserves the structure of inputs during mutation by
detecting and using relation fields.
  Our technique, FrameShift, is simple, fast, and does not require additional
instrumentation beyond standard coverage feedback. We implement our technique
in two state-of-the-art fuzzers, AFL++ and LibAFL, and perform a 12+ CPU-year
fuzzer evaluation, finding that FrameShift improves the performance of the
fuzzer in each configuration, sometimes increasing coverage by more than 50%.
Furthermore, through a series of case studies, we show that our technique is
versatile enough to find important structural relationships in a variety of
formats, even generalizing beyond C/C++ targets to both Rust and Python.

</details>


### [19] [A Systematization of Security Vulnerabilities in Computer Use Agents](https://arxiv.org/abs/2507.05445)
*Daniel Jones,Giorgio Severi,Martin Pouliot,Gary Lopez,Joris de Gruyter,Santiago Zanella-Beguelin,Justin Song,Blake Bullwinkel,Pamela Cortez,Amanda Minnich*

Main category: cs.CR

TL;DR: 本文系统分析了计算机使用代理（CUAs）的安全风险，揭示了七类独特威胁，并深入研究了三种具体攻击场景，提出了安全评估框架和设计原则。


<details>
  <summary>Details</summary>
Motivation: CUAs的广泛应用引入了传统威胁模型未涵盖的新攻击面和信任边界，但其安全边界尚未被充分理解。

Method: 通过对抗条件下的系统威胁分析和实际CUA测试，识别了七类风险，并深入分析了三种具体攻击场景。

Result: 揭示了当前CUA实现中的架构缺陷，包括输入来源跟踪不足、接口-动作绑定薄弱以及代理内存和委托控制不足。

Conclusion: 提出了针对CUAs的安全评估框架和设计原则，以支持其在对抗性和高风险环境中的安全部署。

Abstract: Computer Use Agents (CUAs), autonomous systems that interact with software
interfaces via browsers or virtual machines, are rapidly being deployed in
consumer and enterprise environments. These agents introduce novel attack
surfaces and trust boundaries that are not captured by traditional threat
models. Despite their growing capabilities, the security boundaries of CUAs
remain poorly understood. In this paper, we conduct a systematic threat
analysis and testing of real-world CUAs under adversarial conditions. We
identify seven classes of risks unique to the CUA paradigm, and analyze three
concrete exploit scenarios in depth: (1) clickjacking via visual overlays that
mislead interface-level reasoning, (2) indirect prompt injection that enables
Remote Code Execution (RCE) through chained tool use, and (3) CoT exposure
attacks that manipulate implicit interface framing to hijack multi-step
reasoning. These case studies reveal deeper architectural flaws across current
CUA implementations. Namely, a lack of input provenance tracking, weak
interface-action binding, and insufficient control over agent memory and
delegation. We conclude by proposing a CUA-specific security evaluation
framework and design principles for safe deployment in adversarial and
high-stakes settings.

</details>


### [20] [Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice](https://arxiv.org/abs/2507.05512)
*Gehao Zhang,Eugene Bagdasarian,Juan Zhai,Shiqing Ma*

Main category: cs.CR

TL;DR: 该论文探讨了N-gram水印方案在代码中的鲁棒性不足问题，并通过理论和实验证明其在代码混淆攻击下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 区分AI生成代码与人工编写代码对作者归属、内容追踪和滥用检测至关重要，但现有水印方案在代码混淆攻击下的鲁棒性未充分评估。

Method: 论文通过形式化建模代码混淆，证明N-gram水印在满足分布一致性假设时无法保持鲁棒性，并在实验中验证了这一点。

Result: 实验表明，所有水印检测器在混淆代码上的检测能力接近随机（AUROC≈0.5），且存在混淆器能使检测AUROC不超过0.6。

Conclusion: N-gram水印方案在代码混淆攻击下表现脆弱，论文提出了可能的鲁棒水印改进方向。

Abstract: Distinguishing AI-generated code from human-written code is becoming crucial
for tasks such as authorship attribution, content tracking, and misuse
detection. Based on this, N-gram-based watermarking schemes have emerged as
prominent, which inject secret watermarks to be detected during the generation.
  However, their robustness in code content remains insufficiently evaluated.
Most claims rely solely on defenses against simple code transformations or code
optimizations as a simulation of attack, creating a questionable sense of
robustness. In contrast, more sophisticated schemes already exist in the
software engineering world, e.g., code obfuscation, which significantly alters
code while preserving functionality. Although obfuscation is commonly used to
protect intellectual property or evade software scanners, the robustness of
code watermarking techniques against such transformations remains largely
unexplored.
  In this work, we formally model the code obfuscation and prove the
impossibility of N-gram-based watermarking's robustness with only one intuitive
and experimentally verified assumption, distribution consistency, satisfied.
Given the original false positive rate of the watermarking detection, the ratio
that the detector failed on the watermarked code after obfuscation will
increase to 1 - fpr.
  The experiments have been performed on three SOTA watermarking schemes, two
LLMs, two programming languages, four code benchmarks, and four obfuscators.
Among them, all watermarking detectors show coin-flipping detection abilities
on obfuscated codes (AUROC tightly surrounds 0.5). Among all models,
watermarking schemes, and datasets, both programming languages own obfuscators
that can achieve attack effects with no detection AUROC higher than 0.6 after
the attack. Based on the theoretical and practical observations, we also
proposed a potential path of robust code watermarking.

</details>


### [21] [PROTEAN: Federated Intrusion Detection in Non-IID Environments through Prototype-Based Knowledge Sharing](https://arxiv.org/abs/2507.05524)
*Sara Chennoufi,Yufei Han,Gregory Blanc,Emiliano De Cristofaro,Christophe Kiennert*

Main category: cs.CR

TL;DR: PROTEAN是一个基于原型学习的框架，旨在解决联邦学习中数据异构性问题，提升入侵检测的准确性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 分布式网络中，参与者面临多样且快速演变的网络攻击，联邦学习（FL）因其隐私保护特性成为有前景的解决方案，但数据异构性限制了其效果。

Method: PROTEAN通过交换不同攻击类型的类原型，促进直接知识共享，解决了非独立同分布（non-IID）攻击分布的问题。

Result: 在IIoT和5G数据集上的实验表明，PROTEAN在效用和隐私方面表现优异，能有效应对数据异构性并提升入侵检测系统的理解能力。

Conclusion: PROTEAN为联邦入侵检测系统提供了一种高效且隐私保护的解决方案，显著提升了攻击检测能力。

Abstract: In distributed networks, participants often face diverse and fast-evolving
cyberattacks. This makes techniques based on Federated Learning (FL) a
promising mitigation strategy. By only exchanging model updates, FL
participants can collaboratively build detection models without revealing
sensitive information, e.g., network structures or security postures. However,
the effectiveness of FL solutions is often hindered by significant data
heterogeneity, as attack patterns often differ drastically across organizations
due to varying security policies. To address these challenges, we introduce
PROTEAN, a Prototype Learning-based framework geared to facilitate
collaborative and privacy-preserving intrusion detection. PROTEAN enables
accurate detection in environments with highly non-IID attack distributions and
promotes direct knowledge sharing by exchanging class prototypes of different
attack types among participants. This allows organizations to better understand
attack techniques not present in their data collections. We instantiate PROTEAN
on two cyber intrusion datasets collected from IIoT and 5G-connected
participants and evaluate its performance in terms of utility and privacy,
demonstrating its effectiveness in addressing data heterogeneity while
improving cyber attack understanding in federated intrusion detection systems
(IDSs).

</details>


### [22] [AI Agent Smart Contract Exploit Generation](https://arxiv.org/abs/2507.05558)
*Arthur Gervais,Liyi Zhou*

Main category: cs.CR

TL;DR: A1是一个基于LLM的端到端漏洞利用生成系统，通过六种领域工具实现自主漏洞发现，成功率为62.96%，并验证了其经济效益和不对称性。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用LLM生成端到端的漏洞利用方案，并验证其在真实场景中的效果和经济可行性。

Method: A1系统通过六种工具支持LLM自主发现漏洞，包括理解合约行为、生成策略、测试和优化。

Result: 在36个真实漏洞合约中，A1成功率为62.96%，提取总金额达933万美元，且攻击者比防御者更具经济优势。

Conclusion: A1展示了LLM在漏洞利用中的高效性，但揭示了攻击者与防御者之间的经济不对称性，引发了对AI代理偏向性的担忧。

Abstract: We present A1, an agentic execution driven system that transforms any LLM
into an end-to-end exploit generator. A1 has no hand-crafted heuristics and
provides the agent with six domain-specific tools that enable autonomous
vulnerability discovery. The agent can flexibly leverage these tools to
understand smart contract behavior, generate exploit strategies, test them on
blockchain states, and refine approaches based on execution feedback. All
outputs are concretely validated to eliminate false positives.
  The evaluation across 36 real-world vulnerable contracts on Ethereum and
Binance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the
VERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional
vulnerable contracts, with 5 cases occurring after the strongest model's
training cutoff date. Across all 26 successful cases, A1 extracts up to 8.59
million USD per case and 9.33 million USD total. Through 432 experiments across
six LLMs, we analyze iteration-wise performance showing diminishing returns
with average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations
2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo
analysis of 19 historical attacks shows success probabilities of 85.9%-88.8%
without detection delays.
  We investigate whether an attacker or a defender benefits most from deploying
A1 as a continuous on-chain scanning system. Our model shows that OpenAI's
o3-pro maintains profitability up to a 30.0 days scanning delay at 0.100%
vulnerability incidence rates, while faster models require >=1.000% rates to
break-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability
rates, attackers achieve an on-chain scanning profitability at a $6000 exploit
value, while defenders require $60000, raising fundamental questions about
whether AI agents inevitably favor exploitation over defense.

</details>


### [23] [iThermTroj: Exploiting Intermittent Thermal Trojans in Multi-Processor System-on-Chips](https://arxiv.org/abs/2507.05576)
*Mehdi Elahi,Mohamed R. Elshamy,Abdel-Hameed Badawy,Ahmad Patooghy*

Main category: cs.CR

TL;DR: 论文提出了一种间歇性热特洛伊木马攻击（iThermTroj），并设计了基于微型机器学习分类器的运行时异常检测方法，显著提高了攻击检测率和保护分辨率。


<details>
  <summary>Details</summary>
Motivation: 热特洛伊木马攻击对SoC的安全性和可靠性构成严重威胁，尤其是当攻击更隐蔽且间歇性操作时，现有检测机制难以应对。

Method: 通过深入分析特洛伊木马激活和持续时间场景，提出了一组微型机器学习分类器用于运行时异常检测。

Result: 在攻击操纵80%、60%和40%热数据时，检测率分别提高了29.4%、17.2%和14.3%，保护分辨率提升至0.8摄氏度。

Conclusion: 该方法有效提高了对间歇性热特洛伊木马攻击的检测能力，为SoC安全提供了更可靠的保护。

Abstract: Thermal Trojan attacks present a pressing concern for the security and
reliability of System-on-Chips (SoCs), especially in mobile applications. The
situation becomes more complicated when such attacks are more evasive and
operate sporadically to stay hidden from detection mechanisms. In this paper,
we introduce Intermittent Thermal Trojans (iThermTroj) that exploit the chips'
thermal information in a random time-triggered manner. According to our
experiments, iThermTroj attack can easily bypass available threshold-based
thermal Trojan detection solutions. We investigate SoC vulnerabilities to
variations of iThermTroj through an in-depth analysis of Trojan activation and
duration scenarios. We also propose a set of tiny Machine Learning classifiers
for run-time anomaly detection to protect SoCs against such intermittent
thermal Trojan attacks. Compared to existing methods, our approach improves the
attack detection rate by 29.4\%, 17.2\%, and 14.3\% in scenarios where
iThermTroj manipulates up to 80\%, 60\%, and 40\% of SoC's thermal data,
respectively. Additionally, our method increases the full protection resolution
to 0.8 degrees Celsius, meaning that any temperature manipulations exceeding
$\pm 0.8$ degrees will be detected with 100\% accuracy.

</details>


### [24] [DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective](https://arxiv.org/abs/2507.05622)
*Shuo Shao,Yiming Li,Mengren Zheng,Zhiyang Hu,Yukun Chen,Boheng Li,Yu He,Junfeng Guo,Tianwei Zhang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: 论文提出了一个对抗视角下的数据集审计方法评估框架，揭示了现有方法在对抗攻击下的脆弱性，并提出了新的攻击策略和基准测试DATABench。


<details>
  <summary>Details</summary>
Motivation: 解决数据集审计方法在对抗攻击下的鲁棒性问题，填补现有研究的空白。

Method: 提出分类法区分内部特征（IF）和外部特征（EF），并设计两种攻击类型：逃避攻击和伪造攻击。进一步提出系统性攻击策略，并构建基准测试DATABench。

Result: 评估显示现有审计方法在对抗环境下均不够鲁棒或具有区分性。

Conclusion: 亟需开发更安全可靠的数据集审计方法以抵御复杂对抗攻击。

Abstract: The widespread application of Deep Learning across diverse domains hinges
critically on the quality and composition of training datasets. However, the
common lack of disclosure regarding their usage raises significant privacy and
copyright concerns. Dataset auditing techniques, which aim to determine if a
specific dataset was used to train a given suspicious model, provide promising
solutions to addressing these transparency gaps. While prior work has developed
various auditing methods, their resilience against dedicated adversarial
attacks remains largely unexplored. To bridge the gap, this paper initiates a
comprehensive study evaluating dataset auditing from an adversarial
perspective. We start with introducing a novel taxonomy, classifying existing
methods based on their reliance on internal features (IF) (inherent to the
data) versus external features (EF) (artificially introduced for auditing).
Subsequently, we formulate two primary attack types: evasion attacks, designed
to conceal the use of a dataset, and forgery attacks, intending to falsely
implicate an unused dataset. Building on the understanding of existing methods
and attack objectives, we further propose systematic attack strategies:
decoupling, removal, and detection for evasion; adversarial example-based
methods for forgery. These formulations and strategies lead to our new
benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9
representative auditing methods. Extensive evaluations using DATABench reveal
that none of the evaluated auditing methods are sufficiently robust or
distinctive under adversarial settings. These findings underscore the urgent
need for developing a more secure and reliable dataset auditing method capable
of withstanding sophisticated adversarial manipulation. Code is available at
https://github.com/shaoshuo-ss/DATABench.

</details>


### [25] [How Not to Detect Prompt Injections with an LLM](https://arxiv.org/abs/2507.05630)
*Sarthak Choudhary,Divyam Anshumaan,Nils Palumbo,Somesh Jha*

Main category: cs.CR

TL;DR: 论文揭示了已知答案检测（KAD）防御框架的结构性漏洞，并提出了一种名为DataFlip的自适应攻击方法，能够高效绕过KAD防御。


<details>
  <summary>Details</summary>
Motivation: LLM应用易受提示注入攻击，现有KAD防御框架虽表现优异，但存在设计缺陷，需深入分析其弱点。

Method: 通过形式化分析KAD框架，设计DataFlip攻击方法，利用其结构性漏洞绕过防御。

Result: DataFlip攻击成功率达88%，检测率低至1.5%，无需白盒访问或优化过程。

Conclusion: KAD防御框架存在根本性安全漏洞，需重新设计防御机制以应对此类攻击。

Abstract: LLM-integrated applications and agents are vulnerable to prompt injection
attacks, in which adversaries embed malicious instructions within seemingly
benign user inputs to manipulate the LLM's intended behavior. Recent defenses
based on $\textit{known-answer detection}$ (KAD) have achieved near-perfect
performance by using an LLM to classify inputs as clean or contaminated. In
this work, we formally characterize the KAD framework and uncover a structural
vulnerability in its design that invalidates its core security premise. We
design a methodical adaptive attack, $\textit{DataFlip}$, to exploit this
fundamental weakness. It consistently evades KAD defenses with detection rates
as low as $1.5\%$ while reliably inducing malicious behavior with success rates
of up to $88\%$, without needing white-box access to the LLM or any
optimization procedures.

</details>


### [26] [DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning](https://arxiv.org/abs/2507.05649)
*Kaixiang Zhao,Joseph Yousry Attalla,Qian Lou,Yushun Dong*

Main category: cs.CR

TL;DR: DESIGN框架通过服务器端输入图剪枝和自适应多项式激活方案，显著提升了加密GNN推理的效率，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有FHE GNN方法效率低下，忽视了输入数据冗余和统一计算策略的问题。

Method: DESIGN采用分层优化策略，包括基于加密度统计的节点重要性评分、同态分区生成多级重要性掩码，以及自适应多项式激活方案。

Result: 实验表明，DESIGN显著加速了FHE GNN推理，同时保持了竞争性的模型准确性。

Conclusion: DESIGN为安全图分析提供了一个高效的解决方案。

Abstract: Graph Neural Networks (GNNs) have achieved state-of-the-art performance in
various graph-based learning tasks. However, enabling privacy-preserving GNNs
in encrypted domains, such as under Fully Homomorphic Encryption (FHE),
typically incurs substantial computational overhead, rendering real-time and
privacy-preserving inference impractical. In this work, we propose DESIGN
(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel
framework for efficient encrypted GNN inference. DESIGN tackles the critical
efficiency limitations of existing FHE GNN approaches, which often overlook
input data redundancy and apply uniform computational strategies. Our framework
achieves significant performance gains through a hierarchical optimization
strategy executed entirely on the server: first, FHE-compatible node importance
scores (based on encrypted degree statistics) are computed from the encrypted
graph. These scores then guide a homomorphic partitioning process, generating
multi-level importance masks directly under FHE. This dynamically generated
mask facilitates both input graph pruning (by logically removing unimportant
elements) and a novel adaptive polynomial activation scheme, where activation
complexity is tailored to node importance levels. Empirical evaluations
demonstrate that DESIGN substantially accelerates FHE GNN inference compared to
state-of-the-art methods while maintaining competitive model accuracy,
presenting a robust solution for secure graph analytics.

</details>


### [27] [TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data](https://arxiv.org/abs/2507.05660)
*Aravind Cheruvu,Shravya Kanchi,Sifat Muhammad Abdullah,Nicholas Kong,Daphne Yao,Murtuza Jadliwala,Bimal Viswanath*

Main category: cs.CR

TL;DR: TuneShield是一个防御框架，用于在LLM微调过程中减少毒性，同时保持对话质量。


<details>
  <summary>Details</summary>
Motivation: 定制LLM时，处理不可信训练数据中的毒性是一个重要挑战。

Method: 利用LLM的毒性分类能力生成‘治愈数据’，并通过对齐过程优化模型。

Result: TuneShield有效减少毒性攻击，保持对话质量，对抗自适应攻击表现优异。

Conclusion: TuneShield在减少毒性和对抗攻击方面表现出色，适用于对话学习场景。

Abstract: Recent advances in foundation models, such as LLMs, have revolutionized
conversational AI. Chatbots are increasingly being developed by customizing
LLMs on specific conversational datasets. However, mitigating toxicity during
this customization, especially when dealing with untrusted training data,
remains a significant challenge. To address this, we introduce TuneShield, a
defense framework designed to mitigate toxicity during chatbot fine-tuning
while preserving conversational quality. TuneShield leverages LLM-based
toxicity classification, utilizing the instruction-following capabilities and
safety alignment of LLMs to effectively identify toxic samples, outperforming
industry API services. TuneShield generates synthetic conversation samples,
termed 'healing data', based on the identified toxic samples, using them to
mitigate toxicity while reinforcing desirable behavior during fine-tuning. It
performs an alignment process to further nudge the chatbot towards producing
desired responses. Our findings show that TuneShield effectively mitigates
toxicity injection attacks while preserving conversational quality, even when
the toxicity classifiers are imperfect or biased. TuneShield proves to be
resilient against adaptive adversarial and jailbreak attacks. Additionally,
TuneShield demonstrates effectiveness in mitigating adaptive toxicity injection
attacks during dialog-based learning (DBL).

</details>


### [28] [Polyadic encryption](https://arxiv.org/abs/2507.05683)
*Steven Duplij,Qiang Guo*

Main category: cs.CR

TL;DR: 提出了一种基于多元代数结构和信号处理方法的加密/解密新方法。


<details>
  <summary>Details</summary>
Motivation: 利用多元代数结构和信号处理技术，开发更高效的加密/解密方法。

Method: 使用整数振幅信号传递信息，通过多元技术将明文转换为特殊整数序列，接收方通过特定规则和方程组恢复明文。

Result: 实现了基于多元代数和信号处理的加密/解密流程。

Conclusion: 该方法为加密技术提供了新的思路，具有潜在的应用价值。

Abstract: A novel original procedure of encryption/decryption based on the polyadic
algebraic structures and on signal processing methods is proposed. First, we
use signals with integer amplitudes to send information. Then we use polyadic
techniques to transfer the plaintext into series of special integers. The
receiver restores the plaintext using special rules and systems of equations.

</details>


### [29] [Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset](https://arxiv.org/abs/2507.05728)
*Ruofei Wang,Peiqi Duan,Boxin Shi,Renjie Wan*

Main category: cs.CR

TL;DR: 提出了一种针对事件数据集的不可学习事件流生成方法，通过异步事件误差最小化噪声干扰数据，防止未经授权的训练。


<details>
  <summary>Details</summary>
Motivation: 随着在线事件数据集的增多，保护数据免受未经授权使用成为重要问题。

Method: 提出异步事件误差最小化噪声和投影策略，生成稀疏的不可学习事件流（UEvs）。

Result: 实验表明该方法有效防止数据滥用，同时保留合法用途的实用性。

Conclusion: UEvs有助于推动安全可信的事件数据集共享。

Abstract: With more event datasets being released online, safeguarding the event
dataset against unauthorized usage has become a serious concern for data
owners. Unlearnable Examples are proposed to prevent the unauthorized
exploitation of image datasets. However, it's unclear how to create unlearnable
asynchronous event streams to prevent event misuse. In this work, we propose
the first unlearnable event stream generation method to prevent unauthorized
training from event datasets. A new form of asynchronous event error-minimizing
noise is proposed to perturb event streams, tricking the unauthorized model
into learning embedded noise instead of realistic features. To be compatible
with the sparse event, a projection strategy is presented to sparsify the noise
to render our unlearnable event streams (UEvs). Extensive experiments
demonstrate that our method effectively protects event data from unauthorized
exploitation, while preserving their utility for legitimate use. We hope our
UEvs contribute to the advancement of secure and trustworthy event dataset
sharing. Code is available at: https://github.com/rfww/uevs.

</details>


### [30] [Automated Reasoning for Vulnerability Management by Design](https://arxiv.org/abs/2507.05794)
*Avi Shaked,Nan Messe*

Main category: cs.CR

TL;DR: 提出了一种基于形式化基础的自动化推理机制，用于系统化管理系统设计中的漏洞态势，并集成到开源安全设计工具中。


<details>
  <summary>Details</summary>
Motivation: 当前漏洞管理方法缺乏对系统设计漏洞态势的系统化推理支持，需改进以有效管理漏洞和设计安全控制。

Method: 开发了一种形式化基础的自动化推理机制，并将其集成到开源安全设计工具中。

Result: 该机制帮助设计师识别特定系统设计中的漏洞，明确指定缓解选项，并系统化管理漏洞态势。

Conclusion: 提出的自动化推理机制为系统设计师提供了系统化漏洞管理工具，提升了安全控制设计的效率。

Abstract: For securing systems, it is essential to manage their vulnerability posture
and design appropriate security controls. Vulnerability management allows to
proactively address vulnerabilities by incorporating pertinent security
controls into systems designs. Current vulnerability management approaches do
not support systematic reasoning about the vulnerability postures of systems
designs. To effectively manage vulnerabilities and design security controls, we
propose a formally grounded automated reasoning mechanism. We integrate the
mechanism into an open-source security design tool and demonstrate its
application through an illustrative example driven by real-world challenges.
The automated reasoning mechanism allows system designers to identify
vulnerabilities that are applicable to a specific system design, explicitly
specify vulnerability mitigation options, declare selected controls, and thus
systematically manage vulnerability postures.

</details>


### [31] [LDP$^3$: An Extensible and Multi-Threaded Toolkit for Local Differential Privacy Protocols and Post-Processing Methods](https://arxiv.org/abs/2507.05872)
*Berkay Kemal Balioglu,Alireza Khodaie,Mehmet Emre Gursoy*

Main category: cs.CR

TL;DR: LDP$^3$是一个开源、可扩展的多线程工具包，用于优化本地差分隐私（LDP）协议和后处理方法的选择，显著提升数据效用和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LDP协议和后处理方法选择困难以及缺乏全面评估工具的问题。

Method: 开发LDP$^3$工具包，包含多种LDP协议、后处理方法和效用指标，采用模块化和多线程设计。

Result: 实验表明，LDP$^3$能显著提升数据效用，并通过并行化大幅减少执行时间。

Conclusion: LDP$^3$为LDP研究和实践提供了高效、灵活的工具，优化了协议和后处理方法的选择。

Abstract: Local differential privacy (LDP) has become a prominent notion for
privacy-preserving data collection. While numerous LDP protocols and
post-processing (PP) methods have been developed, selecting an optimal
combination under different privacy budgets and datasets remains a challenge.
Moreover, the lack of a comprehensive and extensible LDP benchmarking toolkit
raises difficulties in evaluating new protocols and PP methods. To address
these concerns, this paper presents LDP$^3$ (pronounced LDP-Cube), an
open-source, extensible, and multi-threaded toolkit for LDP researchers and
practitioners. LDP$^3$ contains implementations of several LDP protocols, PP
methods, and utility metrics in a modular and extensible design. Its modular
design enables developers to conveniently integrate new protocols and PP
methods. Furthermore, its multi-threaded nature enables significant reductions
in execution times via parallelization. Experimental evaluations demonstrate
that: (i) using LDP$^3$ to select a good protocol and post-processing method
substantially improves utility compared to a bad or random choice, and (ii) the
multi-threaded design of LDP$^3$ brings substantial benefits in terms of
efficiency.

</details>


### [32] [Post-Processing in Local Differential Privacy: An Extensive Evaluation and Benchmark Platform](https://arxiv.org/abs/2507.05875)
*Alireza Khodaie,Berkay Kemal Balioglu,Mehmet Emre Gursoy*

Main category: cs.CR

TL;DR: 论文通过广泛实验比较了不同后处理方法在本地差分隐私（LDP）下的性能，发现后处理在小隐私预算时显著提升数据效用，但其效果随隐私预算增加而减弱。最优后处理方法受多种因素影响，作者还开发了开源平台LDP$^3$以支持研究和实践。


<details>
  <summary>Details</summary>
Motivation: 本地差分隐私（LDP）协议在保护用户数据隐私的同时降低了数据效用，后处理方法旨在缓解此问题，但其性能比较和适用条件尚不明确。

Method: 研究设计了包含6种LDP协议、7种后处理方法、4种效用指标和6个数据集的广泛基准测试，通过实验评估后处理方法在不同条件下的表现。

Result: 实验表明，后处理在小隐私预算时显著提升效用，但随着隐私预算增加效果减弱。最优后处理方法受LDP协议、隐私预算、数据特征和效用指标等多因素影响。

Conclusion: 研究揭示了后处理方法的适用条件，并开发了开源平台LDP$^3$，为研究和实践提供了模块化、可扩展的工具。

Abstract: Local differential privacy (LDP) has recently gained prominence as a powerful
paradigm for collecting and analyzing sensitive data from users' devices.
However, the inherent perturbation added by LDP protocols reduces the utility
of the collected data. To mitigate this issue, several post-processing (PP)
methods have been developed. Yet, the comparative performance of PP methods
under diverse settings remains underexplored. In this paper, we present an
extensive benchmark comprising 6 popular LDP protocols, 7 PP methods, 4 utility
metrics, and 6 datasets to evaluate the behaviors and optimality of PP methods
under diverse conditions. Through extensive experiments, we show that while PP
can substantially improve utility when the privacy budget is small (i.e.,
strict privacy), its benefit diminishes as the privacy budget grows. Moreover,
our findings reveal that the optimal PP method depends on multiple factors,
including the choice of LDP protocol, privacy budget, data characteristics
(such as distribution and domain size), and the specific utility metric. To
advance research in this area and assist practitioners in identifying the most
suitable PP method for their setting, we introduce LDP$^3$, an open-source
benchmark platform. LDP$^3$ contains all methods used in our experimental
analysis, and it is designed in a modular, extensible, and multi-threaded way
for future use and development.

</details>


### [33] [The Impact of Event Data Partitioning on Privacy-aware Process Discovery](https://arxiv.org/abs/2507.06008)
*Jungeun Lim,Stephan A. Fahrenkrog-Petersen,Xixi Lu,Jan Mendling,Minseok Song*

Main category: cs.CR

TL;DR: 提出了一种结合匿名化和事件数据分区的管道，通过事件抽象进行分区，以在保护隐私的同时减少效用损失。


<details>
  <summary>Details</summary>
Motivation: 信息系统的事件日志包含敏感信息，匿名化会降低日志的效用，尤其是在复杂日志中。

Method: 提出了一种管道，结合匿名化和事件数据分区，利用事件抽象进行分区，分别匿名化子日志。

Result: 实验验证表明，事件分区可以提升基于直接跟随关系的匿名化技术的效用。

Conclusion: 事件分区方法在保护隐私的同时有效减少了匿名化对日志效用的负面影响。

Abstract: Information systems support the execution of business processes. The event
logs of these executions generally contain sensitive information about
customers, patients, and employees. The corresponding privacy challenges can be
addressed by anonymizing the event logs while still retaining utility for
process discovery. However, trading off utility and privacy is difficult: the
higher the complexity of event log, the higher the loss of utility by
anonymization. In this work, we propose a pipeline that combines anonymization
and event data partitioning, where event abstraction is utilized for
partitioning. By leveraging event abstraction, event logs can be segmented into
multiple parts, allowing each sub-log to be anonymized separately. This
pipeline preserves privacy while mitigating the loss of utility. To validate
our approach, we study the impact of event partitioning on two anonymization
techniques using three real-world event logs and two process discovery
techniques. Our results demonstrate that event partitioning can bring
improvements in process discovery utility for directly-follows-based
anonymization techniques.

</details>


### [34] [Enter, Exit, Page Fault, Leak: Testing Isolation Boundaries for Microarchitectural Leaks](https://arxiv.org/abs/2507.06039)
*Oleksii Oleksenko,Flavien Solt,Cédric Fournet,Jana Hofmann,Boris Köpf,Stavros Volos*

Main category: cs.CR

TL;DR: 该论文开发了一种工具，用于测试微架构隔离缺陷，发现了新的漏洞并验证了已知漏洞，为处理器设计提供了主动安全验证方法。


<details>
  <summary>Details</summary>
Motivation: 现有CPU隔离机制主要关注架构层面，忽视了微架构侧信道攻击（如Meltdown和Foreshadow），导致软件需要不断修补漏洞，难以确保完全隔离。

Method: 扩展了基于模型的关系测试（MRT）方法，设计了新的测试用例生成器、执行沙箱、泄漏模型和分析技术，以检测跨域信息泄漏。

Result: 在六款x86-64 CPU上测试，发现了四个新漏洞并验证了多个已知漏洞，仅有两个误报。

Conclusion: 该方法揭示了当前隔离机制的缺陷，并为处理器设计提供了从被动修补转向主动安全验证的途径。

Abstract: CPUs provide isolation mechanisms like virtualization and privilege levels to
protect software. Yet these focus on architectural isolation while typically
overlooking microarchitectural side channels, exemplified by Meltdown and
Foreshadow. Software must therefore supplement architectural defenses with
ad-hoc microarchitectural patches, which are constantly evolving as new attacks
emerge and defenses are proposed. Such reactive approach makes ensuring
complete isolation a daunting task, and leaves room for errors and oversights.
  We address this problem by developing a tool that stress tests
microarchitectural isolation between security domains such as virtual machines,
kernel, and processes, with the goal of detecting flaws in the isolation
boundaries. The tool extends model-based relational testing (MRT) methodology
to enable detection of cross-domain information leakage. We design a new test
case generator and execution sandbox to handle multi-domain execution, new
leakage models to encode expected leaks, and new analysis techniques to manage
nondeterminism.
  We use this tool to perform an in-depth testing campaign on six x86-64 CPUs
for leakage across different isolation boundaries. The testing campaign exposed
four new leaks and corroborated numerous known ones, with only two false
positives throughout the entire campaign. These results show critical gaps in
current isolation mechanisms as well as validate a robust methodology for
detecting microarchitectural flaws. As such, this approach enables a shift from
reactive patching to proactive security validation in processor design.

</details>


### [35] [CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations](https://arxiv.org/abs/2507.06043)
*Xiaohu Li,Yunfeng Ning,Zepeng Bao,Mayi Xu,Jianhao Chen,Tieyun Qian*

Main category: cs.CR

TL;DR: 论文提出了一种结合攻击与防御的框架，利用GAN学习LLM内部的安全判断边界，实现了高效的越狱攻击与防御。


<details>
  <summary>Details</summary>
Motivation: 研究LLM的安全保护机制，揭示其脆弱性，并提出一种结合攻击与防御的方法以增强模型安全性。

Method: 基于LLM中间层嵌入的线性可分性，利用GAN学习安全判断边界，实现攻击与防御的结合。

Result: 在三种流行LLM上平均越狱成功率为88.85%，防御成功率为84.17%。

Conclusion: 该方法验证了有效性，揭示了LLM内部安全机制，为增强模型安全性提供了新思路。

Abstract: Security alignment enables the Large Language Model (LLM) to gain the
protection against malicious queries, but various jailbreak attack methods
reveal the vulnerability of this security mechanism. Previous studies have
isolated LLM jailbreak attacks and defenses. We analyze the security protection
mechanism of the LLM, and propose a framework that combines attack and defense.
Our method is based on the linearly separable property of LLM intermediate
layer embedding, as well as the essence of jailbreak attack, which aims to
embed harmful problems and transfer them to the safe area. We utilize
generative adversarial network (GAN) to learn the security judgment boundary
inside the LLM to achieve efficient jailbreak attack and defense. The
experimental results indicate that our method achieves an average jailbreak
success rate of 88.85\% across three popular LLMs, while the defense success
rate on the state-of-the-art jailbreak dataset reaches an average of 84.17\%.
This not only validates the effectiveness of our approach but also sheds light
on the internal security mechanisms of LLMs, offering new insights for
enhancing model security The code and data are available at
https://github.com/NLPGM/CAVGAN.

</details>


### [36] [Wrapless: The trustless lending protocol on top of Bitcoin](https://arxiv.org/abs/2507.06064)
*Oleksandr Kurbatov,Kyrylo Baybula,Yaroslava Chopa,Sergey Kozlov,Oleg Komendant,Illia Dovgopoly,Dmitrii Kurbatov,Zakhar Naumets,Yulia Artikulova,Pavel Kravchenko,Volodymyr Dubinin,Lasha Antadze,Yaroslav Panasenko,Mykhailo Velykodnyi*

Main category: cs.CR

TL;DR: Wrapless是一种借贷协议，允许比特币作为抵押品，无需依赖可信的包装机制。


<details>
  <summary>Details</summary>
Motivation: 解决比特币作为抵押品时对可信包装机制的依赖问题。

Method: 通过比特币区块链上的“贷款通道”实现，支持图灵完备智能合约的区块链均可使用。

Result: 协议设计确保各方操纵贷款规则在经济上不合理。

Conclusion: 仍需进一步研究以使其更接近传统AMM金融工具。

Abstract: This paper presents Wrapless -- a lending protocol that enables the
collateralization of bitcoins without requiring a trusted wrapping mechanism.
The protocol facilitates a "loan channel" on the Bitcoin blockchain, allowing
bitcoins to be locked as collateral for loans issued on any blockchain that
supports Turing-complete smart contracts. The protocol is designed in a way
that makes it economically irrational for each involved party to manipulate the
loan rules. There is still a significant research area to bring the protocol
closer to traditional AMM financial instruments.

</details>


### [37] [Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI](https://arxiv.org/abs/2507.06092)
*Shravya Kanchi,Neal Mangaokar,Aravind Cheruvu,Sifat Muhammad Abdullah,Shirin Nilizadeh,Atul Prakash,Bimal Viswanath*

Main category: cs.CR

TL;DR: 研究探讨了利用生成式AI（GenAI）生成合成数据以解决安全任务中机器学习分类器的数据挑战，并显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注算法改进，而数据挑战对分类器性能的负面影响未得到足够重视。

Method: 提出通过GenAI生成合成数据增强训练集，并在7个安全任务中评估了6种GenAI方法及新方案Nimai。

Result: GenAI显著提升分类器性能（最高32.6%），并支持快速适应概念漂移，但某些任务中GenAI初始化困难。

Conclusion: GenAI在安全任务中潜力巨大，但需针对特定任务特性优化工具开发。

Abstract: Machine learning-based supervised classifiers are widely used for security
tasks, and their improvement has been largely focused on algorithmic
advancements. We argue that data challenges that negatively impact the
performance of these classifiers have received limited attention. We address
the following research question: Can developments in Generative AI (GenAI)
address these data challenges and improve classifier performance? We propose
augmenting training datasets with synthetic data generated using GenAI
techniques to improve classifier generalization. We evaluate this approach
across 7 diverse security tasks using 6 state-of-the-art GenAI methods and
introduce a novel GenAI scheme called Nimai that enables highly controlled data
synthesis. We find that GenAI techniques can significantly improve the
performance of security classifiers, achieving improvements of up to 32.6% even
in severely data-constrained settings (only ~180 training samples).
Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to
concept drift post-deployment, requiring minimal labeling in the adjustment
process. Despite successes, our study finds that some GenAI schemes struggle to
initialize (train and produce data) on certain security tasks. We also identify
characteristics of specific tasks, such as noisy labels, overlapping class
distributions, and sparse feature vectors, which hinder performance boost using
GenAI. We believe that our study will drive the development of future GenAI
tools designed for security tasks.

</details>


### [38] [Fun with flags: How Compilers Break and Fix Constant-Time Code](https://arxiv.org/abs/2507.06112)
*Antoine Geimer,Clementine Maurice*

Main category: cs.CR

TL;DR: 论文分析了编译器优化如何破坏常数时间编程，识别了导致泄漏的关键优化步骤，并提出了一种无需修改编译器或源代码的缓解方法。


<details>
  <summary>Details</summary>
Motivation: 开发者依赖常数时间编程防止时序侧信道攻击，但编译器优化可能重新引入泄漏，目前缺乏具体可操作的解决方案。

Method: 构建编译器引入的常数时间违规数据集，分析GCC和LLVM的优化步骤，识别主要泄漏来源。

Result: 发现少数优化步骤是泄漏的主要原因，并提出通过编译器标志禁用这些步骤的缓解方法。

Conclusion: 该方法显著减少泄漏且性能开销小，为开发者提供了可直接部署的防御方案。

Abstract: Developers rely on constant-time programming to prevent timing side-channel
attacks. But these efforts can be undone by compilers, whose optimizations may
silently reintroduce leaks. While recent works have measured the extent of such
leakage, they leave developers without actionable insights: which optimization
passes are responsible, and how to disable them without modifying the compiler
remains unclear.
  In this paper, we conduct a qualitative analysis of how compiler
optimizations break constant-time code. We construct a dataset of
compiler-introduced constant-time violations and analyze the internals of two
widely used compilers, GCC and LLVM, to identify the specific optimization
passes responsible. Our key insight is that a small set of passes are at the
root of most leaks. To the best of our knowledge, we are also the first to
characterize how the interactions between these passes contribute to leakage.
Based on this analysis, we propose an original and practical mitigation that
requires no source code modification or custom compiler: disabling selected
optimization passes via compiler flags. We show that this approach
significantly reduces leakage with minimal performance overhead, offering an
immediately deployable defense for developers.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [Strongly Solving $7 \times 6$ Connect-Four on Consumer Grade Hardware](https://arxiv.org/abs/2507.05267)
*Markus Böck*

Main category: cs.AI

TL;DR: 本文通过符号搜索方法（基于二元决策图）为Connect-Four游戏生成强解，并实现了高效的89.6GB查找表。


<details>
  <summary>Details</summary>
Motivation: 尽管Connect-Four游戏已有数学解，但传统认为强解形式的查找表不可行。本文旨在探索符号搜索方法生成强解的可行性。

Method: 采用基于二元决策图的符号搜索方法，高效实现生成查找表，并包含alpha-beta搜索以优化最快胜利或最慢失败的策略。

Result: 成功生成89.6GB的查找表，耗时47小时（单CPU核心，128GB内存），适用于标准7×6棋盘。

Conclusion: 符号搜索方法可行且高效，为Connect-Four提供了强解，并开源了相关工具。

Abstract: While the game Connect-Four has been solved mathematically and the best move
can be effectively computed with search based methods, a strong solution in the
form of a look-up table was believed to be infeasible. In this paper, we
revisit a symbolic search method based on binary decision diagrams to produce
strong solutions. With our efficient implementation we were able to produce a
89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main
memory for the standard $7 \times 6$ board size. In addition to this
win-draw-loss evaluation, we include an alpha-beta search in our open source
artifact to find the move which achieves the fastest win or slowest loss.

</details>


### [40] [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
*Yue Wang,Miao Zhou,Guijing Huang,Rui Zhuo,Chao Yi,Zhenliang Ma*

Main category: cs.AI

TL;DR: Chat2SPaT是一种方法，通过大型语言模型（LLMs）将用户对交通信号控制计划的半结构化描述转换为精确的信号相位和时序（SPaT）结果，准确率超过94%。


<details>
  <summary>Details</summary>
Motivation: 减少手动输入交通信号控制计划的繁琐工作，提高计划管理的效率和准确性。

Method: 利用LLMs理解用户描述并生成JSON格式的相位序列和属性，再通过Python脚本组装完整的信号控制计划。

Result: 在测试数据集（300多个计划描述）中，Chat2SPaT的准确率超过94%（中英文均适用）。

Conclusion: Chat2SPaT为交通从业者和研究人员提供了易于使用的计划管理工具，展示了LLMs在智能交通系统（ITS）中的潜力。

Abstract: Pre-timed traffic signal control, commonly used for operating signalized
intersections and coordinated arterials, requires tedious manual work for
signaling plan creating and updating. When the time-of-day or day-of-week plans
are utilized, one intersection is often associated with multiple plans, leading
to further repetitive manual plan parameter inputting. To enable a
user-friendly traffic signal control plan management process, this study
proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous
descriptions on the signal control plan to exact signal phase and timing (SPaT)
results, which could further be transformed into structured stage-based or
ring-based plans to interact with intelligent transportation system (ITS)
software and traffic signal controllers. With curated prompts, Chat2SPaT first
leverages large language models' (LLMs) capability of understanding users' plan
descriptions and reformulate the plan as a combination of phase sequence and
phase attribute results in the json format. Based on LLM outputs, python
scripts are designed to locate phases in a cycle, address nuances of traffic
signal control, and finally assemble the complete traffic signal control plan.
Within a chat, the pipeline can be utilized iteratively to conduct further plan
editing. Experiments show that Chat2SPaT can generate plans with an accuracy of
over 94% for both English and Chinese cases, using a test dataset with over 300
plan descriptions. As the first benchmark for evaluating LLMs' capability of
understanding traffic signal control plan descriptions, Chat2SPaT provides an
easy-to-use plan management pipeline for traffic practitioners and researchers,
serving as a potential new building block for a more accurate and versatile
application of LLMs in the field of ITS. The source codes, prompts and test
dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.

</details>


### [41] [Fuzzy Classification Aggregation for a Continuum of Agents](https://arxiv.org/abs/2507.05297)
*Zijun Meng*

Main category: cs.AI

TL;DR: 证明了对于连续个体分类的最优、独立且零一致的模糊分类聚合函数必须是加权算术平均。


<details>
  <summary>Details</summary>
Motivation: 研究模糊分类聚合函数的性质，特别是在多对象多类型分类中的最优性、独立性和一致性。

Method: 通过数学证明，分析连续个体分类的聚合函数，并验证其必须满足加权算术平均的条件。

Result: 证明了在给定条件下，模糊分类聚合函数只能是加权算术平均。

Conclusion: 加权算术平均是满足最优、独立和零一致条件的唯一模糊分类聚合函数。

Abstract: We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean.

</details>


### [42] [OLG++: A Semantic Extension of Obligation Logic Graph](https://arxiv.org/abs/2507.05488)
*Subhasis Dasgupta,Jon Stephens,Amarnath Gupta*

Main category: cs.AI

TL;DR: OLG++是Obligation Logic Graph（OLG）的语义扩展，用于建模市政和跨辖区背景下的法规和法律规则。它引入了更丰富的节点和边类型，支持复杂的法律义务、例外和层次结构的表示，并通过食品业务法规示例展示了其表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有的法律知识表示模型（如LegalRuleML）在表达空间约束、子类关系和例外结构时存在不足，OLG++旨在填补这一空白。

Method: OLG++扩展了OLG，增加了空间、时间、群体、可废止性和逻辑分组等构造，支持上下文条件、优先级和复杂触发器的结构化推理。

Result: OLG++在表达法律义务和例外方面优于现有模型（如LegalRuleML），并通过属性图查询支持法律问题回答。

Conclusion: OLG++为法律知识表示提供了更丰富的语义工具，适用于复杂的法律规则建模和推理。

Abstract: We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)
for modeling regulatory and legal rules in municipal and interjurisdictional
contexts. OLG++ introduces richer node and edge types, including spatial,
temporal, party group, defeasibility, and logical grouping constructs, enabling
nuanced representations of legal obligations, exceptions, and hierarchies. The
model supports structured reasoning over rules with contextual conditions,
precedence, and complex triggers. We demonstrate its expressiveness through
examples from food business regulations, showing how OLG++ supports legal
question answering using property graph queries. OLG++ also improves over
LegalRuleML by providing native support for subClassOf, spatial constraints,
and reified exception structures. Our examples show that OLG++ is more
expressive than prior graph-based models for legal knowledge representation.

</details>


### [43] [Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents](https://arxiv.org/abs/2507.05495)
*Prahaladh Chandrahasan,Jiahe Jin,Zhihan Zhang,Tevin Wang,Andy Tang,Lucy Mo,Morteza Ziyadi,Leonardo F. R. Ribeiro,Zimeng Qiu,Markus Dreyer,Akari Asai,Chenyan Xiong*

Main category: cs.AI

TL;DR: 论文介绍了Deep Research Comparator平台，用于评估和比较深度研究代理，支持细粒度反馈和排名计算。


<details>
  <summary>Details</summary>
Motivation: 解决深度研究代理在生成报告和中间步骤评估上的挑战。

Method: 开发了Deep Research Comparator平台和Simple Deepresearch代理框架，支持多代理比较和反馈收集。

Result: 收集了17位注释者的真实用户偏好数据，验证了平台的有效性。

Conclusion: 平台为深度研究代理的开发提供了实用工具，支持评估和改进。

Abstract: Effectively evaluating deep research agents that autonomously search the web,
analyze information, and generate reports remains a major challenge,
particularly when it comes to assessing long reports and giving detailed
feedback on their intermediate steps. To address these gaps, we introduce Deep
Research Comparator, a platform that offers a holistic framework for deep
research agent hosting, side-by-side comparison, fine-grained human feedback
collection, and ranking calculation. Given a user query, our platform displays
the final reports from two different agents along with their intermediate steps
during generation. Annotators can evaluate the overall quality of final reports
based on side-by-side comparison, and also provide detailed feedback separately
by assessing intermediate steps or specific text spans within the final report.
Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This
scaffold serves as a baseline that facilitates the easy integration of various
large language models to transform them into deep research agents for
evaluation. To demonstrate the platform's utility for deep research agent
development, we have collected real user preference data from 17 annotators on
three deep research agents. A demo video of our platform can be found at
https://www.youtube.com/watch?v=g4d2dnbdseg.

</details>


### [44] [Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality](https://arxiv.org/abs/2507.05515)
*Haochen Huang,Jiahuan Pei,Mohammad Aliannejadi,Xin Sun,Moonisa Ahsan,Pablo Cesar,Chuang Yu,Zhaochun Ren,Junxiao Wang*

Main category: cs.AI

TL;DR: 论文介绍了针对AR训练的视觉语言模型（VLM）数据集，评估了九种先进模型，发现其在细粒度任务上表现不佳，呼吁改进数据集和基准。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在AR训练中的应用，填补研究空白，并为盲人和视障用户提供平等的AI学习机会。

Method: 构建了系统化的视觉语言任务数据集，并评估了九种先进的VLM模型。

Result: 即使是GPT-4o等先进模型在细粒度装配任务上表现不佳，状态检测的最高F1分数仅为40.54%。

Conclusion: 需改进数据集和基准以提升细粒度视觉语言对齐，研究具有广泛的社会意义，支持盲人和视障用户。

Abstract: Vision-language models (VLMs) are essential for enabling AI-powered smart
assistants to interpret and reason in multimodal environments. However, their
application in augmented reality (AR) training remains largely unexplored. In
this work, we introduce a comprehensive dataset tailored for AR training,
featuring systematized vision-language tasks, and evaluate nine
state-of-the-art VLMs on it. Our results reveal that even advanced models,
including GPT-4o, struggle with fine-grained assembly tasks, achieving a
maximum F1 score of just 40.54% on state detection. These findings highlight
the demand for enhanced datasets, benchmarks, and further research to improve
fine-grained vision-language alignment. Beyond technical contributions, our
work has broader social implications, particularly in empowering blind and
visually impaired users with equitable access to AI-driven learning
opportunities. We provide all related resources, including the dataset, source
code, and evaluation results, to support the research community.

</details>


### [45] [Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System](https://arxiv.org/abs/2507.05519)
*Gopal Gupta,Abhiramon Rajasekharan,Alexis R. Tudor,Elmer Salazar,Joaquín Arias*

Main category: cs.AI

TL;DR: 使用ASP中的默认否定和强否定优雅表达义务模态逻辑，并通过全局约束解决其悖论。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在ASP中实现义务模态逻辑，解决其经典悖论问题。

Method: 利用ASP的默认否定和强否定表示模态算子，并通过全局约束表达义务和禁止。

Result: 提出的表示方法能优雅解决义务模态逻辑的多种悖论。

Conclusion: ASP为义务模态逻辑的实现提供了有效且优雅的解决方案。

Abstract: We consider the problem of implementing deontic modal logic. We show how
(deontic) modal operators can be expressed elegantly using default negation
(negation-as-failure) and strong negation present in answer set programming
(ASP). We propose using global constraints of ASP to represent obligations and
impermissibilities of deontic modal logic. We show that our proposed
representation results in the various paradoxes of deontic modal logic being
elegantly resolved.

</details>


### [46] [Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis](https://arxiv.org/abs/2507.05520)
*Karishma Thakrar,Shreyas Basavatia,Akshay Daftardar*

Main category: cs.AI

TL;DR: 2025 ImageCLEF MEDIQA-MAGIC挑战赛关注多模态皮肤病问答与分割，提出结合微调多模态模型、结构化推理层和检索增强生成的方法，取得高准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决远程医疗中诊断决策的异步性和高准确性需求，模拟皮肤科医生的系统性推理模式。

Method: 结合微调开源多模态模型（Qwen、Gemma、LLaMA）、结构化推理层和检索增强生成（agentic RAG）。

Result: 团队获得第二名，提交作品排名第六，表现竞争力和高准确性。

Conclusion: 该方法为自动化诊断支持系统提供了更可靠的路径，适用于实际医疗场景。

Abstract: The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized
by researchers from Microsoft, Stanford University, and the Hospital Clinic of
Barcelona, focuses on multimodal dermatology question answering and
segmentation, using real-world patient queries and images. This work addresses
the Closed Visual Question Answering (CVQA) task, where the goal is to select
the correct answer to multiple-choice clinical questions based on both
user-submitted images and accompanying symptom descriptions. The proposed
approach combines three core components: (1) fine-tuning open-source multimodal
models from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)
introducing a structured reasoning layer that reconciles and adjudicates
between candidate model outputs, and (3) incorporating agentic
retrieval-augmented generation (agentic RAG), which adds relevant information
from the American Academy of Dermatology's symptom and condition database to
fill in gaps in patient context. The team achieved second place with a
submission that scored sixth, demonstrating competitive performance and high
accuracy. Beyond competitive benchmarks, this research addresses a practical
challenge in telemedicine: diagnostic decisions must often be made
asynchronously, with limited input and with high accuracy and interpretability.
By emulating the systematic reasoning patterns employed by dermatologists when
evaluating skin conditions, this architecture provided a pathway toward more
reliable automated diagnostic support systems.

</details>


### [47] [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
*Jiahuan Pei,Fanghua Ye,Xin Sun,Wentao Deng,Koen Hindriks,Junxiao Wang*

Main category: cs.AI

TL;DR: 论文提出WikiHowAgent，一个基于大语言模型的多智能体工作流，用于模拟教学互动，并通过数据集和评估协议验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在可扩展性和利用大规模多样化课程内容方面存在不足，且缺乏评估教学质量的框架。

Method: 提出WikiHowAgent，整合教师和学习者智能体、交互管理器和评估器，支持程序化学习并评估教学质量。

Result: 实验结果表明该工作流在多样化场景中有效，并提供了跨领域的大语言模型能力洞察。

Conclusion: 论文开源了数据集和实现，为AI4Education领域提供了实用工具和资源。

Abstract: Large language models (LLMs) have advanced virtual educators and learners,
bridging NLP with AI4Education. Existing work often lacks scalability and fails
to leverage diverse, large-scale course content, with limited frameworks for
assessing pedagogic quality. To this end, we propose WikiHowAgent, a
multi-agent workflow leveraging LLMs to simulate interactive teaching-learning
conversations. It integrates teacher and learner agents, an interaction
manager, and an evaluator to facilitate procedural learning and assess
pedagogic quality. We introduce a dataset of 114,296 teacher-learner
conversations grounded in 14,287 tutorials across 17 domains and 727 topics.
Our evaluation protocol combines computational and rubric-based metrics with
human judgment alignment. Results demonstrate the workflow's effectiveness in
diverse setups, offering insights into LLM capabilities across domains. Our
datasets and implementations are fully open-sourced.

</details>


### [48] [Red Teaming AI Red Teaming](https://arxiv.org/abs/2507.05538)
*Subhabrata Majumdar,Brian Pendleton,Abhishek Gupta*

Main category: cs.AI

TL;DR: 本文批判性地审视了AI红队测试的实践，指出其当前在生成式AI中过于关注模型级缺陷，而忽略了更广泛的社会技术系统。作者提出一个两层次框架，并建议多学科团队以应对系统性风险。


<details>
  <summary>Details</summary>
Motivation: 探讨AI红队测试的局限性，尤其是其忽视社会技术系统和涌现行为的缺陷，旨在推动更全面的红队测试方法。

Method: 提出一个两层次框架：宏观系统级红队测试和微观模型级红队测试，结合网络安全经验和系统理论。

Result: 强调有效的AI红队测试需多学科团队，关注涌现风险、系统性漏洞及技术与社会因素的相互作用。

Conclusion: 呼吁扩展红队测试的范围，以更好地应对AI系统中的复杂性和系统性风险。

Abstract: Red teaming has evolved from its origins in military applications to become a
widely adopted methodology in cybersecurity and AI. In this paper, we take a
critical look at the practice of AI red teaming. We argue that despite its
current popularity in AI governance, there exists a significant gap between red
teaming's original intent as a critical thinking exercise and its narrow focus
on discovering model-level flaws in the context of generative AI. Current AI
red teaming efforts focus predominantly on individual model vulnerabilities
while overlooking the broader sociotechnical systems and emergent behaviors
that arise from complex interactions between models, users, and environments.
To address this deficiency, we propose a comprehensive framework
operationalizing red teaming in AI systems at two levels: macro-level system
red teaming spanning the entire AI development lifecycle, and micro-level model
red teaming. Drawing on cybersecurity experience and systems theory, we further
propose a set of recommendations. In these, we emphasize that effective AI red
teaming requires multifunctional teams that examine emergent risks, systemic
vulnerabilities, and the interplay between technical and social factors.

</details>


### [49] [SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation](https://arxiv.org/abs/2507.05541)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.AI

TL;DR: 论文提出了一种基于大语言模型（如GPT-4o-mini）生成反事实解释（CFs）的方法，用于增强机器学习的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 反事实解释（CFs）能提供人类可理解的预测解释，并可用于干预异常预防和数据增强。

Method: 在零样本和三样本设置下，使用LLM生成CFs，并在两个数据集（压力预测和心脏病检测）上进行评估。

Result: 相比传统方法（如DiCE、CFNOW、NICE），LLM方法在合理性（99%）、有效性（0.99）和稀疏性上表现优异，且能提升下游分类器性能（平均准确率提高5%）。

Conclusion: 基于提示的生成技术有望提升临床和生理预测任务的可解释性和鲁棒性。

Abstract: Counterfactual explanations (CFs) offer human-centric insights into machine
learning predictions by highlighting minimal changes required to alter an
outcome. Therefore, CFs can be used as (i) interventions for abnormality
prevention and (ii) augmented data for training robust models. In this work, we
explore large language models (LLMs), specifically GPT-4o-mini, for generating
CFs in a zero-shot and three-shot setting. We evaluate our approach on two
datasets: the AI-Readi flagship dataset for stress prediction and a public
dataset for heart disease detection. Compared to traditional methods such as
DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high
plausibility (up to 99%), strong validity (up to 0.99), and competitive
sparsity. Moreover, using LLM-generated CFs as augmented samples improves
downstream classifier performance (an average accuracy gain of 5%), especially
in low-data regimes. This demonstrates the potential of prompt-based generative
techniques to enhance explainability and robustness in clinical and
physiological prediction tasks. Code base: github.com/anonymous/SenseCF.

</details>


### [50] [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566)
*David Bensaïd,Noam Rotstein,Roy Velich,Daniel Bensaïd,Ron Kimmel*

Main category: cs.AI

TL;DR: SingLoRA提出了一种新的低秩适应方法，通过单低秩矩阵分解解决LoRA中的尺度冲突问题，提升训练稳定性并减少参数数量。


<details>
  <summary>Details</summary>
Motivation: LoRA在参数高效微调中存在尺度冲突问题，导致训练不稳定和性能下降。

Method: SingLoRA将权重更新重新表述为单低秩矩阵与其转置的乘积，消除尺度冲突并减少参数。

Result: 在多个任务中表现优异，如LLama 7B微调MNLI准确率达91.3%，优于LoRA和LoRA+，且参数更少。

Conclusion: SingLoRA通过简单设计解决了LoRA的稳定性问题，并在性能和效率上均有显著提升。

Abstract: Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient
fine-tuning of large pretrained models. LoRA augments the pre-trained weights
of a model by adding the product of two smaller matrices that together form a
low-rank matrix update. Recent research has shown that scale disparities
between these two matrices often cause unstable training dynamics, leading to
suboptimal performance. In this paper, we propose SingLoRA, which reformulates
low-rank adaptation by learning the weights update as a decomposition of a
single low-rank matrix multiplied by its transpose. This simple design
inherently removes inter-matrix scale conflicts, ensuring stable optimization,
and roughly halves the parameter count. We analyze SingLoRA within the
infinite-width neural network framework, showing that it guarantees stable
feature learning by construction. Extensive experiments on multiple tasks
validate these benefits. In common sense reasoning, fine-tuning LLama 7B on
MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+
(90.2%) - while using only 60% of their parameter budget. In image generation,
fine-tuning Stable Diffusion with SingLoRA significantly improves image
fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to
scores of 0.148 and 0.143 for DoRA and LoRA, respectively.

</details>


### [51] [Towards Measurement Theory for Artificial Intelligence](https://arxiv.org/abs/2507.05587)
*Elija Perrier*

Main category: cs.AI

TL;DR: 提出一个关于人工智能测量的正式理论框架，旨在支持系统比较、风险分析及能力评估。


<details>
  <summary>Details</summary>
Motivation: 为AI研究、实践和监管提供统一的测量标准，以便比较系统、连接前沿评估与风险分析技术，并明确AI能力的测量依赖性。

Method: 提出分层的测量堆栈，区分直接与间接可观测性，构建可校准的AI现象分类体系。

Result: 为AI测量提供理论框架，支持系统比较、风险分析及能力评估。

Conclusion: 该理论框架为AI测量提供统一路径，促进更科学的评估和监管。

Abstract: We motivate and outline a programme for a formal theory of measurement of
artificial intelligence. We argue that formalising measurement for AI will
allow researchers, practitioners, and regulators to: (i) make comparisons
between systems and the evaluation methods applied to them; (ii) connect
frontier AI evaluations with established quantitative risk analysis techniques
drawn from engineering and safety science; and (iii) foreground how what counts
as AI capability is contingent upon the measurement operations and scales we
elect to use. We sketch a layered measurement stack, distinguish direct from
indirect observables, and signpost how these ingredients provide a pathway
toward a unified, calibratable taxonomy of AI phenomena.

</details>


### [52] [MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models](https://arxiv.org/abs/2507.05591)
*Wei Zhang,Juan Chen,En Zhu,Wenhong Cheng,YunPeng Li,Yanbo J. Wang*

Main category: cs.AI

TL;DR: 提出了一种新型多模态大语言模型（MLlm-DR），用于可解释的抑郁症诊断，结合小型LLM和轻量查询模块（LQ-former），在CMDC和E-DAIC-WOZ数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症诊断方法缺乏解释性，且多模态LLM在访谈数据上表现不佳，限制了临床应用的推广。

Method: MLlm-DR整合小型LLM生成抑郁症评分及解释，LQ-former捕捉语音和视觉特征，通过精细训练提升逻辑推理能力。

Result: 在CMDC和E-DAIC-WOZ数据集上达到最优性能。

Conclusion: MLlm-DR为可解释的抑郁症诊断提供了有效解决方案，具有临床应用的潜力。

Abstract: Automated depression diagnosis aims to analyze multimodal information from
interview videos to predict participants' depression scores. Previous studies
often lack clear explanations of how these scores were determined, limiting
their adoption in clinical practice. While the advent of LLMs provides a
possible pathway for explainable depression diagnosis, current LLMs capable of
processing multimodal data lack training on interview data, resulting in poor
diagnostic performance when used directly. In this paper, we propose a novel
multimodal large language model (MLlm-DR) that can understand multimodal
information inputs and supports explainable depression diagnosis. MLlm-DR
integrates a smaller LLMs and a lightweight query module (LQ-former).
Specifically, the smaller LLMs is designed to generate depression scores and
corresponding evaluation rationales. To enhance its logical reasoning for
domain-specific tasks while maintaining practicality, we constructed a robust
training dataset to fine-tune it. Meanwhile, the LQ-former captures
depression-related features from speech and visual data, aiding the model's
ability to process multimodal information, to achieve comprehensive depression
diagnosis. Our approach achieves state-of-the-art results on two
interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its
effectiveness and superiority.

</details>


### [53] [Domain adaptation of large language models for geotechnical applications](https://arxiv.org/abs/2507.05613)
*Lei Fan,Fangxue Liu,Cheng Chen*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型（LLMs）在岩土工程中的适应与应用，探讨了关键方法（如提示工程、检索增强生成等）及前沿应用（如地质解释、风险评估等），并分析了其优势与局限。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，其在岩土工程中的应用潜力巨大，但需领域特定适应。本文旨在填补这一研究空白，为实践者和学术界提供参考。

Method: 通过综述方法，总结了LLMs在岩土工程中的适应技术（如领域自适应预训练、微调）和应用场景（如设计计算、数值建模）。

Result: 研究发现，LLMs在岩土工程中能显著提升效率，但也存在领域知识不足等局限。未来研究方向包括改进模型适应性和扩展应用范围。

Conclusion: 本文为LLMs在岩土工程中的实践和未来研究提供了重要参考，推动了这一跨学科领域的发展。

Abstract: Recent developments in large language models (LLMs) are opening up new
opportunities in geotechnical engineering and engineering geology. While
general-purpose LLMs possess broad capabilities, effective application in
geotechnics often requires domain-specific adaptation. Such tailored LLMs are
increasingly employed to streamline geotechnical workflows. This paper presents
the first survey of the adaptation and application of LLMs in geotechnical
engineering. It outlines key methodologies for adaptation to geotechnical
domain, including prompt engineering, retrieval-augmented generation,
domain-adaptive pretraining, and fine-tuning. The survey examines the
state-of-the-art applications of geotechnical-adapted LLMs, including
geological interpretation, subsurface characterization, site planning, design
calculations, numerical modeling, safety and risk assessment, and educational
tutoring. It also analyzes benefits and limitations of geotechnical-adapted
LLMs, and identifies promising directions for future research in this
interdisciplinary discipline. The findings serve as a valuable resource for
practitioners seeking to integrate LLMs into geotechnical practice, while also
providing a foundation to stimulate further investigation within the academic
community.

</details>


### [54] [ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion](https://arxiv.org/abs/2507.05624)
*Wei Zhang,Juan Chen,Yanbo J. Wang,En Zhu,Xuan Yang,Yiduo Wang*

Main category: cs.AI

TL;DR: 提出了一种基于注意力的扩散模型（ADMC），用于解决多模态情感和意图识别中缺失模态的问题，通过独立训练特征提取网络和生成缺失模态特征，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态数据中因传感器故障或数据不完整导致的缺失模态问题，传统方法存在过耦合和生成不精确的缺陷。

Method: 提出ADMC框架，独立训练各模态特征提取网络，利用注意力扩散网络（ADN）生成与真实多模态分布一致的缺失特征。

Result: 在IEMOCAP和MIntRec基准测试中取得最优结果，适用于缺失和完整模态场景。

Conclusion: ADMC通过避免过耦合和精确生成缺失特征，显著提升了多模态情感和意图识别的性能。

Abstract: Multimodal emotion and intent recognition is essential for automated
human-computer interaction, It aims to analyze users' speech, text, and visual
information to predict their emotions or intent. One of the significant
challenges is that missing modalities due to sensor malfunctions or incomplete
data. Traditional methods that attempt to reconstruct missing information often
suffer from over-coupling and imprecise generation processes, leading to
suboptimal outcomes. To address these issues, we introduce an Attention-based
Diffusion model for Missing Modalities feature Completion (ADMC). Our framework
independently trains feature extraction networks for each modality, preserving
their unique characteristics and avoiding over-coupling. The Attention-based
Diffusion Network (ADN) generates missing modality features that closely align
with authentic multimodal distribution, enhancing performance across all
missing-modality scenarios. Moreover, ADN's cross-modal generation offers
improved recognition even in full-modality contexts. Our approach achieves
state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating
its effectiveness in both missing and complete modality scenarios.

</details>


### [55] [Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses](https://arxiv.org/abs/2507.05629)
*Yuan An,John Liu,Niyam Acharya,Ruhma Hashmi*

Main category: cs.AI

TL;DR: 研究探讨了利用大型语言模型（LLM）自动生成检索练习问题对学生学习的影响，结果显示LLM生成的问题显著提高了学生的知识保留率。


<details>
  <summary>Details</summary>
Motivation: 解决教师手动生成高质量检索练习问题耗时费力的问题，尤其是在快速发展的技术学科中。

Method: 通过两项大学数据科学课程的实证研究，比较学生使用LLM生成的多选题与未使用时的学习效果。

Result: 使用LLM生成问题的学生知识保留率显著提高（89% vs. 73%）。

Conclusion: LLM生成的检索练习问题能有效支持学习，但需教师手动验证以确保质量。

Abstract: Retrieval practice is a well-established pedagogical technique known to
significantly enhance student learning and knowledge retention. However,
generating high-quality retrieval practice questions is often time-consuming
and labor intensive for instructors, especially in rapidly evolving technical
subjects. Large Language Models (LLMs) offer the potential to automate this
process by generating questions in response to prompts, yet the effectiveness
of LLM-generated retrieval practice on student learning remains to be
established. In this study, we conducted an empirical study involving two
college-level data science courses, with approximately 60 students. We compared
learning outcomes during one week in which students received LLM-generated
multiple-choice retrieval practice questions to those from a week in which no
such questions were provided. Results indicate that students exposed to
LLM-generated retrieval practice achieved significantly higher knowledge
retention, with an average accuracy of 89%, compared to 73% in the week without
such practice. These findings suggest that LLM-generated retrieval questions
can effectively support student learning and may provide a scalable solution
for integrating retrieval practice into real-time teaching. However, despite
these encouraging outcomes and the potential time-saving benefits, cautions
must be taken, as the quality of LLM-generated questions can vary. Instructors
must still manually verify and revise the generated questions before releasing
them to students.

</details>


### [56] [LLMs are Introvert](https://arxiv.org/abs/2507.05638)
*Litian Zhang,Xiaoming Zhang,Bingyu Yan,Ziyi Zhou,Bo Zhang,Zhenyu Guan,Xi Zhang,Chaozhuo Li*

Main category: cs.AI

TL;DR: 论文探讨了利用大语言模型（LLM）模拟信息传播中的心理和行为动态，提出SIP-CoT机制增强LLM的社会智能和真实性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体和生成式AI的快速发展加剧了错误信息的传播，传统模型无法充分捕捉在线互动的复杂性，而现有高级方法又忽视了用户心理和行为动态。

Method: 提出基于社会信息处理理论（SIP）的SIP-CoT机制，结合情感引导记忆，改进LLM对社交线索的解释、目标个性化和反馈评估。

Result: 实验表明，SIP-CoT增强的LLM代理能更有效地处理社会信息，行为、态度和情感更接近真实人类互动。

Conclusion: 研究揭示了当前LLM模拟传播的局限性，并证明SIP-CoT和情感记忆的整合能显著提升LLM代理的社会智能和真实性。

Abstract: The exponential growth of social media and generative AI has transformed
information dissemination, fostering connectivity but also accelerating the
spread of misinformation. Understanding information propagation dynamics and
developing effective control strategies is essential to mitigate harmful
content. Traditional models, such as SIR, provide basic insights but
inadequately capture the complexities of online interactions. Advanced methods,
including attention mechanisms and graph neural networks, enhance accuracy but
typically overlook user psychology and behavioral dynamics. Large language
models (LLMs), with their human-like reasoning, offer new potential for
simulating psychological aspects of information spread. We introduce an
LLM-based simulation environment capturing agents' evolving attitudes,
emotions, and responses. Initial experiments, however, revealed significant
gaps between LLM-generated behaviors and authentic human dynamics, especially
in stance detection and psychological realism. A detailed evaluation through
Social Information Processing Theory identified major discrepancies in
goal-setting and feedback evaluation, stemming from the lack of emotional
processing in standard LLM training. To address these issues, we propose the
Social Information Processing-based Chain of Thought (SIP-CoT) mechanism
enhanced by emotion-guided memory. This method improves the interpretation of
social cues, personalization of goals, and evaluation of feedback. Experimental
results confirm that SIP-CoT-enhanced LLM agents more effectively process
social information, demonstrating behaviors, attitudes, and emotions closer to
real human interactions. In summary, this research highlights critical
limitations in current LLM-based propagation simulations and demonstrates how
integrating SIP-CoT and emotional memory significantly enhances the social
intelligence and realism of LLM agents.

</details>


### [57] [City-Level Foreign Direct Investment Prediction with Tabular Learning on Judicial Data](https://arxiv.org/abs/2507.05651)
*Tianxing Wu,Lizhe Cao,Shuang Wang,Jiming Wang,Shutong Zhu,Yerong Wu,Yuqing Feng*

Main category: cs.AI

TL;DR: 论文提出了一种基于司法数据的表格学习方法（TLJD），用于城市级外国直接投资（FDI）预测，解决了传统经济数据可能被操纵的问题。


<details>
  <summary>Details</summary>
Motivation: 为促进联合国可持续发展目标，FDI对经济增长和创新至关重要，但传统基于经济数据的预测可能不可靠。

Method: 利用大规模司法数据构建司法绩效评估指标体系，并提出TLJD方法，结合行数据和列数据进行指标编码，使用混合专家模型调整权重。

Result: 实验表明，TLJD在跨城市和跨时间任务中优于其他十种先进基线方法，R2达到至少0.92。

Conclusion: TLJD通过司法数据提升了FDI预测的可靠性，为地方政府决策提供了新工具。

Abstract: To advance the United Nations Sustainable Development Goal on promoting
sustained, inclusive, and sustainable economic growth, foreign direct
investment (FDI) plays a crucial role in catalyzing economic expansion and
fostering innovation. Precise city-level FDI prediction is quite important for
local government and is commonly studied based on economic data (e.g., GDP).
However, such economic data could be prone to manipulation, making predictions
less reliable. To address this issue, we try to leverage large-scale judicial
data which reflects judicial performance influencing local investment security
and returns, for city-level FDI prediction. Based on this, we first build an
index system for the evaluation of judicial performance over twelve million
publicly available adjudication documents according to which a tabular dataset
is reformulated. We then propose a new Tabular Learning method on Judicial Data
(TLJD) for city-level FDI prediction. TLJD integrates row data and column data
in our built tabular dataset for judicial performance indicator encoding, and
utilizes a mixture of experts model to adjust the weights of different
indicators considering regional variations. To validate the effectiveness of
TLJD, we design cross-city and cross-time tasks for city-level FDI predictions.
Extensive experiments on both tasks demonstrate the superiority of TLJD (reach
to at least 0.92 R2) over the other ten state-of-the-art baselines in different
evaluation metrics.

</details>


### [58] [Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology](https://arxiv.org/abs/2507.05716)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: 研究比较了人类专家与两种AI模型（通用型和推理型）生成的皮肤病治疗方案，发现评估结果因评估者（人类或AI）而异，揭示了临床经验与算法逻辑之间的差距。


<details>
  <summary>Details</summary>
Motivation: 评估AI生成的治疗方案质量，尤其是在AI扩展到诊断之外的领域时，探索人类与AI评估的差异。

Method: 10名皮肤科医生、通用AI（GPT-4o）和推理AI（o3）为5个复杂皮肤病案例生成治疗方案，匿名后由人类专家和高级AI（Gemini 2.5 Pro）评分。

Result: 人类专家评分显著高于AI方案，而AI评委则相反，推理AI（o3）在AI评委中排名第一。

Conclusion: 治疗方案质量感知取决于评估者性质，未来需开发可解释的人机协同系统以弥合理念差距。

Abstract: Background: Evaluating AI-generated treatment plans is a key challenge as AI
expands beyond diagnostics, especially with new reasoning models. This study
compares plans from human experts and two AI models (a generalist and a
reasoner), assessed by both human peers and a superior AI judge.
  Methods: Ten dermatologists, a generalist AI (GPT-4o), and a reasoning AI
(o3) generated treatment plans for five complex dermatology cases. The
anonymized, normalized plans were scored in two phases: 1) by the ten human
experts, and 2) by a superior AI judge (Gemini 2.5 Pro) using an identical
rubric.
  Results: A profound 'evaluator effect' was observed. Human experts scored
peer-generated plans significantly higher than AI plans (mean 7.62 vs. 7.16;
p=0.0313), ranking GPT-4o 6th (mean 7.38) and the reasoning model, o3, 11th
(mean 6.97). Conversely, the AI judge produced a complete inversion, scoring AI
plans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313). It
ranked o3 1st (mean 8.20) and GPT-4o 2nd, placing all human experts lower.
  Conclusions: The perceived quality of a clinical plan is fundamentally
dependent on the evaluator's nature. An advanced reasoning AI, ranked poorly by
human experts, was judged as superior by a sophisticated AI, revealing a deep
gap between experience-based clinical heuristics and data-driven algorithmic
logic. This paradox presents a critical challenge for AI integration,
suggesting the future requires synergistic, explainable human-AI systems that
bridge this reasoning gap to augment clinical care.

</details>


### [59] [An autonomous agent for auditing and improving the reliability of clinical AI models](https://arxiv.org/abs/2507.05755)
*Lukas Kuhn,Florian Buettner*

Main category: cs.AI

TL;DR: ModelAuditor是一个自省代理，通过对话和模拟临床相关分布变化，识别AI模型在真实医疗场景中的潜在失败模式，并提供可解释报告和改进策略。


<details>
  <summary>Details</summary>
Motivation: AI模型在临床实践中可能因硬件、光照或人口统计变化而失效，目前缺乏高效工具进行可靠性审计。

Method: ModelAuditor通过多智能体架构，选择任务特定指标，模拟分布变化，并生成报告。

Result: 在三种临床场景中，ModelAuditor成功识别失败模式，性能恢复15-25%，成本低于0.5美元。

Conclusion: ModelAuditor为临床AI模型提供了一种高效、低成本的可靠性审计工具。

Abstract: The deployment of AI models in clinical practice faces a critical challenge:
models achieving expert-level performance on benchmarks can fail
catastrophically when confronted with real-world variations in medical imaging.
Minor shifts in scanner hardware, lighting or demographics can erode accuracy,
but currently reliability auditing to identify such catastrophic failure cases
before deployment is a bespoke and time-consuming process. Practitioners lack
accessible and interpretable tools to expose and repair hidden failure modes.
Here we introduce ModelAuditor, a self-reflective agent that converses with
users, selects task-specific metrics, and simulates context-dependent,
clinically relevant distribution shifts. ModelAuditor then generates
interpretable reports explaining how much performance likely degrades during
deployment, discussing specific likely failure modes and identifying root
causes and mitigation strategies. Our comprehensive evaluation across three
real-world clinical scenarios - inter-institutional variation in
histopathology, demographic shifts in dermatology, and equipment heterogeneity
in chest radiography - demonstrates that ModelAuditor is able correctly
identify context-specific failure modes of state-of-the-art models such as the
established SIIM-ISIC melanoma classifier. Its targeted recommendations recover
15-25% of performance lost under real-world distribution shift, substantially
outperforming both baseline models and state-of-the-art augmentation methods.
These improvements are achieved through a multi-agent architecture and execute
on consumer hardware in under 10 minutes, costing less than US$0.50 per audit.

</details>


### [60] [Real-time monitoring of the SoH of lithium-ion batteries](https://arxiv.org/abs/2507.05765)
*Bruno Jammes,Edgar Hernando Sepúlveda-Oviedo,Corinne Alonso*

Main category: cs.AI

TL;DR: 提出了一种基于充电末段放电脉冲分析的新方法，用于实时监测电池健康状态（SoH），在微电网中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统方法在微电网中受限，需开发新方法实时监测电池SoH。

Method: 通过分析充电末段的放电脉冲，利用等效电路模型参数估计SoH。

Result: 实验数据显示，预测SoH的误差约1%，解释性评分接近0.9。

Conclusion: 该方法有望集成到电池管理系统（BMS）中，优化持续运行下的电池管理。

Abstract: Real-time monitoring of the state of health (SoH) of batteries remains a
major challenge, particularly in microgrids where operational constraints limit
the use of traditional methods. As part of the 4BLife project, we propose an
innovative method based on the analysis of a discharge pulse at the end of the
charge phase. The parameters of the equivalent electrical model describing the
voltage evolution across the battery terminals during this current pulse are
then used to estimate the SoH. Based on the experimental data acquired so far,
the initial results demonstrate the relevance of the proposed approach. After
training using the parameters of two batteries with a capacity degradation of
around 85%, we successfully predicted the degradation of two other batteries,
cycled down to approximately 90% SoH, with a mean absolute error of around 1%
in the worst case, and an explainability score of the estimator close to 0.9.
If these performances are confirmed, this method can be easily integrated into
battery management systems (BMS) and paves the way for optimized battery
management under continuous operation.

</details>


### [61] [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/2507.05791)
*Yan Yang,Dongxu Li,Yutong Dai,Yuhao Yang,Ziyang Luo,Zirui Zhao,Zhiyuan Hu,Junzhe Huang,Amrita Saha,Zeyuan Chen,Ran Xu,Liyuan Pan,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: 论文提出了一种名为GTA1的GUI代理，通过测试时缩放方法和强化学习解决任务规划和视觉元素交互的挑战，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理在任务规划中的模糊性和复杂界面中视觉元素交互的准确性挑战。

Method: 引入测试时缩放方法选择最佳动作提案，并利用强化学习模型提高视觉元素交互的准确性。

Result: GTA1在多个基准测试中表现优异，如Screenspot-Pro（50.1%）、Screenspot-V2（92.4%）和OSWorld-G（67.7%）。

Conclusion: GTA1通过测试时缩放和强化学习方法显著提升了GUI代理的性能，代码和模型已开源。

Abstract: Graphical user interface (GUI) agents autonomously operate across platforms
(e.g., Linux) to complete tasks by interacting with visual elements.
Specifically, a user instruction is decomposed into a sequence of action
proposals, each corresponding to an interaction with the GUI. After each
action, the agent observes the updated GUI environment to plan the next step.
However, two main challenges arise: i) resolving ambiguity in task planning
(i.e., the action proposal sequence), where selecting an appropriate plan is
non-trivial, as many valid ones may exist; ii) accurately grounding actions in
complex and high-resolution interfaces, i.e., precisely interacting with visual
targets.
  This paper investigates the two aforementioned challenges with our GUI
Test-time Scaling Agent, namely GTA1. First, to select the most appropriate
action proposal, we introduce a test-time scaling method. At each step, we
sample multiple candidate action proposals and leverage a judge model to
evaluate and select the most suitable one. It trades off computation for better
decision quality by concurrent sampling, shortening task execution steps, and
improving overall performance. Second, we propose a model that achieves
improved accuracy when grounding the selected action proposal to its
corresponding visual elements. Our key insight is that reinforcement learning
(RL) facilitates visual grounding through inherent objective alignments,
rewarding successful clicks on interface elements.
  Experimentally, our method establishes state-of-the-art performance across
diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%
accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When
paired with a planner applying our test-time scaling strategy, it exhibits
state-of-the-art agentic performance (e.g., 45.2% task success rate on
OSWorld). We open-source our code and models here.

</details>


### [62] [Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity](https://arxiv.org/abs/2507.05816)
*Shuai Zhao,Yulin Zhang,Luwei Xiao,Xinyi Wu,Yanhao Jia,Zhongliang Guo,Xiaobao Wu,Cong-Duy Nguyen,Guoming Zhang,Anh Tuan Luu*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在预测早产儿视网膜病变（ROP）风险方面的能力，提出了CROP数据集和Affective-ROPTester评估框架，发现外部知识增强和情感提示工程能显著提升模型性能并减少偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多领域取得进展，但其在ROP风险预测中的应用尚未充分探索，因此需要系统评估其能力和情感偏见。

Method: 提出CROP数据集和Affective-ROPTester框架，结合指令、思维链和上下文学习三种提示策略，并融入情感元素。

Result: LLMs在仅依赖内部知识时表现有限，但外部输入显著提升性能；情感偏见明显，积极情感提示有助于减少偏见。

Conclusion: 情感敏感的提示工程对提升诊断可靠性至关重要，Affective-ROPTester为临床语言模型评估提供了有效框架。

Abstract: Despite the remarkable progress of large language models (LLMs) across
various domains, their capacity to predict retinopathy of prematurity (ROP)
risk remains largely unexplored. To address this gap, we introduce a novel
Chinese benchmark dataset, termed CROP, comprising 993 admission records
annotated with low, medium, and high-risk labels. To systematically examine the
predictive capabilities and affective biases of LLMs in ROP risk
stratification, we propose Affective-ROPTester, an automated evaluation
framework incorporating three prompting strategies: Instruction-based,
Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme
assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and
ICL schemes leverage external medical knowledge to enhance predictive accuracy.
Crucially, we integrate emotional elements at the prompt level to investigate
how different affective framings influence the model's ability to predict ROP
and its bias patterns. Empirical results derived from the CROP dataset yield
two principal observations. First, LLMs demonstrate limited efficacy in ROP
risk prediction when operating solely on intrinsic knowledge, yet exhibit
marked performance gains when augmented with structured external inputs.
Second, affective biases are evident in the model outputs, with a consistent
inclination toward overestimating medium- and high-risk cases. Third, compared
to negative emotions, positive emotional framing contributes to mitigating
predictive bias in model outputs. These findings highlight the critical role of
affect-sensitive prompt engineering in enhancing diagnostic reliability and
emphasize the utility of Affective-ROPTester as a framework for evaluating and
mitigating affective bias in clinical language modeling systems.

</details>


### [63] [CogniPlay: a work-in-progress Human-like model for General Game Playing](https://arxiv.org/abs/2507.05868)
*Aloïs Rautureau,Éric Piette*

Main category: cs.AI

TL;DR: 论文探讨了AI系统在游戏中表现优异但仍缺乏人类直觉决策的问题，提出了基于认知心理学的模型CogniPlay。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在游戏中表现超越人类，但其决策过程与人类直觉不同，研究旨在填补这一差距。

Method: 结合认知心理学和前人研究，提出新模型CogniPlay，应用于通用游戏（GGP）。

Result: 模型CogniPlay为工作进展，旨在模拟人类直觉决策。

Conclusion: CogniPlay有望在通用游戏中实现更接近人类的决策模式。

Abstract: While AI systems have equaled or surpassed human performance in a wide
variety of games such as Chess, Go, or Dota 2, describing these systems as
truly "human-like" remains far-fetched. Despite their success, they fail to
replicate the pattern-based, intuitive decision-making processes observed in
human cognition. This paper presents an overview of findings from cognitive
psychology and previous efforts to model human-like behavior in artificial
agents, discusses their applicability to General Game Playing (GGP) and
introduces our work-in-progress model based on these observations: CogniPlay.

</details>


### [64] [Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better](https://arxiv.org/abs/2507.05886)
*Aaron Bembenek*

Main category: cs.AI

TL;DR: 提出了一种名为“神经符号转换系统”的计算模型，旨在结合符号算法和大型语言模型（LLMs），以构建更强大的自动推理工具。


<details>
  <summary>Details</summary>
Motivation: 当前构建神经符号自动推理系统的方法缺乏传统符号算法的强保证，且未能充分结合神经网络与符号推理的潜力。

Method: 提出神经符号转换系统，将符号状态与直觉配对，并在符号和直觉上并行执行状态转换。

Result: 该模型有望扩展逻辑推理能力，同时保留符号算法的强保证。

Conclusion: 该计算模型可具体化为逻辑编程语言，为神经符号自动推理工具提供理论基础。

Abstract: There is growing excitement about building software verifiers, synthesizers,
and other Automated Reasoning (AR) tools by combining traditional symbolic
algorithms and Large Language Models (LLMs). Unfortunately, the current
practice for constructing such neurosymbolic AR systems is an ad hoc
programming model that does not have the strong guarantees of traditional
symbolic algorithms, nor a deep enough synchronization of neural networks and
symbolic reasoning to unlock the full potential of LLM-powered reasoning. I
propose Neurosymbolic Transition Systems as a principled computational model
that can underlie infrastructure for building neurosymbolic AR tools. In this
model, symbolic state is paired with intuition, and state transitions operate
over symbols and intuition in parallel. I argue why this new paradigm can scale
logical reasoning beyond current capabilities while retaining the strong
guarantees of symbolic algorithms, and I sketch out how the computational model
I propose can be reified in a logic programming language.

</details>


### [65] [Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection](https://arxiv.org/abs/2507.05891)
*Robert Leppich,Michael Stenger,André Bauer,Samuel Kounev*

Main category: cs.AI

TL;DR: 论文提出了一种将时间序列预测分解为三个核心阶段的方法，通过评估不同模块的有效性，实现了高精度和计算效率的提升。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在Transformer时代虽有进步，但仍面临序列表示、信息提取和目标投影的挑战，需针对不同任务解决独特问题。

Method: 将预测流程分解为输入序列表示、信息提取与记忆构建、目标投影三阶段，评估卷积层和自注意力机制等模块的有效性。

Result: 在七个基准数据集上实现了最先进的预测精度，同时显著提升了计算效率，减少了训练和推理时间及参数量。

Conclusion: 通过系统分解和模块化设计，论文提出的方法在时间序列预测中实现了高效和高精度的平衡。

Abstract: With the advent of Transformers, time series forecasting has seen significant
advances, yet it remains challenging due to the need for effective sequence
representation, memory construction, and accurate target projection. Time
series forecasting remains a challenging task, demanding effective sequence
representation, meaningful information extraction, and precise future
projection. Each dataset and forecasting configuration constitutes a distinct
task, each posing unique challenges the model must overcome to produce accurate
predictions. To systematically address these task-specific difficulties, this
work decomposes the time series forecasting pipeline into three core stages:
input sequence representation, information extraction and memory construction,
and final target projection. Within each stage, we investigate a range of
architectural configurations to assess the effectiveness of various modules,
such as convolutional layers for feature extraction and self-attention
mechanisms for information extraction, across diverse forecasting tasks,
including evaluations on seven benchmark datasets. Our models achieve
state-of-the-art forecasting accuracy while greatly enhancing computational
efficiency, with reduced training and inference times and a lower parameter
count. The source code is available at
https://github.com/RobertLeppich/REP-Net.

</details>


### [66] [MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation](https://arxiv.org/abs/2507.05894)
*Fathinah Izzati,Xinyue Li,Yuxuan Wu,Gus Xia*

Main category: cs.AI

TL;DR: 论文提出MusiScene模型，通过音乐场景想象（MSI）任务生成与音乐匹配的场景描述，并用于提升视频背景音乐生成（VBMG）。


<details>
  <summary>Details</summary>
Motivation: 探索音乐语言模型是否能像人类一样通过音乐想象场景，弥补现有音乐标注模型仅关注音乐元素的不足。

Method: 构建大规模视频-音频标注数据集（3,371对），微调Music Understanding LLaMA以创建MusiScene模型，并进行全面评估。

Result: MusiScene在生成上下文相关标注方面优于MU-LLaMA，并成功应用于视频背景音乐生成任务。

Conclusion: MusiScene通过跨模态信息训练，有效实现了音乐场景想象，为音乐与视频的结合提供了新思路。

Abstract: Humans can imagine various atmospheres and settings when listening to music,
envisioning movie scenes that complement each piece. For example, slow,
melancholic music might evoke scenes of heartbreak, while upbeat melodies
suggest celebration. This paper explores whether a Music Language Model, e.g.
MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),
which requires cross-modal information from video and music to train. To
improve upon existing music captioning models which focusing solely on musical
elements, we introduce MusiScene, a music captioning model designed to imagine
scenes that complement each music. In this paper, (1) we construct a
large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music
Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct
comprehensive evaluations and prove that our MusiScene is more capable of
generating contextually relevant captions compared to MU-LLaMA. We leverage the
generated MSI captions to enhance Video Background Music Generation (VBMG) from
text.

</details>


### [67] [BlueLM-2.5-3B Technical Report](https://arxiv.org/abs/2507.05934)
*Baojiao Xiong,Boheng Chen,Chengzhi Wang,Daxiong Luo,Dongsheng Xu,Dongyang Liu,Fan Yang,Fangyuan Li,Fei Teng,Feng Wang,Fukang Qin,Fuquan Peng,Guanxin Tan,Guozhi Wang,Haibo Yu,Haohao Gao,Heng Liu,Hongbo Yang,Hongjian Zou,Houzheng Shen,Hu Meng,Huan Li,Hui Tan,Jiali Chen,Jianzhao Chen,Jinliang Zhu,Kai Wang,Lei Wu,Liangbing Liu,Liuyang Bian,Liyan He,Long Liu,Peiwen Li,Penggang Shi,Qi Ding,Rui Hu,Shuai Cao,Shuai Ren,Shuang Peng,Teng Xie,Weiji Chen,Weilin Xiang,Weixin Wu,Xi Yin,Xiaoxin Chen,Xu Chen,Yafei Wen,Yan Hu,Yanzhou Yang,Yina Xie,Yinghao Chen,Yixuan Liao,Yu Geng,Yuanjiang Ouyang,Yuanzhuo Yang,Yuehua He,Yushuai Peng,Zhaoxiong Wang,Zheng Wang,Zhibo Zhou,Ziyang Wu*

Main category: cs.AI

TL;DR: BlueLM-2.5-3B是一个紧凑的多模态大语言模型，支持思考和普通模式，并在边缘设备上高效运行。


<details>
  <summary>Details</summary>
Motivation: 开发高性能、适用于边缘设备的多模态大语言模型，同时保持文本和多模态任务的竞争力。

Method: 通过多样化数据整理、关键数据重采样、混合异构强化学习和高性能训练基础设施开发模型。

Result: 在3B参数规模下，模型在多模态和纯文本任务中表现优异，数据效率高。

Conclusion: BlueLM-2.5-3B为高性能边缘设备MLLM的发展提供了重要贡献。

Abstract: We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large
Language Model (MLLM) designed for efficient edge-device deployment, offering
strong general-purpose and reasoning capabilities. To the best of our
knowledge, this is the first 3B-scale MLLM to support both thinking and
non-thinking modes, while also enabling explicit control over thinking token
budget. BlueLM-2.5-3B is developed through diversified data curation, key data
resampling, hybrid heterogeneous reinforcement learning, and a high-performance
training infrastructure. Our model achieves superior multimodal capacity while
preserving competitive pure-text performance with only 2.9 billion parameters.
We conduct comprehensive evaluations across a broad range of multimodal and
text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable
performance to Qwen3-4B on text-only benchmarks, and trails the larger
Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In
non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal
benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.
All of the aforementioned performance is achieved with substantially less total
training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to
the advancement of high-performance, on-device MLLMs and provides meaningful
insights to the research community.

</details>


### [68] [A Wireless Foundation Model for Multi-Task Prediction](https://arxiv.org/abs/2507.05938)
*Yucheng Sheng,Jiacheng Wang,Xingyu Zhou,Le Liang,Hao Ye,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: 提出了一种统一的基础模型，用于无线网络中的多任务预测，支持不同预测区间，并通过分解、编码和因果Transformer实现泛化能力。


<details>
  <summary>Details</summary>
Motivation: 移动通信网络的复杂性和动态性增加，准确预测关键系统参数对PHY层和MAC层任务至关重要，传统深度学习方法泛化能力不足。

Method: 采用单变量分解统一异构任务，编码粒度以实现区间感知，使用因果Transformer作为主干，并引入补丁掩码策略支持任意输入长度。

Result: 在大规模数据集上训练后，模型在未见场景中表现出强泛化能力，并在新任务上实现零样本性能，超越传统全样本基线。

Conclusion: 提出的基础模型在无线网络多任务预测中具有广泛适用性和优越性能。

Abstract: With the growing complexity and dynamics of the mobile communication
networks, accurately predicting key system parameters, such as channel state
information (CSI), user location, and network traffic, has become essential for
a wide range of physical (PHY)-layer and medium access control (MAC)-layer
tasks. Although traditional deep learning (DL)-based methods have been widely
applied to such prediction tasks, they often struggle to generalize across
different scenarios and tasks. In response, we propose a unified foundation
model for multi-task prediction in wireless networks that supports diverse
prediction intervals. The proposed model enforces univariate decomposition to
unify heterogeneous tasks, encodes granularity for interval awareness, and uses
a causal Transformer backbone for accurate predictions. Additionally, we
introduce a patch masking strategy during training to support arbitrary input
lengths. After trained on large-scale datasets, the proposed foundation model
demonstrates strong generalization to unseen scenarios and achieves zero-shot
performance on new tasks that surpass traditional full-shot baselines.

</details>


### [69] [Enhancing the Interpretability of Rule-based Explanations through Information Retrieval](https://arxiv.org/abs/2507.05976)
*Alessandro Umbrico,Guido Bologna,Luca Coraci,Francesca Fracasso,Silvia Gola,Gabriella Cortellessa*

Main category: cs.AI

TL;DR: 提出一种基于属性的方法，提升可解释AI在乳腺癌淋巴结放疗后淋巴水肿风险评估中的透明度和可接受性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的AI技术缺乏透明度，限制了其在医疗决策中的可解释性和接受度。

Method: 采用基于规则的预测模型，结合信息检索技术中的标准指标，对属性进行统计分析，计算其对预测的相关性。

Result: 用户研究表明，与原始可解释AI模型相比，该方法生成的输出具有更高的可解释性和实用性。

Conclusion: 该方法有效提升了AI预测在医疗风险评估中的透明度和用户接受度。

Abstract: The lack of transparency of data-driven Artificial Intelligence techniques
limits their interpretability and acceptance into healthcare decision-making
processes. We propose an attribution-based approach to improve the
interpretability of Explainable AI-based predictions in the specific context of
arm lymphedema's risk assessment after lymph nodal radiotherapy in breast
cancer. The proposed method performs a statistical analysis of the attributes
in the rule-based prediction model using standard metrics from Information
Retrieval techniques. This analysis computes the relevance of each attribute to
the prediction and provides users with interpretable information about the
impact of risk factors. The results of a user study that compared the output
generated by the proposed approach with the raw output of the Explainable AI
model suggested higher levels of interpretability and usefulness in the context
of predicting lymphedema risk.

</details>


### [70] [Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening](https://arxiv.org/abs/2507.05984)
*Zhijun Guo,Alvina Lai,Julia Ive,Alexandru Petcu,Yutong Wang,Luyuan Qi,Johan H Thygesen,Kezhi Li*

Main category: cs.AI

TL;DR: HopeBot是一款基于大型语言模型的聊天机器人，用于抑郁症筛查，与传统静态问卷PHQ-9相比，更具互动性和适应性。研究表明其评分与传统方法高度一致，且用户反馈积极。


<details>
  <summary>Details</summary>
Motivation: 传统抑郁症筛查工具（如PHQ-9）缺乏互动性和适应性，限制了其效果。

Method: 开发了HopeBot，结合检索增强生成和实时澄清功能，进行PHQ-9问卷的交互式管理。

Result: 132名参与者测试显示，HopeBot与传统方法评分高度一致（ICC=0.91），71%用户更信任聊天机器人。

Conclusion: 语音驱动的LLM聊天机器人可作为抑郁症筛查的可扩展、低负担辅助工具。

Abstract: Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively
screen depression but lack interactivity and adaptability. We developed
HopeBot, a chatbot powered by a large language model (LLM) that administers the
PHQ-9 using retrieval-augmented generation and real-time clarification. In a
within-subject study, 132 adults in the United Kingdom and China completed both
self-administered and chatbot versions. Scores demonstrated strong agreement
(ICC = 0.91; 45% identical). Among 75 participants providing comparative
feedback, 71% reported greater trust in the chatbot, highlighting clearer
structure, interpretive guidance, and a supportive tone. Mean ratings (0-10)
were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,
and 7.4 for recommendation helpfulness; the latter varied significantly by
employment status and prior mental-health service use (p < 0.05). Overall,
87.1% expressed willingness to reuse or recommend HopeBot. These findings
demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden
adjuncts for routine depression screening.

</details>


### [71] [CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation](https://arxiv.org/abs/2507.06013)
*Kushal Gajjar,Harshit Sikchi,Arpit Singh Gautam,Marc Hammons,Saurabh Jha*

Main category: cs.AI

TL;DR: CogniSQL-R1-Zero是一个基于强化学习的框架，用于生成准确的SQL查询，通过轻量级奖励信号（执行正确性和格式合规性）实现高效学习，并在Text2SQL基准测试中达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 自然语言转SQL（Text-to-SQL）是语言理解和结构化数据访问的核心挑战，尽管大语言模型（LLMs）在流畅性上有所提升，但生成复杂查询的正确SQL仍然困难。

Method: 采用强化学习（RL）框架，通过轻量级奖励信号（执行正确性和格式合规性）训练模型，避免中间监督和复杂奖励设计。

Result: 在Text2SQL基准测试（BIRD bench）中，CogniSQL-R1-Zero超越了包括SFT CodeS-7B、DeepSeek-Coder 236B和Mistral 123B在内的基线模型，尽管其基于较小的7B模型。

Conclusion: 该研究展示了强化学习在高效、可扩展的Text-to-SQL生成中的潜力，并发布了两套数据集以支持进一步研究。

Abstract: Translating natural language into SQL (Text-to-SQL) remains a core challenge
at the intersection of language understanding and structured data access.
Although large language models (LLMs) have improved fluency, generating correct
and executable SQL, especially for complex queries, continues to be
challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)
framework and model that produces accurate SQL using a lightweight reward
signal based on execution correctness and format-tag compliance. By avoiding
intermediate supervision, hybrid pipelines and complex reward shaping, our
method encourages stable learning and stronger alignment with the ultimate task
objective-producing executable programs. CogniSQL-R1-Zero achieves
state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,
outperforming prior supervised and instruction-tuned baselines including SFT
CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a
significantly smaller 7B backbone. This result underscores the scalability and
efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs
(40 GB VRAM each). To support further research in efficient and interpretable
Text-to-SQL modeling, we release two curated datasets: (i) a collection of
5,024 reasoning traces with varying context lengths, and (ii) a
positive-sampled corpus of 36,356 corpus of weakly supervised queries, each
annotated with six semantically diverse reasoning paths. Together, these
contributions advance scalable, execution-aligned Text-to-SQL generation.

</details>


### [72] [Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions](https://arxiv.org/abs/2507.06029)
*Courtney Ford,Mark T. Keane*

Main category: cs.AI

TL;DR: FGNS方法通过结合局部和全局特征重要性选择代表性样本，显著提升了非专家用户对模型错误的识别能力，同时保持了预测一致性。


<details>
  <summary>Details</summary>
Motivation: 当前XAI方法对非专家用户生成清晰解释的能力不足，FGNS旨在通过改进样本选择提升解释性。

Method: FGNS是一种后处理方法，利用局部和全局特征重要性选择类代表性样本。

Result: 在用户研究中，FGNS显著提升了非专家的决策速度和准确性，且所选邻居更符合类别特征。

Conclusion: FGNS是迈向更人性化模型评估的一步，但解释质量与用户信任之间的差距仍需进一步研究。

Abstract: Explainable AI (XAI) methods often struggle to generate clear, interpretable
outputs for users without domain expertise. We introduce Feature-Guided
Neighbor Selection (FGNS), a post hoc method that enhances interpretability by
selecting class-representative examples using both local and global feature
importance. In a user study (N = 98) evaluating Kannada script classifications,
FGNS significantly improved non-experts' ability to identify model errors while
maintaining appropriate agreement with correct predictions. Participants made
faster and more accurate decisions compared to those given traditional k-NN
explanations. Quantitative analysis shows that FGNS selects neighbors that
better reflect class characteristics rather than merely minimizing
feature-space distance, leading to more consistent selection and tighter
clustering around class prototypes. These results support FGNS as a step toward
more human-aligned model assessment, although further work is needed to address
the gap between explanation quality and perceived trust.

</details>


### [73] [On Lockean beliefs that are deductively closed and minimal change](https://arxiv.org/abs/2507.06042)
*Tommaso Flaminio,Lluis Godo,Ramón Pino Pérez,Lluis Subirana*

Main category: cs.AI

TL;DR: 论文在Lockean理论框架下，研究了基于概率置信度的信念集，提出了两种使其闭合于经典逻辑演绎的特征，并提出了一种最小化修正的更新方法。


<details>
  <summary>Details</summary>
Motivation: Lockean信念集在经典逻辑演绎下通常不闭合，这限制了其在信念修正理论等领域的应用。论文旨在解决这一问题。

Method: 提供了两种使信念集闭合于经典逻辑演绎的特征，并提出了一种基于最小修正的概率更新方法。

Result: 展示了如何通过最小修正实现信念集的演绎闭合。

Conclusion: 论文为Lockean信念集的闭合性和修正提供了理论支持，扩展了其应用范围。

Abstract: Within the formal setting of the Lockean thesis, an agent belief set is
defined in terms of degrees of confidence and these are described in
probabilistic terms. This approach is of established interest, notwithstanding
some limitations that make its use troublesome in some contexts, like, for
instance, in belief change theory. Precisely, Lockean belief sets are not
generally closed under (classical) logical deduction. The aim of the present
paper is twofold: on one side we provide two characterizations of those belief
sets that are closed under classical logic deduction, and on the other we
propose an approach to probabilistic update that allows us for a minimal
revision of those beliefs, i.e., a revision obtained by making the fewest
possible changes to the existing belief set while still accommodating the new
information. In particular, we show how we can deductively close a belief set
via a minimal revision.

</details>


### [74] [FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models](https://arxiv.org/abs/2507.06057)
*Bo Pang,Yalu Ouyang,Hangfei Xu,Ziqi Jia,Panpan Li,Shengzhao Wen,Lu Wang,Shiyong Li,Yanpeng Wang*

Main category: cs.AI

TL;DR: FEVO框架通过多阶段增强方法提升LLM在金融领域的性能，包括持续预训练、监督微调和强化学习，并在多个基准测试中取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究在将LLM应用于需要大量领域知识的金融领域方面仍有限，因此开发了FEVO框架以填补这一空白。

Method: FEVO采用持续预训练（CPT）扩展金融知识，监督微调（SFT）培养结构化推理模式，强化学习（RL）整合知识与推理。

Result: FEVO-R32B在五个金融基准测试中表现最优，且显著优于仅使用RL训练的模型。

Conclusion: FEVO框架有效提升了LLM在金融领域的性能，验证了金融知识扩展和结构化推理的重要性。

Abstract: Advancements in reasoning for large language models (LLMs) have lead to
significant performance improvements for LLMs in various fields such as
mathematics and programming. However, research applying these advances to the
financial domain, where considerable domain-specific knowledge is necessary to
complete tasks, remains limited. To address this gap, we introduce FEVO
(Financial Evolution), a multi-stage enhancement framework developed to enhance
LLM performance in the financial domain. FEVO systemically enhances LLM
performance by using continued pre-training (CPT) to expand financial domain
knowledge, supervised fine-tuning (SFT) to instill structured, elaborate
reasoning patterns, and reinforcement learning (RL) to further integrate the
expanded financial domain knowledge with the learned structured reasoning. To
ensure effective and efficient training, we leverage frontier reasoning models
and rule-based filtering to curate FEVO-Train, high-quality datasets
specifically designed for the different post-training phases. Using our
framework, we train the FEVO series of models -- C32B, S32B, R32B -- from
Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and
general capabilities, with results showing that FEVO-R32B achieves
state-of-the-art performance on five financial benchmarks against much larger
models as well as specialist models. More significantly, FEVO-R32B demonstrates
markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct
using only RL), thus validating the effectiveness of financial domain knowledge
expansion and structured, logical reasoning distillation

</details>


### [75] [AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study](https://arxiv.org/abs/2507.06077)
*Iman Rahimi,Isha Patel*

Main category: cs.AI

TL;DR: 该论文提出了一种结合LSTM、遗传算法和SHAP的AI框架，用于医疗设施的高效能源管理，显著提升了预测准确性和能源效率。


<details>
  <summary>Details</summary>
Motivation: 医疗设施能源需求波动大，传统方法效率低下且成本高，亟需智能解决方案。

Method: 采用LSTM进行时间序列预测，结合遗传算法优化参数，利用SHAP增强模型可解释性。

Result: LSTM在MAE和RMSE上显著优于ARIMA和Prophet模型，遗传算法优化了负载平衡，SHAP提高了透明度。

Conclusion: 该框架为医疗能源管理提供了高效、可扩展的解决方案，未来可探索实时部署和强化学习结合。

Abstract: This paper tackles the urgent need for efficient energy management in
healthcare facilities, where fluctuating demands challenge operational
efficiency and sustainability. Traditional methods often prove inadequate,
causing inefficiencies and higher costs. To address this, the study presents an
AI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm
(GA), and SHAP (Shapley Additive Explanations), specifically designed for
healthcare energy management. Although LSTM is widely used for time-series
forecasting, its application in healthcare energy prediction remains
underexplored. The results reveal that LSTM significantly outperforms ARIMA and
Prophet models in forecasting complex, non-linear demand patterns. LSTM
achieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)
of 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:
87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm
is applied to optimize model parameters and improve load balancing strategies,
enabling adaptive responses to real-time energy fluctuations. SHAP analysis
further enhances model transparency by explaining the influence of different
features on predictions, fostering trust in decision-making processes. This
integrated LSTM-GA-SHAP approach offers a robust solution for improving
forecasting accuracy, boosting energy efficiency, and advancing sustainability
in healthcare facilities. Future research may explore real-time deployment and
hybridization with reinforcement learning for continuous optimization. Overall,
the study establishes a solid foundation for using AI in healthcare energy
management, highlighting its scalability, efficiency, and resilience potential.

</details>


### [76] [OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety](https://arxiv.org/abs/2507.06134)
*Sanidhya Vijayvargiya,Aditya Bharat Soni,Xuhui Zhou,Zora Zhiruo Wang,Nouha Dziri,Graham Neubig,Maarten Sap*

Main category: cs.AI

TL;DR: OpenAgentSafety是一个模块化框架，用于评估AI代理在真实工具交互中的安全性，覆盖八类关键风险，支持多任务和多用户场景，发现主流LLM在51.2%至72.7%的任务中存在不安全行为。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理安全性评估多依赖模拟环境或狭窄任务域，无法全面反映真实风险，亟需更全面的评估框架。

Method: 提出OpenAgentSafety框架，支持真实工具交互（如浏览器、代码执行环境等），结合规则分析和LLM评估，检测显性和隐性不安全行为。

Result: 实验显示，主流LLM在安全漏洞任务中不安全行为比例高达51.2%（Claude-Sonnet-3.7）至72.7%（o3-mini）。

Conclusion: OpenAgentSafety揭示了AI代理在真实场景中的显著安全风险，强调实际部署前需加强安全保障。

Abstract: Recent advances in AI agents capable of solving complex, everyday tasks, from
scheduling to customer service, have enabled deployment in real-world settings,
but their possibilities for unsafe behavior demands rigorous evaluation. While
prior benchmarks have attempted to assess agent safety, most fall short by
relying on simulated environments, narrow task domains, or unrealistic tool
abstractions. We introduce OpenAgentSafety, a comprehensive and modular
framework for evaluating agent behavior across eight critical risk categories.
Unlike prior work, our framework evaluates agents that interact with real
tools, including web browsers, code execution environments, file systems, bash
shells, and messaging platforms; and supports over 350 multi-turn, multi-user
tasks spanning both benign and adversarial user intents. OpenAgentSafety is
designed for extensibility, allowing researchers to add tools, tasks, websites,
and adversarial strategies with minimal effort. It combines rule-based analysis
with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.
Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe
behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%
with o3-mini, highlighting critical safety vulnerabilities and the need for
stronger safeguards before real-world deployment.

</details>


### [77] [The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains](https://arxiv.org/abs/2507.06187)
*Scott Geng,Hamish Ivison,Chun-Liang Li,Maarten Sap,Jerry Li,Ranjay Krishna,Pang Wei Koh*

Main category: cs.AI

TL;DR: 通过配对偏好数据（即使单个数据点较弱）提升语言模型性能，提出delta学习假设，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在强监督数据稀缺时，如何利用弱数据点提升模型性能。

Method: 提出delta学习假设，通过配对弱数据点（如3B和1.5B模型的输出）训练8B模型。

Result: 在11个基准测试中，性能与Tulu 3相当，且成本更低。

Conclusion: delta学习为模型训练提供了一种更简单、更经济的方法，即使数据点较弱也能有效提升性能。

Abstract: Improvements in language models are often driven by improving the quality of
the data we train them on, which can be limiting when strong supervision is
scarce. In this work, we show that paired preference data consisting of
individually weak data points can enable gains beyond the strength of each
individual data point. We formulate the delta learning hypothesis to explain
this phenomenon, positing that the relative quality delta between points
suffices to drive learning via preference tuning--even when supervised
finetuning on the weak data hurts. We validate our hypothesis in controlled
experiments and at scale, where we post-train 8B models on preference data
generated by pairing a small 3B model's responses with outputs from an even
smaller 1.5B model to create a meaningful delta. Strikingly, on a standard
11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the
performance of Tulu 3, a state-of-the-art open model tuned from the same base
model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta
learning enables simpler and cheaper open recipes for state-of-the-art
post-training. To better understand delta learning, we prove in logistic
regression that the performance gap between two weak teacher models provides
useful signal for improving a stronger student. Overall, our work shows that
models can learn surprisingly well from paired data that might typically be
considered weak.

</details>


### [78] [Identifiability in Causal Abstractions: A Hierarchy of Criteria](https://arxiv.org/abs/2507.06213)
*Clément Yvernes,Emilie Devijver,Marianne Clausel,Eric Gaussier*

Main category: cs.AI

TL;DR: 论文探讨了在观测数据中识别治疗效果时，因果图通常需要完全指定，但实际中难以实现。作者提出使用因果抽象（简化的因果表示）来解决这一问题，并研究了在此框架下的可识别性标准。


<details>
  <summary>Details</summary>
Motivation: 在复杂或高维环境中，完全指定的因果图往往未知，限制了因果效应的识别。因果抽象提供了一种部分保留因果信息的简化方法。

Method: 作者形式化了因果抽象的集合，并提出了多个可识别性标准，将其组织成层次结构以明确关系。

Result: 通过层次化的可识别性标准，论文展示了在不同因果知识水平下可以识别的内容。

Conclusion: 该框架为因果效应的识别提供了更灵活的工具，尤其是在缺乏完整因果知识的情况下。

Abstract: Identifying the effect of a treatment from observational data typically
requires assuming a fully specified causal diagram. However, such diagrams are
rarely known in practice, especially in complex or high-dimensional settings.
To overcome this limitation, recent works have explored the use of causal
abstractions-simplified representations that retain partial causal information.
In this paper, we consider causal abstractions formalized as collections of
causal diagrams, and focus on the identifiability of causal queries within such
collections. We introduce and formalize several identifiability criteria under
this setting. Our main contribution is to organize these criteria into a
structured hierarchy, highlighting their relationships. This hierarchical view
enables a clearer understanding of what can be identified under varying levels
of causal knowledge. We illustrate our framework through examples from the
literature and provide tools to reason about identifiability when full causal
knowledge is unavailable.

</details>


### [79] [Aligned Textual Scoring Rules](https://arxiv.org/abs/2507.06221)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Michael J. Curry*

Main category: cs.AI

TL;DR: 本文提出了一种对齐评分规则（ASR），通过优化均方误差来对齐人类偏好，同时保持评分规则的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的正确评分规则在文本信息获取中不一定与人类偏好一致，因此需要设计一种既能保持正确性又能对齐人类偏好的评分规则。

Method: 设计ASR，通过最小化正确评分规则与参考评分（如人类评分）之间的均方误差来优化对齐性。

Result: 实验表明，ASR在保持正确性的同时，比之前的方法更符合人类偏好。

Conclusion: ASR是一种有效的评分规则，既能保证正确性，又能更好地对齐人类偏好。

Abstract: Scoring rules elicit probabilistic predictions from a strategic agent by
scoring the prediction against a ground truth state. A scoring rule is proper
if, from the agent's perspective, reporting the true belief maximizes the
expected score. With the development of language models, Wu and Hartline (2024)
proposes a reduction from textual information elicitation to the numerical
(i.e. probabilistic) information elicitation problem, which achieves provable
properness for textual elicitation. However, not all proper scoring rules are
well aligned with human preference over text. Our paper designs the Aligned
Scoring rule (ASR) for text by optimizing and minimizing the mean squared error
between a proper scoring rule and a reference score (e.g. human score). Our
experiments show that our ASR outperforms previous methods in aligning with
human preference while maintaining properness.

</details>
