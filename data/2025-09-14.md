<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: 这篇论文介绍了Python中的glob模块作为一种基础但缺乏文档记载的文件访问工具，通过通配符模式进行文件搜索、筛选和数据导入，支持可扩展的计算工作流。


<details>
  <summary>Details</summary>
Motivation: 解决计算研究中文件模式访问的基础但缺乏文档记载的问题，向研究人员和实践者提供一个简洁的参考指南，帮助他们更好地理解和使用glob模块来支持数据科学、商业分析和人工智能应用。

Method: 通过具体的Python示例和广泛使用的库（如pandas、scikit-learn、matplotlib）来展示glob模块在大规模数据导入、组织数据分析、AI数据集构建以及可复现研究实践中的应用。

Result: 论文展示了glob如何促进高效的文件遍历和与分析管道的集成，并将其定位为可复现研究和数据工程中的方法论基石。

Conclusion: glob模块是Python基础研究工作流中文件模式匹配的默认引用工具，通过基础概念与应用实践的结合，为研究人员和实践者提供了价值。

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [2] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: 本文通过系统映射研究分析了54项关于教育聊天机器人在编程教学中的应用研究，发现Python教学占主导地位，主要关注基础编程概念，采用多种教学方法和架构。


<details>
  <summary>Details</summary>
Motivation: 教育聊天机器人作为编程教学支持工具日益重要，但缺乏对现有研究发展的系统分析，需要了解其开发和应用现状以指导未来工具开发。

Method: 采用系统映射研究(SMS)方法，从3,216篇初始文献中筛选出54项研究，基于五个研究子问题进行分析：聊天机器人类型、编程语言、教育内容、交互模型和应用场景。

Result: 研究发现聊天机器人主要用于Python教学，重点关注基础编程概念，采用多样化的教学方法和技术架构，揭示了文献中的趋势和空白。

Conclusion: 该研究为编程教学新教育工具的开发提供了重要见解，识别了当前研究趋势和需要填补的空白领域。

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [3] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: 这篇论文提出了一种多代理LLM架构GeoJSON Agents，通过函数调用和代码生成两种方法处理空间数据，显著提升了GIS自动化的性能和可扩展性。代码生成方法达到97.14%的准确率，显著超越了通用模型。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在任务自动化和自然语言理解方面取得了显著进步，但缺乏GIS专业知识仍然限制了其在空间数据处理方面的能力。需要一种方案来克服这些限制，提高GIS自动化的性能。

Method: 提出GeoJSON Agents多代理LLM架构，包含任务解析、代理协作和结果集成三个组件。Planner代理将自然语言任务转换为结构化的GeoJSON命令，Worker代理通过函数调用或代码生成执行空间数据处理，最后系统集成多轮执行结果为标准GeoJSON文件。

Result: 在70个不同复杂度任务的实验中，基于函数调用的GeoJSON Agent达到85.71%的准确率，基于代码生成的代理达到97.14%的准确率，都显著超越了最佳通用模型的表现（48.57%）。代码生成方法更灵活，函数调用方法执行更稳定。

Conclusion: 这是首个为GeoJSON数据设计的LLM多代理框架，并对比了两种主流LLM增强方法的优势和限制。为提高GeoAI系统性能提供了新的视角和方法。

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [4] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG是一个基于检索增强生成的框架，通过自然语言查询和Java代码分析实现可解释的Android恶意软件检测，达到96%的检测准确率和83.81%的行为识别准确率。


<details>
  <summary>Details</summary>
Motivation: Android恶意应用采用复杂的规避策略，将恶意逻辑隐藏在合法功能中，传统分析方法难以恢复深度隐藏的行为或提供人类可读的解释。

Method: 使用检索增强生成(RAG)框架，首先生成方法级代码片段的摘要并建立向量索引，然后通过行为导向的问题检索相关代码片段进行深度分析，最终生成包含恶意行为及其代码实现的人类可读报告。

Result: 实验结果显示达到96%的恶意软件检测准确率和83.81%的行为识别准确率，基于更新的VirusTotal扫描和人工验证，专家评估确认了生成报告的实际效用。

Conclusion: TraceRAG框架成功地将自然语言查询与代码分析相结合，为Android恶意软件检测提供了可解释且高效的解决方案，在准确性和实用性方面都表现出色。

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [5] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: 提出了LLM能效基准测试，使用vLLM模拟真实生产场景，评估模型大小、架构和并发请求量对推理能效的影响。


<details>
  <summary>Details</summary>
Motivation: LLM部署和使用消耗大量能源，对气候产生影响，需要收集更多关于LLM能效的信息，现有基准测试往往无法代表真实生产场景。

Method: 使用vLLM（高吞吐量、生产就绪的LLM服务后端）构建能效基准测试，模拟真实使用条件，分析模型大小、架构和并发请求量等因素。

Result: 证明了可以创建更能反映实际部署条件的能效基准测试，为开发者构建更可持续的AI系统提供有价值见解。

Conclusion: LLM能效基准测试能够更好地评估实际生产环境中的能源效率，有助于推动可持续AI系统的发展。

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [6] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA是一个基于先进推理模型的浏览器扩展工具，帮助开发者和研究人员进行代码理解、重构和质量检测，通过用户研究证明其实用性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有代码分析工具需要项目设置、缺乏上下文感知且需要大量手动操作，CLARA旨在解决这些问题

Method: 开发浏览器扩展工具，使用最先进的推理模型，通过定性评估和10名开发者的用户研究来验证

Result: CLARA在代码理解和分析任务中表现出有用性、准确性和实用性

Conclusion: CLARA是一个开源工具，为代码理解和分析提供了有效的解决方案

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [7] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: 这篇论文提出了ReDef数据集，通过revert commits和GPT辅助筛选构建了高信度的软件缺陷预测数据集，并系统评估了预训练语言模型在代码修改理解方面的能力和局限性。


<details>
  <summary>Details</summary>
Motivation: 解决现有JIT-SDP数据集标签噪声大、精确度低的问题，以及探索预训练语言模型是否真正理解代码修改语义。

Method: 通过revert commits标记缺陷修改，历史检查验证清洁修改，GPT辅助多次投票审核筛选模糊实例，构建高信度ReDef数据集。对CodeBERT、CodeT5+、UniXcoder进行五种编码策略微调，并通过变换添加/删除块、反转diff构建反事实干扰来探针模型敏感性。

Result: 构建了3,164个缺陷修改和10,268个清洁修改的高信度数据集。紧凑diff格式编码在所有PLM中都明显优于整体函数格式，但反事实干扰实验显示模型依赖表面线索而非真正语义理解。

Conclusion: 当前预训练语言模型在理解代码修改方面能力有限，虽然在标准测试中表现良好，但实际上依赖表面特征而非深层语义理解。

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [8] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: 这篇论文提出了一种将大语言模型与传统软件工程技术结合的方法论，通过场景基于编程范式来提高LLM在软件开发中的可靠性和可验证性


<details>
  <summary>Details</summary>
Motivation: 虽然LLM能大大缩短开发时间并生成质量较高的代码，但它们经常会导入重大错误且以说服力强的信心呈现错误代码，容易误导开发者

Method: 采用场景基于编程(SBP)范式，这是一种事件驱动的软件工程方法，允许开发者将专业知识注入LLM，并检查和验证其输出

Result: 通过Connect4游戏案例研究验证，组合LLM和SBP能够创建强大的继续机器人，能够击败现有的强大继续机器者，并在某些情况下实现了形式化验证

Conclusion: 该方法论能够在保持LLM优势的同时提高代码质量和可靠性，为LLM在软件开发周期中的更可靠集成提供了有效途径

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [9] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 首次大规模研究Git公开仓库历史修改行为，发现1.22M仓库存在8.7M个重写历史，包括许可证和秘密移除等问题，并提供自动化检测工具GitHistorian。


<details>
  <summary>Details</summary>
Motivation: 调查Git公共分支中的历史修改行为，这些修改对下游用户造成推拉流程突破、仓库完整性风险和供应链攻击漏洞。

Method: 分析Software Heritage归档的111M个仓库，识别和分类重写历史行为，进行两个目标案例研究，开发GitHistorian自动化检测工具。

Result: 在1.22M仓库中发现8.7M个重写历史，包括许可证违规更改、密码秘密移除等风险行为，显示了项目管理和安全管理中的不良实践。

Conclusion: 公共Git仓库中的历史修改带来重大风险，GitHistorian工具能够帮助开发者识别和避免这些风险。

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [10] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: 研究评估了CodeBERT在产业和开源软件中的漏洞检测性能，开发了集成到CI/CD流程的AI-DO推荐系统，通过调查验证了工具的实用性。


<details>
  <summary>Details</summary>
Motivation: 深度学习漏洞检测技术在产业环境中遇到了可访问性、信任度、系统集成和工作流程等挑战，需要研究如何将学术研究转化为可应用于产业的解决方案。

Method: 首先评估CodeBERT在产业和开源软件中的漏洞检测性能，分析跨域汇总能力，探索类不平衡处理策略。然后开发AI-DO系统，将精调的CodeBERT集成到CI/CD流程中，并通过调查评估业务专家的感知有用性。

Result: 结果显示：基于产业数据训练的模型在同域内检测准确，但在开源代码上性能下降；基于开源数据精调的深度学习模型经适当的下采样处理后能提高漏洞检测能力。

Conclusion: 研究成功将学术研究转化为可集成到产业工作流程中的实用工具，为漏洞检测技术的实际应用提供了有价值的见解和方法。

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [11] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: 这篇论文研究了容器安全分析工具在遇到隐藏/不完整容器时的限制，提出了一种具有弹性的分析方法ORCA，能够显著提高对隐藏容器的文件覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发依赖开源库和第三方组件，容器化环境带来安全风险。当容器文件系统发生无意修改时，会导致不完整的容器镜像，影响SCA工具的可靠性。

Method: 分析了600个流行容器，研究云端和开源SCA工具的限制，提出了一种具有弹性的容器分析方法，并开发了开源实现ORCA。

Result: 发现隐藏容器存在于知名注册表和可信镜像中，许多工具无法分析这种容器。ORCA能够有效检测隐藏容器的内容，与Docker Scout和Syft相比文件覆盖率提高了40%。

Conclusion: 论文提出的ORCA方法能够有效解决当前SCA工具在分析隐藏容器时的限制，显著提高了容器安全分析的可靠性和效果。

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [12] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench是一个专门评估长上下文LLM在复杂软件开发场景中表现的基准测试，包含8000个评估场景、10种编程语言、10K到1M tokens的上下文长度，涵盖8个任务类别和17个评估指标。


<details>
  <summary>Details</summary>
Motivation: 随着支持百万tokens上下文窗口的长上下文语言模型出现，需要专门的基准测试来评估这些模型在真实复杂软件开发场景中的能力，填补现有代码评估基准在长上下文能力评估方面的空白。

Method: 通过5阶段流水线系统生成多样化的高质量评估场景，涵盖10种编程语言，上下文长度从10K到1M tokens，包含8个任务类别：架构理解、跨文件重构、多会话开发等。

Result: 对最先进的长上下文模型评估显示存在显著的性能差距，表明复杂软件开发中的长上下文理解仍然是一个重大未解决的挑战。

Conclusion: LoCoBench揭示了长上下文LLM在复杂软件开发中的局限性，为该领域的研究提供了重要的评估工具和基准，表明需要更多关注长上下文理解能力的提升。

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [13] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: SmartDetector是一个新颖的智能合约函数相似性检测方法，通过将AST分解为语句树进行细粒度语义比较，在三个真实数据集上平均F1分数达到95.88%，比现有方法提升14.01%。


<details>
  <summary>Details</summary>
Motivation: 智能合约开发中开源代码的广泛重用虽然提高了编程效率，但显著放大了漏洞传播风险。现有的AST方法难以处理复杂树结构，而深度学习方法又忽视了代码语法和可解释性。

Method: 将智能合约函数的AST分解为一系列较小的语句树，使用分类器通过比较语句树对来计算相似性分数，并采用余弦扩散过程高效搜索最优超参数。

Result: 在三个大型真实数据集上的实验表明，SmartDetector平均F1分数达到95.88%，比现有最先进方法平均提升14.01%。

Conclusion: SmartDetector通过细粒度语句级比较有效解决了智能合约函数相似性检测问题，在性能和可解释性方面都表现出色。

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [14] [Cross-Service Token: Finding Attacks in 5G Core Networks](https://arxiv.org/abs/2509.08992)
*Anqi Chen,Riccardo Preatoni,Alessandro Brighente,Mauro Conti,Cristina Nita-Rotaru*

Main category: cs.CR

TL;DR: FivGeeFuzz是一个基于语法的模糊测试框架，用于发现5G核心网络中服务化接口的安全漏洞，在free5GC中发现了8个未知漏洞


<details>
  <summary>Details</summary>
Motivation: 5G核心网络采用服务化架构，网络功能通过HTTP API通信，云环境面临内部威胁增加的风险，需要研究安全漏洞以防止攻击者利用受损网络功能获取未授权资源访问

Method: 从3GPP API规范自动推导语法生成畸形、意外或语义不一致的输入，集成自动化bug检测与手动验证和根本原因分析

Result: 在free5GC中发现8个先前未知的漏洞，导致运行时崩溃、错误处理不当和未授权资源访问，包括严重的跨服务令牌攻击

Conclusion: FivGeeFuzz框架有效发现了5G核心网络服务化接口的安全漏洞，所有漏洞都得到确认，大部分已修复

Abstract: 5G marks a major departure from previous cellular architectures, by
transitioning from a monolithic design of the core network to a Service-Based
Architecture (SBA) where services are modularized as Network Functions (NFs)
which communicate with each other via standard-defined HTTP-based APIs called
Service-Based Interfaces (SBIs). These NFs are deployed in private and public
cloud infrastructure, and an access control framework based on OAuth restricts
how they communicate with each other and obtain access to resources. Given the
increased vulnerabilities of clouds to insiders, it is important to study the
security of the 5G Core services for vulnerabilities that allow attackers to
use compromised NFs to obtain unauthorized access to resources.
  We present FivGeeFuzz, a grammar-based fuzzing framework designed to uncover
security flaws in 5G core SBIs. FivGeeFuzz automatically derives grammars from
3GPP API specifications to generate malformed, unexpected, or semantically
inconsistent inputs, and it integrates automated bug detection with manual
validation and root-cause analysis. We evaluate our approach on free5GC, the
only open-source 5G core implementing Release 17-compliant SBIs with an access
control mechanism. Using FivGeeFuzz, we discovered 8 previously unknown
vulnerabilities in free5GC, leading to runtime crashes, improper error
handling, and unauthorized access to resources, including a very severe attack
we call Cross-Service Token Attack. All bugs were confirmed by the free5GC
team, 7 have already been patched, and the remaining one has a patch under
development.

</details>


### [15] [When FinTech Meets Privacy: Securing Financial LLMs with Differential Private Fine-Tuning](https://arxiv.org/abs/2509.08995)
*Sichen Zhu,Hoyeung Leung,Xiaoyi Wang,Jia Wei,Honghui Xu*

Main category: cs.CR

TL;DR: DPFinLLM是一个专为金融应用设计的轻量级隐私保护大语言模型，结合差分隐私机制和精简架构，在保护敏感金融数据隐私的同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在边缘设备金融应用中的部署增加，保护敏感金融数据的隐私成为重要挑战，需要开发既能保护隐私又高效的解决方案。

Method: 提出DPFinLLM模型，结合强大的差分隐私机制和受先进模型启发的精简架构，专门用于设备端金融应用的安全高效数据处理。

Result: 在多个金融情感数据集上的广泛实验验证了DPFinLLM的有效性，即使在严格隐私约束下也能达到与完全微调模型相当的性能。

Conclusion: DPFinLLM能够有效保护用户数据免受隐私泄露，同时在各种金融任务中确保高性能，为设备端金融AI应用提供了可行的隐私保护解决方案。

Abstract: The integration of Large Language Models (LLMs) into financial technology
(FinTech) has revolutionized the analysis and processing of complex financial
data, driving advancements in real-time decision-making and analytics. With the
growing trend of deploying AI models on edge devices for financial
applications, ensuring the privacy of sensitive financial data has become a
significant challenge. To address this, we propose DPFinLLM, a
privacy-enhanced, lightweight LLM specifically designed for on-device financial
applications. DPFinLLM combines a robust differential privacy mechanism with a
streamlined architecture inspired by state-of-the-art models, enabling secure
and efficient processing of financial data. This proposed DPFinLLM can not only
safeguard user data from privacy breaches but also ensure high performance
across diverse financial tasks. Extensive experiments on multiple financial
sentiment datasets validate the effectiveness of DPFinLLM, demonstrating its
ability to achieve performance comparable to fully fine-tuned models, even
under strict privacy constraints.

</details>


### [16] [Beyond Tag Collision: Cluster-based Memory Management for Tag-based Sanitizers](https://arxiv.org/abs/2509.09089)
*Mengfei Xie,Yan Lin,Hongtao Wu,Jianming Fu,Chenke Luo,Guojun Peng*

Main category: cs.CR

TL;DR: ClusterTag是一种基于集群的内存分配器，通过将内存对象分组到独立集群中并采用集群级堆随机化，有效缓解了标签式消毒器中标签碰撞问题，在保持低性能开销的同时提供确定性安全检测结果。


<details>
  <summary>Details</summary>
Motivation: 标签式消毒器使用有限的标签编码空间来验证指针-对象一致性，但现有方法在时间和空间维度上难以分配唯一标签，导致标签碰撞和潜在的内存违规检测失败。

Method: ClusterTag将内存对象划分为多个独立集群，限制标签碰撞范围；设计集群级堆随机化方案，在集群间引入随机地址间隔，打破标签空间的熵限制。

Result: 在Juliet数据集上的安全评估显示，ClusterTag在500次重复测试中表现确定性结果（报告5652个漏洞，遗漏1530个），而现有标签分配策略都存在概率性假阴性。在最小、平均和不可预测性三个碰撞距离指标上均实现平衡改进。

Conclusion: ClusterTag通过集群化内存分配和随机化策略，有效解决了标签式消毒器的标签碰撞问题，在保持低性能开销（1%以内）的同时提供了更可靠的内存安全检测。

Abstract: Tag-based sanitizers attach a small "key" to each pointer and a matching
"lock" tag to its target memory object, enabling runtime verification of
pointer-object consistency and helping developers to detect potential memory
violations. However, the limited tag encoding space challenges existing studies
in assigning distinct tags to memory objects across temporal and spatial
dimensions, leading to potential tag collisions. In this paper, we present
ClusterTag, a novel cluster-based memory allocator aimed at simultaneously
mitigating tag collisions in both temporal and spatial dimensions. The core
design of ClusterTag effectively balances the significant mismatch between tag
encoding space and memory objects: it divides memory objects into multiple
independent clusters, thereby limiting tag collisions to finite chunks within
each cluster. To mitigate tag collisions across clusters, we design a
cluster-grained heap randomization scheme. This approach introduces random
address intervals between clusters and further breaks the entropy limitation of
the tag space. ClusterTag has been implemented as an independent memory
allocator that seamlessly integrates with tag-based sanitizers such as HWASan,
and maintains comparable performance overhead (within 1%) at various
randomization densities. Security evaluations on the Juliet dataset indicate
that ClusterTag exhibits deterministic results across 500 repeated tests (5,652
reported and 1,530 missed), while the existing three types of tag assignment
strategies all exhibit probabilistic false negatives due to tag collisions.
Quantitative analysis across three tag collision distance metrics-minimum,
average, and unpredictability-demonstrates that ClusterTag achieves balanced
improvements across all three, whereas prior tag assignment schemes (random,
staggered, fixed) show significant trade-offs in at least one metric.

</details>


### [17] [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)
*Honglan Yu,Yibin Wang,Feifei Dai,Dong Liu,Haihui Fan,Xiaoyan Gu*

Main category: cs.CR

TL;DR: CMIF是一个保护隐私的高效模型推理框架，通过在客户端TEE中部署嵌入层，在GPU服务器上运行后续层，并优化Report-Noisy-Max机制，在降低TEE推理开销的同时保护用户数据隐私。


<details>
  <summary>Details</summary>
Motivation: 解决TEE中高推理延迟和DP方法影响LLM性能的问题，传统分区方法在处理LLM密集非线性层时产生显著通信开销，而DP方法添加随机噪声会损害模型性能和语义理解。

Method: 在客户端TEE中机密部署嵌入层，后续层部署在GPU服务器上；优化Report-Noisy-Max机制以保护敏感输入；在Llama系列模型上进行广泛实验验证。

Result: CMIF显著减少了TEE中的额外推理开销，同时有效保护了用户数据隐私，仅对模型性能造成轻微影响。

Conclusion: CMIF框架成功解决了TEE和DP方法在LLM隐私保护推理中的局限性，实现了机密性和效率的平衡，为大规模语言模型的隐私保护推理提供了有效解决方案。

Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.

</details>


### [18] [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097)
*Honghui Xu,Shiva Shrestha,Wei Chen,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: DP-FedLoRA是一个隐私增强的联邦微调框架，结合LoRA适配和差分隐私技术，在边缘设备上实现高效通信的隐私保护LLM微调。


<details>
  <summary>Details</summary>
Motivation: 随着设备端大语言模型系统的普及，联邦微调需要处理敏感的用户特定数据，在联邦学习框架中存在显著的隐私担忧。

Method: 每个客户端使用高斯噪声对LoRA矩阵进行本地裁剪和扰动，满足(ε,δ)-差分隐私要求，并提供理论分析证明更新的无偏性和噪声引入的方差界限。

Result: 在主流基准测试中，DP-FedLoRA在提供强大隐私保证的同时，展现出具有竞争力的性能表现。

Conclusion: 该框架为在设备端环境中实现可扩展和隐私保护的LLM部署铺平了道路。

Abstract: As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.

</details>


### [19] [AgriSentinel: Privacy-Enhanced Embedded-LLM Crop Disease Alerting System](https://arxiv.org/abs/2509.09103)
*Chanti Raju Mylay,Bobin Deng,Zhipeng Cai,Honghui Xu*

Main category: cs.CR

TL;DR: AgriSentinel是首个隐私增强的嵌入式LLM作物病害预警系统，通过差分隐私保护作物图像数据，轻量级深度学习模型实现移动端病害分类，集成LLM提供具体管理建议。


<details>
  <summary>Details</summary>
Motivation: 现有作物病害预警系统忽视数据隐私、市场价格权力和农民友好性，使农民面临隐私泄露和经济剥削风险，需要开发更全面的解决方案。

Method: 采用差分隐私机制保护敏感数据，开发轻量级深度学习病害分类模型优化移动设备，集成微调的本地大语言模型提供具体管理建议。

Result: 实验验证系统能有效保护数据隐私，保持高分类性能，并提供实用的病害管理策略。

Conclusion: AgriSentinel为自动化作物病害预警和管理提供了强大、农民友好的解决方案，有助于改善农业决策和提高作物生产力。

Abstract: Crop diseases pose significant threats to global food security, agricultural
productivity, and sustainable farming practices, directly affecting farmers'
livelihoods and economic stability. To address the growing need for effective
crop disease management, AI-based disease alerting systems have emerged as
promising tools by providing early detection and actionable insights for timely
intervention. However, existing systems often overlook critical aspects such as
data privacy, market pricing power, and farmer-friendly usability, leaving
farmers vulnerable to privacy breaches and economic exploitation. To bridge
these gaps, we propose AgriSentinel, the first Privacy-Enhanced Embedded-LLM
Crop Disease Alerting System. AgriSentinel incorporates a differential privacy
mechanism to protect sensitive crop image data while maintaining classification
accuracy. Its lightweight deep learning-based crop disease classification model
is optimized for mobile devices, ensuring accessibility and usability for
farmers. Additionally, the system includes a fine-tuned, on-device large
language model (LLM) that leverages a curated knowledge pool to provide farmers
with specific, actionable suggestions for managing crop diseases, going beyond
simple alerting. Comprehensive experiments validate the effectiveness of
AgriSentinel, demonstrating its ability to safeguard data privacy, maintain
high classification performance, and deliver practical, actionable disease
management strategies. AgriSentinel offers a robust, farmer-friendly solution
for automating crop disease alerting and management, ultimately contributing to
improved agricultural decision-making and enhanced crop productivity.

</details>


### [20] [CryptGNN: Enabling Secure Inference for Graph Neural Networks](https://arxiv.org/abs/2509.09107)
*Pritam Sen,Yao Ma,Cristian Borcea*

Main category: cs.CR

TL;DR: CryptGNN是一个基于安全多方计算的图神经网络安全推理方案，保护客户端数据和模型参数隐私


<details>
  <summary>Details</summary>
Motivation: 解决云环境中第三方GNN模型推理时的隐私保护问题，防止云服务提供商和模型所有者获取敏感数据

Method: 使用分布式安全多方计算技术实现安全消息传递和特征变换层，支持任意数量的SMPC参与方，无需可信服务器

Result: 理论分析和实验证明CryptGNN具有安全性和高效性，即使P-1个云参与方合谋也能保证安全

Conclusion: CryptGNN为MLaaS场景下的GNN推理提供了有效的隐私保护解决方案

Abstract: We present CryptGNN, a secure and effective inference solution for
third-party graph neural network (GNN) models in the cloud, which are accessed
by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its
secure message passing and feature transformation layers using distributed
secure multi-party computation (SMPC) techniques. CryptGNN protects the
client's input data and graph structure from the cloud provider and the
third-party model owner, and it protects the model parameters from the cloud
provider and the clients. CryptGNN works with any number of SMPC parties, does
not require a trusted server, and is provably secure even if P-1 out of P
parties in the cloud collude. Theoretical analysis and empirical experiments
demonstrate the security and efficiency of CryptGNN.

</details>


### [21] [Character-Level Perturbations Disrupt LLM Watermarks](https://arxiv.org/abs/2509.09112)
*Zhaoxi Zhang,Xiaomei Zhang,Yanjun Zhang,He Zhang,Shirui Pan,Bo Liu,Asif Qumer Gill,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 这篇论文探讨了LLM水印消除的强效方法，发现字符级批力比词级批力更有效，并提出基于遗传算法的指导性攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究评估水印消除的方法往往不优，造成了需要大规模批力或强大攻击者才能有效消除的误解。本文希望找到更有效的消除方法。

Method: 首先形式化LLM水印系统模型，定义两种实际威胁模型。分析不同类型批力的攻击范围，发现字符级批力通过打断标记化过程可以同时影响多个标记。提出基于遗传算法的指导性消除攻击。

Result: 字符级批力在最严格的威胁模型下显示出显著更高的消除效果。在有限的黑盒查询条件下，遗传算法方法展现出强大的消除性能。还提出了能够穿透固定防御的适应性复合攻击。

Conclusion: 现有LLM水印方案存在重大漏洞，字符级攻击方法显示出超越传统方法的效果。研究结果强调了开发更稳健水印机制的紧迫性。

Abstract: Large Language Model (LLM) watermarking embeds detectable signals into
generated text for copyright protection, misuse prevention, and content
detection. While prior studies evaluate robustness using watermark removal
attacks, these methods are often suboptimal, creating the misconception that
effective removal requires large perturbations or powerful adversaries.
  To bridge the gap, we first formalize the system model for LLM watermark, and
characterize two realistic threat models constrained on limited access to the
watermark detector. We then analyze how different types of perturbation vary in
their attack range, i.e., the number of tokens they can affect with a single
edit. We observe that character-level perturbations (e.g., typos, swaps,
deletions, homoglyphs) can influence multiple tokens simultaneously by
disrupting the tokenization process. We demonstrate that character-level
perturbations are significantly more effective for watermark removal under the
most restrictive threat model. We further propose guided removal attacks based
on the Genetic Algorithm (GA) that uses a reference detector for optimization.
Under a practical threat model with limited black-box queries to the watermark
detector, our method demonstrates strong removal performance. Experiments
confirm the superiority of character-level perturbations and the effectiveness
of the GA in removing watermarks under realistic constraints. Additionally, we
argue there is an adversarial dilemma when considering potential defenses: any
fixed defense can be bypassed by a suitable perturbation strategy. Motivated by
this principle, we propose an adaptive compound character-level attack.
Experimental results show that this approach can effectively defeat the
defenses. Our findings highlight significant vulnerabilities in existing LLM
watermark schemes and underline the urgency for the development of new robust
mechanisms.

</details>


### [22] [IoTFuzzSentry: A Protocol Guided Mutation Based Fuzzer for Automatic Vulnerability Testing in Commercial IoT Devices](https://arxiv.org/abs/2509.09158)
*Priyanka Rushikesh Chaudhary,Rajib Ranjan Maiti*

Main category: cs.CR

TL;DR: 本文提出了IoTFuzzSentry协议模糊测试工具，用于发现物联网设备中的安全漏洞，在商业设备中发现了4类漏洞并公开了2个CVE编号


<details>
  <summary>Details</summary>
Motivation: 物联网设备在运行阶段常运行轻量级服务器处理用户交互，传输层或应用层安全机制的实现缺陷可能导致未经授权访问和数据泄露等威胁

Method: 开发了基于变异的模糊测试工具IoTFuzzSentry，集成到Cotopaxi测试工具中，向物联网通信注入精心构造的传输层和应用层数据包

Result: 在商用物联网设备中发现了4类漏洞（凭证泄露、视频流窃取、图像窃取、命令注入），公开了2个CVE编号，证明6个其他设备也存在类似漏洞

Conclusion: IoTFuzzSentry能够发现非常规安全威胁，帮助物联网厂商以可忽略的开销自动加强商业化设备的安全性

Abstract: Protocol fuzzing is a scalable and cost-effective technique for identifying
security vulnerabilities in deployed Internet of Things devices. During their
operational phase, IoT devices often run lightweight servers to handle user
interactions, such as video streaming or image capture in smart cameras.
Implementation flaws in transport or application-layer security mechanisms can
expose IoT devices to a range of threats, including unauthorized access and
data leakage. This paper addresses the challenge of uncovering such
vulnerabilities by leveraging protocol fuzzing techniques that inject crafted
transport and application-layer packets into IoT communications. We present a
mutation-based fuzzing tool, named IoTFuzzSentry, to identify specific
non-trivial vulnerabilities in commercial IoT devices. We further demonstrate
how these vulnerabilities can be exploited in real-world scenarios. We
integrated our fuzzing tool into a well-known testing tool Cotopaxi and
evaluated it with commercial-off-the-shelf IoT devices such as IP cameras and
Smart Plug. Our evaluation revealed vulnerabilities categorized into 4 types
(IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live
Image, IoT Command Injection) and we show their exploits using three IoT
devices. We have responsibly disclosed all these vulnerabilities to the
respective vendors. So far, we have published two CVEs, CVE-2024-41623 and
CVE-2024-42531, and one is awaiting. To extend the applicability, we have
investigated the traffic of six additional IoT devices and our analysis shows
that these devices can have similar vulnerabilities, due to the presence of a
similar set of application protocols. We believe that IoTFuzzSentry has the
potential to discover unconventional security threats and allow IoT vendors to
strengthen the security of their commercialized IoT devices automatically with
negligible overhead.

</details>


### [23] [Enhancing Cyber Threat Hunting -- A Visual Approach with the Forensic Visualization Toolkit](https://arxiv.org/abs/2509.09185)
*Jihane Najar,Marinos Tsantekidis,Aris Sotiropoulos,Vassilis Prevelakis*

Main category: cs.CR

TL;DR: 提出了Forensic Visualization Toolkit (FVT)工具，用于数字取证调查和网络安全威胁狩猎，通过高级可视化增强安全态势感知。


<details>
  <summary>Details</summary>
Motivation: 在动态网络威胁环境中，传统安全措施可能无法检测到高级威胁，需要主动的威胁狩猎方法来识别和缓解这些威胁。

Method: 开发了Forensic Visualization Toolkit工具，提供数字证据分析和交互式可视化功能，支持网络安全专业人员进行主动威胁检测。

Result: FVT工具已集成到多个欧盟资助的研究项目中，并通过实际场景验证了其能够显著增强网络安全专业人员识别、分析和响应威胁的能力。

Conclusion: FVT是一个强大的数字取证和威胁狩猎工具，通过直观的交互式可视化界面，有效提升了网络安全防御能力。

Abstract: In today's dynamic cyber threat landscape, organizations must take proactive
steps to bolster their cybersecurity defenses. Cyber threat hunting is a
proactive and iterative process aimed at identifying and mitigating advanced
threats that may go undetected by traditional security measures. Rather than
waiting for automated security systems to flag potential threats, threat
hunting involves actively searching for signs of malicious activity within an
organization's network. In this paper, we present the Forensic Visualization
Toolkit, a powerful tool designed for digital forensics investigations,
analysis of digital evidence, and advanced visualizations to enhance
cybersecurity situational awareness and risk management and empower security
analysts with an intuitive and interactive tool. Through practical, real-world
scenarios, we demonstrate how FVT significantly amplifies the capabilities of
cybersecurity professionals, enabling them to effectively identify, analyze,
and respond to threats. Furthermore, it is important to highlight that FVT has
been integrated into, utilized, and continually enhanced within various
EU-funded research projects over recent years.

</details>


### [24] [Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for Automated Penetration Testing](https://arxiv.org/abs/2509.09207)
*Wuyuao Mai,Geng Hong,Qi Liu,Jinsong Chen,Jiarun Dai,Xudong Pan,Yuan Zhang,Min Yang*

Main category: cs.CR

TL;DR: 提出了第一个真实世界渗透测试基准TermiBench和新型多智能体框架TermiAgent，解决了传统AI渗透测试在简化CTF环境中评估不准确的问题，在真实条件下显著提升了渗透测试能力。


<details>
  <summary>Details</summary>
Motivation: 传统渗透测试成本高、耗时长且依赖专家人力，现有AI驱动的渗透测试代理评估基于过度简化的CTF环境，性能估计远离真实世界实践，需要建立真实世界的基准测试框架。

Method: 引入TermiBench基准测试（510个主机、25种服务、30个CVE），提出TermiAgent多智能体框架，采用定位记忆激活机制解决长上下文遗忘问题，通过结构化代码理解构建可靠的漏洞利用库。

Result: 现有系统在真实条件下几乎无法获得系统shell，TermiAgent在评估中优于最先进代理，表现出更强的渗透测试能力，减少执行时间和财务成本，甚至在笔记本电脑规模部署中也具有实用性。

Conclusion: 该工作提供了第一个开源的真实世界自主渗透测试基准和新型智能体框架，为AI驱动的渗透测试建立了里程碑，解决了现有方法的局限性并展示了实际应用价值。

Abstract: Penetration testing is critical for identifying and mitigating security
vulnerabilities, yet traditional approaches remain expensive, time-consuming,
and dependent on expert human labor. Recent work has explored AI-driven
pentesting agents, but their evaluation relies on oversimplified
capture-the-flag (CTF) settings that embed prior knowledge and reduce
complexity, leading to performance estimates far from real-world practice. We
close this gap by introducing the first real-world, agent-oriented pentesting
benchmark, TermiBench, which shifts the goal from 'flag finding' to achieving
full system control. The benchmark spans 510 hosts across 25 services and 30
CVEs, with realistic environments that require autonomous reconnaissance,
discrimination between benign and exploitable services, and robust exploit
execution. Using this benchmark, we find that existing systems can hardly
obtain system shells under realistic conditions.
  To address these challenges, we propose TermiAgent, a multi-agent penetration
testing framework. TermiAgent mitigates long-context forgetting with a Located
Memory Activation mechanism and builds a reliable exploit arsenal via
structured code understanding rather than naive retrieval. In evaluations, our
work outperforms state-of-the-art agents, exhibiting stronger penetration
testing capability, reducing execution time and financial cost, and
demonstrating practicality even on laptop-scale deployments. Our work delivers
both the first open-source benchmark for real-world autonomous pentesting and a
novel agent framework that establishes a milestone for AI-driven penetration
testing.

</details>


### [25] [A Cyber-Twin Based Honeypot for Gathering Threat Intelligence](https://arxiv.org/abs/2509.09222)
*Muhammad Azmi Umer,Zhan Xuna,Yan Lin Aung,Aditya P. Mathur,Jianying Zhou*

Main category: cs.CR

TL;DR: 基于水处理厂网络双生的蜘蛋系统，用于吸引和分析网络攻击，为关键基础设施提供威胁情报


<details>
  <summary>Details</summary>
Motivation: 关键基础设施容易受到网络攻击，需要有效的防护技术，通过蜘蛋系统获取攻击情报以提升防护能力

Method: 开发了一个基于水处理厂网络双生的蜘蛋系统，作为实际水厂的实际复制品，用于吸引潜在攻击者

Result: 该蜘蛋系统已经运行并多次受到攻击，包括详细描述的劫持病毒攻击，攻击行为被记录和分析

Conclusion: 通过网络双生蜘蛋系统可以有效收集攻击情报，分享给水处理厂管理方以改善防护系统，提高关键基础设施的安全性

Abstract: Critical Infrastructure (CI) is prone to cyberattacks. Several techniques
have been developed to protect CI against such attacks. In this work, we
describe a honeypot based on a cyber twin for a water treatment plant. The
honeypot is intended to serve as a realistic replica of a water treatment plant
that attracts potential attackers. The attacks launched on the honeypot are
recorded and analyzed for threat intelligence. The intelligence so obtained is
shared with the management of water treatment plants, who in turn may use it to
improve plant protection systems. The honeypot used here is operational and has
been attacked on several occasions using, for example, a ransomware attack that
is described in detail.

</details>


### [26] [What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection](https://arxiv.org/abs/2509.09291)
*Biwei Yan,Yue Zhang,Minghui Xu,Runyu Pan,Jinku Li,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: VerifiaBLE系统利用LLM将BLE应用代码转换为可验证的形式化模型，实现对Android BLE应用的大规模安全验证，发现仅有10.2%的应用完整实现了加密、随机性和认证三项核心保护


<details>
  <summary>Details</summary>
Motivation: BLE应用层安全漏洞日益增多，开发者经常忽视加密、认证和新鲜性等关键保护措施。传统形式化验证需要手动构建模型，难以进行大规模分析

Method: 将BLE应用安全分析重构为语义翻译问题，使用LLM作为翻译器将BLE特定代码转换为ProVerif等工具可验证的进程模型。结合静态分析、提示引导的LLM翻译和符号验证来检查加密、随机性和认证三个核心安全特性

Result: 在1,050个Android BLE应用中，仅10.2%的应用实现了所有三项保护，53.9%的应用完全忽略了这些保护措施

Conclusion: 使用LLM作为结构化翻译器可以降低形式化方法的门槛，在安全关键领域实现可扩展的验证

Abstract: The application layer of Bluetooth Low Energy (BLE) is a growing source of
security vulnerabilities, as developers often neglect to implement critical
protections such as encryption, authentication, and freshness. While formal
verification offers a principled way to check these properties, the manual
effort of constructing formal models makes it impractical for large-scale
analysis. This paper introduces a key insight: BLE application security
analysis can be reframed as a semantic translation problem, i.e., from
real-world code to formal models. We leverage large language models (LLMs) not
to directly detect vulnerabilities, but to serve as translators that convert
BLE-specific code into process models verifiable by tools like ProVerif. We
implement this idea in VerifiaBLE, a system that combines static analysis,
prompt-guided LLM translation, and symbolic verification to check three core
security features: encryption, randomness, and authentication. Applied to 1,050
Android BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\% of apps
implement all three protections, while 53.9\% omit them entirely. Our work
demonstrates that using LLMs as structured translators can lower the barrier to
formal methods, unlocking scalable verification across security-critical
domains.

</details>


### [27] [On the Security of SSH Client Signatures](https://arxiv.org/abs/2509.09331)
*Fabian Bäumer,Marcus Brinkmann,Maximilian Radoy,Jörg Schwenk,Juraj Somorovsky*

Main category: cs.CR

TL;DR: 该论文通过收集和分析SSH客户端公钥，首次系统性地研究了SSH客户端密钥的安全状况，发现了包括弱随机性、确定性nonce漏洞在内的多个安全问题，其中PuTTY的ECDSA签名漏洞被分配了CVE编号。


<details>
  <summary>Details</summary>
Motivation: SSH客户端密钥被广泛用于认证，但相比于服务器，客户端的安全性难以通过互联网扫描来评估。研究旨在填补这一空白，系统分析SSH客户端密钥的安全状况。

Method: 1) 从GitHub、GitLab等开放开发平台收集SSH客户端公钥进行纵向安全测试；2) 对24个流行SSH客户端进行黑盒实验，分析其签名算法实现。

Result: 收集了31,622,338个密钥，发现RSA签名向EdDSA迁移的趋势。检测到98个短密钥、139个弱随机性生成的密钥、149个有公因子的密钥。首次发现PuTTY ECDSA确定性nonce漏洞，仅需58个有效签名即可恢复私钥(CVE-2024-31497)。

Conclusion: 虽然大多数SSH客户端密钥是安全的，但仍存在显著的安全问题，特别是弱随机性和确定性nonce生成算法漏洞。研究推动了PuTTY等客户端修复关键安全漏洞。

Abstract: Administrators and developers use SSH client keys and signatures for
authentication, for example, to access internet backbone servers or to commit
new code on platforms like GitHub. However, unlike servers, SSH clients cannot
be measured through internet scans. We close this gap in two steps. First, we
collect SSH client public keys. Such keys are regularly published by their
owners on open development platforms like GitHub and GitLab. We systematize
previous non-academic work by subjecting these keys to various security tests
in a longitudinal study. Second, in a series of black-box lab experiments, we
analyze the implementations of algorithms for SSH client signatures in 24
popular SSH clients for Linux, Windows, and macOS.
  We extracted 31,622,338 keys from three public sources in two scans. Compared
to previous work, we see a clear tendency to abandon RSA signatures in favor of
EdDSA signatures. Still, in January 2025, we found 98 broken short keys, 139
keys generated from weak randomness, and 149 keys with common or small
factors-the large majority of the retrieved keys exposed no weakness.
  Weak randomness can not only compromise a secret key through its public key,
but also through signatures. It is well-known that a bias in random nonces in
ECDSA can reveal the secret key through public signatures. For the first time,
we show that the use of deterministic nonces in ECDSA can also be dangerous:
The private signing key of a PuTTY client can be recovered from just 58 valid
signatures if ECDSA with NIST curve P-521 is used. PuTTY acknowledged our
finding in CVE-2024-31497, and they subsequently replaced the nonce generation
algorithm.

</details>


### [28] [[Extended] Ethics in Computer Security Research: A Data-Driven Assessment of the Past, the Present, and the Possible Future](https://arxiv.org/abs/2509.09351)
*Harshini Sri Ramulu,Helen Schmitt,Bogdan Rerich,Rachel Gonzalez Rodriguez,Tadayoshi Kohno,Yasemin Acar*

Main category: cs.CR

TL;DR: 本文分析了计算机安全研究中伦理问题的现状，发现当前研究缺乏明确的伦理指导框架，伦理报告不一致，主要集中在机构审批、人类受试者保护和负责任披露，而缺乏对利益平衡的讨论。


<details>
  <summary>Details</summary>
Motivation: 计算机安全领域经常讨论伦理问题，但研究人员缺乏明确的指导来做出、记录和评估研究中的伦理决策，特别是在道德正确性或可接受性不明确的情况下。

Method: 通过回顾2024年发表的1154篇顶级安全论文的伦理讨论情况，并对24位计算机安全和隐私研究人员进行半结构化访谈研究。

Result: 发现伦理报告水平不一致，强烈关注机构或伦理委员会批准、人类受试者保护和负责任披露，但缺乏对利益平衡的讨论。研究人员有强烈的伦理研究意愿，但在价值观、伦理框架、决策过程和结果方面缺乏一致性。

Conclusion: 提出了改进计算机安全研究伦理状况的建议，包括建立更一致的伦理标准和框架，加强对利益平衡的讨论，以及提供更明确的伦理指导。

Abstract: Ethical questions are discussed regularly in computer security. Still,
researchers in computer security lack clear guidance on how to make, document,
and assess ethical decisions in research when what is morally right or
acceptable is not clear-cut. In this work, we give an overview of the
discussion of ethical implications in current published work in computer
security by reviewing all 1154 top-tier security papers published in 2024,
finding inconsistent levels of ethics reporting with a strong focus of
reporting institutional or ethics board approval, human subjects protection,
and responsible disclosure, and a lack of discussion of balancing harms and
benefits. We further report on the results of a semi-structured interview study
with 24 computer security and privacy researchers (among whom were also:
reviewers, ethics committee members, and/or program chairs) and their ethical
decision-making both as authors and during peer review, finding a strong desire
for ethical research, but a lack of consistency in considered values, ethical
frameworks (if articulated), decision-making, and outcomes. We present an
overview of the current state of the discussion of ethics and current de-facto
standards in computer security research, and contribute suggestions to improve
the state of ethics in computer security research.

</details>


### [29] [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424)
*Zhiyu He,Maojiang Wang,Xinwen Gao,Yuchuan Luo,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: ENSI是一个非交互式安全推理框架，通过密码协议与LLM架构协同设计，显著提升大语言模型加密推理效率，实现矩阵乘法8倍加速和softmax推理2.6倍加速。


<details>
  <summary>Details</summary>
Motivation: 密码协议与大型语言模型集成存在重大挑战，协议复杂性和LLM大规模参数限制了实际可用性，需要解决加密矩阵乘法和softmax计算的高计算需求问题。

Method: 采用协同设计方法，优化编码策略集成CKKS方案与轻量级BitNet模型，首创sigmoid注意力机制与同态加密集成，在RMSNorm过程中嵌入Bootstrapping操作减少调用频率。

Result: 实验评估显示ENSI在CPU上实现矩阵乘法约8倍加速，softmax推理2.6倍加速，bootstrapping比例降至仅1%。

Conclusion: ENSI框架通过密码协议和模型架构的协同创新设计，有效解决了LLM安全推理中的计算效率瓶颈，为隐私保护机器学习提供了实用解决方案。

Abstract: Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.

</details>


### [30] [Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts](https://arxiv.org/abs/2509.09488)
*Felix Mächtle,Ashwath Shetty,Jonas Sander,Nils Loose,Sören Pirk,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: 这篇论文揭露了散布模型中的提示盗窃攻击，通过利用PyTorch随机器种子范围限制的漏洞，开发了种子恢复工具SeedSnitch和基因算法基础的提示盗窃方法PromptPirate，并提出了有效的防御措施。


<details>
  <summary>Details</summary>
Motivation: 散布模型创造的图像包含重要的知识产权和经济价值，提示盗窃成为严重的安全隐私问题。论文研究了数值优化方法在提示恢复中的根本限制，并发现了随机器种子生成漏洞。

Method: 识别并利用PyTorch CPU上随机种子范围限制在2^32的漏洞(CWE-339)，开发SeedSnitch工具用于种子暴力破解，然后使用基因算法基础的PromptPirate方法进行提示盗窃。

Result: 在CivitAI平台的大规模实验中，约95%图像的种子可在140分钟内被破解。PromptPirate方法在LPIPS相似性上比现有方法提高8-11%。

Conclusion: 论文提出了简单有效的防御措施，能够防止种子盗窃和优化基础的提示盗窃。已经与开发者进行协调治理工作来修补这个关键漏洞。

Abstract: Diffusion models have significantly advanced text-to-image generation,
enabling the creation of highly realistic images conditioned on textual prompts
and seeds. Given the considerable intellectual and economic value embedded in
such prompts, prompt theft poses a critical security and privacy concern. In
this paper, we investigate prompt-stealing attacks targeting diffusion models.
We reveal that numerical optimization-based prompt recovery methods are
fundamentally limited as they do not account for the initial random noise used
during image generation. We identify and exploit a noise-generation
vulnerability (CWE-339), prevalent in major image-generation frameworks,
originating from PyTorch's restriction of seed values to a range of $2^{32}$
when generating the initial random noise on CPUs. Through a large-scale
empirical analysis conducted on images shared via the popular platform CivitAI,
we demonstrate that approximately 95% of these images' seed values can be
effectively brute-forced in 140 minutes per seed using our seed-recovery tool,
SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic
algorithm-based optimization method explicitly designed for prompt stealing.
PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and
CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.
Furthermore, we introduce straightforward and effective countermeasures that
render seed stealing, and thus optimization-based prompt stealing, ineffective.
We have disclosed our findings responsibly and initiated coordinated mitigation
efforts with the developers to address this critical vulnerability.

</details>


### [31] [What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets](https://arxiv.org/abs/2509.09564)
*Meghan Wilkinson,Robert H Thomson*

Main category: cs.CR

TL;DR: 本文分析了网络入侵检测数据集中的良性流量结构，通过无监督聚类技术发现良性流量存在有意义的子类别，这可能提高多类分类性能。


<details>
  <summary>Details</summary>
Motivation: 直接使用数据集的标签训练入侵检测算法时，大多数研究将良性流量处理为单一类别，而忽略了其内部可能存在的有意义子结构。

Method: 使用HDBSCAN、Mean Shift等无监督聚类技术分析NSL-KDD、UNSW-NB15和CIC-IDS 2017等常用入侵检测数据集中的良性流量结构，识别其中的子类别。

Result: 研究发现良性流量在多个数据集中都存在明显的子结构，这些子类别可能提高多类分类的性能。

Conclusion: 应该重新考虑入侵检测数据集中良性流量的结构，通过无监督聚类发现其中的子类别可以改善监督学习算法的表现。

Abstract: Supervised machine learning techniques rely on labeled data to achieve high
task performance, but this requires the labels to capture some meaningful
differences in the underlying data structure. For training network intrusion
detection algorithms, most datasets contain a series of attack classes and a
single large benign class which captures all non-attack network traffic. A
review of intrusion detection papers and guides that explicitly state their
data preprocessing steps identified that the majority took the labeled
categories of the dataset at face value when training their algorithms. The
present paper evaluates the structure of benign traffic in several common
intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and
determines whether there are meaningful sub-categories within this traffic
which may improve overall multi-classification performance using common machine
learning techniques. We present an overview of some unsupervised clustering
techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they
differentially cluster the benign traffic space.

</details>


### [32] [Bridging the Gap in Phishing Detection: A Comprehensive Phishing Dataset Collector](https://arxiv.org/abs/2509.09592)
*Aditya Kulkarni,Shahil Manishbhai Patel,Shivam Pradip Tirmare,Vivek Balachandran,Tamal Das*

Main category: cs.CR

TL;DR: 这篇论文提出了一种新的资源收集工具，用于采集涉及网页钻的多种资源（URL、源代码、截图、CSS、JavaScript等），以解决现有数据集中资源类型不全面和数据偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 电信网页存在时间短、收集困难，现有数据集库缺乏资源类型的全面性，导致模型偏差，影响钻洗检测效果。

Method: 开发了一种资源收集工具，利用PhishTank作为主要的活跃钻网页URL来源，能够采集比PyWebCopy更多的网页资源类型。

Result: 生成了包含4,056个正常网页和5,666个钻网页URL的样本数据集，包含相关资源，并分析了与钻网页类别相关性最高的特征。

Conclusion: 该工具提供了全面的资源集，能够帮助研究人员开发更有效的钻网页检测方法，解决了现有数据集资源类型不全面的问题。

Abstract: To combat phishing attacks -- aimed at luring web users to divulge their
sensitive information -- various phishing detection approaches have been
proposed. As attackers focus on devising new tactics to bypass existing
detection solutions, researchers have adapted by integrating machine learning
and deep learning into phishing detection. Phishing dataset collection is vital
to developing effective phishing detection approaches, which highly depend on
the diversity of the gathered datasets. The lack of diversity in the dataset
results in a biased model. Since phishing websites are often short-lived,
collecting them is also a challenge. Consequently, very few phishing webpage
dataset repositories exist to date. No single repository comprehensively
consolidates all phishing elements corresponding to a phishing webpage, namely,
URL, webpage source code, screenshot, and related webpage resources. This paper
introduces a resource collection tool designed to gather various resources
associated with a URL, such as CSS, Javascript, favicons, webpage images, and
screenshots. Our tool leverages PhishTank as the primary source for obtaining
active phishing URLs. Our tool fetches several additional webpage resources
compared to PyWebCopy Python library, which provides webpage content for a
given URL. Additionally, we share a sample dataset generated using our tool
comprising 4,056 legitimate and 5,666 phishing URLs along with their associated
resources. We also remark on the top correlated phishing features with their
associated class label found in our dataset. Our tool offers a comprehensive
resource set that can aid researchers in developing effective phishing
detection approaches.

</details>


### [33] [CryptoGuard: An AI-Based Cryptojacking Detection Dashboard Prototype](https://arxiv.org/abs/2509.09638)
*Amitabh Chakravorty,Jess Kropczynski,Nelly Elsayed*

Main category: cs.CR

TL;DR: 提出CryptoGuard前端原型，AI驱动的加密货币安全仪表板，通过用户中心设计帮助用户监控登录和交易活动，检测可疑行为。


<details>
  <summary>Details</summary>
Motivation: 随着加密货币的广泛采用，加密劫持已成为钱包用户的重要安全威胁，需要开发用户友好的安全工具。

Method: 采用用户中心设计流程，构建高保真可点击原型，基于Figma模拟关键用户交互，优先考虑非技术用户的直观体验。

Result: 开发出概念性AI功能的原型，展示视觉警报和报告功能，虽然AI功能是概念性的，但原型验证了界面设计理念。

Conclusion: 实用安全工具不仅需要强大的后端功能，还需要用户中心设计来有效沟通风险并促使用户采取行动，桥接了加密劫持检测研究与以人为本的界面设计。

Abstract: With the widespread adoption of cryptocurrencies, cryptojacking has become a
significant security threat to crypto wallet users. This paper presents a
front-end prototype of an AI-powered security dashboard, namely, CryptoGuard.
Developed through a user-centered design process, the prototype was constructed
as a high-fidelity, click-through model from Figma mockups to simulate key user
interactions. It is designed to assist users in monitoring their login and
transaction activity, identifying any suspicious behavior, and enabling them to
take action directly within the wallet interface. The dashboard is designed for
a general audience, prioritizing an intuitive user experience for non-technical
individuals. Although its AI functionality is conceptual, the prototype
demonstrates features like visual alerts and reporting. This work is positioned
explicitly as a design concept, bridging cryptojacking detection research with
human-centered interface design. This paper also demonstrates how usability
heuristics can directly inform a tool's ability to support rapid and confident
decision-making under real-world threats. This paper argues that practical
security tools require not only robust backend functionality but also a
user-centric design that communicates risk and empowers users to take
meaningful action.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 本文提出了区间二型贝叶斯定理，通过保守方法处理输入区间的不一致性，并开发了将专家提供的区间编码为二型模糊隶属函数的新算法。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推断假设精确输入值，但实际应用中专家通常提供区间范围估计，需要扩展贝叶斯定理来处理这种不确定性。

Method: 开发了区间二型贝叶斯定理版本，使用保守方法避免输入不一致性；提出了将专家区间编码为二型模糊隶属函数的新算法。

Result: 成功扩展了贝叶斯定理到区间二型版本，能够有效处理专家提供的区间输入信息，避免产生无效输出结果。

Conclusion: 该方法为处理现实世界中不确定性信息提供了有效的贝叶斯推断框架，扩展了传统精确值假设的局限性。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [35] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [36] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 使用多个专门化的LLM模型以分解方式将自然语言问题描述转换为MiniZinc约束模型，通过不同代理分别处理不同类型的全局约束，最终组装成完整模型


<details>
  <summary>Details</summary>
Motivation: 自然语言描述转换为MiniZinc约束模型需要逻辑推理和约束编程专业知识，这一过程很具挑战性

Method: 采用多代理机制，每个专门化LLM代理负责检测和生成特定类型的全局约束代码，最后由组装代理整合成完整模型

Result: 初步实验显示该框架在多个LLM上都表现更好，性能超过了一次性提示和思维链提示等基准方法

Conclusion: 通过分解任务降低复杂度的多代理方法有效，文章还提出了未来工作路线图和改进方向

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [37] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 论文提出了一种截断交叉熵(TCE)损失函数来缓解生成式AI模型在合成数据上重复训练导致的模型崩溃问题，通过降低对高置信度预测的权重来显著延迟模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在训练中的比例不断增加，模型在合成数据上重复训练会导致模型崩溃现象，现有缓解策略有限。研究发现模型对其生成数据的过度自信是崩溃的关键驱动因素。

Method: 提出截断交叉熵(TCE)损失函数，在训练过程中降低高置信度预测的权重，构建模型无关的框架将损失函数设计与模型崩溃缓解联系起来。

Result: TCE显著延迟递归训练中的模型崩溃，可将模型崩溃前的保真度间隔延长2.3倍以上，且该方法在不同模态上具有通用性。

Conclusion: 损失函数设计为在合成数据时代保持生成模型质量提供了简单而强大的工具，通过控制模型对自生成数据的置信度来有效缓解模型崩溃。

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [38] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 该论文研究可解释AI中的不确定性解释和全局解释，通过测试算法校准信任的能力，探讨直观可视化方法是否能提高用户满意度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 虽然XAI的某些领域已有深入研究，但不确定性解释和全局解释往往被忽视。研究者希望构建XAI方案的通用指南，并特别关注这些被忽略的方面。

Method: 选择了一个能同时涵盖不确定性、鲁棒性和全局XAI概念的算法，测试其校准信任的能力，并检查直观可视化方法的效果。

Result: 研究发现，尽管算法本身可能复杂难懂，但旨在提供更直观视觉理解的算法能够提供更高的用户满意度和人类可解释性。

Conclusion: 该研究强调了在XAI中关注不确定性解释和全局解释的重要性，并证明直观可视化方法在提高用户信任和满意度方面的有效性。

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [39] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于上下文条件提示的方法来优化大语言模型在冷启动推荐任务中的表现，通过最优示例注入和指令结构化显著提升了few-shot场景下的推荐精度。


<details>
  <summary>Details</summary>
Motivation: 冷启动用户问题限制了推荐系统的有效性，因为缺乏历史行为信息。需要一种有效的方法来优化大语言模型在推荐任务中的指令提示。

Method: 提出了上下文条件提示公式P(u, Ds)→R̂，其中u是冷启动用户画像，Ds是精选支持集。使用基于transformer的自回归LLM（BioGPT、LLaMA-2、GPT-4），采用token级对齐和嵌入空间正则化技术。

Result: 实验证明最优示例注入和指令结构化能显著提高precision@k和NDCG分数，特别是在低数据设置下。提示的及时组合不仅影响语法结构，还直接控制注意力规模和推理过程中的解码器行为。

Conclusion: 基于提示的适应方法可以视为解决基于LLM的推荐管道中冷启动问题的一种有效途径。

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [40] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 人类、LLM和贝叶斯模型在动态协商中表现出不同行为模式，虽然可能达到相似的绩效水平，但其过程和对齐性存在显著差异


<details>
  <summary>Details</summary>
Motivation: 评估自主协调代理在动态多代理环境中的协商过程，比较不同类型代理（人类、LLM、贝叶斯模型）的优势和行为差异

Method: 在动态协商设置中进行直接比较，包括216名人类参与者、GPT-4o、Gemini 1.5 Pro和贝叶斯代理，捕获结果和行为动态

Result: 贝叶斯代理通过敌对性优化获得最高剩余，但拒绝交易频繁；LLM偏向保守、进退的交易策略，拒绝率低；人类采用更具战略性、冒险和公平导向的行为

Conclusion: 绩效平等作为代理评估标准可能隐藏了过程和对齐性的根本差异，这对实际部署至关重要

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [41] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 本文提出了一种用于反洗钱检测的机器学习流水线，在IMI大数据与AI竞赛中获得第二名，AUROC达到0.961


<details>
  <summary>Details</summary>
Motivation: 反洗钱是金融机构的优先事项，机器学习在此领域具有巨大潜力，需要开发系统化的方法来识别高风险银行客户

Method: 采用16步设计和统计分析流程，构建SQLite数据库，开发基于SQL的特征工程算法，集成预训练模型并提供可解释AI模块

Result: 流水线在包含195,789个客户ID的数据集上实现了0.961的平均AUROC（标准差0.005），在竞赛中获得第二名

Conclusion: 提出的综合系统化方法能够有效开发稳健的机器学习流水线，为金融机构的反洗钱检测提供了实用解决方案

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [42] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 这篇论文提出了一个基于神经科学原理的计算框架，用于提升自治AI系统的空间推理能力，包含六个核心模块并分析了相关方法、数据集和应用领域。


<details>
  <summary>Details</summary>
Motivation: 当前自治AI系统在空间推理能力方面存在显著缺口，主要限于符号和序列处理，而人类空间智能则基于多感官知觉、空间记忆和认知地图，能够在非结构化环境中做出灵活的上下文感知决策。缩小这一差距对于推进自治空间智能至关重要。

Method: 首先审视计算神经科学中的空间神经模型，然后提出一个基于神经科学原理的新计算框架。该框架将核心生物功能映射到六个重要计算模块：生物受感器受灵的多模态感知、多感官整合、自我中心-绝对坐标转换、人工认知地图、空间记忆和空间推理。

Result: 这些模块共同构成了一个视角化的自治空间推理能力景观，适用于虚拟和物理环境。论文进行了框架引导的方法分析，评估了最新方法与每个模块的相关性，并识别了阻碍更多神经科学基础空间推理模块发展的关键缺口。

Conclusion: 论文还分析了新兴的标准测试集和数据集，探讨了从虚拟到体现系统（如机器人学）的潜在应用领域，并给出了可能的研究方向。这份工作为研究社区提供了基于神经科学的视角和结构化路径，有助于推进自治空间智能的发展。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [43] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 提出ProgD方法，通过动态异构图建模和渐进多尺度解码策略，解决多智能体运动预测中交互关系动态演化的挑战，在多个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有多智能体联合预测方法忽视了交互关系的动态演化特性，无法准确捕捉未来场景中不断变化的社会交互

Method: 使用动态异构图进行场景建模，设计渐进式多尺度解码策略，通过因子化架构处理时空依赖关系并逐步消除未来运动的不确定性

Result: 在INTERACTION多智能体预测基准中排名第一，在Argoverse 2多世界预测基准上也达到最先进性能

Conclusion: ProgD方法通过动态建模交互演化和渐进消除不确定性，显著提升了多智能体运动预测的准确性和一致性

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [44] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: 基于区块链的多自治理层结构，通过行为追踪、声誉评估和恶意行为预测三个模块，解决LLM自主代理的可靠性和负责制挑战


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的自主代理在各领域带来重大机遇，但其不可预测行为和异构能力造成了法治和负责制挑战

Method: 提出一种区块链启用的层状监管架构，包含代理层、区块链数据层和监管应用层，设计了行为追踪仲裁、动态声誉评估和恶意行为预测三个核心模块

Result: 为大规模代理生态系统建立了可信、弹性和可扩展的监管机制系统基础

Conclusion: 该框架有效解决了自主代理的监管挑战，为多代理系统中区块链监管框架的未来研究指明了方向

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [45] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 提出了NbQA数据集和Jupiter框架，通过从真实Jupyter笔记本提取高质量工具使用任务和解决方案，结合MCTS搜索增强多步推理，显著提升了LLM在数据科学任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多步推理和工具使用方面存在困难，限制了其在复杂数据分析任务中的有效性，需要更好的数据集和推理框架来提升性能。

Method: 1) 从真实Jupyter笔记本提取工具型数据分析任务和可执行多步解决方案，构建NbQA数据集；2) 提出Jupiter框架，将数据分析建模为搜索问题，使用MCTS生成多样化解决方案轨迹进行价值模型学习；3) 推理时结合价值模型和节点访问计数高效生成可执行多步计划。

Result: Qwen2.5-7B和14B-Instruct模型在NbQA上分别解决了77.82%和86.38%的任务，性能达到或超过GPT-4o和先进代理框架，在多步推理任务中展现出更好的泛化能力和工具使用推理能力。

Conclusion: 通过构建高质量数据集和结合MCTS的搜索框架，显著提升了LLM在数据科学工作流中的多步推理和工具使用能力，为自动化数据分析提供了有效解决方案。

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [46] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 知识图构建与LLM整合的技术对比研究，评估spaCy、OpenIE和GraphRAG三种方法在问答系统中的效果


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理复杂文本的主题性和整体理解方面有限，需要知识图来提升问答系统的深度分析能力

Method: 对spaCy、Stanford CoreNLP-OpenIE和GraphRAG三种开源知识图构建方法进行综合技术对比研究，分析其能力、发展状态和对LLM问答性能的影响

Result: 实验结果显示OpenIE提供最全面的三元组覆盖，而GraphRAG在推理能力方面表现最优

Conclusion: 讨论了各方法的优势和限制，并提出了改进知识图基于问答系统的未来发展方向

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [47] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 将MCTS轨迹用于GRPO策略优化，提出基于树状结构的优势估计方法，解决优势饱和和奖励信号崩溃问题


<details>
  <summary>Details</summary>
Motivation: 探索如何将传统用于训练价值或奖励模型的MCTS轨迹重新用于改进基于偏好的强化学习中的策略优化

Method: 提出分阶段GRPO训练范式，使用部分揭示的MCTS rollout生成补全，引入新颖的树状结构优势估计方法

Result: 结构化优势估计可以稳定更新并更好地反映组合推理质量，但仍面临优势饱和和奖励信号崩溃等挑战

Conclusion: 提出了启发式和统计解决方案来缓解这些问题，并讨论了在分阶段或树状奖励结构下学习的开放挑战

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [48] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: LightAgent是一个轻量级但功能强大的多智能体框架，解决了现有框架在灵活性和简单性之间的权衡问题，集成了内存、工具和思维树等核心功能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，多智能体系统在各种应用场景中取得了显著进展，但在设计通用、鲁棒和高效的智能体部署平台方面仍存在重大挑战。

Method: 提出了LightAgent框架，集成Memory (mem0)、Tools和Tree of Thought (ToT)等核心功能，同时保持极轻量级结构，是一个完全开源的解决方案。

Result: LightAgent能够无缝集成到主流聊天平台，使开发者能够轻松构建自学习智能体。

Conclusion: LightAgent有效解决了现有框架在灵活性和简单性之间的权衡问题，为多智能体系统提供了一个轻量级但功能强大的部署平台。

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [49] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 该论文研究锦标赛中获胜者的认证解释问题，通过识别最小支持子锦标赛来解释为什么某个候选人在不同锦标赛规则下必然获胜。


<details>
  <summary>Details</summary>
Motivation: 锦标赛模型广泛应用于表示候选人或团队之间的成对优势关系，需要为获胜者提供可认证的解释，这是可解释AI的核心概念。

Method: 识别最小支持子锦标赛（候选人在其中是必要获胜者的最小子锦标赛），针对多种常见锦标赛规则（如top cycle、uncovered set等）分析最小支持的大小并设计多项式时间算法。

Result: 对于除加权uncovered set外的所有规则，都能找到多项式时间算法计算最小支持，而加权uncovered set的问题是NP完全的。

Conclusion: 最小支持子锦标赛能够提供紧凑、可认证且直观的解释，为锦标赛获胜者的解释问题提供了有效的解决方案。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [50] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 研究探索在无法进行显式沟通的环境下，空间协调的三个维度（探索多样性、移动专门化、适应性空间距离）如何影响团队性能。结果显示空间专门化正向预测性能，适应性空间距离呈倒U型关系，中等水平的适应最优。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同地同步团队或知识工作的协调，而许多实际团队（消防员、军队、警察等）需要在缺乏视觉线索或显式沟通的情况下协谁物理移动。本文研究隐式空间协谁在角色基础团队中的作用。

Method: 分析34个四人团队（136名参与者）在搜索营救任务中的数据，参与者被分配专门角色。通过测量空间距离、分布模式和移动对齐等关系性指标来分析团队协作。

Result: 空间专门化正向预测团队性能，适应性空间距离呈现边际倒U型关系（中等适应水平最优）。这些指标的时间动态能够区分高体现和低体现团队。

Conclusion: 研究结果为隐式空间协谁提供了见解，强调了平衡适应策略的重要性，对于培训和AI辅助团队支持系统具有重要意义。

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [51] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: TAM Bench是一个用于评估基于LLM的端到端机器学习代理的多样化、真实结构化基准测试，包含150个AutoML任务，具有浏览器自动化采集、难度建模和多维评估框架三大创新。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在任务覆盖、领域多样性、难度建模和评估严谨性方面存在局限，无法充分评估LLM代理在真实环境中的端到端ML工作流能力。

Method: 1) 基于浏览器自动化和LLM的任务采集系统从Kaggle等平台自动收集结构化ML挑战；2) 基于排行榜的难度建模机制；3) 包含性能、格式合规性、约束遵循和任务泛化的多维评估框架。

Result: 构建了包含150个AutoML任务的基准测试，提供Lite(18任务)、Medium和Full三个版本，其中Lite版本在模态和难度级别上平衡覆盖，适合日常基准测试。

Conclusion: TAM Bench为评估LLM-based ML代理提供了更加全面、真实和结构化的测试环境，解决了现有基准的局限性。

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [52] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 一种新的深度强化学习架构，通过视觉-语言模型和层状奖励函数集成普通常识，实现资源高效的语义探索


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在平衡高效探索和语义理解方面遇到困难，小型策略缺乏足够的认知能力，导致在语义探索中依赖人类驾驶

Method: 集成VLM普通常识通过层状奖励函数，将VLM查询模型化为专门动作，经理性地使用外部指导，结合课程学习策略确保稳定学习

Result: 实验结果显示代理实现了显著提高的物体发现率，学会了向语义丰富区域导航，并掌握了战略性地引导环境信息的能力

Conclusion: 通过展示一种实用可扩展的方法，将普通常识语义推理嵌入自主代理，为实现全面智能自主探索提供了新方法

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [53] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: TORSO方法通过引导LLM利用内部推理能力生成响应，无需手动构建few-shot示例，在多个基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有few-shot提示方法过度依赖提供的示例，限制了模型固有推理能力的利用，且构建任务特定的few-shot提示成本高且在不同任务间存在不一致性

Method: 提出Template-Oriented Reasoning (TORSO)方法，引导大语言模型利用内部推理能力生成适当响应，无需手动构建few-shot示例

Result: 实验结果表明TORSO在多样化LLM基准测试中实现了强劲性能，并生成了合理的推理过程

Conclusion: TORSO方法有效解决了few-shot提示的局限性，能够更好地激发LLM的内在推理能力，在多种任务上取得优异表现

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [54] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 这篇技术报告分析了LLM在法律领域中的“幽灵现象”问题，评估了RAG策略的有效性和局限性，并提出以验证性和可追溯性为核心的“咨询式AI”新范式。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在法律应用中产生假信息的“幽灵现象”问题，这在法律运用中带来严重风险和伦理挑战。

Method: 通过分析幽灵现象的根源、表现形式和RAG策略的有效性，提出整体性优化方案，并考察了相关伦理和监管问题。

Result: 发现RAG策略有限但存在局限性，人类监督在法律AI应用中仍然不可或缺。

Conclusion: 解决方案不在于增量改进生成模型，而是应采用以验证性和可追溯性为核心的“咨询式AI”新范式，作为增强专业判断的工具而非替代。

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [55] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM是一个自演化的分布式内存框架，通过可验证写入、动态内存调度和跨域知识扩散来解决多智能体系统中内存管理的噪声积累、内存膨胀和泛化限制问题。


<details>
  <summary>Details</summary>
Motivation: 长期多智能体系统产生大量轨迹和交互数据，现有内存管理方法存在噪声积累、内存膨胀不可控和跨域泛化能力有限的问题。

Method: SEDM框架包含：1）基于可重现回放的可验证写入准入机制；2）根据经验效用动态排序和整合条目的自调度内存控制器；3）抽象可重用见解的跨域知识扩散机制。

Result: 在基准数据集上评估显示，SEDM相比强基线方法提高了推理准确率，减少了token开销，并能将从事实验证中提取的知识增强多跳推理能力。

Conclusion: SEDM是一个可扩展且可持续的内存机制，适用于开放式多智能体协作，代码将在项目后期发布。

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [56] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 量子模型在组合泛化任务中相比经典组合模型表现更好，特别是在图像描述任务中使用多热编码时取得了良好的概念验证结果


<details>
  <summary>Details</summary>
Motivation: 组合泛化是人类认知的关键能力，但当前AI工具如视觉语言模型缺乏这种能力。量子模型训练效率更高，可能提升在这类任务中的性能

Method: 将组合张量模型的表示解释在希尔伯特空间中，训练变分量子电路来学习这些表示。使用两种图像编码技术：多热编码(MHE)和基于CLIP模型的角/振幅编码

Result: 使用噪声MHE编码获得了良好的概念验证结果。在CLIP图像向量上的表现较为混合，但仍优于经典组合模型

Conclusion: 量子模型在组合泛化任务中展现出潜力，特别是在特定编码方式下能够超越经典方法，为量子AI在复杂认知任务中的应用提供了可能性

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [57] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: Auras是一个算法-系统协同设计的推理框架，通过解耦感知和生成模块并提供受控的流水线并行化，显著提升具身AI系统的推理频率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统顺序计算模式虽然能保证准确性，但无法满足具身AI系统在动态环境中对高频输入输出处理的需求，限制了实时应用的性能。

Method: 将感知和生成模块解耦，建立公共上下文共享机制，提供受控的流水线并行化，解决数据陈旧性问题。

Result: 平均吞吐量提升2.54倍，同时达到原始准确率的102.7%，实现了高吞吐量和高准确性的平衡。

Conclusion: Auras框架有效克服了顺序计算的限制，为具身AI系统提供了高频率、高吞吐量的推理能力，同时保证了系统准确性。

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [58] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 论文研究发现，大语言模型在长任务执行中存在自我条件效应，模型规模扩大并不能解决执行错误累积问题，而思维模型能更好地执行长序列任务。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在长序列任务中表现不佳的原因，探索单步准确率提升如何转化为长任务执行能力的指数级改进。

Method: 通过显式提供知识和计划来隔离执行能力，分析不同规模模型在多步任务中的表现，比较传统LLM与思维模型的执行差异。

Result: 发现模型存在自我条件效应（错误累积），模型规模扩大不能解决此问题，但思维模型能显著提升长任务执行能力。

Conclusion: 执行能力是长任务成功的关键，模型规模扩展和序列测试时计算对长视野任务有巨大益处，思维模型是解决长序列执行问题的有效途径。

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>
